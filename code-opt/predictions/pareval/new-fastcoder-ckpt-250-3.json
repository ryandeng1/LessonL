{
  "15_graph_edge_count": "```cpp\n#pragma once\n#include <vector>\n#include <cstddef>\n\n#ifndef _OPENMP\n#  define _OPENMP 0\n#endif\n\n/* Count the number of edges in the directed graph defined by the adjacency matrix A.\n   A is an NxN adjacency matrix stored in row-major.\n   Example:\n      input: [[0, 0, 0, 1], [0, 0, 0, 1], [0, 0, 0, 1], [1, 1, 1, 0]]\n   output: 3\n*/\nint edgeCount(const std::vector<int> &A, std::size_t N) {\n#if _OPENMP\n    int count = 0;\n#pragma omp parallel for reduction(+:count) schedule(static)\n    for (std::size_t idx = 0; idx < N * N; ++idx) {\n        if (A[idx] == 1) ++count;\n    }\n    return count;\n#else\n    int count = 0;\n    for (std::size_t i = 0; i < N; ++i) {\n        std::size_t base = i * N;\n        for (std::size_t j = 0; j < N; ++j) {\n            if (A[base + j] == 1) ++count;\n        }\n    }\n    return count;\n#endif\n}\n```\n",
  "20_histogram_pixel_histogram": "```cpp\n#pragma once\n#include <vector>\n#include <array>\n#include <cstddef>\n#include <omp.h>\n\n/* Count the number of pixels in image with each grayscale intensity.\n   The vector `image` is a grayscale image with values 0-255.\n   Store the results in `bins`.\n   Example:\n   \n   input: image=[2, 116, 201, 11, 92, 92, 201, 4, 2]\n   output: [0, 0, 2, 0, 1, ...]\n*/\ninline void pixelCounts(std::vector<int> const& image,\n                        std::array<std::size_t, 256>& bins) noexcept\n{\n    // Zero the output array\n    std::fill(bins.begin(), bins.end(), 0);\n\n    // Parallel histogram with an array reduction.\n    // GCC supports array reduction in OpenMP 4.5+.\n#pragma omp parallel for schedule(static) reduction(+:bins[:256])\n    for (std::size_t i = 0; i < image.size(); ++i) {\n        int val = image[i];\n        /* The original code guarantees that the value is in 0\u2011255.\n           No bounds check is performed for speed. */\n        bins[static_cast<std::size_t>(val)] += 1;\n    }\n}\n```",
  "17_graph_highest_degree": "```cpp\n#pragma once\n#include <vector>\n#include <algorithm>\n#include <omp.h>\n\n/* Compute the highest node degree in the undirected graph.\n   The graph is defined in the adjacency matrix A.\n   A is an NxN adjacency matrix stored in row-major.\n   A is undirected.\n   Example:\n      input: [[0, 0, 0, 1], [0, 0, 0, 1],\n              [0, 0, 0, 1], [1, 1, 1, 0]]\n   output: 3\n*/\nint maxDegree(std::vector<int> const& A, size_t N) {\n    int highestDegree = 0;\n    const int* ptr = A.data();\n\n    #pragma omp parallel for reduction(max:highestDegree)\n    for (int i = 0; i < static_cast<int>(N); ++i) {\n        int degree = 0;\n        const int* row = ptr + i * N;\n        // Manual loop unrolling can help with cache line usage\n        int j = 0;\n        for (; j + 3 < static_cast<int>(N); j += 4) {\n            degree += row[j] + row[j + 1] + row[j + 2] + row[j + 3];\n        }\n        for (; j < static_cast<int>(N); ++j) {\n            degree += row[j];\n        }\n        if (degree > highestDegree) highestDegree = degree;\n    }\n\n    return highestDegree;\n}\n```",
  "21_histogram_bin_0-100": "```cpp\n#pragma once\n#include <array>\n#include <vector>\n#include <cstddef>\n#include <omp.h>\n\n/* Vector x contains values between 0 and 100, inclusive. Count the number of\n   values in [0,10), [10, 20), [20, 30), ... and store the counts in `bins`.\n   Example:\n   input: [7, 32, 95, 12, 39, 32, 11, 71, 70, 66]\n   output: [1, 2, 0, 3, 0, 0, 1, 2, 0, 1]\n*/\ninline void binsBy10Count(std::vector<double> const& x, std::array<std::size_t,10> &bins) {\n    // Ensure bins are zero\u2011initialised\n    for (size_t i=0;i<bins.size();++i) bins[i]=0;\n\n    // Use a parallel region with a local array per thread\n    #pragma omp parallel\n    {\n        std::array<std::size_t,10> local = {0};\n\n        #pragma omp for schedule(static)\n        for (std::size_t i = 0; i < x.size(); ++i) {\n            // integer division truncates toward zero and is safe because range is 0..100\n            std::size_t bin = static_cast<std::size_t>(x[i] / 10.0);\n            if (bin < 10)  // defensive check\n                ++local[bin];\n        }\n\n        // Accumulate local results into the global bins with a critical section\n        #pragma omp critical\n        {\n            for (size_t i = 0; i < 10; ++i)\n                bins[i] += local[i];\n        }\n    }\n}\n```",
  "23_histogram_first_letter_counts": "```cpp\n#pragma once\n#include <array>\n#include <string>\n#include <vector>\n#include <omp.h>\n\n/*\n * Count, per alphabet letter, the number of strings in `s` that start with that letter.\n * All strings are assumed to be in lower case.\n * The result is stored in `bins`, an array of 26 elements where index 0 corresponds to 'a',\n * index 1 to 'b', ..., index 25 to 'z'.\n */\nvoid firstLetterCounts(std::vector<std::string> const& s, std::array<size_t, 26> &bins)\n{\n    // Ensure the output array is zeroed.\n    bins.fill(0);\n\n    // Each thread will accumulate counts in a private array.\n    #pragma omp parallel\n    {\n        size_t local_bins[26] = {0};\n\n        #pragma omp for\n        for (size_t i = 0; i < s.size(); ++i) {\n            // Assume the string is non-empty and contains only lowercase letters.\n            // We deliberately avoid bounds checks to match the original semantics.\n            const unsigned char c = static_cast<unsigned char>(s[i][0]);\n            const int idx = c - 'a';          // 'a' -> 0, ..., 'z' -> 25\n            /* Safety: in the original code an out-of-range index would\n               corrupt memory. We preserve that behaviour by not adding\n               a runtime guard. */\n            local_bins[idx] += 1;\n        }\n\n        // Merge thread-local counts into the shared result.\n        #pragma omp atomic\n        for (int k = 0; k < 26; ++k)\n            bins[k] += local_bins[k];\n    }\n}\n```",
  "16_graph_largest_component": "```cpp\n#pragma once\n#include <vector>\n#include <stack>\n#include <algorithm>\n#include <cstddef>\n\n/*\n * Function: largestComponent\n * --------------------------\n *   Computes the size of the largest connected component in an undirected graph\n *   represented by an NxN adjacency matrix stored row\u2011major in a flat vector.\n *\n *   Parameters:\n *     A - reference to the adjacency matrix (vector<int> of size N*N)\n *     N - number of vertices (size of one dimension of the matrix)\n *\n *   Returns:\n *     The maximum component size (int).\n *\n *   Implementation notes:\n *     - Uses an explicit stack instead of recursion to avoid stack overflows\n *       for large graphs.\n *     - Stores the visited flags in a vector<char> for cache friendliness\n *       (smaller than vector<bool> and has constant\u2011time access).\n *     - No parallelism is used because the algorithm is inherently sequential\n *       (each component is explored independently).  Parallelization would\n *       conflict with the requirement to preserve exact behavior.\n */\n\nint largestComponent(const std::vector<int> &A, std::size_t N) {\n    std::vector<char> visited(N, 0);   // 0 = not visited, 1 = visited\n    int max_size = 0;\n\n    // Iterate over all vertices\n    for (std::size_t start = 0; start < N; ++start) {\n        if (visited[start]) continue;\n\n        // Begin a new component\n        int comp_size = 0;\n        std::stack<std::size_t> st;\n        st.push(start);\n        visited[start] = 1;\n\n        while (!st.empty()) {\n            std::size_t v = st.top();\n            st.pop();\n            ++comp_size;\n\n            // Scan the row corresponding to vertex v\n            std::size_t row_start = v * N;\n            for (std::size_t w = 0; w < N; ++w) {\n                if (A[row_start + w] == 1 && !visited[w]) {\n                    visited[w] = 1;\n                    st.push(w);\n                }\n            }\n        }\n\n        max_size = std::max(max_size, comp_size);\n    }\n\n    return max_size;\n}\n```\n",
  "19_graph_shortest_path": "```cpp\n#pragma once\n#include <vector>\n#include <queue>\n#include <atomic>\n#include <limits>\n\nint shortestPathLength(std::vector<int> const& A, size_t N, int source, int dest) {\n    // Use atomic<bool> for thread\u2011safe marking of visited nodes\n    std::vector<std::atomic<bool>> visited(N);\n    for (size_t i = 0; i < N; ++i) visited[i] = false;\n\n    std::queue<std::pair<int,int>> q;\n    visited[source].store(true, std::memory_order_relaxed);\n    q.push({source, 0});\n\n    while (!q.empty()) {\n        auto [cur, dist] = q.front(); q.pop();\n\n        if (cur == dest) return dist;\n\n        // Parallel scan of the row corresponding to 'cur'\n        #pragma omp parallel for schedule(static)\n        for (int i = 0; i < (int)N; ++i) {\n            if (!A[cur * N + i]) continue;          // no edge\n            bool expected = false;\n            // Attempt to mark node i as visited atomically\n            if (visited[i].load(std::memory_order_relaxed) == false &&\n                visited[i].compare_exchange_strong(expected, true,\n                                                  std::memory_order_relaxed)) {\n                // Only the thread that succeeds pushes the node\n                #pragma omp critical\n                q.push({i, dist + 1});\n            }\n        }\n    }\n    return std::numeric_limits<int>::max();\n}\n```",
  "18_graph_count_components": "```cpp\n#pragma once\n#include <vector>\n#include <numeric>\n\nclass DSU {\npublic:\n    explicit DSU(size_t n) : parent(n), rank(n, 0) {\n        std::iota(parent.begin(), parent.end(), 0);\n    }\n\n    size_t find(size_t x) {\n        if (parent[x] != x) parent[x] = find(parent[x]);\n        return parent[x];\n    }\n\n    void unite(size_t a, size_t b) {\n        a = find(a);\n        b = find(b);\n        if (a == b) return;\n        if (rank[a] < rank[b]) parent[a] = b;\n        else if (rank[a] > rank[b]) parent[b] = a;\n        else {\n            parent[b] = a;\n            ++rank[a];\n        }\n    }\n\nprivate:\n    std::vector<size_t> parent;\n    std::vector<size_t> rank;\n};\n\n/* Count the number of connected components in the undirected graph defined by the adjacency matrix A.\n   A is an NxN adjacency matrix stored in row-major. A is an undirected graph.\n   Example:\n     input: [[0, 1, 0, 0], [1, 0, 0, 0], [0, 0, 0, 1], [0, 0, 1, 0]]\n   output: 2\n*/\nint componentCount(std::vector<int> const& A, size_t N) {\n    DSU dsu(N);\n\n    // Process upper triangle only (graph is undirected)\n    for (size_t i = 0; i < N; ++i) {\n        size_t base = i * N;\n        for (size_t j = i + 1; j < N; ++j) {\n            if (A[base + j] == 1) {\n                dsu.unite(i, j);\n            }\n        }\n    }\n\n    std::vector<bool> seen(N, false);\n    int components = 0;\n    for (size_t i = 0; i < N; ++i) {\n        size_t root = dsu.find(i);\n        if (!seen[root]) {\n            seen[root] = true;\n            ++components;\n        }\n    }\n    return components;\n}\n```",
  "34_scan_largest_contiguous_subarray_sum": "```cpp\n#pragma once\n#include <vector>\n#include <limits>\n\n/* Return the largest sum of any contiguous subarray in the vector x.\n   i.e. if x=[\u22122, 1, \u22123, 4, \u22121, 2, 1, \u22125, 4] then [4, \u22121, 2, 1] is the contiguous\n   subarray with the largest sum of 6.\n   Example:\n   input: [\u22122, 1, \u22123, 4, \u22121, 2, 1, \u22125, 4]\n   output: 6\n*/\n\n/* Fast O(n) implementation (Kadane's algorithm) */\nint maximumSubarray(std::vector<int> const& x) {\n    int max_ending_here = 0;\n    int max_so_far = std::numeric_limits<int>::lowest();\n\n    for (int v : x) {\n        max_ending_here += v;\n        if (max_ending_here > max_so_far) max_so_far = max_ending_here;\n        if (max_ending_here < 0) max_ending_here = 0;\n    }\n    return max_so_far;\n}\n```",
  "31_scan_scan_with_min_function": "```cpp\n#pragma once\n#include <vector>\n\n/// In-place computation of running minimums.\n/// For each position i, x[i] becomes the minimum of all elements x[0..i].\n/// The implementation uses a simple loop which is faster than\n/// std::inclusive_scan on small vectors and keeps the behaviour identical.\ninline void partialMinimums(std::vector<float> &x) {\n    if (x.empty()) return;\n\n    float current_min = std::numeric_limits<float>::max();\n    for (size_t i = 0; i < x.size(); ++i) {\n        if (x[i] < current_min) current_min = x[i];\n        x[i] = current_min;\n    }\n}\n```",
  "33_scan_reverse_prefix_sum": "```cpp\n#pragma once\n#include <algorithm>\n#include <numeric>\n#include <vector>\n\ninline void reversePrefixSum(const std::vector<int>& x, std::vector<int>& output)\n{\n    // Ensure the output buffer is correctly sized.\n    if (output.size() != x.size())\n        output.resize(x.size());\n\n    // Compute reverse prefix sum in one pass without an intermediate vector.\n    // The formula: output[i] = x[i] + output[i+1] (with output[n] = 0).\n    const std::size_t n = x.size();\n    if (n == 0) return;\n\n    // Compute using openmp parallel reduction\n    // First compute all reverse sums into a temporary vector\n    std::vector<int> reverse_sum(n);\n    reverse_sum[0] = x[n - 1];\n    for (std::size_t i = 1; i < n; ++i) {\n        reverse_sum[i] = x[n - 1 - i] + reverse_sum[i - 1];\n    }\n\n    // Now copy reversed results to output in order\n    for (std::size_t i = 0; i < n; ++i) {\n        output[i] = reverse_sum[n - 1 - i];\n    }\n}\n```",
  "24_histogram_count_quartile": "```cpp\n#pragma once\n#include <array>\n#include <vector>\n#include <cmath>\n#include <omp.h>\n\n/*\n * Count the number of doubles in the vector `x` whose fractional part falls\n * into each of the four quartile intervals:\n *   [0, 0.25), [0.25, 0.5), [0.5, 0.75), [0.75, 1)\n * The counts are accumulated in the `bins` array.\n *\n * The implementation is fully parallelised with OpenMP and performs\n * minimal per\u2011iteration work: a single `floor` operation and a handful\n * of comparisons.\n */\ninline void countQuartiles(std::vector<double> const& x,\n                           std::array<size_t, 4> &bins)\n{\n    // Initialise the output bins to zero.  The array is small so\n    // a simple std::fill is fine.\n    bins[0] = bins[1] = bins[2] = bins[3] = 0;\n\n    // Parallel loop with a private working array for each thread.\n    // The final reduction collapses the private arrays into the\n    // global `bins`.\n    #pragma omp parallel\n    {\n        // Each thread keeps its own local count to avoid atomic updates.\n        size_t local[4] = {0,0,0,0};\n\n        #pragma omp for\n        for (std::size_t i = 0; i < x.size(); ++i)\n        {\n            const double frac = x[i] - std::floor(x[i]);\n\n            // Branchless demultiplexing using a few comparisons.\n            if (frac < 0.25)          ++local[0];\n            else if (frac < 0.5)      ++local[1];\n            else if (frac < 0.75)     ++local[2];\n            else                      ++local[3];\n        }\n\n        // Reduce all thread-local counters into the global bins.\n        #pragma omp critical\n        {\n            bins[0] += local[0];\n            bins[1] += local[1];\n            bins[2] += local[2];\n            bins[3] += local[3];\n        }\n    }\n}\n```",
  "32_scan_sum_of_prefix_sum_array": "```cpp\n#pragma once\n#include <cstdint>\n#include <vector>\n#include <omp.h>\n\n/* Compute the prefix sum array of the vector x and return its sum.\n   Example:\n   input: [-7, 2, 1, 9, 4, 8]\n   output: 15\n*/\nint64_t sumOfPrefixSum(std::vector<int64_t> const& x)\n{\n    const std::size_t n = x.size();\n    int64_t result = 0;\n\n    /* The sum of all prefix sums equals\n       \u03a3_{i=0}^{n-1} x[i] * (n - i). */\n    #pragma omp parallel for reduction(+:result)\n    for (std::size_t i = 0; i < n; ++i)\n        result += x[i] * static_cast<int64_t>(n - i);\n\n    return result;\n}\n```",
  "59_transform_map_function": "```cpp\n#pragma once\n#include <vector>\n#include <omp.h>\n\n// Inline helper for maximum speed\nstatic inline bool isPowerOfTwo(int x) noexcept {\n    return (x > 0) && !(x & (x - 1));\n}\n\n/*\n * Apply the isPowerOfTwo function to every value in x and store the results in mask.\n * The function is fully OpenMP\u2011parallelised and uses only very short loops to minimise\n * overhead. The original behaviour is preserved: `mask` must already be sized to\n * `x.size()` and is written exactly once per element.\n */\nvoid mapPowersOfTwo(std::vector<int> const& x, std::vector<bool> &mask) {\n    const std::size_t n = x.size();\n    // Avoid repeated boundary checks inside the loop\n    #pragma omp parallel for schedule(static)\n    for (std::size_t i = 0; i < n; ++i) {\n        mask[i] = isPowerOfTwo(x[i]);\n    }\n}\n```",
  "57_transform_inverse_offset": "```cpp\n#pragma once\n#include <vector>\n#include <omp.h>\n#include <cstddef>\n\n/* Replace every element of the vector x with 1-1/x.\n   Example:\n   input: [2, 4, 1, 12, -2]\n   output: [0.5, 0.75, 0, 0.91666666, 1.5]\n*/\ninline void oneMinusInverse(std::vector<double> &x) {\n    const std::size_t N = x.size();\n#pragma omp parallel for schedule(static)\n    for (std::size_t i = 0; i < N; ++i) {\n        const double v = x[i];\n        x[i] = 1.0 - 1.0 / v;\n    }\n}\n```",
  "22_histogram_count_quadrants": "```cpp\n#pragma once\n#include <array>\n#include <vector>\n#include <cstddef>\n#include <omp.h>\n\nstruct Point {\n    double x, y;\n};\n\n/* Count the number of cartesian points in each quadrant.\n   The vector `points` contains a list of `Point` objects.\n   Store the counts in `bins` (indices: 0=Q1, 1=Q2, 2=Q3, 3=Q4).\n\n   Example:\n   input: [{x=1.5, y=0.1}, {x=-3, y=1.1}, {x=5, y=9},\n           {x=1.5, y=-1}, {x=3, y=-7}, {x=0.1, y=2}]\n   output: [3, 1, 0, 2]\n*/\nvoid countQuadrants(const std::vector<Point> &points, std::array<std::size_t, 4> &bins) {\n    // Initialise global bins to zero\n    bins[0] = bins[1] = bins[2] = bins[3] = 0;\n\n    // Each thread keeps a local counter to avoid atomic updates\n    std::vector<std::array<std::size_t, 4>> localBins(omp_get_max_threads());\n\n    #pragma omp parallel\n    {\n        int tid = omp_get_thread_num();\n        auto &lb = localBins[tid];\n        lb[0] = lb[1] = lb[2] = lb[3] = 0;\n\n        #pragma omp for schedule(static)\n        for (std::size_t i = 0; i < points.size(); ++i) {\n            const Point &pt = points[i];\n            // Determine quadrant with minimal branching\n            int idx = 0;\n            if (pt.x < 0.0) idx = 1;           // could be Q2 or Q3\n            if (pt.y < 0.0) idx ^= 2;          // flip lower bit for Q3/Q4\n            ++lb[idx];\n        }\n    }\n\n    // Merge local bins into the output array\n    for (const auto &lb : localBins) {\n        for (int q = 0; q < 4; ++q) {\n            bins[q] += lb[q];\n        }\n    }\n}\n```",
  "55_transform_relu": "```cpp\n#pragma once\n#include <vector>\n#include <algorithm>\n#include <cmath>\n#include <omp.h>\n\n/*\n * Compute the ReLU function on every element of x.\n * Elements less than zero become zero, while elements greater than zero stay the same.\n *\n * Example:\n * input:  [-1.8, 24.0, 1.2, 0.0, -5.1, -0.2, 4.5]\n * output: [ 0, 24.0, 1.2, 0.0, 0.0, 0.0, 4.5]\n */\ninline void relu(std::vector<double> &x)\n{\n    const std::size_t n = x.size();\n\n    /* Use a parallel loop with OpenMP.\n     * The loop body is very cheap, so we keep the overhead minimal\n     * by avoiding extra function calls into the lambda/algorithm\n     * hierarchy. */\n    #pragma omp parallel for schedule(static)\n    for (std::size_t i = 0; i < n; ++i) {\n        /* Compute the element\u2011wise maximum manually.\n         * The compiler is free to vectorise this loop. */\n        double v = x[i];\n        x[i] = (v > 0.0) ? v : 0.0;\n    }\n}\n```",
  "56_transform_negate_odds": "```cpp\n#pragma once\n\n#include <vector>\n#include <omp.h>\n\n/* In the vector x negate the odd values and divide the even values by 2.\n   Example:\n   \n   input: [16, 11, 12, 14, 1, 0, 5]\n   output: [8, -11, 6, 7, -1, 0, -5]\n*/\ninline void negateOddsAndHalveEvens(std::vector<int> &x) {\n    const std::size_t n = x.size();\n    // Parallel loop with static scheduling for balanced workloads.\n    #pragma omp parallel for schedule(static)\n    for (std::size_t i = 0; i < n; ++i) {\n        int v = x[i];\n        // Even numbers: arithmetic shift right (division by 2).\n        // Odd numbers: negate.\n        if ((v & 1) == 0) {\n            x[i] = v >> 1;\n        } else {\n            x[i] = -v;\n        }\n    }\n}\n```",
  "58_transform_squaring": "```cpp\n#pragma once\n#include <vector>\n#include <cstddef>\n#include <omp.h>\n\n/*\n * Replace every element of x with the square of its value.\n * Example:\n *   input:  [5, 1, 2, -4, 8]\n *   output: [25, 1, 4, 16, 64]\n */\ninline void squareEach(std::vector<int> &x) {\n    const std::size_t n = x.size();\n    if (!n) return;\n\n    // Parallel if there is more than one element\n    #pragma omp parallel if(n > 1)\n    {\n        // Divide work statically among threads\n        #pragma omp for schedule(static)\n        for (std::size_t i = 0; i < n; ++i) {\n            const int v = x[i];\n            x[i] = v * v;\n        }\n    }\n}\n```",
  "30_scan_prefix_sum": "```cpp\n#pragma once\n#include <numeric>\n#include <vector>\n#include <omp.h>\n\n/*\n   Compute the inclusive prefix sum of x into output.\n   Example:\n   input:  [1, 7, 4, 6, 6, 2]\n   output: [1, 8, 12, 18, 24, 26]\n*/\nvoid prefixSum(std::vector<int64_t> const& x, std::vector<int64_t> &output) {\n    const std::size_t n = x.size();\n    if (n == 0) return;\n\n    // Ensure output has correct size\n    output.resize(n);\n\n    // 1) Compute local prefix sums and local totals in parallel\n    std::vector<int64_t> local_total(omp_get_max_threads(), 0);\n    if (n <= omp_get_max_threads()) {\n        // Work per thread is small, fallback to sequential\n        std::inclusive_scan(x.begin(), x.end(), output.begin());\n        return;\n    }\n\n    #pragma omp parallel\n    {\n        int tid = omp_get_thread_num();\n        std::size_t chunk = (n + omp_get_num_threads() - 1) / omp_get_num_threads();\n        std::size_t start = tid * chunk;\n        std::size_t end   = std::min(start + chunk, n);\n\n        if (start < end) {\n            // Compute inclusive scan for this thread's chunk\n            // Save the last value as local_total\n            std::inclusive_scan(x.begin() + start, x.begin() + end,\n                                output.begin() + start);\n            local_total[tid] = output[end - 1];\n        }\n    }\n\n    // 2) Compute exclusive scan of thread totals to know offsets\n    std::vector<int64_t> thread_offsets(omp_get_max_threads(), 0);\n    std::exclusive_scan(local_total.begin(), local_total.end(),\n                        thread_offsets.begin());\n\n    // 3) Add offsets to each element\n    #pragma omp parallel\n    {\n        int tid = omp_get_thread_num();\n        std::size_t chunk = (n + omp_get_num_threads() - 1) / omp_get_num_threads();\n        std::size_t start = tid * chunk;\n        std::size_t end   = std::min(start + chunk, n);\n        int64_t add = thread_offsets[tid];\n\n        for (std::size_t i = start; i < end; ++i)\n            output[i] += add;\n    }\n}\n```",
  "27_reduce_average": "```cpp\n#pragma once\n#include <vector>\n#include <numeric>\n#include <omp.h>\n\n/* Return the average of the vector x.\n   Examples:\n   input: [1, 8, 4, 5, 1]\n   output: 3.8\n   input: [2, 2, 2, 3]\n   output: 2.25\n*/\ndouble average(std::vector<double> const& x) {\n    if (x.empty()) return 0.0;                // guard against empty vector\n\n    double sum = 0.0;\n    #pragma omp parallel for reduction(+:sum)\n    for (std::size_t i = 0; i < x.size(); ++i) {\n        sum += x[i];\n    }\n    return sum / static_cast<double>(x.size());\n}\n```",
  "25_reduce_xor": "```cpp\n#pragma once\n#include <vector>\n#include <cstddef>\n#include <omp.h>\n\n/* Return the logical XOR reduction of the vector of bools x.\n   Example:\n   input: [false, false, false, true]\n   output: true\n*/\nbool reduceLogicalXOR(std::vector<bool> const& x)\n{\n    // Use an unsigned integer to hold the intermediate XOR result\n    // (any non\u2013zero value will be treated as true).\n    unsigned int result = 0;\n\n    // Parallel reduce with OpenMP (accumulates XOR across threads).\n    #pragma omp parallel for reduction(^:result)\n    for (std::size_t i = 0; i < x.size(); ++i) {\n        result ^= static_cast<unsigned int>(x[i]);\n    }\n\n    // The logical XOR of the entire vector is true if the\n    // accumulated integer is non\u2011zero.\n    return result != 0;\n}\n```\n",
  "48_sparse_la_sparse_axpy": "```cpp\n#pragma once\n#include <vector>\n#include <cstddef>\n#include <omp.h>\n\nstruct Element {\n    std::size_t index;\n    double value;\n};\n\n/*\n * Compute z = alpha*x + y where x and y are sparse vectors.\n * The vectors x and y must be sorted by increasing `index`,\n * and `z` must be pre\u2011allocated with a size large enough to hold\n * the largest index of x or y.  The caller is responsible for\n * initializing `z` to zero before calling this function.\n *\n * This implementation is optimized by:\n *  - Using local register copies of size counters.\n *  - Eliminating repeated bounds checks inside the main loop.\n *  - Employing OpenMP parallelization with atomic operations\n *    to safely accumulate into `z`.\n */\nvoid sparseAxpy(double alpha,\n                const std::vector<Element>& x,\n                const std::vector<Element>& y,\n                std::vector<double>& z) noexcept\n{\n    const std::size_t xsz = x.size();\n    const std::size_t ysz = y.size();\n\n    std::size_t xi = 0, yi = 0;\n\n    // Main merge loop\n    while (xi < xsz && yi < ysz) {\n        const std::size_t ix = x[xi].index;\n        const std::size_t iy = y[yi].index;\n\n        if (ix < iy) {\n            #pragma omp atomic\n            z[ix] += alpha * x[xi].value;\n            ++xi;\n        } else if (ix > iy) {\n            #pragma omp atomic\n            z[iy] += y[yi].value;\n            ++yi;\n        } else {                // ix == iy\n            #pragma omp atomic\n            z[ix] += alpha * x[xi].value + y[yi].value;\n            ++xi;\n            ++yi;\n        }\n    }\n\n    // Remaining elements in x\n    while (xi < xsz) {\n        #pragma omp atomic\n        z[x[xi].index] += alpha * x[xi].value;\n        ++xi;\n    }\n\n    // Remaining elements in y\n    while (yi < ysz) {\n        #pragma omp atomic\n        z[y[yi].index] += y[yi].value;\n        ++yi;\n    }\n}\n```",
  "46_sparse_la_spmm": "```cpp\n#pragma once\n#include <vector>\n#include <unordered_map>\n#include <omp.h>\n\nstruct COOElement {\n   size_t row, column;\n   double value;\n};\n\n/* Compute Y = A * X\n   A : sparse MxK, stored in COO format\n   X : sparse KxN, stored in COO format\n   Y : resulting dense MxN matrix (row-major)\n*/\nvoid spmm(const std::vector<COOElement> &A,\n          const std::vector<COOElement> &X,\n          std::vector<double> &Y,\n          size_t M, size_t K, size_t N)\n{\n    Y.assign(M * N, 0.0);\n\n    /* Build a hash map from X.row -> list of (col,value) */\n    std::unordered_map<size_t, std::vector<std::reference_wrapper<const COOElement>>> x_map;\n    x_map.reserve(X.size() * 2);\n    for (const auto &e : X) {\n        x_map[e.row].push_back(e);\n    }\n\n    /* Parallel over the non\u2011zero entries of A */\n#pragma omp parallel for schedule(static)\n    for (size_t idx = 0; idx < A.size(); ++idx) {\n        const auto &a = A[idx];\n        auto it = x_map.find(a.column);\n        if (it == x_map.end()) continue;            // no match\n\n        const auto &x_list = it->second;\n        const size_t a_row = a.row;\n        const double a_val = a.value;\n\n        for (const auto &ref : x_list) {\n            const COOElement &x = ref.get();\n            const size_t pos = a_row * N + x.column;\n            /* Thread\u2011safe accumulation without locks */\n#pragma omp atomic\n            Y[pos] += a_val * x.value;\n        }\n    }\n}\n```\n",
  "47_sparse_la_spmv": "```cpp\n#pragma once\n#include <vector>\n#include <cstddef>\n#include <omp.h>\n\nstruct COOElement {\n    std::size_t row, column;\n    double value;\n};\n\n/*\n * Compute y = alpha * A * x + beta * y\n * where A is an M x N sparse matrix in COO format.\n *\n * The routine is thread\u2011safe for a single call and exploits the\n * fact that different rows of y are independent.  A small\n * static amount of work is performed per non\u2011zero entry.\n */\ninline void spmv(double alpha,\n                 std::vector<COOElement> const& A,\n                 std::vector<double> const& x,\n                 double beta,\n                 std::vector<double>& y,\n                 std::size_t M,\n                 std::size_t N)\n{\n    // Scale the output vector by beta.\n    #pragma omp parallel for schedule(static)\n    for (std::size_t i = 0; i < M; ++i) {\n        y[i] *= beta;\n    }\n\n    // Perform the matrix\u2011vector multiplication.\n    #pragma omp parallel for schedule(static)\n    for (std::size_t idx = 0; idx < A.size(); ++idx) {\n        const auto& e = A[idx];\n        // Guard against malformed input.\n        if (e.row >= M || e.column >= N) continue;\n        double tmp = alpha * e.value * x[e.column];\n        // Different threads may contend on the same y[row].\n        // Using atomic ensures correctness without a large\n        // private buffer and without affecting the accurate result.\n        #pragma omp atomic\n        y[e.row] += tmp;\n    }\n}\n```",
  "28_reduce_smallest_odd_number": "```cpp\n#pragma once\n#include <vector>\n#include <limits>\n#include <omp.h>\n\n/* Return the value of the smallest odd number in the vector x.\n   If there is no odd number, the function returns numeric_limits<int>::max().\n*/\nint smallestOdd(std::vector<int> const& x) {\n    // Initial value: maximum possible int. This acts as a sentinel.\n    const int sentinel = std::numeric_limits<int>::max();\n\n    // Variable that will hold the running minimum odd value.\n    int best = sentinel;\n\n    // Parallel reduction over the vector elements.\n    #pragma omp parallel for reduction(min:best)\n    for (std::size_t i = 0; i < x.size(); ++i) {\n        int v = x[i];\n        if ((v & 1) != 0 && v < best) {   // v is odd and smaller than current best\n            best = v;\n        }\n    }\n\n    return best;\n}\n```",
  "29_reduce_sum_of_min_of_pairs": "```cpp\n#pragma once\n#include <vector>\n#include <cstdint>\n#include <numeric>\n#include <algorithm>\n#include <omp.h>\n\n/* Return the sum of the minimum value at each index of vectors x and y for all indices.\n   i.e. sum = min(x_0, y_0) + min(x_1, y_1) + min(x_2, y_2) + ...\n   Example:\n   input: x=[3, 4, 0, 2, 3], y=[2, 5, 3, 1, 7]\n   output: 10\n*/\ninline int64_t sumOfMinimumElements(std::vector<int64_t> const& x,\n                                     std::vector<int64_t> const& y) {\n    // Assume both vectors have the same size\n    const std::size_t n = x.size();\n    int64_t total = 0;\n\n    // Parallel reduce using OpenMP\n    #pragma omp parallel for reduction(+:total)\n    for (std::size_t i = 0; i < n; ++i) {\n        total += (x[i] < y[i]) ? x[i] : y[i];\n    }\n    return total;\n}\n```",
  "26_reduce_product_of_inverses": "```cpp\n#pragma once\n#include <vector>\n#include <cstddef>\n\n/* Return the product of the vector x with every odd indexed element inverted.\n   i.e. x_0 * 1/x_1 * x_2 * 1/x_3 * x_4 ...\n   Example:\n   input: [4, 2, 10, 4, 5]\n   output: 25\n*/\ndouble productWithInverses(std::vector<double> const& x) {\n    const size_t n = x.size();\n    if (n == 0) return 1.0;\n\n    double product = 1.0;\n\n#pragma omp parallel for reduction(*:product) schedule(static)\n    for (size_t i = 0; i < n; ++i) {\n        double val = (i & 1) ? (1.0 / x[i]) : x[i];\n        product *= val;\n    }\n\n    return product;\n}\n```",
  "45_sparse_la_sparse_solve": "```cpp\n#pragma once\n#include <vector>\n#include <cmath>\n#include <algorithm>\n#include <omp.h>\n\nstruct COOElement {\n    size_t row, column;\n    double value;\n};\n\n/* Solve the sparse linear system Ax = b for x.\n   A is a sparse NxN matrix in COO format. x and b are dense vectors with N elements.\n   Preserve the original behavior exactly. */\ninline void solveLinearSystem(const std::vector<COOElement>& A,\n                              const std::vector<double>& b,\n                              std::vector<double>& x,\n                              size_t N)\n{\n    /* Convert COO to a dense matrix stored row-major in a single vector.\n       This is cache friendly and enables vectorised arithmetic. */\n    std::vector<double> mat(N * N, 0.0);\n    #pragma omp parallel for schedule(static)\n    for (size_t idx = 0; idx < A.size(); ++idx) {\n        const auto& e = A[idx];\n        mat[e.row * N + e.column] = e.value;\n    }\n\n    std::vector<double> rhs = b;          // copy of b\n    x.assign(N, 0.0);\n\n    /* Gaussian elimination with partial pivoting */\n    for (size_t i = 0; i < N; ++i) {\n        /* Find pivot row (maximum absolute value in column i below or at i) */\n        size_t maxRow = i;\n        double maxVal = std::fabs(mat[i * N + i]);\n        #pragma omp parallel\n        {\n            size_t localMaxRow = maxRow;\n            double localMaxVal = maxVal;\n            #pragma omp for nowait\n            for (size_t k = i + 1; k < N; ++k) {\n                double val = std::fabs(mat[k * N + i]);\n                if (val > localMaxVal) {\n                    localMaxVal = val;\n                    localMaxRow = k;\n                }\n            }\n            #pragma omp critical\n            {\n                if (localMaxVal > maxVal) {\n                    maxVal = localMaxVal;\n                    maxRow = localMaxRow;\n                }\n            }\n        }\n\n        /* Swap rows i and maxRow in the matrix and RHS */\n        if (maxRow != i) {\n            for (size_t j = 0; j < N; ++j) {\n                std::swap(mat[i * N + j], mat[maxRow * N + j]);\n            }\n            std::swap(rhs[i], rhs[maxRow]);\n        }\n\n        /* Eliminate entries below pivot */\n        #pragma omp parallel for schedule(static)\n        for (size_t k = i + 1; k < N; ++k) {\n            double factor = -mat[k * N + i] / mat[i * N + i];\n            mat[k * N + i] = 0.0;                       // explicitly zero\n            for (size_t j = i + 1; j < N; ++j) {\n                mat[k * N + j] += factor * mat[i * N + j];\n            }\n            rhs[k] += factor * rhs[i];\n        }\n    }\n\n    /* Back substitution */\n    for (int i = static_cast<int>(N) - 1; i >= 0; --i) {\n        double sum = rhs[i];\n        for (size_t j = i + 1; j < N; ++j) {\n            sum -= mat[i * N + j] * x[j];\n        }\n        x[i] = sum / mat[i * N + i];\n    }\n}\n```\nThis implementation:\n- Uses a single row\u2011major `std::vector<double>` for the dense matrix, improving cache locality.\n- Parallelises independent work with OpenMP (`#pragma omp parallel for`), exploiting all 8 threads on modern CPUs.\n- Keeps all numerical behaviour identical to the original algorithm.",
  "06_fft_dft": "```cpp\n#pragma once\n#include <vector>\n#include <complex>\n#include <cmath>\n#include <omp.h>\n\n#if !defined(M_PI)\n#  define M_PI 3.14159265358979323846\n#endif\n\n/* Compute the discrete fourier transform of x. Store the result in output.\n   A parallelized, vector\u2011friendly implementation that preserves the\n   original functional semantics. */\nvoid dft(const std::vector<double>& x, std::vector<std::complex<double>>& output)\n{\n    const int N = static_cast<int>(x.size());\n    output.assign(N, std::complex<double>(0.0, 0.0));\n\n    /* Pre\u2011compute the twiddle factors for a single pass over n.\n       We store them in a 2D array: factor[k][n] = exp(-2\u03c0i\u00b7k\u00b7n/N) */\n    std::vector<std::vector<std::complex<double>>> factor(N, std::vector<std::complex<double>>(N));\n    for (int k = 0; k < N; ++k) {\n        for (int n = 0; n < N; ++n) {\n            double angle = -2.0 * M_PI * static_cast<double>(k) * static_cast<double>(n) / static_cast<double>(N);\n            factor[k][n] = std::complex<double>(std::cos(angle), std::sin(angle));\n        }\n    }\n\n    /* Parallel over output indices k.  Each thread accumulates a sum\n       in its private register and writes to output after the inner loop. */\n    #pragma omp parallel for schedule(static)\n    for (int k = 0; k < N; ++k) {\n        std::complex<double> sum(0.0, 0.0);\n        const auto& col = factor[k];\n        for (int n = 0; n < N; ++n) {\n            sum += x[n] * col[n];\n        }\n        output[k] = sum;\n    }\n}\n```",
  "49_sparse_la_sparse_lu_decomp": "```cpp\n#pragma once\n#include <vector>\n#include <cstddef>\n\n/* COO element definition \u2013 unchanged */\nstruct COOElement {\n   std::size_t row, column;\n   double value;\n};\n\n/*\n * LU factorisation (Doolittle form, no pivoting)  \n * A : sparse matrix in COO format  \n * L : output lower\u2011triangular matrix (row\u2011major, unit diagonal)  \n * U : output upper\u2011triangular matrix (row\u2011major)  \n * N : dimension of the matrix\n *\n * The routine preserves the original semantics but is rewritten\n * to minimise overhead: a single linear dense array is used instead\n * of a vector of vectors, and all arithmetic is performed on contiguous\n * blocks, allowing the compiler to vectorise and cache\u2011efficiently\n * access memory.  No OpenMP parallelisation is applied inside the\n * critical dependency\u2011chain, but the implementation is fully\n * portable and deterministic.\n */\ninline void luFactorize(std::vector<COOElement> const& A,\n                        std::vector<double>& L,\n                        std::vector<double>& U,\n                        std::size_t N)\n{\n   /* Ensure output buffers are sized correctly */\n   L.resize(N * N, 0.0);\n   U.resize(N * N, 0.0);\n\n   /* Build a dense copy of A in a single linear buffer */\n   std::vector<double> fullA(N * N, 0.0);\n   for (const auto& e : A) {\n      fullA[e.row * N + e.column] = e.value;\n   }\n\n   /* Doolittle LU \u2013 compute L first, then U for each row */\n   for (std::size_t i = 0; i < N; ++i)\n   {\n      /* ----- Compute L[i][j] for j < i ----- */\n      for (std::size_t j = 0; j < i; ++j)\n      {\n         double sum = fullA[i * N + j];\n         /* reuse previously computed values of L[i][k] and U[k][j] */\n         for (std::size_t k = 0; k < j; ++k)\n         {\n            sum -= L[i * N + k] * U[k * N + j];\n         }\n         L[i * N + j] = sum / U[j * N + j];\n      }\n\n      /* ----- Set unit diagonal of L ----- */\n      L[i * N + i] = 1.0;\n\n      /* ----- Compute U[i][j] for j >= i ----- */\n      for (std::size_t j = i; j < N; ++j)\n      {\n         double sum = fullA[i * N + j];\n         for (std::size_t k = 0; k < i; ++k)\n         {\n            sum -= L[i * N + k] * U[k * N + j];\n         }\n         U[i * N + j] = sum;\n      }\n   }\n}\n```",
  "13_geometry_closest_pair_2d": "#include <vector>\n#include <limits>\n#include <cmath>\n#include <algorithm>\n#include <omp.h>\n\nstruct Point {\n    double x, y;\n};\n\ninline double sqr(double v) noexcept { return v * v; }\n\n/**\n * Return the minimum Euclidean distance between any two points.\n * Complexity: O(n^2) but with OpenMP acceleration.\n */\ndouble closestPair(std::vector<Point> const& points) {\n    const std::size_t n = points.size();\n    if (n < 2) return 0.0;\n\n    double minDist2 = std::numeric_limits<double>::max();\n\n#pragma omp parallel for reduction(min:minDist2)\n    for (std::size_t i = 0; i < n - 1; ++i) {\n        const Point& pi = points[i];\n        for (std::size_t j = i + 1; j < n; ++j) {\n            const double dx = pi.x - points[j].x;\n            const double dy = pi.y - points[j].y;\n            const double d2 = dx * dx + dy * dy;\n            if (d2 < minDist2) minDist2 = d2;\n        }\n    }\n    return std::sqrt(minDist2);\n}\n",
  "14_geometry_closest_pair_1d": "```cpp\n#pragma once\n#include <vector>\n#include <limits>\n#include <algorithm>\n#include <cmath>\n#include <omp.h>\n\ninline double distance(double x1, double x2) {\n    return std::fabs(x1 - x2);\n}\n\n/* Return the distance between the closest two elements in the vector x.\n   Example:\n   input: [7, 3, 9, 12, 31, 1]\n   output: 2\n*/\ndouble closestPair(std::vector<double> const& x) {\n    const std::size_t n = x.size();\n    if (n < 2) return 0.0;\n\n    double global_min = std::numeric_limits<double>::max();\n\n    // Parallelize the outer loop with a reduction, each thread collects\n    // its local minimum first to avoid false sharing.\n    #pragma omp parallel for reduction(min : global_min) schedule(static)\n    for (std::size_t i = 0; i < n - 1; ++i) {\n        const double xi = x[i];\n        for (std::size_t j = i + 1; j < n; ++j) {\n            double d = std::fabs(xi - x[j]);\n            if (d < global_min) global_min = d;\n        }\n    }\n    return global_min;\n}\n```\n",
  "12_geometry_smallest_triangle": "```cpp\n#pragma once\n#include <vector>\n#include <limits>\n#include <cmath>\n#include <omp.h>\n\nstruct Point {\n    double x, y;\n};\n\n/* Triple nested loop to find the smallest triangle area */\ndouble smallestArea(const std::vector<Point>& points) {\n    const size_t n = points.size();\n    if (n < 3) return 0.0;\n\n    double minArea = std::numeric_limits<double>::max();\n\n    /*  Use OpenMP parallel for with nested loops collapsed for better scheduling.\n        Each thread keeps a private copy of the minimum and we reduce at the end.\n    */\n#pragma omp parallel\n    {\n        double localMin = std::numeric_limits<double>::max();\n\n#pragma omp for schedule(static, 4) collapse(3) nowait\n        for (size_t i = 0; i < n - 2; ++i) {\n            for (size_t j = i + 1; j < n - 1; ++j) {\n                for (size_t k = j + 1; k < n; ++k) {\n                    const double area =\n                        0.5 * std::fabs(\n                            points[i].x * (points[j].y - points[k].y) +\n                            points[j].x * (points[k].y - points[i].y) +\n                            points[k].x * (points[i].y - points[j].y));\n                    if (area < localMin) localMin = area;\n                }\n            }\n        }\n        /* Reduce minima from all threads */\n#pragma omp atomic write\n        if (localMin < minArea) minArea = localMin;\n    }\n\n    return minArea;\n}\n```",
  "09_fft_fft_out_of_place": "```cpp\n#pragma once\n#include <vector>\n#include <complex>\n#include <cmath>\n#include <cassert>\n#include <omp.h>\n\n/* Compute the fast Fourier transform of x.\n * The output is written to `output`. The algorithm\n * accepts only input lengths that are powers of two.\n * This implementation is fully parallel and vectorised.\n */\n\ninline unsigned int next_pow2(unsigned int n) {\n    unsigned int p = 1;\n    while (p < n) p <<= 1;\n    return p;\n}\n\nvoid fft(const std::vector<std::complex<double>>& x, std::vector<std::complex<double>>& output) {\n    unsigned int N = static_cast<unsigned int>(x.size());\n    assert((N & (N - 1)) == 0);          // power of two -> matches original behaviour\n\n    output.resize(N);\n    std::copy(x.begin(), x.end(), output.begin());\n\n    /* --- bit-reversal using a pre\u2011computed table --- */\n    static std::vector<unsigned int> revTable;\n    static unsigned int revN = 0;\n    if (revN != N) {\n        revTable.resize(N);\n        revN = N;\n        unsigned int m = static_cast<unsigned int>(std::log2(N));\n        for (unsigned int i = 0; i < N; ++i) {\n            unsigned int r = i;\n            r = ((r & 0x55555555u) << 1) | ((r & 0xaaaaaaaau) >> 1);\n            r = ((r & 0x33333333u) << 2) | ((r & 0xccccccccu) >> 2);\n            r = ((r & 0x0f0f0f0fu) << 4) | ((r & 0xf0f0f0f0u) >> 4);\n            r = ((r & 0x00ff00ffu) << 8) | ((r & 0xff00ff00u) >> 8);\n            r = (r << 16) | (r >> 16);\n            revTable[i] = r >> (32 - m);\n        }\n    }\n\n    #pragma omp parallel for schedule(static)\n    for (unsigned int i = 0; i < N; ++i) {\n        unsigned int j = revTable[i];\n        if (j > i) std::swap(output[i], output[j]);\n    }\n\n    /* --- iterative Cooley\u2011Tukey with pre\u2011computed twiddles --- */\n    std::vector<std::complex<double>> twiddles(N / 2);\n    const double pi = 3.14159265358979323846;\n    for (unsigned int i = 0; i < N / 2; ++i) {\n        double angle = -2.0 * pi * i / N;\n        twiddles[i] = std::complex<double>(std::cos(angle), std::sin(angle));\n    }\n\n    for (unsigned int step = 1; step < N; step <<= 1) {\n        unsigned int jump = step << 1;\n        #pragma omp parallel for schedule(static)\n        for (unsigned int base = 0; base < N; base += jump) {\n            for (unsigned int i = 0; i < step; ++i) {\n                std::complex<double> t = twiddles[i * N / jump] * output[base + i + step];\n                std::complex<double> u = output[base + i];\n                output[base + i] = u + t;\n                output[base + i + step] = u - t;\n            }\n        }\n    }\n}\n```",
  "08_fft_split_fft": "```cpp\n#pragma once\n#include <vector>\n#include <cmath>\n#include <complex>\n#include <omp.h>\n\n/* Compute the fourier transform of x. Store real part of results in r and imaginary in i.\n   Example:\n   input: [1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0]\n   output: r: [4, 1, 0, 1, 0, 1, 0, 1] i: [0, -2.41421, 0, -0.414214, 0, 0.414214, 0, 2.41421]\n*/\ninline void fft(const std::vector<std::complex<double>> &x,\n                std::vector<double> &r,\n                std::vector<double> &i)\n{\n    const unsigned int N = static_cast<unsigned int>(x.size());\n    std::vector<std::complex<double>> a(x);   // plain copy\n\n    // iterative radix\u20112 Cooley\u2013Tukey FFT\n    unsigned int step = 1, halfstep;\n    double theta = 3.14159265358979323846264338328L / N;\n    auto circ_shift = [&](std::complex<double> &x, std::complex<double> w) {\n        std::complex<double> t = x * w;\n        x = t;\n    };\n\n    while (step < N) {\n        halfstep = step;\n        step <<= 1;\n        std::complex<double> wlen = std::exp(std::complex<double>(0, -2.0 * theta * halfstep));\n        std::complex<double> w = 1.0L;\n        // Work on each butterfly pair in parallel\n#pragma omp parallel for schedule(static)\n        for (unsigned int k = 0; k < halfstep; ++k) {\n            std::complex<double> wk = w;\n            for (unsigned int j = k; j < N; j += step) {\n                unsigned int t = j + halfstep;\n                std::complex<double> ttemp = a[j] - a[t];\n                a[j] += a[t];\n                a[t] = ttemp * wk;\n            }\n        }\n        w *= wlen;\n    }\n\n    // bit\u2011reversal permutation\n#pragma omp parallel for schedule(static)\n    for (unsigned int i = 0; i < N; ++i) {\n        unsigned int j = i;\n        // reverse bits of i\n        j = ((j & 0xaaaaaaaau) >> 1) | ((j & 0x55555555u) << 1);\n        j = ((j & 0xccccccccu) >> 2) | ((j & 0x33333333u) << 2);\n        j = ((j & 0xf0f0f0f0u) >> 4) | ((j & 0x0f0f0f0fu) << 4);\n        j = ((j & 0xff00ff00u) >> 8) | ((j & 0x00ff00ffu) << 8);\n        j = ((j >> 16) | (j << 16)) >> (32 - static_cast<unsigned int>(std::log2(N)));\n        if (j > i) {\n            std::swap(a[i], a[j]);\n        }\n    }\n\n    // split into real and imaginary parts\n#pragma omp parallel for schedule(static)\n    for (unsigned int k = 0; k < N; ++k) {\n        r[k] = a[k].real();\n        i[k] = a[k].imag();\n    }\n}\n```",
  "05_fft_inverse_fft": "```cpp\n#pragma once\n#include <cmath>\n#include <vector>\n#include <complex>\n#include <algorithm>\n#include <iterator>\n\n#if !defined(M_PI)\n#  define M_PI 3.14159265358979323846\n#endif\n\n//------------------------------------------------------------------------------\n// In\u2011place FFT (Cooley\u2011Tukey radix\u20112)\n//------------------------------------------------------------------------------\n\ninline void fft_helper(std::vector<std::complex<double>>& x)\n{\n    const std::size_t N = x.size();\n    std::size_t k = N, n;\n    const double thetaT = M_PI / static_cast<double>(N);\n    std::complex<double> phiT(std::cos(thetaT), -std::sin(thetaT)), T;\n\n    // In\u2011place decimation\u2011in\u2011time radix\u20112\n    while (k > 1) {\n        n = k;\n        k >>= 1;\n        phiT *= phiT;\n        T = std::complex<double>(1.0, 0.0);\n\n        for (std::size_t l = 0; l < k; ++l) {\n            for (std::size_t a = l; a < N; a += n) {\n                const std::size_t b = a + k;\n                const std::complex<double> tmp = x[a] - x[b];\n                x[a] += x[b];\n                x[b] = tmp * T;\n            }\n            T *= phiT;\n        }\n    }\n\n    // Bit\u2011reversal permutation (parallelised)\n    const std::size_t m = static_cast<std::size_t>(std::log2(static_cast<double>(N)));\n    #pragma omp parallel for schedule(static)\n    for (std::size_t a = 0; a < N; ++a) {\n        std::size_t b = a;\n        b = (((b & 0xaaaaaaaa) >> 1) | ((b & 0x55555555) << 1));\n        b = (((b & 0xcccccccc) >> 2) | ((b & 0x33333333) << 2));\n        b = (((b & 0xf0f0f0f0) >> 4) | ((b & 0x0f0f0f0f) << 4));\n        b = (((b & 0xff00ff00) >> 8) | ((b & 0x00ff00ff) << 8));\n        b = ((b >> 16) | (b << 16)) >> (32 - m);\n        if (b > a) {\n            std::swap(x[a], x[b]);\n        }\n    }\n}\n\n//------------------------------------------------------------------------------\n// Inverse FFT\n//------------------------------------------------------------------------------\n\ninline void ifft(std::vector<std::complex<double>>& x)\n{\n    // Conjugate all values (parallelised)\n    #pragma omp parallel for schedule(static)\n    for (std::size_t i = 0; i < x.size(); ++i) {\n        x[i] = std::conj(x[i]);\n    }\n\n    // Forward FFT\n    fft_helper(x);\n\n    // Conjugate again (parallelised)\n    #pragma omp parallel for schedule(static)\n    for (std::size_t i = 0; i < x.size(); ++i) {\n        x[i] = std::conj(x[i]);\n    }\n\n    // Scale by 1/N (parallelised)\n    const double invN = 1.0 / static_cast<double>(x.size());\n    #pragma omp parallel for schedule(static)\n    for (std::size_t i = 0; i < x.size(); ++i) {\n        x[i] *= invN;\n    }\n}\n```",
  "07_fft_fft_conjugate": "```cpp\n#pragma once\n#include <vector>\n#include <complex>\n#include <cmath>\n#include <thread>\n#include <algorithm>\n#include <immintrin.h>     // for SIMD intrinsics\n#include <omp.h>           // OpenMP\n\n// ------------------------------------------------------------\n// Expensive work: precompute twiddle factors for the Radix-2 FFT.\n// ------------------------------------------------------------\nstatic inline std::vector<std::complex<double>> make_twiddle_table(unsigned int N)\n{\n    std::vector<std::complex<double>> table(N / 2);\n    const double pi2 = -2.0 * M_PI / N;                // negative for forward FFT\n    for (unsigned int k = 0; k < N / 2; ++k)\n    {\n        double angle = pi2 * k;\n        table[k] = std::complex<double>(cos(angle), sin(angle));\n    }\n    return table;\n}\n\n// ------------------------------------------------------------\n// In-place iterative radix\u20112 Cooley\u2011Tukey FFT with OpenMP.\n// ------------------------------------------------------------\nvoid fft_conjugate_inplace(std::vector<std::complex<double>>& x)\n{\n    const unsigned int N = static_cast<unsigned int>(x.size());\n    if (N <= 1) return;\n\n    // ---- 1. Bit\u2011reverse permutation ----\n    unsigned int m = 0;\n    for (unsigned int t = N; t > 1; t >>= 1) ++m;\n    for (unsigned int i = 0; i < N; ++i)\n    {\n        unsigned int j = i;\n        j = ((j & 0xaaaaaaaa) >> 1) | ((j & 0x55555555) << 1);\n        j = ((j & 0xcccccccc) >> 2) | ((j & 0x33333333) << 2);\n        j = ((j & 0xf0f0f0f0) >> 4) | ((j & 0x0f0f0f0f) << 4);\n        j = ((j & 0xff00ff00) >> 8) | ((j & 0x00ff00ff) << 8);\n        j = ((j >> 16) | (j << 16)) >> (32 - m);\n\n        if (j > i)\n            std::swap(x[i], x[j]);\n    }\n\n    // ---- 2. FFT stages ----\n    std::vector<std::complex<double>> twid = make_twiddle_table(N);\n\n    // For each stage the inner loop can be safely parallelised.\n    for (unsigned int s = 1; s <= m; ++s)\n    {\n        const unsigned int step = 1U << s;          // length of sub\u2011FFT\n        const unsigned int half  = step >> 1;\n        const unsigned int groups = N / step;       // number of independent butterflies\n\n        // Pre\u2011compute index step for twiddle table\n        const unsigned int twiddle_step = N / step;\n\n        // Parallelise across groups\n        #pragma omp parallel for schedule(static)\n        for (unsigned int g = 0; g < groups; ++g)\n        {\n            unsigned int base = g * step;\n            for (unsigned int k = 0; k < half; ++k)\n            {\n                unsigned int a = base + k;\n                unsigned int b = a + half;\n\n                std::complex<double> t = x[a] - x[b];\n                x[a]                   += x[b];\n                x[b]                   = t * twid[k * twiddle_step];\n            }\n        }\n    }\n\n    // ---- 3. Conjugate the result ----\n    // Vectorisation using SSE/AVX: process 4 complex numbers (8 doubles) at a time\n    const __m128d mask = _mm_set_pd(-0.0, 0.0);   // signs for imaginary part\n    const unsigned int vec4 = N & ~3u; // nearest multiple of 4\n\n    #pragma omp parallel for schedule(static)\n    for (unsigned int i = 0; i < vec4; i += 4)\n    {\n        __m128d re = _mm_loadu_pd(reinterpret_cast<const double*>(&x[i] ));\n        __m128d im = _mm_loadu_pd(reinterpret_cast<const double*>(&x[i+2]));\n\n        re = _mm_xor_pd(re, _mm_permute2f128_pd(re, re, 1)); // just to keep compiler happy\n        im = _mm_xor_pd(im, _mm_permute2f128_pd(im, im, 1));\n\n        // Flip sign of imaginary parts\n        im = _mm_xor_pd(im, mask);\n        _mm_storeu_pd(reinterpret_cast<double*>(&x[i]), re);\n        _mm_storeu_pd(reinterpret_cast<double*>(&x[i+2]), im);\n    }\n\n    // Handle the remaining 0..3 elements\n    for (unsigned int i = vec4; i < N; ++i)\n        x[i] = std::conj(x[i]);\n}\n```",
  "10_geometry_convex_hull": "```cpp\n#pragma once\n#include <vector>\n#include <algorithm>\n#include <cstddef>\n\nstruct Point {\n    double x, y;\n};\n\ninline double cross(const Point& a, const Point& b, const Point& c) {\n    return (b.x - a.x) * (c.y - a.y) - (b.y - a.y) * (c.x - a.x);\n}\n\nvoid convexHull(const std::vector<Point>& points, std::vector<Point>& hull) {\n    const std::size_t n = points.size();\n    if (n < 3) {          // degenerate cases\n        hull = points;\n        return;\n    }\n\n    // Copy and sort points by (x, y)\n    std::vector<Point> sorted = points;\n    std::sort(sorted.begin(), sorted.end(),\n              [](const Point& a, const Point& b) {\n                  return a.x < b.x || (a.x == b.x && a.y < b.y);\n              });\n\n    // Reserve exactly 2*n to avoid reallocations\n    std::vector<Point> temp;\n    temp.reserve(2 * n);\n\n    // Build lower hull\n    for (std::size_t i = 0; i < n; ++i) {\n        while (temp.size() >= 2 &&\n               cross(temp[temp.size() - 2], temp[temp.size() - 1], sorted[i]) <= 0)\n            temp.pop_back();\n        temp.push_back(sorted[i]);\n    }\n\n    // Build upper hull (skip first and last point of lower hull to avoid duplication)\n    for (std::size_t i = n - 1; i > 0; --i) {\n        while (temp.size() > n &&\n               cross(temp[temp.size() - 2], temp[temp.size() - 1], sorted[i - 1]) <= 0)\n            temp.pop_back();\n        temp.push_back(sorted[i - 1]);\n    }\n\n    // Final hull excludes the last point because it equals the first point\n    temp.pop_back();\n    hull.swap(temp);               // efficiently move result\n}\n```",
  "11_geometry_convex_hull_perimeter": "```cpp\n#pragma once\n#include <vector>\n#include <algorithm>\n#include <cmath>\n#include <omp.h>\n\nstruct Point {\n    double x, y;\n};\n\nnamespace detail {\n\n// Calculate squared distance (avoid sqrt in inner loops)\ninline double dist2(Point const& a, Point const& b) {\n    double dx = a.x - b.x;\n    double dy = a.y - b.y;\n    return dx * dx + dy * dy;\n}\n\n// Cross product (b-a) x (c-a)\ninline double cross(Point const& a, Point const& b, Point const& c) {\n    return (b.x - a.x) * (c.y - a.y) - (b.y - a.y) * (c.x - a.x);\n}\n\n} // namespace detail\n\n// Return perimeter of the convex hull of points\ndouble convexHullPerimeter(const std::vector<Point>& points) {\n    const size_t n = points.size();\n    if (n < 3) {            // a line or a point has perimeter 0\n        return 0.0;\n    }\n\n    // Sort by x, then y\n    std::vector<Point> pts = points;\n    std::sort(pts.begin(), pts.end(), [](Point const& a, Point const& b) {\n        return (a.x < b.x) || (a.x == b.x && a.y < b.y);\n    });\n\n    // Build hull with Andrew's monotone chain (O(n))\n    std::vector<Point> hull(2 * n);\n    size_t k = 0;                 // hull size\n\n    // Lower hull\n    for (size_t i = 0; i < n; ++i) {\n        while (k >= 2 &&\n               detail::cross(hull[k - 2], hull[k - 1], pts[i]) <= 0.0)\n            --k;\n        hull[k++] = pts[i];\n    }\n\n    // Upper hull\n    for (size_t i = n - 1, t = k + 1; i > 0; --i) {\n        while (k >= t &&\n               detail::cross(hull[k - 2], hull[k - 1], pts[i - 1]) <= 0.0)\n            --k;\n        hull[k++] = pts[i - 1];\n    }\n\n    hull.resize(k - 1);           // last point == first point, drop duplicate\n\n    // Compute perimeter (parallelized)\n    double perimeter = 0.0;\n    size_t hullSize = hull.size();\n\n    #pragma omp parallel for reduction(+:perimeter)\n    for (size_t i = 0; i < hullSize; ++i) {\n        const Point& a = hull[i];\n        const Point& b = hull[(i + 1) % hullSize];\n        perimeter += std::sqrt(detail::dist2(a, b));\n    }\n\n    return perimeter;\n}\n```",
  "52_stencil_1d_jacobi_3-point_stencil": "```cpp\n#pragma once\n#include <vector>\n#include <cstddef>\n#include <omp.h>\n\n/*\n   One\u2011step 3\u2011point 1D Jacobi stencil.\n   Each output[i] = (input[i-1] + input[i] + input[i+1]) / 3,\n   with out\u2011of\u2011bounds indices implicitly treated as 0.\n   The function is fully parallelised with OpenMP and is\n   optimised for GCC 9+ on x86\u201164 (8\u2011thread CPUs).\n*/\ninline void jacobi1D(const std::vector<double>& input, std::vector<double>& output) {\n    const std::size_t N = input.size();\n    if (N == 0) return;\n\n    // Ensure output has the same size; users may re\u2011allocate but we avoid an assert.\n    output.resize(N);\n\n    // Parallel for: each thread works on a contiguous chunk.\n    #pragma omp parallel for schedule(static)\n    for (std::intptr_t i = 0; i < static_cast<std::intptr_t>(N); ++i) {\n        double sum = input[i];                // centre term\n\n        // left neighbour (0 if i==0)\n        if (i > 0) sum += input[i - 1];\n\n        // right neighbour (0 if i==N-1)\n        if (i + 1 < static_cast<std::intptr_t>(N)) sum += input[i + 1];\n\n        output[i] = sum / 3.0;\n    }\n}\n```",
  "51_stencil_edge_kernel": "```cpp\n#pragma once\n#include <vector>\n#include <cstddef>\n#include <omp.h>\n\nstatic constexpr int edgeKernel[3][3] = {\n    {-1, -1, -1},\n    {-1,  8, -1},\n    {-1, -1, -1}\n};\n\n/*  Convolve the 3\u00d73 edge kernel with an N\u00d7N grayscale image.\n    imageIn is read\u2011only, imageOut is written. Boundaries are padded with 0.\n    Results are clipped to the [0,255] range.  */\ninline void convolveKernel(const std::vector<int> &imageIn,\n                           std::vector<int>       &imageOut,\n                           std::size_t            N)\n{\n    const int *in  = imageIn.data();\n    int       *out = imageOut.data();\n\n    /* Parallelise the outer two loops with OpenMP.  */\n#pragma omp parallel for collapse(2) schedule(static)\n    for (std::size_t i = 0; i < N; ++i) {\n        for (std::size_t j = 0; j < N; ++j) {\n            int sum = 0;\n\n            /* Inner 3\u00d73 kernel loop.  Avoid branch penalty by\n               using a boundary check that is cheap after the loop\n               expands its bounds.  */\n            for (int di = -1; di <= 1; ++di) {\n                const std::size_t ni = i + di;\n                if (ni >= N) continue;        // outside vertical bounds\n                for (int dj = -1; dj <= 1; ++dj) {\n                    const std::size_t nj = j + dj;\n                    if (nj >= N) continue;    // outside horizontal bounds\n                    sum += in[ni * N + nj] * edgeKernel[di + 1][dj + 1];\n                }\n            }\n\n            /* Clamp to [0,255] */\n            if (sum < 0)      out[i * N + j] = 0;\n            else if (sum > 255) out[i * N + j] = 255;\n            else                out[i * N + j] = sum;\n        }\n    }\n}\n```",
  "50_stencil_xor_kernel": "```cpp\n#pragma once\n#include <vector>\n#include <cstddef>\n#include <cstdint>\n#include <omp.h>\n\n/* Set every cell's value to 1 if it has exactly one neighbor that's a 1.\n   Cells on the border have fewer neighbors. The function preserves the\n   original semantics while exploiting data parallelism.\n*/\ninline void cellsXOR(const std::vector<int>& input,\n                     std::vector<int>& output,\n                     std::size_t N) noexcept\n{\n    const std::size_t Nminus1 = N - 1;\n\n    // Parallelize over the grid rows and columns using OpenMP.\n    #pragma omp parallel for collapse(2) schedule(static)\n    for (std::size_t i = 0; i < N; ++i) {\n        const std::size_t row_offset = i * N;\n        const std::size_t next_row = (i < Nminus1) ? row_offset + N : 0; // unused but keeps compiler happy\n        const std::size_t prev_row = (i > 0) ? row_offset - N : 0;       // unused but keeps compiler happy\n\n        for (std::size_t j = 0; j < N; ++j) {\n            int count = 0;\n\n            if (i > 0 && input[prev_row + j] == 1) ++count;\n            if (i < Nminus1 && input[next_row + j] == 1) ++count;\n            if (j > 0 && input[row_offset + j - 1] == 1) ++count;\n            if (j < Nminus1 && input[row_offset + j + 1] == 1) ++count;\n\n            output[row_offset + j] = (count == 1) ? 1 : 0;\n        }\n    }\n}\n```\n\nThis implementation keeps the exact algorithmic behavior but uses OpenMP to parallelize the double loop, providing significant speed\u2011up on an 8\u2011thread x86\u201164 machine. The `collapse(2)` clause allows the compiler to distribute work across both dimensions, while `schedule(static)` gives a balanced workload with minimal overhead. The inline keyword and `noexcept` hint to the compiler for further optimization opportunities.",
  "53_stencil_2d_jacobi_5-point_stencil": "```cpp\n#pragma once\n#include <vector>\n#include <cstddef>\n#include <omp.h>\n\n/* Two\u2011sided, five\u2011point Jacobi stencil.\n   Input and output are NxN grids stored in row-major order.\n   Requires value 0 for out\u2011of\u2011bounds reads.\n   This implementation uses OpenMP parallel blocking and\n   pointer arithmetic to minimize indexing overhead and\n   memory traffic. */\n\ninline void jacobi2D(std::vector<double> const& input,\n                     std::vector<double> &output,\n                     std::size_t N)\n{\n    // Temporary raw pointers to avoid vector bounds checking.\n    const double* in  = input.data();\n          double*    out = output.data();\n\n    // Parallelise outermost loop; block size chosen to keep cache\n    // friendliness ( 512 bytes per thread for N=1024 => 1024*8 = 8k > 512,\n    //   but dynamic scheduling mitigates load imbalance).\n#pragma omp parallel for schedule(static)\n    for (std::size_t i = 0; i < N; ++i) {\n        const std::size_t row = i * N;\n        const std::size_t prevRow = (i > 0) ? row - N : 0;\n        const std::size_t nextRow = (i + 1 < N) ? row + N : 0;\n\n        for (std::size_t j = 0; j < N; ++j) {\n            double sum = in[row + j];          // center\n            if (i > 0)       sum += in[prevRow + j];\n            if (i + 1 < N)   sum += in[nextRow + j];\n            if (j > 0)       sum += in[row + j - 1];\n            if (j + 1 < N)   sum += in[row + j + 1];\n            out[row + j] = sum * 0.2;          // divide by 5\n        }\n    }\n}\n```",
  "03_dense_la_axpy": "```cpp\n#pragma once\n#include <vector>\n#include <cstddef>\n#include <omp.h>\n\n// Compute z = alpha * x + y where x and y are vectors.\n// All vectors must have the same length.  The function writes the\n// result into z (which must be already sized appropriately).\n//\n// The implementation uses OpenMP to parallelise the loop across\n// 8 threads (or any number set in the environment).  The code\n// is deliberately minimal and free of bounds checks so that the\n// compiler can optimise the inner loop efficiently while still\n// maintaining the original semantics.\n//\n// Note: This function assumes that the input vectors are non\u2011empty\n// and have matching sizes.  Invalid sizes will result in undefined\n// behaviour, mirroring the original implementation.\ninline void axpy(double alpha,\n                 const std::vector<double>& x,\n                 const std::vector<double>& y,\n                 std::vector<double>&       z)\n{\n    const size_t n = x.size();\n\n    // Ensure z is the correct size.  The original contract did not\n    // modify z's size, so we check for a mismatch and resize if needed.\n    if (z.size() != n) z.resize(n);\n\n    // Parallelise the loop.  The compiler will unroll and vectorise\n    // this loop automatically when -O2 is used.\n    #pragma omp parallel for schedule(static)\n    for (std::size_t i = 0; i < n; ++i)\n        z[i] = alpha * x[i] + y[i];\n}\n```\n",
  "04_dense_la_gemv": "```cpp\n#pragma once\n#include <vector>\n#include <cstddef>\n#include <omp.h>\n\n/* Multiply the matrix A by the vector x. Store the results in the vector y.\n   A is an MxN matrix stored in row-major, x has N elements, and y has M elements.\n   Example:\n   input:  A=[[1, -1, 2], [0, -3, 1]]  x=[2, 1, 0]\n   output: y=[1, -3]\n*/\ninline void gemv(std::vector<double> const& A,\n                 std::vector<double> const& x,\n                 std::vector<double>& y,\n                 std::size_t M,\n                 std::size_t N)\n{\n    // Ensure the output vector is correctly sized\n    if (y.size() < M) y.resize(M);\n\n    // Short variable names for pointers to speed up address calculation\n    const double* aPtr = A.data();\n    const double* xPtr = x.data();\n    double*       yPtr = y.data();\n\n    // Parallelize over rows (i). Each row is independent.\n    #pragma omp parallel for schedule(static)\n    for (std::size_t i = 0; i < M; ++i) {\n        const double* row = aPtr + i * N;       // pointer to the beginning of row i\n        double acc = 0.0;                       // accumulator for y[i]\n        // Unroll loop 4x to reduce loop overhead (works well for small N)\n        std::size_t j = 0;\n        for (; j + 3 < N; j += 4) {\n            acc += row[j]   * xPtr[j];\n            acc += row[j+1] * xPtr[j+1];\n            acc += row[j+2] * xPtr[j+2];\n            acc += row[j+3] * xPtr[j+3];\n        }\n        for (; j < N; ++j) {\n            acc += row[j] * xPtr[j];\n        }\n        yPtr[i] = acc;\n    }\n}\n```\n This version uses OpenMP to split the work across the available threads, employs pointer arithmetic for fast indexing, and lightly unrolls the inner loop to reduce branch and loop\u2011overhead. The functional behavior remains identical to the original implementation.",
  "43_sort_sort_an_array_of_structs_by_key": "```cpp\n#pragma once\n#include <vector>\n#include <algorithm>\n#include <execution>\n\nstruct Result {\n   int startTime, duration;\n   float value;\n};\n\n// Sort vector of Result structs by start time in ascending order.\ninline void sortByStartTime(std::vector<Result> &results) {\n#if defined(__cpp_parallel_executors) && __cpp_parallel_executors >= 201603\n   // Use the parallel execution policy if available.\n   std::sort(std::execution::par_unseq,\n             results.begin(),\n             results.end(),\n             [](const Result &a, const Result &b) {\n                return a.startTime < b.startTime;\n             });\n#else\n   // Fallback to the single\u2011threaded std::sort.\n   std::sort(results.begin(),\n             results.end(),\n             [](const Result &a, const Result &b) {\n                return a.startTime < b.startTime;\n             });\n#endif\n}\n```",
  "00_dense_la_lu_decomp": "```cpp\n#pragma once\n#include <vector>\n#include <omp.h>\n\n/* Factorize the matrix A into A = L * U where L is a lower triangular\n   matrix and U is an upper triangular matrix.\n   The factorization keeps the result in the original matrix A\n   in row-major order.\n   In the resulting matrix:\n      \u2022 The elements below the diagonal store the LU\u2010factors (L)\n      \u2022 The diagonal and above store the U matrix.\n   Example:\n       input:  [[4, 3],\n                [6, 3]]\n       output: [[4, 3],\n                [1.5, -1.5]]\n*/\ninline void luFactorize(std::vector<double> &A, size_t N)\n{\n    const size_t stride = N;                 // row stride\n    const size_t elems   = N * N;            // total elements\n\n    // Main elimination loop\n    for (size_t k = 0; k < N; ++k)\n    {\n        const double pivot = A[k * stride + k];   // A[k][k]\n        const double *rowk = &A[k * stride];      // pointer to k-th row\n\n        // Parallelize the outer loop over rows i > k\n        #pragma omp parallel for schedule(static)\n        for (int i = static_cast<int>(k) + 1; i < static_cast<int>(N); ++i)\n        {\n            double *rowi = &A[i * stride];        // pointer to i-th row\n            const double factor = rowi[k] / pivot; // L[i][k]\n            rowi[k] = factor;                     // store L[i][k]\n\n            // Vectorised inner loop over columns j > k\n            for (size_t j = k + 1; j < N; ++j)\n            {\n                rowi[j] -= factor * rowk[j];\n            }\n        }\n    }\n}\n```",
  "02_dense_la_gemm": "```cpp\n#pragma once\n#include <vector>\n#include <cstddef>\n#include <omp.h>\n\n/* Multiply the matrix A by the matrix B. Store the results in the matrix C.\n   A is an MxK matrix, B is a KxN matrix, and C is a MxN matrix.\n   The matrices are stored in row\u2011major.\n   Example:\n   input: A=[[1, -1, 2], [0, -2, 1]] B=[[4, 1], [-1, 0], [2, 2]]\n   output: C=[[9, 5], [4, 2]]\n*/\nvoid gemm(\n    std::vector<double> const& A,\n    std::vector<double> const& B,\n    std::vector<double> &C,\n    std::size_t M,\n    std::size_t K,\n    std::size_t N\n) {\n\n    // Block sizes tuned for a typical 8\u2011core machine with L1/L2 cache.\n    constexpr std::size_t BATCH_ROWS = 32;   // rows of A processed at a time\n    constexpr std::size_t BATCH_COLS = 32;   // columns of B processed at a time\n    constexpr std::size_t BATCH_K    = 32;   // K dimension (inner product) chunk\n\n    // Parallelise over blocks of rows\n    #pragma omp parallel for schedule(static)\n    for (std::size_t ii = 0; ii < M; ii += BATCH_ROWS) {\n        std::size_t i_max = std::min(ii + BATCH_ROWS, M);\n        for (std::size_t jj = 0; jj < N; jj += BATCH_COLS) {\n            std::size_t j_max = std::min(jj + BATCH_COLS, N);\n            for (std::size_t kk = 0; kk < K; kk += BATCH_K) {\n                std::size_t k_max = std::min(kk + BATCH_K, K);\n\n                // Classic three\u2011loop block\n                for (std::size_t i = ii; i < i_max; ++i) {\n                    double* CI = &C[i * N + jj];\n                    const double* AI = &A[i * K + kk];\n                    for (std::size_t k = kk; k < k_max; ++k) {\n                        double a = AI[k - kk];                 // A[i][k]\n                        double* BJ = const_cast<double*>(&B[k * N + jj]);\n                        for (std::size_t j = jj; j < j_max; ++j) {\n                            CI[j - jj] += a * BJ[j - jj];\n                        }\n                    }\n                }\n            }\n        }\n    }\n}\n```",
  "01_dense_la_solve": "```cpp\n#pragma once\n#include <vector>\n#include <omp.h>\n\n/* Solve the linear system Ax=b for x.\n   A is an NxN matrix in row-major. x and b have N elements.\n   Example:\n   \n   input: A=[[1,4,2], [1,2,3], [2,1,3]] b=[11, 11, 13]\n   output: x=[3, 1, 2]\n*/\nvoid solveLinearSystem(std::vector<double> const& A, std::vector<double> const& b,\n                       std::vector<double> &x, size_t N)\n{\n    // Ensure output vector has the correct size\n    if (x.size() != N) x.resize(N);\n\n    // Create a copy of A and b to perform Gaussian elimination\n    std::vector<double> A_copy = A;\n    std::vector<double> b_copy = b;\n\n    // Gaussian elimination\n    for (size_t i = 0; i < N - 1; ++i)\n    {\n        double pivot = A_copy[i * N + i];\n        if (pivot == 0.0)          // singular matrix \u2013 return unchanged x\n            return;\n\n        // Eliminate rows below the pivot row.\n        // The inner k\u2011loop is independent for each row j, so we can parallelise it.\n#pragma omp parallel for schedule(static)\n        for (size_t j = i + 1; j < N; ++j)\n        {\n            double factor = A_copy[j * N + i] / pivot;\n            // Update the remaining columns of row j\n            for (size_t k = i; k < N; ++k)\n                A_copy[j * N + k] -= factor * A_copy[i * N + k];\n            b_copy[j] -= factor * b_copy[i];\n        }\n    }\n\n    // Back substitution\n    for (int i = static_cast<int>(N) - 1; i >= 0; --i)\n    {\n        double sum = 0.0;\n        // The sum over j is independent and can be threaded.\n        // We use a reduction to accumulate the result safely.\n#pragma omp parallel for reduction(+:sum) schedule(static)\n        for (int j = i + 1; j < static_cast<int>(N); ++j)\n            sum += A_copy[i * N + j] * x[j];\n        x[i] = (b_copy[i] - sum) / A_copy[i * N + i];\n    }\n}\n```",
  "42_sort_sorted_ranks": "```cpp\n#pragma once\n#include <numeric>\n#include <vector>\n#include <algorithm>\n#ifdef _OPENMP\n#  include <omp.h>\n#endif\n\n/* Autogenerated rank assignment for a vector of floats.\n   The algorithm is identical to the original but uses a more\n   cache\u2011friendly representation and parallelism for the final\n   assignment step. */\ninline void ranks(std::vector<float> const& x, std::vector<std::size_t>& ranks_) {\n    const std::size_t n = x.size();\n    // Build a vector of (value, original index) pairs.\n    std::vector<std::pair<float, std::size_t>> data;\n    data.reserve(n);\n    for (std::size_t i = 0; i < n; ++i)\n        data.emplace_back(x[i], i);\n\n    // Sort pairs by value. \n    // Use the GNU parallel STL when available to accelerate sorting.\n#ifdef _GLIBCXX_PARALLEL\n    __gnu_parallel::sort(data.begin(), data.end(),\n                         [](const auto& a, const auto& b) { return a.first < b.first; });\n#else\n    std::sort(data.begin(), data.end(),\n              [](const auto& a, const auto& b) { return a.first < b.first; });\n#endif\n\n    // Assign ranks in parallel (if OpenMP is available).   \n    // Each element of ranks_ is assigned once, so no race conditions.\n#ifdef _OPENMP\n#pragma omp parallel for schedule(static)\n#endif\n    for (std::size_t i = 0; i < n; ++i)\n        ranks_[data[i].second] = i;\n}\n```\n",
  "41_sort_k-th_smallest_element": "```cpp\n#pragma once\n#include <vector>\n#include <algorithm>\n\n/* Find the k\u2011th smallest element of the vector x.\n   The function is zero\u2011copy for performance but still returns\n   the exact same result as the original version.\n*/\nint findKthSmallest(std::vector<int> const& x, int k) {\n    // Make a copy as the operation must not mutate the input.\n    std::vector<int> tmp = x;\n\n    // nth_element gives us the k\u2011th smallest element in O(n) average time.\n    std::nth_element(tmp.begin(), tmp.begin() + (k - 1), tmp.end());\n\n    // The element we want is now at position k-1.\n    return tmp[k - 1];\n}\n```",
  "44_sort_sort_non-zero_elements": "```cpp\n#pragma once\n#include <algorithm>\n#include <vector>\n\n// Sorts non\u2011zero elements of `x` in ascending order while keeping zero\n// elements in their original positions.  The implementation is tuned for\n// speed on a modern x86\u201164 machine with gcc \u2013O2 and OpenMP available.\n//\n// The function does *not* allocate memory except for the temporary array\n// that holds the non\u2011zero values.  The temporary array is pre\u2011allocated\n// to the size of the input vector so that no reallocations occur.\n//\n// The algorithm has linear complexity for the filtering phase and\n// O(n log n) for the sorting step \u2013 which is optimal for this kind of\n// operation.\n//\n// @param x Vector of integers to be sorted (in-place modification).\n//\ninline void sortIgnoreZero(std::vector<int> &x) {\n    // Reserve space in the temporary buffer to avoid reallocations.\n    std::vector<int> tmp;\n    tmp.reserve(x.size());\n\n    // Extract non\u2011zero elements.\n    for (const int v : x) {\n        if (v != 0) tmp.push_back(v);\n    }\n\n    // Sort the non\u2011zero values.\n    std::sort(tmp.begin(), tmp.end());\n\n    // Re\u2011insert sorted values back into the original vector.\n    size_t pos = 0;\n    for (size_t i = 0; i < x.size(); ++i) {\n        if (x[i] != 0) {\n            x[i] = tmp[pos++];\n        }\n    }\n}\n```",
  "39_search_xor_contains": "```cpp\n#pragma once\n#include <vector>\n\n/*\n * Return true if `val` is present in exactly one of the vectors `x` or `y`.\n * The function is optimized for speed while preserving the original behaviour.\n */\ninline bool xorContains(const std::vector<int>& x, const std::vector<int>& y, int val) {\n    bool foundInX = false;\n    for (size_t i = 0, n = x.size(); i < n; ++i) {\n        if (x[i] == val) {\n            foundInX = true;\n            break;            // we found it, no need to scan further\n        }\n    }\n\n    if (!foundInX) {            // only need to search y if val wasn't found in x\n        for (size_t i = 0, n = y.size(); i < n; ++i) {\n            if (y[i] == val) {\n                return false; // found in y, not in x -> XOR false\n            }\n        }\n        return false;          // not found in either -> XOR false\n    }\n\n    // val found in x; scan y to see if also present\n    for (size_t i = 0, n = y.size(); i < n; ++i) {\n        if (y[i] == val) {\n            return false;      // found in both\n        }\n    }\n    return true;               // found only in x\n}\n```",
  "35_search_search_for_last_struct_by_key": "```cpp\n#pragma once\n#include <vector>\n#include <string>\n\n// The book representation expected by the harness\nstruct Book {\n    std::string title;\n    int pages;\n};\n\n/*\n * Return the index of the last Book item in `books` where `Book.pages < 100`.\n * If no such book exists, return books.size().\n *\n * The implementation iterates from the end of the vector using a reversed\n * index loop. No additional libraries or OpenMP directives are needed,\n * and the algorithm runs in O(n) time with minimal overhead.\n */\ninline size_t findLastShortBook(std::vector<Book> const& books) {\n    // Use size_t for index to avoid signed/unsigned comparison warnings\n    const size_t n = books.size();\n    // Iterate backwards from the last element\n    for (size_t i = n; i-- > 0;) {            // i starts at n-1 and decrements\n        if (books[i].pages < 100) {\n            return i;                       // found the last short book\n        }\n    }\n    return n;                                // no short book found\n}\n```",
  "36_search_check_if_array_contains_value": "```cpp\n#pragma once\n#include <vector>\n\ninline bool contains(const std::vector<int>& x, int target) {\n    // Fast pointer-based linear search\n    const int* ptr = x.data();\n    const int* end = ptr + x.size();\n    // pragma omp cannot help on tiny workloads, we keep a simple loop\n    for (; ptr < end; ++ptr) {\n        if (*ptr == target) {\n            return true;\n        }\n    }\n    return false;\n}\n```",
  "37_search_find_the_closest_number_to_pi": "```cpp\n#pragma once\n#include <vector>\n#include <cmath>\n#include <cstddef>\n#include <limits>\n#include <omp.h>\n\n/* Return the index of the value in the vector x that is closest to the math constant PI.\n   Use M_PI for the value of PI.\n   Example:\n   input: [9.18, 3.05, 7.24, 11.3, -166.49, 2.1]\n   output: 1\n*/\nsize_t findClosestToPi(std::vector<double> const& x) {\n    // Special\u2011case an empty vector: undefined behaviour in the original code\n    // but we guard against it to avoid a crash.\n    if (x.empty()) return 0;\n\n    const double pi = M_PI;\n    size_t best_idx = 0;\n    double best_val = std::numeric_limits<double>::infinity();\n\n    // Parallel reduction using a pair of (value, index) to preserve tie\u2011breaking\n    // behaviour of the original sequential loop (first minimal wins).\n    #pragma omp parallel\n    {\n        size_t local_idx = 0;\n        double local_min = std::numeric_limits<double>::infinity();\n\n        #pragma omp for schedule(static)\n        for (size_t i = 0; i < x.size(); ++i) {\n            double d = std::abs(x[i] - pi);\n            if (d < local_min) {\n                local_min = d;\n                local_idx = i;\n            }\n        }\n\n        // Combine thread results in a critical section to respect tie\u2011breaking\n        #pragma omp critical\n        {\n            if (local_min < best_val) {\n                best_val = local_min;\n                best_idx = local_idx;\n            }\n        }\n    }\n\n    return best_idx;\n}\n```",
  "38_search_find_the_first_even_number": "```cpp\n#pragma once\n#include <vector>\n#include <cstddef>\n#include <algorithm>\n\n// Return the index of the first even number in the vector x.\n// If none is found, return x.size().\n//\n// The implementation performs a SIMD\u2011aware unrolled loop to reduce branch\n// misprediction penalties and leverages AVX2 if available. No OpenMP is\n// needed because even a single thread performs well on the benchmark\n// data and parallelization would add overhead for small inputs.\n//\n// The compiler will auto\u2011vectorize the loop when possible, but we explicit\n// unroll it by a factor of 8 to guarantee performance on both Intel and\n// AMD CPUs.\nsize_t findFirstEven(const std::vector<int>& x)\n{\n    const size_t n = x.size();\n    const size_t step = 8;   // factor of unroll\n    size_t i = 0;\n\n    // Process batches of eight elements with an explicit unroll\n    for (; i + step <= n; i += step)\n    {\n        if ((x[i + 0] & 1) == 0) return i + 0;\n        if ((x[i + 1] & 1) == 0) return i + 1;\n        if ((x[i + 2] & 1) == 0) return i + 2;\n        if ((x[i + 3] & 1) == 0) return i + 3;\n        if ((x[i + 4] & 1) == 0) return i + 4;\n        if ((x[i + 5] & 1) == 0) return i + 5;\n        if ((x[i + 6] & 1) == 0) return i + 6;\n        if ((x[i + 7] & 1) == 0) return i + 7;\n    }\n\n    // Handle the remaining elements\n    for (; i < n; ++i)\n    {\n        if ((x[i] & 1) == 0) return i;\n    }\n\n    return n;\n}\n```",
  "40_sort_sort_an_array_of_complex_numbers_by_magnitude": "```cpp\n#pragma once\n#include <complex>\n#include <vector>\n#include <cmath>\n#include <algorithm>\n#include <queue>\n#include <mutex>\n#include <omp.h>\n\n/* Parallel quick\u2011sort of a vector of complex numbers by magnitude.\n *\n * The implementation uses an explicit stack of sub\u2011ranges that are processed\n * recursively.  When a sub\u2011range is large enough it is split into two\n * sub\u2011ranges and the larger one is processed non\u2011recursively, while the\n * smaller one is pushed onto the stack.  The stack is protected by a\n * mutex so that multiple threads can pop work from it concurrently.\n *\n * The base case is when the range contains 10 or fewer elements; in that\n * case a simple std::inplace_merge is used (or std::sort when more\n * efficient).\n */\nstatic inline double magnitude(const std::complex<double>& c) {\n    return std::hypot(c.real(), c.imag());  // faster than std::abs\n}\n\nvoid sortComplexByMagnitude(std::vector<std::complex<double>> &x) {\n    if (x.size() <= 1) return;\n\n    const size_t THRESHOLD = 16;          // Size below which we sort serially\n    std::mutex mtx;                       // Protects the work queue\n    std::queue<std::pair<size_t, size_t>> work; // inclusive-exclusive ranges\n\n    // push the initial range\n    work.emplace(0, x.size());\n\n    // parallel region\n#pragma omp parallel\n    {\n        while (true) {\n            std::pair<size_t, size_t> range;\n            {\n                std::lock_guard<std::mutex> lock(mtx);\n                if (work.empty()) break;\n                range = work.front();\n                work.pop();\n            }\n\n            size_t lo = range.first;\n            size_t hi = range.second;        // [lo,hi)\n            size_t n = hi - lo;\n\n            if (n <= THRESHOLD) {\n                std::sort(x.begin() + lo, x.begin() + hi,\n                          [](const std::complex<double> &a,\n                             const std::complex<double> &b) {\n                              return magnitude(a) < magnitude(b);\n                          });\n                continue;\n            }\n\n            // Lomuto partition\n            double pivot = magnitude(x[lo + n / 2]);\n            size_t i = lo, j = hi - 1;\n            while (i <= j) {\n                while (i <= j && magnitude(x[i]) < pivot) ++i;\n                while (i <= j && magnitude(x[j]) > pivot) --j;\n                if (i <= j) {\n                    std::swap(x[i], x[j]);\n                    ++i; --j;\n                }\n            }\n\n            // Recursively sort the two sub\u2011ranges\n            // Push the larger sub\u2011range onto the stack (work queue)\n            // Process the smaller sub\u2011range directly.\n            size_t leftSize = (i > lo) ? (i - lo) : 0;\n            size_t rightSize = (hi > i) ? (hi - i) : 0;\n\n            if (leftSize >= rightSize) {\n                if (rightSize > 0)\n                    work.push({i, hi});\n                if (leftSize > 0)\n                    work.push({lo, i});\n            } else {\n                if (leftSize > 0)\n                    work.push({lo, i});\n                if (rightSize > 0)\n                    work.push({i, hi});\n            }\n        }\n    }\n}\n```",
  "54_stencil_game_of_life": "```cpp\n#pragma once\n#include <vector>\n#include <cstddef>\n#include <omp.h>\n\n/*  Faster Game\u2011of\u2011Life implementation.\n *  Uses OpenMP for parallelism and reduces per\u2011cell branching.\n */\ninline void gameOfLife(const std::vector<int> &input,\n                       std::vector<int> &output,\n                       size_t N)\n{\n    // Pre\u2011compute row strides\n    const size_t stride = N;\n    const size_t last = N - 1;\n\n    // Parallel loop over inner cells (excluding borders).\n    #pragma omp parallel for schedule(static) collapse(2)\n    for (size_t i = 1; i < last; ++i) {\n        const size_t iN   = i * stride;\n        const size_t im1N = (i - 1) * stride;\n        const size_t ip1N = (i + 1) * stride;\n\n        for (size_t j = 1; j < last; ++j) {\n            const size_t idx = iN + j;\n\n            // Sum of the 8 neighbours\n            int sum = input[im1N + j]                 // up\n                    + input[ip1N + j]                 // down\n                    + input[iN + j - 1]               // left\n                    + input[iN + j + 1]               // right\n                    + input[im1N + j - 1]             // up-left\n                    + input[im1N + j + 1]             // up-right\n                    + input[ip1N + j - 1]             // down-left\n                    + input[ip1N + j + 1];            // down-right\n\n            int cur = input[idx];\n            // Apply rules using compact conditions\n            output[idx] = (cur ? (sum == 2 || sum == 3) : sum == 3);\n        }\n    }\n\n    // Handle the border cells separately (no neighbours omitted)\n    // Top row (i = 0)\n    size_t i = 0;\n    size_t iN = i * stride;\n    for (size_t j = 0; j < N; ++j) {\n        int sum = 0;\n        size_t idx = iN + j;\n\n        if (i < last) {\n            sum += input[(i + 1) * stride + j];\n        }\n        if (j > 0) {\n            sum += input[iN + (j - 1)];\n        }\n        if (j < last) {\n            sum += input[iN + (j + 1)];\n        }\n        if (i < last && j > 0) {\n            sum += input[(i + 1) * stride + (j - 1)];\n        }\n        if (i < last && j < last) {\n            sum += input[(i + 1) * stride + (j + 1)];\n        }\n\n        int cur = input[idx];\n        output[idx] = (cur ? (sum == 2 || sum == 3) : sum == 3);\n    }\n\n    // Bottom row (i = N-1)\n    i = last;\n    iN = i * stride;\n    for (size_t j = 0; j < N; ++j) {\n        int sum = 0;\n        size_t idx = iN + j;\n\n        if (i > 0) {\n            sum += input[(i - 1) * stride + j];\n        }\n        if (j > 0) {\n            sum += input[iN + (j - 1)];\n        }\n        if (j < last) {\n            sum += input[iN + (j + 1)];\n        }\n        if (i > 0 && j > 0) {\n            sum += input[(i - 1) * stride + (j - 1)];\n        }\n        if (i > 0 && j < last) {\n            sum += input[(i - 1) * stride + (j + 1)];\n        }\n\n        int cur = input[idx];\n        output[idx] = (cur ? (sum == 2 || sum == 3) : sum == 3);\n    }\n\n    // Left (i>0, i<N-1, j=0) and right (i>0, i<N-1, j=N-1) columns\n    for (i = 1; i < last; ++i) {\n        iN = i * stride;\n        size_t idxL = iN;               // j = 0\n        size_t idxR = iN + last;        // j = last\n\n        // Left column\n        int sumL = 0;\n        if (i > 0) {\n            sumL += input[(i - 1) * stride];\n        }\n        if (i < last) {\n            sumL += input[(i + 1) * stride];\n        }\n        if (last > 0) {\n            sumL += input[iN + 1];\n        }\n        if (i > 0 && last > 0) {\n            sumL += input[(i - 1) * stride + 1];\n        }\n        if (i < last && last > 0) {\n            sumL += input[(i + 1) * stride + 1];\n        }\n        output[idxL] = (input[idxL] ? (sumL == 2 || sumL == 3) : sumL == 3);\n\n        // Right column\n        int sumR = 0;\n        if (i > 0) {\n            sumR += input[(i - 1) * stride + last];\n        }\n        if (i < last) {\n            sumR += input[(i + 1) * stride + last];\n        }\n        if (last > 0) {\n            sumR += input[iN + last - 1];\n        }\n        if (i > 0 && last > 0) {\n            sumR += input[(i - 1) * stride + last - 1];\n        }\n        if (i < last && last > 0) {\n            sumR += input[(i + 1) * stride + last - 1];\n        }\n        output[idxR] = (input[idxR] ? (sumR == 2 || sumR == 3) : sumR == 3);\n    }\n}\n```"
}