{
  "23_histogram_first_letter_counts": "```cpp\n#pragma once\n#include <array>\n#include <string>\n#include <vector>\n#include <omp.h>   // OpenMP support\n\n// Count the number of strings in `s` that start with each lowercase\n// letter and add the result to the existing `bins` array.\n// The function preserves the exact behaviour of the original serial implementation.\ninline void firstLetterCounts(std::vector<std::string> const& s, std::array<size_t, 26> &bins)\n{\n    const size_t n = s.size();\n\n    // Parallel region: each thread gathers local counts, then merges into `bins`.\n    #pragma omp parallel\n    {\n        // Local histogram for each thread\n        std::array<size_t, 26> local{};  // zero\u2011initialised\n\n        // Distribute the work among threads\n        #pragma omp for schedule(static)\n        for (size_t i = 0; i < n; ++i)\n        {\n            const char c = s[i][0];          // first character (no bounds checking)\n            const int  index = c - 'a';      // alphabet index 0..25\n            local[index] += 1;               // atomic local update\n        }\n\n        // Merge local histograms into the global `bins` array\n        #pragma omp critical\n        {\n            for (int k = 0; k < 26; ++k)\n                bins[k] += local[k];\n        }\n    }\n}\n```",
  "15_graph_edge_count": "```cpp\n#pragma once\n#include <vector>\n#include <queue>\n#include <stack>\n#include <deque>\n#include <limits>\n#include <cstddef>\n\nint edgeCount(std::vector<int> const& A, size_t N) {\n    const int* __restrict data = A.data();\n    const size_t total = N * N;\n    int count = 0;\n    #pragma omp parallel for reduction(+:count) schedule(static)\n    for (size_t idx = 0; idx < total; ++idx) {\n        if (data[idx] == 1) ++count;\n    }\n    return count;\n}\n```",
  "20_histogram_pixel_histogram": "```cpp\n#pragma once\n#include <vector>\n#include <array>\n#include <omp.h>\n\ninline void pixelCounts(const std::vector<int>& image, std::array<size_t, 256>& bins)\n{\n    const size_t n = image.size();\n    if (n == 0) return;\n\n    #pragma omp parallel\n    {\n        size_t local[256] = {0};\n\n        #pragma omp for schedule(static)\n        for (size_t i = 0; i < n; ++i) {\n            // The original code assumes the value is in [0,255].\n            local[image[i]] += 1;\n        }\n\n        #pragma omp critical\n        {\n            for (int b = 0; b < 256; ++b) {\n                bins[b] += local[b];\n            }\n        }\n    }\n}\n```",
  "59_transform_map_function": "```cpp\n#pragma once\n\n#include <vector>\n#include <cstddef> // for std::size_t\n\n// Determine whether a 32\u2011bit signed integer is a power of two.\n// Inlined for zero overhead; noexcept guarantees no exception.\ninline bool isPowerOfTwo(int x) noexcept\n{\n    return (x > 0) && !(x & (x - 1));\n}\n\n/*\n * Apply the isPowerOfTwo function to every element of `x` and store the\n * resulting boolean values in `mask`.  The function preserves the exact\n * semantics of the original implementation while adding OpenMP parallelism\n * for large inputs.\n *\n * Example:\n *\n *   input:  [8, 0, 9, 7, 15, 64, 3]\n *   output: [true, false, false, false, false, true, false]\n */\ninline void mapPowersOfTwo(const std::vector<int> &x, std::vector<bool> &mask)\n{\n    const std::size_t N = x.size();\n\n    // Threshold below which parallel overhead outweighs the benefit.\n    constexpr std::size_t PARALLEL_THRESHOLD = 64;\n\n    if (N > PARALLEL_THRESHOLD)\n    {\n        // Parallel for-loop \u2013 OpenMP guarantees no data races because each\n        // iteration writes to a distinct element of `mask`.\n        #pragma omp parallel for schedule(static)\n        for (std::size_t i = 0; i < N; ++i)\n        {\n            mask[i] = isPowerOfTwo(x[i]);\n        }\n    }\n    else\n    {\n        // Serial fallback for small inputs.\n        for (std::size_t i = 0; i < N; ++i)\n        {\n            mask[i] = isPowerOfTwo(x[i]);\n        }\n    }\n}\n```",
  "34_scan_largest_contiguous_subarray_sum": "```cpp\n#pragma once\n#include <vector>\n#include <limits>\n\nint maximumSubarray(const std::vector<int> &x)\n{\n    int best = std::numeric_limits<int>::lowest();\n    int current = 0;\n    const int *p = x.data();\n    const int *end = p + x.size();\n    for (; p != end; ++p)\n    {\n        current += *p;\n        if (current > best) best = current;\n        if (current < 0) current = 0;\n    }\n    return best;\n}\n```",
  "17_graph_highest_degree": "```cpp\n#pragma once\n#include <vector>\n#include <algorithm>\n#include <limits>\n#include <omp.h>\n#include <cstddef>\n\n/* Compute the highest node degree in the undirected graph. The graph is defined in the adjacency matrix A.\n   A is an NxN adjacency matrix stored in row\u2011major. A is undirected.\n   Example:\n      input: [[0, 0, 0, 1], [0, 0, 0, 1], [0, 0, 0, 1], [1, 1, 1, 0]]\n   output: 3\n*/\nint maxDegree(std::vector<int> const& A, size_t N) {\n    int maxDeg = 0;\n    if (N == 0) return 0;   // No nodes \u2192 degree 0\n\n    /* Parallel over rows, each thread keeps its own local maximum which are combined with a max reduction. */\n    #pragma omp parallel for schedule(static) reduction(max:maxDeg)\n    for (int i = 0; i < static_cast<int>(N); ++i) {\n        const int* row = A.data() + i * N;\n        int sum = 0;\n        int j = 0;\n        int limit = static_cast<int>(N);\n\n        /* Unroll the inner loop for better vectorisation and branch reduction. */\n        for (; j + 3 < limit; j += 4) {\n            sum += row[j] + row[j + 1] + row[j + 2] + row[j + 3];\n        }\n        for (; j < limit; ++j) {\n            sum += row[j];\n        }\n\n        /* Update local maximum. */\n        if (sum > maxDeg) {\n            maxDeg = sum;\n        }\n    }\n    return maxDeg;\n}\n```\n",
  "57_transform_inverse_offset": "```cpp\n#pragma once\n#include <vector>\n\n#ifdef _OPENMP\n#include <omp.h>\n#endif\n\n/**\n * Replace every element of the vector x with 1-1/x.\n *\n * Example:\n * input:  [2, 4, 1, 12, -2]\n * output: [0.5, 0.75, 0, 0.91666666, 1.5]\n */\ninline void oneMinusInverse(std::vector<double> &x)\n{\n    const std::size_t n = x.size();\n    if (n == 0) return;\n\n#ifdef _OPENMP\n#pragma omp parallel for schedule(static)\n#endif\n    for (std::size_t i = 0; i < n; ++i)\n    {\n        const double v = x[i];\n        x[i] = 1.0 - 1.0 / v;\n    }\n}\n```\n",
  "18_graph_count_components": "```cpp\n#pragma once\n#include <vector>\n#include <cstddef>\n\ninline int componentCount(std::vector<int> const& A, size_t N)\n{\n    // Union\u2013Find data structures\n    std::vector<int> parent(N);\n    std::vector<unsigned char> rank(N, 0);\n    for (size_t i = 0; i < N; ++i) parent[i] = static_cast<int>(i);\n\n    // Find with path compression (half\u2011compression)\n    auto find_root = [&](int v) -> int {\n        while (parent[v] != v) {\n            parent[v] = parent[parent[v]];\n            v = parent[v];\n        }\n        return v;\n    };\n\n    // Union every edge (process only the upper triangular part)\n    for (size_t i = 0; i < N; ++i) {\n        const int* row = &A[i * N];\n        for (size_t j = i + 1; j < N; ++j) {\n            if (row[j] == 1) {\n                int ri = find_root(static_cast<int>(i));\n                int rj = find_root(static_cast<int>(j));\n                if (ri != rj) {\n                    if (rank[ri] < rank[rj]) {\n                        parent[ri] = rj;\n                    } else if (rank[ri] > rank[rj]) {\n                        parent[rj] = ri;\n                    } else {\n                        parent[rj] = ri;\n                        ++rank[ri];\n                    }\n                }\n            }\n        }\n    }\n\n    // Final path compression\n    for (size_t i = 0; i < N; ++i) {\n        parent[i] = find_root(static_cast<int>(i));\n    }\n\n    // Count unique roots\n    std::vector<unsigned char> seen(N, 0);\n    int compCount = 0;\n    for (size_t i = 0; i < N; ++i) {\n        int r = parent[i];\n        if (!seen[r]) {\n            seen[r] = 1;\n            ++compCount;\n        }\n    }\n    return compCount;\n}\n```",
  "56_transform_negate_odds": "```cpp\n#pragma once\n\n#include <vector>\n#ifdef _OPENMP\n#include <omp.h>\n#endif\n\n// In the vector x negate the odd values and divide the even values by 2.\n// Example:\n// input:  [16, 11, 12, 14, 1, 0, 5]\n// output: [8, -11, 6, 7, -1, 0, -5]\ninline void negateOddsAndHalveEvens(std::vector<int> &x)\n{\n    const size_t n = x.size();\n    #pragma omp parallel for schedule(static) if(n > 1024)\n    for (size_t i = 0; i < n; ++i)\n    {\n        const int v = x[i];\n        if ((v & 1) == 0)\n            x[i] = v >> 1;        // divide even values by 2 (arithmetic shift)\n        else\n            x[i] = -v;            // negate odd values\n    }\n}\n```",
  "32_scan_sum_of_prefix_sum_array": "```cpp\n#pragma once\n#include <cstdint>\n#include <vector>\n#include <cstddef>\n\ninline int64_t sumOfPrefixSum(const std::vector<int64_t>& x) {\n    const std::size_t n = x.size();\n    if (n == 0) return 0;\n\n    int64_t result = 0;\n\n    /* Use parallel reduction only for larger inputs to avoid\n       the cost of creating a thread team for tiny vectors. */\n    if (n <= 1024) {\n        for (std::size_t i = 0; i < n; ++i) {\n            result += x[i] * static_cast<int64_t>(n - i);\n        }\n    } else {\n# pragma omp parallel for reduction(+:result)\n        for (std::size_t i = 0; i < n; ++i) {\n            result += x[i] * static_cast<int64_t>(n - i);\n        }\n    }\n\n    return result;\n}\n```\n",
  "58_transform_squaring": "```cpp\n#pragma once\n#include <vector>\n#include <cstddef>\n\ninline void squareEach(std::vector<int> &x)\n{\n    const std::size_t n = x.size();\n    if (n == 0) return;\n\n    constexpr std::size_t threshold = 1 << 12;          // 4096 elems\n    int* data = x.data();                               // raw pointer access\n\n    #pragma omp parallel for schedule(static) simd if (n > threshold)\n    for (std::size_t i = 0; i < n; ++i) {\n        const int v = data[i];\n        data[i] = v * v;                                 // in\u2011place square\n    }\n}\n```\n",
  "27_reduce_average": "```c++\n// Replaced implementation of average using OpenMP parallel reduction\n#pragma once\n#include <vector>\n#include <cstddef>\n\ninline double average(std::vector<double> const& x) {\n    const std::size_t n = x.size();\n    double sum = 0.0;\n    const double* p = x.data();\n\n    #pragma omp parallel for reduction(+:sum) schedule(static)\n    for (std::size_t i = 0; i < n; ++i) {\n        sum += p[i];\n    }\n\n    return sum / static_cast<double>(n);\n}\n```\n",
  "16_graph_largest_component": "```cpp\n#pragma once\n#include <vector>\n\nint largestComponent(std::vector<int> const& A, size_t N) {\n    const int* data = A.data();\n    std::vector<char> visited(N, 0);\n    std::vector<int> stack;\n    stack.reserve(64);\n\n    int maxSize = 0;\n    for (size_t i = 0; i < N; ++i) {\n        if (!visited[i]) {\n            int compSize = 0;\n            stack.clear();\n            stack.push_back(static_cast<int>(i));\n            visited[i] = 1;\n\n            while (!stack.empty()) {\n                int node = stack.back();\n                stack.pop_back();\n                ++compSize;\n\n                const int* row = data + node * N;\n                for (int j = 0; j < static_cast<int>(N); ++j) {\n                    if (row[j] && !visited[j]) {\n                        visited[j] = 1;\n                        stack.push_back(j);\n                    }\n                }\n            }\n            if (compSize > maxSize) maxSize = compSize;\n        }\n    }\n    return maxSize;\n}\n```",
  "46_sparse_la_spmm": "```cpp\n#pragma once\n#include <vector>\n#include <omp.h>\n\nstruct COOElement {\n   size_t row;\n   size_t column;\n   double value;\n};\n\nvoid spmm(std::vector<COOElement> const& A,\n          std::vector<COOElement> const& X,\n          std::vector<double> &Y, size_t M, size_t K, size_t N) {\n    // Zero\u2011initialize the dense result matrix\n    Y.assign(M * N, 0.0);\n\n    /* Build a row\u2011major index for X so we can look up the non\u2011zeros\n       in X that match a given column of A in O(1) time.          */\n    std::vector<std::vector<COOElement>> rowsX(K);\n    for (auto const& x : X)\n        rowsX[x.row].push_back(x);\n\n    /* Build a row\u2011major index for A. Each thread will work on a\n       disjoint set of rows, so there are no write conflicts. */\n    std::vector<std::vector<COOElement>> rowsA(M);\n    for (auto const& a : A)\n        rowsA[a.row].push_back(a);\n\n    double* Ydata = Y.data();\n\n#pragma omp parallel for schedule(static)\n    for (int r = 0; r < static_cast<int>(M); ++r) {\n        const auto& aRow = rowsA[r];\n        if (aRow.empty()) continue;                 // No contribution to this row\n        double* rowY = Ydata + r * N;\n        for (const auto& a : aRow) {\n            const auto& xRow = rowsX[a.column];\n            if (xRow.empty()) continue;              // No matching X entries\n            double valA = a.value;\n            for (const auto& x : xRow) {\n                rowY[x.column] += valA * x.value;   // Accumulate product\n            }\n        }\n    }\n}\n```",
  "24_histogram_count_quartile": "```cpp\n#pragma once\n#include <array>\n#include <vector>\n\ninline void countQuartiles(const std::vector<double>& x, std::array<size_t, 4>& bins) {\n    const std::size_t N = x.size();\n\n    #pragma omp parallel\n    {\n        size_t local[4] = {};            // thread\u2011local counters\n\n        #pragma omp for\n        for (std::size_t i = 0; i < N; ++i) {\n            const double val  = x[i];\n            const double frac = val - static_cast<int>(val);\n\n            if (frac < 0.0) {\n                ++local[0];\n            } else {\n                const size_t idx = static_cast<size_t>(frac * 4.0);\n                ++local[idx];\n            }\n        }\n\n        #pragma omp critical\n        {\n            bins[0] += local[0];\n            bins[1] += local[1];\n            bins[2] += local[2];\n            bins[3] += local[3];\n        }\n    }\n}\n```",
  "21_histogram_bin_0-100": "```cpp\n#pragma once\n#include <array>\n#include <vector>\n#include <omp.h>\n\ninline void binsBy10Count(std::vector<double> const& x,\n                          std::array<size_t, 10> &bins) {\n    const size_t N = x.size();\n    if (N == 0) return;\n\n    /* Per\u2011thread histograms. The vector is over\u2011allocated to the maximum\n       number of threads that may be used, so that each thread can safely\n       index its local array via its thread id. */\n    std::vector<std::array<size_t,10>> local(omp_get_max_threads());\n\n    int used_threads = 0;\n\n    #pragma omp parallel\n    {\n        const int tid = omp_get_thread_num();\n        const int stride = omp_get_num_threads();\n        auto& local_bins = local[tid];\n        local_bins.fill(0);\n\n        for (size_t i = static_cast<size_t>(tid); i < N; i += stride) {\n            const size_t bin = static_cast<size_t>(x[i] / 10.0);\n            ++local_bins[bin];\n        }\n\n        /* Master thread records how many threads were actually used. */\n        if (tid == 0) {\n            used_threads = stride;\n        }\n    }\n\n    /* Merge the per\u2011thread histograms into the shared bins array. */\n    for (int t = 0; t < used_threads; ++t) {\n        for (size_t j = 0; j < 10; ++j) {\n            bins[j] += local[t][j];\n        }\n    }\n}\n```",
  "55_transform_relu": "```cpp\n#pragma once\n#include <vector>\n#include <omp.h>\n\ninline void relu(std::vector<double> &x)\n{\n    const std::size_t n = x.size();\n    if (n == 0)\n        return;\n\n    double* ptr = x.data();\n\n    #pragma omp parallel for schedule(static)\n    for (std::size_t i = 0; i < n; ++i)\n    {\n        const double v = ptr[i];\n        ptr[i] = v > 0.0 ? v : 0.0;\n    }\n}\n```",
  "29_reduce_sum_of_min_of_pairs": "```cpp\n#pragma once\n#include <vector>\n#include <cstddef>\n#include <cstdint>\n#include <omp.h>\n\n/* Return the sum of the minimum value at each index of vectors x and y for all indices.\n   i.e. sum = min(x_0, y_0) + min(x_1, y_1) + min(x_2, y_2) + ...\n   Example:\n   input: x=[3, 4, 0, 2, 3], y=[2, 5, 3, 1, 7]\n   output: 10\n*/\nint64_t sumOfMinimumElements(const std::vector<int64_t>& x, const std::vector<int64_t>& y) {\n    const size_t n = x.size();\n    // Assume vectors are the same length as per original specification.\n    if (n == 0) return 0;\n\n    const int64_t * __restrict xi = x.data();\n    const int64_t * __restrict yi = y.data();\n    int64_t sum = 0;\n\n    #pragma omp parallel for reduction(+:sum) schedule(static)\n    for (size_t i = 0; i < n; ++i) {\n        sum += (xi[i] < yi[i]) ? xi[i] : yi[i];\n    }\n\n    return sum;\n}\n```\n",
  "48_sparse_la_sparse_axpy": "```cpp\n#pragma once\n#include <vector>\n\nstruct Element {\n    size_t index;\n    double value;\n};\n\ninline void sparseAxpy(double alpha,\n                       const std::vector<Element>& x,\n                       const std::vector<Element>& y,\n                       std::vector<double>& z)\n{\n    // Accumulate contributions from vector x\n    #pragma omp parallel for schedule(static)\n    for (size_t i = 0; i < x.size(); ++i) {\n        const Element& e = x[i];\n        double val = alpha * e.value;\n        #pragma omp atomic\n        z[e.index] += val;\n    }\n\n    // Accumulate contributions from vector y\n    #pragma omp parallel for schedule(static)\n    for (size_t i = 0; i < y.size(); ++i) {\n        const Element& e = y[i];\n        #pragma omp atomic\n        z[e.index] += e.value;\n    }\n}\n```\n",
  "25_reduce_xor": "```cpp\n#include <vector>\n#include <omp.h>\n\ninline bool reduceLogicalXOR(const std::vector<bool>& x) {\n    const std::size_t n = x.size();\n    if (n == 0) return false;\n\n    bool res = false;\n    const std::size_t threshold = 1024;  // Threshold to avoid parallel overhead on tiny vectors\n\n    if (n < threshold) {\n        for (std::size_t i = 0; i < n; ++i)\n            res ^= x[i];\n    } else {\n#ifdef _OPENMP\n#pragma omp parallel for reduction(^: res)\n        for (std::ptrdiff_t i = 0; i < static_cast<std::ptrdiff_t>(n); ++i)\n            res ^= x[static_cast<std::size_t>(i)];\n#else\n        for (std::size_t i = 0; i < n; ++i)\n            res ^= x[i];\n#endif\n    }\n    return res;\n}\n```",
  "30_scan_prefix_sum": "```cpp\n#pragma once\n#include <numeric>\n#include <vector>\n#include <cstdint>\n#include <omp.h>\n\nvoid prefixSum(std::vector<int64_t> const& x, std::vector<int64_t> &output) {\n    const size_t n = x.size();\n    if (n == 0) return;\n\n    // Use a pure sequential scan for very small inputs\n    constexpr size_t SEQLIMIT = 8192;   // adjust as needed\n    if (n <= SEQLIMIT) {\n        std::inclusive_scan(x.begin(), x.end(), output.begin());\n        return;\n    }\n\n    const int max_threads = omp_get_max_threads();\n    std::vector<int64_t> block_sums(max_threads, 0);\n\n    #pragma omp parallel\n    {\n        int tid = omp_get_thread_num();\n        int nthreads = omp_get_num_threads();\n\n        size_t size = n;\n        size_t chunk = (size + nthreads - 1) / nthreads;\n        size_t start = tid * chunk;\n        size_t end   = start + chunk;\n        if (end > size) end = size;\n\n        int64_t sum = 0;\n        for (size_t i = start; i < end; ++i) {\n            sum += x[i];\n            output[i] = sum;\n        }\n        block_sums[tid] = sum;\n\n        #pragma omp barrier\n\n        int64_t offset = 0;\n        for (int i = 0; i < tid; ++i)\n            offset += block_sums[i];\n\n        if (offset != 0) {\n            for (size_t i = start; i < end; ++i)\n                output[i] += offset;\n        }\n    }\n}\n```",
  "19_graph_shortest_path": "```cpp\n#pragma once\n#include <vector>\n#include <limits>\n#include <omp.h>\n\nint shortestPathLength(std::vector<int> const& A, size_t N, int source, int dest) {\n    if (source == dest) {\n        return 0;\n    }\n\n    std::vector<int> dist(N, -1);\n    dist[source] = 0;\n\n    std::vector<int> frontier;\n    frontier.reserve(N);\n    frontier.push_back(source);\n\n    const int N_int = static_cast<int>(N);\n    const int maxThreads = omp_get_max_threads();\n    std::vector<std::vector<int>> localNext(maxThreads);\n\n    while (!frontier.empty() && dist[dest] == -1) {\n        // Clear thread-local buffers and reserve approximate capacity\n        for (auto& vec : localNext) {\n            vec.clear();\n            vec.reserve(frontier.size() / static_cast<size_t>(maxThreads) + 10);\n        }\n\n        int* dist_ptr = dist.data();\n\n        #pragma omp parallel\n        {\n            int tid = omp_get_thread_num();\n            std::vector<int>& local = localNext[tid];\n\n            #pragma omp for schedule(dynamic, 16)\n            for (size_t idx = 0; idx < frontier.size(); ++idx) {\n                int cur = frontier[idx];\n                size_t rowOffset = static_cast<size_t>(cur) * N;\n                int dist_cur = dist[cur];\n                const int* row = A.data() + rowOffset;\n\n                for (int j = 0; j < N_int; ++j) {\n                    if (!row[j]) continue;\n                    // Atomically mark the vertex as visited and record distance\n                    if (__sync_bool_compare_and_swap(dist_ptr + j, -1, dist_cur + 1)) {\n                        local.push_back(j);\n                    }\n                }\n            }\n        }\n\n        frontier.clear();\n        for (auto& vec : localNext) {\n            if (!vec.empty()) {\n                frontier.insert(frontier.end(), vec.begin(), vec.end());\n            }\n        }\n    }\n\n    return (dist[dest] == -1) ? std::numeric_limits<int>::max() : dist[dest];\n}\n```\n",
  "28_reduce_smallest_odd_number": "```cpp\n#pragma once\n#include <vector>\n#include <limits>\n#include <cstddef>\n\n/* Return the value of the smallest odd number in vector `x`.\n   Example:\n     input: [7, 9, 5, 2, 8, 16, 4, 1]\n     output: 1\n   If no odd number is present, returns std::numeric_limits<int>::max().\n*/\ninline int smallestOdd(const std::vector<int> &x) {\n    const int sentinel = std::numeric_limits<int>::max();\n    int minOdd = sentinel;\n    const std::size_t n = x.size();\n    const std::size_t threshold = 16384;   // Above this use parallel reduction\n\n    if (n > threshold) {\n        #pragma omp parallel for reduction(min:minOdd)\n        for (std::size_t i = 0; i < n; ++i) {\n            const int v = x[i];\n            if ((v & 1) && v < minOdd) {\n                minOdd = v;\n            }\n        }\n    } else {\n        for (std::size_t i = 0; i < n; ++i) {\n            const int v = x[i];\n            if ((v & 1) && v < minOdd) {\n                minOdd = v;\n            }\n        }\n    }\n\n    return minOdd;\n}\n```\n",
  "47_sparse_la_spmv": "```cpp\n#pragma once\n#include <vector>\n\n/* COO matrix element */\nstruct COOElement {\n    size_t row, column;\n    double value;\n};\n\n/*\n * Compute y = alpha*A*x + beta*y\n *\n * Parameters:\n *   alpha - scalar multiplier for A*x\n *   A     - sparse matrix in COO format\n *   x     - input vector (length N)\n *   beta  - scalar multiplier for existing y\n *   y     - input/output vector (length M)\n *   M     - number of rows of A (and length of y)\n *   N     - number of columns of A (and length of x)\n *\n * The function preserves the exact numerical behaviour of the original\n * implementation while leveraging parallelism and efficient sparse\n * matrix storage.\n */\nvoid spmv(double alpha, const std::vector<COOElement> &A,\n          const std::vector<double> &x, double beta,\n          std::vector<double> &y, size_t M, size_t N)\n{\n    /* 0. Scale y by beta */\n    #pragma omp parallel for schedule(static)\n    for (size_t i = 0; i < M; ++i)\n        y[i] *= beta;\n\n    /* 1. Early exit if nothing to do */\n    if (M == 0 || N == 0 || A.empty())\n        return;\n\n    /* 2. Build CSR\u2011like representation (row pointers) */\n    std::vector<size_t> row_ptr(M + 1, 0);\n    for (const auto &e : A) {\n        if (e.row < M && e.column < N)\n            ++row_ptr[e.row + 1];\n    }\n\n    /* Prefix sum to get start indices for each row */\n    for (size_t i = 1; i <= M; ++i)\n        row_ptr[i] += row_ptr[i - 1];\n\n    const size_t nnz = row_ptr[M];\n    if (nnz == 0)\n        return;                // all rows are zero\n\n    /* Allocate column indices and values */\n    std::vector<size_t> col(nnz);\n    std::vector<double> val(nnz);\n\n    /* Copy of row_ptr to keep current insertion positions */\n    std::vector<size_t> cur(row_ptr);\n\n    /* 3. Fill column and value arrays preserving order within each row */\n    for (const auto &e : A) {\n        if (e.row < M && e.column < N) {\n            size_t r   = e.row;\n            size_t idx = cur[r]++;  // use current start index and advance\n            col[idx]   = e.column;\n            val[idx]   = e.value;\n        }\n    }\n\n    /* 4. Accumulate contributions into y (in parallel by rows) */\n    #pragma omp parallel for schedule(static)\n    for (size_t r = 0; r < M; ++r) {\n        double sum = 0.0;\n        for (size_t idx = row_ptr[r]; idx < row_ptr[r + 1]; ++idx)\n            sum += alpha * val[idx] * x[col[idx]];\n        y[r] += sum;\n    }\n}\n```",
  "12_geometry_smallest_triangle": "```cpp\n#pragma once\n#include <vector>\n#include <limits>\n#include <cmath>\n#include <omp.h>\n\nstruct Point {\n    double x, y;\n};\n\ninline double triangleArea(Point const& A, Point const& B, Point const& C) {\n    return 0.5 * std::fabs(A.x * (B.y - C.y) + B.x * (C.y - A.y) + C.x * (A.y - B.y));\n}\n\ninline double smallestArea(const std::vector<Point>& points) {\n    const std::size_t n = points.size();\n    if (n < 3) return 0.0;\n\n    const Point* p = points.data();\n    double minCrossAbs = std::numeric_limits<double>::infinity();\n\n#pragma omp parallel for collapse(2) schedule(static) reduction(min:minCrossAbs)\n    for (std::size_t i = 0; i < n - 2; ++i) {\n        const Point &a = p[i];\n        for (std::size_t j = i + 1; j < n - 1; ++j) {\n            const Point &b = p[j];\n            const double dx1 = b.x - a.x;\n            const double dy1 = b.y - a.y;\n            for (std::size_t k = j + 1; k < n; ++k) {\n                const Point &c = p[k];\n                const double dx2 = c.x - a.x;\n                const double dy2 = c.y - a.y;\n                double cross = dx1 * dy2 - dx2 * dy1;\n                if (cross < 0.0) cross = -cross;\n                if (cross < minCrossAbs) minCrossAbs = cross;\n            }\n        }\n    }\n\n    return minCrossAbs * 0.5;\n}\n```",
  "33_scan_reverse_prefix_sum": "```cpp\n#pragma once\n#include <vector>\n\n/* Compute the reverse prefix sum of the vector x into output.\n   Examples:\n   input: [1, 7, 4, 6, 6, 2]\n   output: [2, 8, 14, 18, 25, 26]\n   input: [3, 3, 7, 1, -2]\n   output: [-2, -1, 6, 9, 12]\n*/\nvoid reversePrefixSum(std::vector<int> const& x, std::vector<int> &output) {\n    const size_t n = x.size();\n    if (n == 0) return;                // nothing to compute\n\n    int sum = 0;\n    const int*   xptr = x.data();       // source\n    int* outptr = output.data();        // destination\n\n    // Compute in reverse order: sum of suffixes of x\n    for (size_t i = n; i > 0; --i) {\n        sum += xptr[i - 1];             // add element x[i-1]\n        outptr[n - i] = sum;           // place into output[n-i]\n    }\n}\n```\n",
  "22_histogram_count_quadrants": "```cpp\n#pragma once\n#include <array>\n#include <vector>\n\nstruct Point {\n    double x;\n    double y;\n};\n\n/*\n * Count the number of cartesian points in each quadrant.\n * Stores the counts in the supplied `bins` array.\n *\n * The implementation is fully parallelised with OpenMP.\n */\nvoid countQuadrants(std::vector<Point> const& points,\n                    std::array<size_t, 4> &bins) {\n    // Ensure the output array starts from zero.\n    bins.fill(0);\n\n    const size_t n = points.size();\n    if (n == 0) return;               // nothing to count\n\n    const Point* data = points.data(); // faster than operator[]\n    const long long N = static_cast<long long>(n);\n\n#pragma omp parallel\n    {\n        // Local histogram for this thread.\n        std::array<size_t, 4> local = {0, 0, 0, 0};\n\n#pragma omp for schedule(static)\n        for (long long i = 0; i < N; ++i) {\n            const Point &p = data[i];\n\n            if (p.x >= 0.0) {\n                if (p.y >= 0.0)\n                    ++local[0];   // Quadrant I\n                else\n                    ++local[3];   // Quadrant IV\n            } else {\n                if (p.y >= 0.0)\n                    ++local[1];   // Quadrant II\n                else\n                    ++local[2];   // Quadrant III\n            }\n        }\n\n#pragma omp critical\n        {\n            for (size_t j = 0; j < 4; ++j)\n                bins[j] += local[j];\n        }\n    }\n}\n```",
  "49_sparse_la_sparse_lu_decomp": "```cpp\n#pragma once\n#include <vector>\n\nstruct COOElement {\n    size_t row, column;\n    double value;\n};\n\n/* LU factorisation of a sparse matrix A (COO format) into\n   L (unit lower\u2011triangular) and U (upper\u2011triangular) matrices.\n   The output matrices L and U are stored in row\u2011major order.\n*/\nvoid luFactorize(std::vector<COOElement> const& A,\n                 std::vector<double>& L,\n                 std::vector<double>& U,\n                 size_t N)\n{\n    // Allocate and zero the output matrices\n    L.assign(N * N, 0.0);\n    U.assign(N * N, 0.0);\n\n    // Build a dense copy of A\n    std::vector<double> fullA(N * N, 0.0);\n    for (const auto& e : A) {\n        fullA[e.row * N + e.column] = e.value;\n    }\n\n    for (size_t i = 0; i < N; ++i) {\n        double* L_row = &L[i * N];\n        double* U_row = &U[i * N];\n        const double* fullRow = &fullA[i * N];\n\n        /* Lower part: L[i][0 .. i-1] */\n        for (size_t j = 0; j < i; ++j) {\n            double sum = fullRow[j];\n            for (size_t k = 0; k < j; ++k) {\n                sum -= L_row[k] * U[k * N + j];\n            }\n            L_row[j] = sum / U[j * N + j];\n        }\n\n        /* Upper part: U[i][i .. N-1] (parallelised over columns) */\n#pragma omp parallel for schedule(static)\n        for (size_t j = i; j < N; ++j) {\n            double val = fullRow[j];\n            for (size_t k = 0; k < i; ++k) {\n                val -= L_row[k] * U[k * N + j];\n            }\n            U_row[j] = val;\n        }\n\n        /* Diagonal element of L is unity */\n        L_row[i] = 1.0;\n    }\n}\n```",
  "52_stencil_1d_jacobi_3-point_stencil": "```cpp\n#pragma once\n#include <vector>\n\ninline void jacobi1D(const std::vector<double>& input, std::vector<double>& output) {\n    const std::size_t N = input.size();\n    if (N == 0) return;\n\n    const double* in  = input.data();\n    double*       out = output.data();\n\n    constexpr double inv3 = 1.0 / 3.0;\n\n    // First element: handle left boundary (no element to the left)\n    out[0] = (in[0] + (N > 1 ? in[1] : 0.0)) * inv3;\n\n    // Last element: handle right boundary (no element to the right)\n    if (N > 1) {\n        out[N - 1] = (in[N - 1] + in[N - 2]) * inv3;\n    }\n\n    // Interior elements \u2013 parallelized and vectorizable\n    #pragma omp parallel for schedule(static)\n    for (std::size_t i = 1; i + 1 < N; ++i) {\n        out[i] = (in[i - 1] + in[i] + in[i + 1]) * inv3;\n    }\n}\n```\n",
  "06_fft_dft": "```cpp\n#pragma once\n#include <vector>\n#include <complex>\n#include <cmath>\n#include <omp.h>\n\n#if !defined(M_PI)\n# define M_PI 3.14159265358979323846\n#endif\n\n/* Compute the discrete Fourier transform of x.\n *  Example:\n *      input:  [1, 4, 9, 16]\n *      output: [30+0i, -8-12i, -10-0i, -8+12i]\n */\ninline void dft(const std::vector<double> &x, std::vector<std::complex<double>> &output)\n{\n    const int N = static_cast<int>(x.size());\n    output.resize(N);                         // all elements will be overwritten\n\n    const double two_pi = 2.0 * M_PI;\n    const double invN   = static_cast<double>(N);   // single conversion for speed\n\n    /* parallelise the outer loop over k \u2013 each k is independent */\n    #pragma omp parallel for schedule(static)\n    for (int k = 0; k < N; ++k) {\n        double sum_r = 0.0;\n        double sum_i = 0.0;\n\n        for (int n = 0; n < N; ++n) {\n            const double angle = two_pi * n * k / invN;\n            const double  c_re = std::cos(angle);\n            const double  c_im = -std::sin(angle);\n            const double  xn   = x[n];\n            sum_r += xn * c_re;\n            sum_i += xn * c_im;\n        }\n        output[k] = std::complex<double>(sum_r, sum_i);\n    }\n}\n```",
  "14_geometry_closest_pair_1d": "```cpp\n#pragma once\n#include <vector>\n#include <limits>\n#include <algorithm>\n#include <cmath>\n#include <omp.h>\n\n// Compute the absolute difference between two points\ninline double distance(double x1, double x2) {\n    return std::abs(x1 - x2);\n}\n\n// Return the minimum distance between any two elements in the vector.\n/// @param x The input vector containing the points.\n/// @return The minimum absolute difference.\n/// @note For n < 2 the function returns 0, matching the original implementation.\ninline double closestPair(const std::vector<double>& x) {\n    const size_t n = x.size();\n    if (n < 2) return 0.0;\n\n    /* For small arrays use the straightforward O(n\u00b2) algorithm to avoid\n       the overhead of copying and sorting.  The cutoff can be tuned\n       according to typical input sizes. */\n    constexpr size_t bruteForceCutoff = 128;\n    if (n <= bruteForceCutoff) {\n        double minDist = std::numeric_limits<double>::max();\n        for (size_t i = 0; i + 1 < n; ++i) {\n            for (size_t j = i + 1; j < n; ++j) {\n                double d = std::abs(x[j] - x[i]);\n                if (d < minDist) minDist = d;\n            }\n        }\n        return minDist;\n    }\n\n    /* For larger inputs we sort a copy and scan adjacent pairs.\n       This reduces the complexity to O(n log n) and scales well\n       on multi\u2011threaded builds thanks to OpenMP parallel\n       reduction on the linear scan. */\n    std::vector<double> sorted(x);\n    std::sort(sorted.begin(), sorted.end());\n\n    double minDist = std::numeric_limits<double>::max();\n    #pragma omp parallel for reduction(min:minDist)\n    for (size_t i = 0; i + 1 < n; ++i) {\n        double d = std::abs(sorted[i + 1] - sorted[i]);\n        if (d < minDist) minDist = d;\n    }\n\n    return minDist;\n}\n```",
  "13_geometry_closest_pair_2d": "```cpp\n#pragma once\n#include <vector>\n#include <limits>\n#include <cmath>\n#ifdef _OPENMP\n#include <omp.h>\n#endif\n\nstruct Point {\n    double x, y;\n};\n\ndouble distance(Point const& p1, Point const& p2) {\n    return std::sqrt(std::pow(p2.x - p1.x, 2) + std::pow(p2.y - p1.y, 2));\n}\n\ndouble closestPair(std::vector<Point> const& points) {\n    const size_t n = points.size();\n    if (n < 2) return 0.0;\n\n    double minDistSq = std::numeric_limits<double>::infinity();\n\n    #pragma omp parallel for schedule(static) reduction(min:minDistSq)\n    for (size_t i = 0; i < n; ++i) {\n        const double xi = points[i].x;\n        const double yi = points[i].y;\n        for (size_t j = i + 1; j < n; ++j) {\n            const double dx = points[j].x - xi;\n            const double dy = points[j].y - yi;\n            const double dSq = dx * dx + dy * dy;\n            if (dSq < minDistSq) minDistSq = dSq;\n        }\n    }\n\n    return std::sqrt(minDistSq);\n}\n```\n",
  "03_dense_la_axpy": "```cpp\n#pragma once\n#include <vector>\n#include <cstddef>\n#include <omp.h>\n\ninline void axpy(double alpha,\n                 std::vector<double> const &x,\n                 std::vector<double> const &y,\n                 std::vector<double> &z)\n{\n    const std::size_t n = x.size();            // Assume y.size() == n && z.size() == n\n    const double *restrict xp = x.data();\n    const double *restrict yp = y.data();\n    double *restrict zp = z.data();\n\n#pragma omp parallel for schedule(static)\n    for (std::ptrdiff_t i = 0; i < static_cast<std::ptrdiff_t>(n); ++i)\n    {\n        zp[i] = alpha * xp[i] + yp[i];\n    }\n}\n```\n",
  "04_dense_la_gemv": "```cpp\n#pragma once\n#include <vector>\n#include <cstddef>\n#ifdef _OPENMP\n#include <omp.h>\n#endif\n\ninline void gemv(std::vector<double> const& A,\n                 std::vector<double> const& x,\n                 std::vector<double> &y,\n                 size_t M, size_t N)\n{\n    // Pointers with GCC's `restrict` keyword for aliasing guarantees.\n    const double *restrict A_data = A.data();\n    const double *restrict x_data = x.data();\n    double *restrict y_data = y.data();\n\n    // Parallelize over rows.  The schedule is static so each thread gets\n    // an equal amount of work.  Each iteration writes to a distinct\n    // element of `y`, so no synchronization issues arise.\n    #pragma omp parallel for schedule(static)\n    for (size_t i = 0; i < M; ++i) {\n        const double *restrict Arow = A_data + i * N;\n        double sum = 0.0;\n\n        // Allow the compiler to vectorize the inner loop.\n        #pragma omp simd\n        for (size_t j = 0; j < N; ++j) {\n            sum += Arow[j] * x_data[j];\n        }\n        y_data[i] = sum;\n    }\n}\n```",
  "26_reduce_product_of_inverses": "```cpp\n#pragma once\n#include <vector>\n\ninline double productWithInverses(const std::vector<double>& x) {\n    double result = 1.0;\n    const size_t n = x.size();\n    size_t i = 0;\n    // Unrolled loop to reduce branching overhead\n    for (; i + 1 < n; i += 2) {\n        result *= x[i];\n        result *= 1.0 / x[i + 1];\n    }\n    if (i < n) {\n        result *= x[i];\n    }\n    return result;\n}\n```",
  "11_geometry_convex_hull_perimeter": "```cpp\n#pragma once\n#include <vector>\n#include <algorithm>\n#include <cmath>\n#ifdef _OPENMP\n#include <omp.h>\n#endif\n\nstruct Point {\n    double x;\n    double y;\n};\n\ninline double cross(const Point &a, const Point &b, const Point &c) {\n    return (b.x - a.x) * (c.y - a.y) - (b.y - a.y) * (c.x - a.x);\n}\n\ninline double convexHullPerimeter(const std::vector<Point> &points) {\n    const std::size_t n = points.size();\n    if (n < 3) {\n        return 0.0;\n    }\n\n    std::vector<Point> pts(points);\n    auto cmp = [](const Point &a, const Point &b) {\n        return a.x < b.x || (a.x == b.x && a.y < b.y);\n    };\n    std::sort(pts.begin(), pts.end(), cmp);\n\n    std::vector<Point> hull(2 * n);\n    std::size_t k = 0;\n\n    // Build lower hull\n    for (std::size_t i = 0; i < n; ++i) {\n        while (k >= 2 && cross(hull[k - 2], hull[k - 1], pts[i]) <= 0.0) {\n            --k;\n        }\n        hull[k++] = pts[i];\n    }\n\n    // Build upper hull\n    const std::size_t t = k + 1;\n    for (std::size_t i = n - 1; i > 0; --i) {\n        while (k >= t && cross(hull[k - 2], hull[k - 1], pts[i - 1]) <= 0.0) {\n            --k;\n        }\n        hull[k++] = pts[i - 1];\n    }\n\n    hull.resize(k - 1);\n\n    double perimeter = 0.0;\n    const std::size_t m = hull.size();\n\n#if defined(_OPENMP)\n    #pragma omp parallel for reduction(+:perimeter) schedule(static)\n#endif\n    for (std::size_t i = 0; i + 1 < m; ++i) {\n        double dx = hull[i + 1].x - hull[i].x;\n        double dy = hull[i + 1].y - hull[i].y;\n        perimeter += std::sqrt(dx * dx + dy * dy);\n    }\n\n    if (m > 1) {\n        double dx = hull[0].x - hull[m - 1].x;\n        double dy = hull[0].y - hull[m - 1].y;\n        perimeter += std::sqrt(dx * dx + dy * dy);\n    }\n\n    return perimeter;\n}\n```",
  "31_scan_scan_with_min_function": "```cpp\n#pragma once\n#include <vector>\n#include <limits>\n#include <cstddef>\n#include <omp.h>\n\ninline void partialMinimums(std::vector<float> &x)\n{\n    const std::size_t n = x.size();\n    if (n == 0) return;\n\n    const float INF = std::numeric_limits<float>::max();\n\n    // For very small vectors the parallel overhead outweighs the benefit.\n    if (n <= 64) {\n        float cur = INF;\n        for (std::size_t i = 0; i < n; ++i) {\n            float v = x[i];\n            cur = (v < cur) ? v : cur;\n            x[i] = cur;\n        }\n        return;\n    }\n\n    const std::size_t max_threads = static_cast<std::size_t>(omp_get_max_threads());\n    const std::size_t block_size_candidate = n / (max_threads * 4);\n    const std::size_t block_size = block_size_candidate > 64u ? block_size_candidate : 64u;\n    const std::size_t nblocks = (n + block_size - 1) / block_size;\n\n    std::vector<float> block_min(nblocks);\n    std::vector<float> block_prefix(nblocks);\n\n    #pragma omp parallel\n    {\n        const std::size_t tid = static_cast<std::size_t>(omp_get_thread_num());\n        const std::size_t nt  = static_cast<std::size_t>(omp_get_num_threads());\n\n        /* Pass 1 \u2013 compute local inclusive minima for each block\n           and store the block's overall minimum. */\n        for (std::size_t block = tid; block < nblocks; block += nt) {\n            std::size_t start = block * block_size;\n            std::size_t end   = start + block_size;\n            if (end > n) end = n;\n\n            float cur = INF;\n            for (std::size_t i = start; i < end; ++i) {\n                float v = x[i];\n                cur = (v < cur) ? v : cur;\n                x[i] = cur;\n            }\n            block_min[block] = cur;\n        }\n\n        /* Prefix over the per\u2011block minima \u2013 performed by a single thread. */\n        #pragma omp single\n        {\n            block_prefix[0] = INF;\n            for (std::size_t i = 1; i < nblocks; ++i) {\n                float a = block_prefix[i - 1];\n                float b = block_min[i - 1];\n                block_prefix[i] = (a < b) ? a : b;\n            }\n        }  // implicit barrier here\n\n        /* Pass 2 \u2013 combine block prefixes with local prefixes to obtain final values. */\n        for (std::size_t block = tid; block < nblocks; block += nt) {\n            std::size_t start = block * block_size;\n            std::size_t end   = start + block_size;\n            if (end > n) end = n;\n\n            const float pref = block_prefix[block];\n            for (std::size_t i = start; i < end; ++i) {\n                float v = x[i];\n                v = (pref < v) ? pref : v;\n                x[i] = v;\n            }\n        }\n    }\n}\n```\n",
  "00_dense_la_lu_decomp": "```cpp\n#pragma once\n#include <vector>\n#include <cstddef>\n#ifdef _OPENMP\n#  include <omp.h>\n#endif\n\ninline void luFactorize(std::vector<double> &A, size_t N) {\n    double *a = A.data();\n    const size_t n = N;\n\n#pragma omp parallel\n    {\n        for (size_t k = 0; k < n; ++k) {\n            double *rowk = a + k * n;\n            const double pivot = rowk[k];\n            const size_t len = n - k - 1;\n            double *curk = rowk + k + 1;\n\n#pragma omp for schedule(static)\n            for (size_t i = k + 1; i < n; ++i) {\n                double *rowi = a + i * n;\n                const double factor = rowi[k] / pivot;\n                rowi[k] = factor;\n                double *curi = rowi + k + 1;\n#pragma omp simd\n                for (size_t j = 0; j < len; ++j) {\n                    curi[j] -= factor * curk[j];\n                }\n            }\n        }\n    }\n}\n```",
  "51_stencil_edge_kernel": "```cpp\n#pragma once\n#include <vector>\n#include <cstring>              // for std::memcpy\n\nconst int edgeKernel[3][3] = {{-1, -1, -1}, {-1, 8, -1}, {-1, -1, -1}};\n\ninline int clip_value(int v) noexcept { return v < 0 ? 0 : (v > 255 ? 255 : v); }\n\nvoid convolveKernel(std::vector<int> const& imageIn,\n                    std::vector<int> &imageOut, size_t N) {\n    if (N == 0) return;\n\n    const size_t paddedCols = N + 2;\n    const size_t paddedSize = paddedCols * paddedCols;\n    std::vector<int> padded(paddedSize, 0);\n\n    /* copy the input image into the center of the padded array */\n    for (size_t i = 0; i < N; ++i) {\n        std::memcpy(&padded[(i + 1) * paddedCols + 1],\n                    &imageIn[i * N],\n                    static_cast<size_t>(N * sizeof(int)));\n    }\n\n    const int *inPtr  = padded.data();\n    int *outPtr       = imageOut.data();\n\n#pragma omp parallel for collapse(2) schedule(static)\n    for (size_t i = 0; i < N; ++i) {\n#pragma omp simd\n        for (size_t j = 0; j < N; ++j) {\n            const size_t idx = (i + 1) * paddedCols + (j + 1);\n            int sum = inPtr[idx] * 8\n                    - inPtr[idx - 1]\n                    - inPtr[idx + 1]\n                    - inPtr[idx - paddedCols]\n                    - inPtr[idx + paddedCols]\n                    - inPtr[idx - paddedCols - 1]\n                    - inPtr[idx - paddedCols + 1]\n                    - inPtr[idx + paddedCols - 1]\n                    - inPtr[idx + paddedCols + 1];\n            outPtr[i * N + j] = clip_value(sum);\n        }\n    }\n}\n```",
  "01_dense_la_solve": "```cpp\n#pragma once\n#include <vector>\n#include <omp.h>\n\ninline void solveLinearSystem(std::vector<double> const& A,\n                              std::vector<double> const& b,\n                              std::vector<double> &x,\n                              size_t N)\n{\n    if (N == 0) return;           // Empty system \u2013 nothing to do\n\n    // Make local copies of A and b to avoid side\u2011effects\n    std::vector<double> Acopy = A;\n    std::vector<double> bcopy = b;\n\n    double *restrict Adata = Acopy.data();\n    double *restrict bdata = bcopy.data();\n\n    /* Gaussian elimination \u2013 row\u2011wise parallelism */\n    for (size_t i = 0; i < N - 1; ++i) {\n        const double pivot = Adata[i * N + i];\n        if (pivot == 0.0) {\n            // Preserve original behaviour: stop on a zero pivot\n            return;\n        }\n\n        /* Eliminate all rows below the pivot in parallel */\n        #pragma omp parallel for schedule(static)\n        for (size_t j = i + 1; j < N; ++j) {\n            const double factor = Adata[j * N + i] / pivot;\n            double *restrict row_j = Adata + j * N;\n            const double *restrict row_i = Adata + i * N;\n\n            /* Apply the elimination to the current row */\n            #pragma omp simd\n            for (size_t k = i; k < N; ++k) {\n                row_j[k] -= factor * row_i[k];\n            }\n\n            /* Update the RHS entry */\n            bdata[j] -= factor * bdata[i];\n        }\n    }\n\n    /* Back substitution */\n    for (int i = static_cast<int>(N) - 1; i >= 0; --i) {\n        double sum = 0.0;\n        double *restrict Ai = Adata + i * N;\n        for (size_t j = i + 1; j < N; ++j) {\n            sum += Ai[j] * x[j];\n        }\n        x[i] = (bdata[i] - sum) / Ai[i];\n    }\n}\n```",
  "02_dense_la_gemm": "```cpp\n#pragma once\n#include <vector>\n#include <cstddef>\n#include <algorithm>\n#include <omp.h>\n\n/*\n * Matrix\u2013matrix multiplication (GEMM).\n *\n * A : M x K matrix (row-major)\n * B : K x N matrix (row-major)\n * C : M x N matrix (row-major), will contain the result after the call.\n *\n * The function is thread\u2011safe and threads work on separate rows of C.\n * This implementation retains the exact numerical accumulation order of the\n * original reference while exploiting OpenMP parallelism and vectorisation.\n */\ninline void gemm(const std::vector<double> &A,\n                 const std::vector<double> &B,\n                 std::vector<double> &C,\n                 size_t M,\n                 size_t K,\n                 size_t N)\n{\n    // Ensure C has the expected size; this is defensive and has no runtime cost\n    // when the sizes match.\n    if (C.size() != M * N)\n        C.resize(M * N);\n\n    // Parallelise over the outermost dimension (rows of A and C).\n    #pragma omp parallel for schedule(static)\n    for (size_t i = 0; i < M; ++i) {\n        const double *rowA = A.data() + i * K;   // A[i, *]\n        double       *rowC = C.data() + i * N;   // C[i, *]\n\n        for (size_t k = 0; k < K; ++k) {\n            const double a = rowA[k];             // A[i, k]\n            const double *rowB = B.data() + k * N; // B[k, *]\n\n            /* Vectorised inner loop \u2013 each j is independent so a SIMD\n             * vectoriser will produce efficient instructions.\n             */\n            #pragma omp simd\n            for (size_t j = 0; j < N; ++j) {\n                rowC[j] += a * rowB[j];\n            }\n        }\n    }\n}\n```",
  "41_sort_k-th_smallest_element": "```cpp\n#pragma once\n#include <vector>\n#include <parallel/algorithm> // Provides __gnu_parallel::nth_element\n\ninline int findKthSmallest(const std::vector<int>& x, int k)\n{\n    std::vector<int> copy(x);                                 // Copy input\n    __gnu_parallel::nth_element(copy.begin(),\n                                copy.begin() + (k - 1),        // 0\u2011based index\n                                copy.end());                  // Parallel nth_element\n    return copy[k - 1];                                       // The k\u2011th smallest value\n}\n```",
  "08_fft_split_fft": "```cpp\n#pragma once\n#include <vector>\n#include <cmath>\n#include <complex>\n#include <algorithm>\n#include <omp.h>\n\ninline void fft(std::vector<std::complex<double>> const& x,\n                std::vector<double> &r,\n                std::vector<double> &i)\n{\n    const size_t N = x.size();\n    if (r.size() != N) r.resize(N);\n    if (i.size() != N) i.resize(N);\n\n    /* --- Copy input data --- */\n    std::vector<std::complex<double>> x_copy = x;\n\n    unsigned int k = static_cast<unsigned int>(N);\n    unsigned int n;\n    const double thetaT = 3.14159265358979323846264338328L / N;\n    std::complex<double> phiT = std::complex<double>(std::cos(thetaT), -std::sin(thetaT));\n\n    /* --- Decimation\u2011in\u2011time FFT --- */\n    while (k > 1) {\n        n = k;\n        k >>= 1;                     // halve the stride\n        phiT *= phiT;                // square  base twiddle factor\n\n        /* Pre\u2011compute the powers of phiT for this stage */\n        std::vector<std::complex<double>> twiddle(k);\n        twiddle[0] = std::complex<double>(1.0, 0.0);\n        for (unsigned int t = 1; t < k; ++t) {\n            twiddle[t] = twiddle[t-1] * phiT;\n        }\n\n        /* Butterfly stage \u2013 work on each 'l' independently */\n        #pragma omp parallel for schedule(static)\n        for (int l = 0; l < static_cast<int>(k); ++l) {\n            const std::complex<double> T = twiddle[l];\n            for (unsigned int a = static_cast<unsigned int>(l); a < N; a += n) {\n                unsigned int b = a + k;\n                const std::complex<double> t = x_copy[a] - x_copy[b];\n                x_copy[a] += x_copy[b];\n                x_copy[b] = t * T;\n            }\n        }\n    }\n\n    /* --- Bit\u2011reversal permutation --- */\n    const unsigned int m = static_cast<unsigned int>(std::log2(N));\n    if (m > 0) {\n        #pragma omp parallel for schedule(static)\n        for (int a_int = 0; a_int < static_cast<int>(N); ++a_int) {\n            unsigned int a = static_cast<unsigned int>(a_int);\n            unsigned int b = a;\n            b = ((b & 0xaaaaaaaau) >> 1) | ((b & 0x55555555u) << 1);\n            b = ((b & 0xccccccccu) >> 2) | ((b & 0x33333333u) << 2);\n            b = ((b & 0xf0f0f0f0u) >> 4) | ((b & 0x0f0f0f0fu) << 4);\n            b = ((b & 0xff00ff00u) >> 8) | ((b & 0x00ff00ffu) << 8);\n            b = ((b >> 16) | (b << 16)) >> (32 - m);\n            if (b > a) {\n                std::swap(x_copy[a], x_copy[b]);\n            }\n        }\n    }\n\n    /* --- Split into real and imaginary parts --- */\n    for (size_t j = 0; j < N; ++j) {\n        r[j] = x_copy[j].real();\n        i[j] = x_copy[j].imag();\n    }\n}\n```",
  "05_fft_inverse_fft": "```cpp\n#pragma once\n#include <cmath>\n#include <vector>\n#include <complex>\n#include <algorithm>\n#include <cstdint>\n#include <omp.h>  // for OpenMP pragmas\n\n#ifndef M_PI\n#define M_PI 3.14159265358979323846\n#endif\n\n// Reverse the lowest 'm' bits of x\nstatic inline unsigned int reverse_bits(unsigned int x, unsigned int m) {\n    x = (((x & 0xaaaaaaaau) >> 1) | ((x & 0x55555555u) << 1));\n    x = (((x & 0xccccccccu) >> 2) | ((x & 0x33333333u) << 2));\n    x = (((x & 0xf0f0f0f0u) >> 4) | ((x & 0x0f0f0f0fu) << 4));\n    x = (((x & 0xff00ff00u) >> 8) | ((x & 0x00ff00ffu) << 8));\n    x = ((x >> 16) | (x << 16)) >> (32 - m);\n    return x;\n}\n\n// In-place FFT (forward transform)\n// Uses an iterative decimation\u2011in\u2011time Cooley\u2011Tukey algorithm\nstatic inline void fft_helper(std::vector<std::complex<double>>& x) {\n    const std::size_t N = x.size();\n    if (N <= 1) return;\n\n    const unsigned int logN = static_cast<unsigned int>(std::log2(static_cast<double>(N)));\n\n    // Initial twiddle factor for the first stage\n    std::complex<double> phiT(std::cos(M_PI / static_cast<double>(N)),\n                             -std::sin(M_PI / static_cast<double>(N)));\n\n    unsigned int k = static_cast<unsigned int>(N);\n    std::vector<std::complex<double>> w;  // powers of phiT for this stage\n\n    while (k > 1) {\n        unsigned int stride = k;          // step for the inner 'a' loop\n        k >>= 1;                         // new k = stride / 2\n        phiT *= phiT;                    // update twiddle for this stage\n\n        // Pre\u2011compute phiT^l for l = 0 \u2026 k-1\n        w.resize(k);\n        w[0] = std::complex<double>(1.0, 0.0);\n        for (unsigned int l = 1; l < k; ++l)\n            w[l] = w[l - 1] * phiT;\n\n        // Butterfly stage \u2013 parallel loop over l\n        #pragma omp parallel for schedule(static)\n        for (int l = 0; l < static_cast<int>(k); ++l) {\n            const std::complex<double> Tw = w[l];\n            for (std::size_t a = static_cast<std::size_t>(l); a < N; a += stride) {\n                std::size_t b = a + k;\n                std::complex<double> t = x[a] - x[b];\n                x[a] += x[b];\n                x[b] = t * Tw;\n            }\n        }\n    }\n\n    // Bit\u2011reversal reordering (decimation)\n    #pragma omp parallel for schedule(static)\n    for (int i = 0; i < static_cast<int>(N); ++i) {\n        unsigned int j = reverse_bits(static_cast<unsigned int>(i), logN);\n        if (j > static_cast<unsigned int>(i)) {\n            std::swap(x[i], x[j]);\n        }\n    }\n}\n\n// Inverse FFT (in\u2011place)\n// Conjugate \u2192 forward FFT \u2192 conjugate \u2192 scale\nstatic inline void ifft(std::vector<std::complex<double>>& x) {\n    const std::size_t N = x.size();\n    if (N == 0) return;\n\n    // First conjugation\n    #pragma omp parallel for schedule(static)\n    for (int i = 0; i < static_cast<int>(N); ++i) {\n        x[i] = std::conj(x[i]);\n    }\n\n    // Forward FFT\n    fft_helper(x);\n\n    // Second conjugation\n    #pragma omp parallel for schedule(static)\n    for (int i = 0; i < static_cast<int>(N); ++i) {\n        x[i] = std::conj(x[i]);\n    }\n\n    // Scale by 1/N\n    const double invN = 1.0 / static_cast<double>(N);\n    #pragma omp parallel for schedule(static)\n    for (int i = 0; i < static_cast<int>(N); ++i) {\n        x[i] *= invN;\n    }\n}\n```",
  "45_sparse_la_sparse_solve": "#pragma once\n#include <vector>\n#include <cstddef>\n#include <cmath>\n#include <algorithm>\n#ifdef _OPENMP\n#include <omp.h>\n#endif\n\nstruct COOElement {\n    size_t row, column;\n    double value;\n};\n\nvoid solveLinearSystem(std::vector<COOElement> const& A,\n                       std::vector<double> const& b,\n                       std::vector<double> &x,\n                       size_t N) {\n    // Build a dense matrix in row-major order (N\u00d7N)\n    std::vector<double> M(N * N, 0.0);\n    for (const auto& el : A) {\n        M[el.row * N + el.column] = el.value;\n    }\n\n    // Working copy of the right\u2011hand side\n    std::vector<double> b_copy = b;\n    const int N_int = static_cast<int>(N);\n\n    /* Gaussian elimination with partial pivoting.\n       The outer loop over the pivot row is executed by a single thread\n       while the elimination of the rows below the pivot is parallelized. */\n    #pragma omp parallel\n    {\n        #pragma omp single\n        {\n            for (int i = 0; i < N_int; ++i) {\n                const size_t ii = static_cast<size_t>(i);\n\n                /* Pivot selection */\n                size_t pivotRow = ii;\n                double maxAbs = std::abs(M[ii * N + ii]);\n                for (int k = i + 1; k < N_int; ++k) {\n                    double absVal = std::abs(M[static_cast<size_t>(k) * N + ii]);\n                    if (absVal > maxAbs) {\n                        maxAbs = absVal;\n                        pivotRow = static_cast<size_t>(k);\n                    }\n                }\n\n                /* Row swap if needed */\n                if (pivotRow != ii) {\n                    for (int j = 0; j < N_int; ++j) {\n                        std::swap(M[ii * N + j], M[pivotRow * N + j]);\n                    }\n                    std::swap(b_copy[ii], b_copy[pivotRow]);\n                }\n\n                /* Parallel elimination of rows below the pivot */\n                #pragma omp for schedule(static)\n                for (int k = i + 1; k < N_int; ++k) {\n                    const size_t kk = static_cast<size_t>(k);\n                    double c = -M[kk * N + ii] / M[ii * N + ii];\n                    M[kk * N + ii] = 0.0;\n                    double* rk = &M[kk * N + ii + 1];\n                    double* ri = &M[ii * N + ii + 1];\n                    for (size_t j = ii + 1; j < static_cast<size_t>(N); ++j, ++rk, ++ri) {\n                        *rk += c * (*ri);\n                    }\n                    b_copy[kk] += c * b_copy[ii];\n                }\n            }\n        }\n    }\n\n    /* Back substitution */\n    x.resize(N);\n    #pragma omp parallel\n    {\n        #pragma omp single\n        {\n            for (int i = N_int - 1; i >= 0; --i) {\n                const size_t ii = static_cast<size_t>(i);\n                double xi = b_copy[ii] / M[ii * N + ii];\n                x[ii] = xi;\n                #pragma omp for schedule(static)\n                for (int k = 0; k < i; ++k) {\n                    b_copy[k] -= M[k * N + ii] * xi;\n                }\n            }\n        }\n    }\n}\n",
  "54_stencil_game_of_life": "```cpp\n#pragma once\n#include <vector>\n\ninline void gameOfLife(std::vector<int> const& input,\n                       std::vector<int> &output,\n                       size_t N) {\n    const int  *in  = input.data();\n    int        *out = output.data();\n    const size_t n = N;\n\n    #pragma omp parallel for schedule(static)\n    for (size_t i = 0; i < n; ++i) {\n        size_t base   = i * n;\n        const int *rowCurr = in + base;\n        const int *rowPrev = (i > 0)   ? in + (i - 1) * n : nullptr;\n        const int *rowNext = (i + 1 < n) ? in + (i + 1) * n : nullptr;\n\n        for (size_t j = 0; j < n; ++j) {\n            int sum = 0;\n\n            // Vertical neighbors\n            if (rowPrev) sum += rowPrev[j];\n            if (rowNext) sum += rowNext[j];\n\n            // Horizontal neighbors\n            if (j > 0)      sum += rowCurr[j - 1];\n            if (j + 1 < n)  sum += rowCurr[j + 1];\n\n            // Diagonals (upper and lower)\n            if (rowPrev && j > 0)      sum += rowPrev[j - 1];\n            if (rowPrev && j + 1 < n)  sum += rowPrev[j + 1];\n            if (rowNext && j > 0)      sum += rowNext[j - 1];\n            if (rowNext && j + 1 < n)  sum += rowNext[j + 1];\n\n            int cell = rowCurr[j];\n            out[base + j] =\n                (cell == 1) ? ((sum == 2 || sum == 3) ? 1 : 0)\n                            : ((sum == 3) ? 1 : 0);\n        }\n    }\n}\n```",
  "42_sort_sorted_ranks": "```cpp\n#pragma once\n#include <algorithm>\n#include <numeric>\n#include <vector>\n\nvoid ranks(std::vector<float> const& x, std::vector<size_t> &ranks_) {\n    const size_t n = x.size();\n    if (n == 0) return;                 // nothing to do for an empty vector\n\n    std::vector<size_t> indices(n);\n    std::iota(indices.begin(), indices.end(), static_cast<size_t>(0));\n\n    const float* px = x.data();\n    std::sort(indices.begin(), indices.end(),\n              [px](size_t a, size_t b) { return px[a] < px[b]; });\n\n    size_t* pr = ranks_.data();\n    #pragma omp parallel for schedule(static)\n    for (size_t i = 0; i < n; ++i) {\n        pr[indices[i]] = i;\n    }\n}\n```",
  "40_sort_sort_an_array_of_complex_numbers_by_magnitude": "```cpp\n#pragma once\n#include <complex>\n#include <vector>\n#include <algorithm>\n#if defined(_OPENMP)\n#include <omp.h>\n#endif\n\n// Comparator that uses squared magnitude to avoid sqrt.\ninline bool sort_by_magnitude(const std::complex<double>& a,\n                              const std::complex<double>& b) {\n    const double ar = a.real(), ai = a.imag();\n    const double br = b.real(), bi = b.imag();\n    return (ar*ar + ai*ai) < (br*br + bi*bi);\n}\n\n// Recursive parallel sort (taskbased merge sort).\ninline void parallel_sort_impl(std::vector<std::complex<double>>& v,\n                               size_t start, size_t end) {\n    static constexpr size_t threshold = 8192;          // base case size\n    size_t n = end - start;\n    if (n <= threshold) {\n        std::sort(v.begin() + start, v.begin() + end, sort_by_magnitude);\n        return;\n    }\n\n    size_t mid = start + n/2;\n\n#if defined(_OPENMP)\n    #pragma omp task shared(v) firstprivate(start, mid, end)\n    {\n        parallel_sort_impl(v, start, mid);\n    }\n    #pragma omp task shared(v) firstprivate(start, mid, end)\n    {\n        parallel_sort_impl(v, mid, end);\n    }\n    #pragma omp taskwait\n#else\n    parallel_sort_impl(v, start, mid);\n    parallel_sort_impl(v, mid, end);\n#endif\n\n    std::inplace_merge(v.begin() + start, v.begin() + mid,\n                       v.begin() + end, sort_by_magnitude);\n}\n\n// Public API \u2013 sorts by magnitude in ascending order.\ninline void sortComplexByMagnitude(std::vector<std::complex<double>>& x) {\n#if defined(_OPENMP)\n    #pragma omp parallel\n    {\n        #pragma omp single\n        {\n            parallel_sort_impl(x, 0, x.size());\n        }\n    }\n#else\n    std::sort(x.begin(), x.end(), sort_by_magnitude);\n#endif\n}\n```",
  "53_stencil_2d_jacobi_5-point_stencil": "```cpp\n#pragma once\n#include <vector>\n#include <omp.h>\n\nvoid jacobi2D(std::vector<double> const& input, std::vector<double>& output, size_t N) {\n    if (N == 0) return;\n    if (N == 1) {\n        output[0] = input[0] / 5.0;\n        return;\n    }\n\n    const double* in  = input.data();\n    double*       out = output.data();\n\n    #pragma omp parallel for schedule(static)\n    for (size_t i = 0; i < N; ++i) {\n        const double* __restrict__ cur   = in + i * N;\n        double*       __restrict__ outRow = out + i * N;\n\n        if (i == 0) {                     // Top row\n            const double* __restrict__ below = in + N;\n\n            outRow[0] = (cur[0] + cur[1] + below[0]) / 5.0;\n\n            #pragma omp simd\n            for (size_t j = 1; j + 1 < N; ++j) {\n                outRow[j] = (cur[j-1] + cur[j] + cur[j+1] + below[j]) / 5.0;\n            }\n\n            outRow[N-1] = (cur[N-1] + cur[N-2] + below[N-1]) / 5.0;\n\n        } else if (i == N-1) {            // Bottom row\n            const double* __restrict__ above = in + (N-2) * N;\n\n            outRow[0] = (cur[0] + cur[1] + above[0]) / 5.0;\n\n            #pragma omp simd\n            for (size_t j = 1; j + 1 < N; ++j) {\n                outRow[j] = (cur[j-1] + cur[j] + cur[j+1] + above[j]) / 5.0;\n            }\n\n            outRow[N-1] = (cur[N-1] + cur[N-2] + above[N-1]) / 5.0;\n\n        } else {                          // Middle rows\n            const double* __restrict__ above = in + (i-1) * N;\n            const double* __restrict__ below = in + (i+1) * N;\n\n            outRow[0] = (cur[0] + cur[1] + above[0] + below[0]) / 5.0;\n\n            #pragma omp simd\n            for (size_t j = 1; j + 1 < N; ++j) {\n                outRow[j] = (cur[j-1] + cur[j] + cur[j+1] + above[j] + below[j]) / 5.0;\n            }\n\n            outRow[N-1] = (cur[N-1] + cur[N-2] + above[N-1] + below[N-1]) / 5.0;\n        }\n    }\n}\n```\n",
  "36_search_check_if_array_contains_value": "```cpp\n#pragma once\n#include <vector>\n\ninline bool contains(std::vector<int> const& x, int target) {\n    const int *it  = x.data();\n    const int *end = it + x.size();\n\n    for (; it != end; ++it) {\n        if (__builtin_expect(*it == target, 0)) {\n            return true;\n        }\n    }\n    return false;\n}\n```\n",
  "39_search_xor_contains": "```cpp\n#pragma once\n#include <vector>\n\ninline bool containsVector(const std::vector<int> &vec, int val) {\n    constexpr size_t PARALLEL_THRESHOLD = 1024;\n    if (vec.size() < PARALLEL_THRESHOLD) {\n        for (size_t i = 0; i < vec.size(); ++i) {\n            if (vec[i] == val) return true;\n        }\n        return false;\n    }\n\n    bool found = false;\n    #pragma omp parallel for reduction(|| : found)\n    for (size_t i = 0; i < vec.size(); ++i) {\n        if (vec[i] == val) found = true;\n    }\n    return found;\n}\n\ninline bool xorContains(const std::vector<int> &x, const std::vector<int> &y, int val) {\n    return containsVector(x, val) ^ containsVector(y, val);\n}\n```\n",
  "38_search_find_the_first_even_number": "#pragma once\n#include <vector>\n\ninline size_t findFirstEven(std::vector<int> const& x) {\n    const size_t n = x.size();\n\n    /* For very small vectors the overhead of creating an OpenMP region\n       dominates the work, so fall back to a simple loop. Adjusting\n       the threshold depends on the expected workload; 8192 is a\n       reasonable compromise. */\n    constexpr size_t SEQ_THRESHOLD = 8192;\n    if (n <= SEQ_THRESHOLD) {\n        for (size_t i = 0; i < n; ++i) {\n            if ((x[i] & 1) == 0) {\n                return i;\n            }\n        }\n        return n;\n    }\n\n    size_t best = n;  // initialise with \"no even\" sentinel\n\n#pragma omp parallel for reduction(min:best)\n    for (size_t i = 0; i < n; ++i) {\n        if ((x[i] & 1) == 0) {\n            if (i < best) {\n                best = i;\n            }\n        }\n    }\n    return best;\n}\n",
  "35_search_search_for_last_struct_by_key": "```cpp\n#pragma once\n#include <vector>\n#include <string>\n#include <algorithm>\n\nstruct Book {\n    std::string title;\n    int pages;\n};\n\n/*\n * Return the index of the last Book item in the vector `books` where\n * Book.pages is less than 100.\n */\ninline size_t findLastShortBook(const std::vector<Book> &books)\n{\n    const size_t n = books.size();\n    if (n == 0) return 0;\n\n    const Book *const data = books.data();\n\n    /* Use a sequential scan for small vectors to avoid OpenMP overhead. */\n    const size_t threshold = 32768; /* tuned for typical workloads */\n    if (n < threshold)\n    {\n        for (size_t i = n; i-- > 0;)\n        {\n            if (data[i].pages < 100) return i;\n        }\n        return n;\n    }\n\n    long long lastFound = -1;  /* negative value signals \u201cno match\u201d */\n\n    #pragma omp parallel for schedule(static) reduction(max:lastFound)\n    for (long long i = 0; i < static_cast<long long>(n); ++i)\n    {\n        if (data[static_cast<size_t>(i)].pages < 100)\n        {\n            lastFound = std::max(lastFound, i);\n        }\n    }\n\n    return (lastFound >= 0) ? static_cast<size_t>(lastFound) : n;\n}\n```\n",
  "44_sort_sort_non-zero_elements": "```cpp\n#pragma once\n#include <algorithm>\n#include <vector>\n#include <omp.h>\n\ninline void sortIgnoreZero(std::vector<int> &x)\n{\n    const size_t n = x.size();\n    if (n < 2)\n        return;                            // nothing to sort for empty or single\u2011element vectors\n\n    const int numThreads = omp_get_max_threads();          // maximum number of threads available\n    const size_t chunkSize = (n + numThreads - 1) / numThreads; // ceil division\n\n    /* ------------------------------------------------------------\n       1. Count how many non\u2011zero elements each thread will see\n    ------------------------------------------------------------ */\n    std::vector<size_t> localCount(numThreads, 0);\n\n    #pragma omp parallel num_threads(numThreads)\n    {\n        const int tid   = omp_get_thread_num();\n        const size_t start = static_cast<size_t>(tid) * chunkSize;\n        const size_t end   = std::min(start + chunkSize, n);\n\n        size_t cnt = 0;\n        for (size_t i = start; i < end; ++i)\n            if (x[i] != 0)\n                ++cnt;\n\n        localCount[tid] = cnt;\n    }\n\n    /* ------------------------------------------------------------\n       2. Compute offsets: the starting index in the temporary\n          vector for each thread.\n    ------------------------------------------------------------ */\n    std::vector<size_t> offsets(numThreads, 0);\n    size_t total = 0;\n    for (int t = 0; t < numThreads; ++t) {\n        offsets[t] = total;\n        total += localCount[t];\n    }\n\n    if (total == 0)\n        return;                             // vector contains only zeros\n\n    /* ------------------------------------------------------------\n       3. Copy all non\u2011zero values into a temporary vector,\n          keeping the original relative order.\n    ------------------------------------------------------------ */\n    std::vector<int> nonZeroElements(total);\n\n    #pragma omp parallel num_threads(numThreads)\n    {\n        const int tid   = omp_get_thread_num();\n        const size_t start = static_cast<size_t>(tid) * chunkSize;\n        const size_t end   = std::min(start + chunkSize, n);\n        size_t idx = offsets[tid];\n\n        for (size_t i = start; i < end; ++i)\n            if (x[i] != 0)\n                nonZeroElements[idx++] = x[i];\n    }\n\n    /* ------------------------------------------------------------\n       4. Sort the temporary vector.\n    ------------------------------------------------------------ */\n    std::sort(nonZeroElements.begin(), nonZeroElements.end());\n\n    /* ------------------------------------------------------------\n       5. Write back the sorted values into the original positions.\n    ------------------------------------------------------------ */\n    #pragma omp parallel num_threads(numThreads)\n    {\n        const int tid   = omp_get_thread_num();\n        const size_t start = static_cast<size_t>(tid) * chunkSize;\n        const size_t end   = std::min(start + chunkSize, n);\n        size_t idx = offsets[tid];\n\n        for (size_t i = start; i < end; ++i)\n            if (x[i] != 0)\n                x[i] = nonZeroElements[idx++];\n    }\n}\n```\n",
  "10_geometry_convex_hull": "```cpp\n#pragma once\n#include <vector>\n#include <algorithm>\n#include <omp.h>\n#include <utility>\n\nstruct Point {\n    double x, y;\n};\n\nnamespace convex_hull_impl {\n    // Comparator for sorting by x, then y\n    inline bool cmp(const Point& a, const Point& b) {\n        return (a.x < b.x) || (a.x == b.x && a.y < b.y);\n    }\n\n    // Cross product (b-a) x (c-a)\n    inline double cross(const Point& a, const Point& b, const Point& c) {\n        return (b.x - a.x) * (c.y - a.y) - (b.y - a.y) * (c.x - a.x);\n    }\n\n    // Parallel quicksort using OpenMP tasks\n    inline void quickSort(std::vector<Point>& v, int left, int right) {\n        if (left >= right) return;\n        if (right - left + 1 < 64) {\n            std::sort(v.begin() + left, v.begin() + right + 1, cmp);\n            return;\n        }\n        int i = left, j = right;\n        const Point pivot = v[left + (right - left) / 2];\n        while (i <= j) {\n            while (cmp(v[i], pivot)) ++i;\n            while (cmp(pivot, v[j])) --j;\n            if (i <= j) {\n                std::swap(v[i++], v[j--]);\n            }\n        }\n        #pragma omp task shared(v) if (j - left > 8192)\n        quickSort(v, left, j);\n        #pragma omp task shared(v) if (right - i > 8192)\n        quickSort(v, i, right);\n        #pragma omp taskwait\n    }\n\n    inline void sortPointsParallel(std::vector<Point>& v) {\n        if (v.size() <= 1) return;\n        #pragma omp parallel\n        {\n            #pragma omp single\n            quickSort(v, 0, static_cast<int>(v.size()) - 1);\n        }\n    }\n}\n\ninline void convexHull(std::vector<Point> const& points, std::vector<Point> &hull) {\n    if (points.size() < 3) {\n        hull = points;\n        return;\n    }\n    std::vector<Point> pointsSorted = points;\n    convex_hull_impl::sortPointsParallel(pointsSorted);\n    const std::size_t n = pointsSorted.size();\n    std::vector<Point> ans;\n    ans.resize(2 * n);\n    std::size_t k = 0;\n    // Build lower hull\n    for (std::size_t i = 0; i < n; ++i) {\n        while (k >= 2 && convex_hull_impl::cross(ans[k-2], ans[k-1], pointsSorted[i]) <= 0.0) {\n            --k;\n        }\n        ans[k++] = pointsSorted[i];\n    }\n    // Build upper hull\n    std::size_t t = k + 1;\n    for (int i = static_cast<int>(n) - 1; i > 0; --i) {\n        while (k >= t && convex_hull_impl::cross(ans[k-2], ans[k-1], pointsSorted[i-1]) <= 0.0) {\n            --k;\n        }\n        ans[k++] = pointsSorted[i-1];\n    }\n    ans.resize(k - 1);\n    hull = std::move(ans);\n}\n```",
  "37_search_find_the_closest_number_to_pi": "```cpp\n#pragma once\n#include <vector>\n#include <cmath>\n#include <cstddef>\n\n#ifdef _OPENMP\n#include <omp.h>\n#endif\n\ninline size_t findClosestToPi(const std::vector<double>& x) {\n    constexpr double pi = 3.14159265358979323846;          // Pi value\n    const size_t n = x.size();\n    size_t bestIdx = 0;\n    double bestDiff = std::fabs(x[0] - pi);\n\n    bool parallel_done = false;\n#ifdef _OPENMP\n    constexpr size_t parallel_threshold = 8192;            // use parallelism only for large vectors\n    if (n > parallel_threshold) {\n#pragma omp parallel\n        {\n            unsigned int tid = omp_get_thread_num();\n            unsigned int nthreads = omp_get_num_threads();\n            const size_t chunk = (n + nthreads - 1) / nthreads;\n            const size_t start = tid * chunk;\n            const size_t local_end = (start + chunk > n) ? n : start + chunk;\n            const double* data = x.data();\n\n            size_t localIdx = start;\n            double localDiff = std::fabs(data[start] - pi);\n\n            for (size_t i = start + 1; i < local_end; ++i) {\n                double diff = std::fabs(data[i] - pi);\n                if (diff < localDiff) {\n                    localDiff = diff;\n                    localIdx = i;\n                }\n            }\n\n#pragma omp critical\n            {\n                if (localDiff < bestDiff ||\n                    (localDiff == bestDiff && localIdx < bestIdx)) {\n                    bestDiff = localDiff;\n                    bestIdx = localIdx;\n                }\n            }\n        }\n        parallel_done = true;                             // indicate that we already computed the result\n    }\n#endif\n\n    if (!parallel_done) {                                  // fall back to sequential\n        for (size_t i = 1; i < n; ++i) {\n            double diff = std::fabs(x[i] - pi);\n            if (diff < bestDiff) {\n                bestDiff = diff;\n                bestIdx = i;\n            }\n        }\n    }\n\n    return bestIdx;\n}\n```",
  "09_fft_fft_out_of_place": "```cpp\n#pragma once\n#include <vector>\n#include <complex>\n#include <cmath>\n#include <omp.h>\n\n#ifndef M_PI\n#define M_PI 3.14159265358979323846\n#endif\n\nvoid fft(const std::vector<std::complex<double>>& x, std::vector<std::complex<double>>& output) {\n    output = x;\n    const unsigned int N = static_cast<unsigned int>(output.size());\n    if (N <= 1) return;\n\n    double thetaT = 3.14159265358979323846264338328L / static_cast<double>(N);\n    std::complex<double> phiT(std::cos(thetaT), -std::sin(thetaT));\n\n    std::vector<std::complex<double>> twiddles;\n    twiddles.reserve(N >> 1);\n\n    unsigned int k = N;\n    while (k > 1) {\n        unsigned int n = k;\n        k >>= 1;\n        phiT *= phiT;                 // twiddle factor for this stage\n\n        twiddles.resize(k);\n        std::complex<double> w(1.0, 0.0);\n        twiddles[0] = w;\n        for (unsigned int l = 1; l < k; ++l) {\n            w *= phiT;\n            twiddles[l] = w;\n        }\n\n        #pragma omp parallel for schedule(static)\n        for (unsigned int l = 0; l < k; ++l) {\n            std::complex<double> T = twiddles[l];\n            std::complex<double>* pa = output.data() + l;\n            std::complex<double>* pb = pa + k;\n            for (unsigned int a = l; a < N; a += n) {\n                std::complex<double> t = *pa - *pb;\n                *pa += *pb;\n                *pb = t * T;\n                T *= phiT;\n                pa += n;\n                pb += n;\n            }\n        }\n    }\n\n    const unsigned int m = static_cast<unsigned int>(std::log2(static_cast<double>(N)));\n\n    #pragma omp parallel for schedule(static)\n    for (unsigned int a = 0; a < N; ++a) {\n        unsigned int b = a;\n        b = ((b & 0xaaaaaaaau) >> 1) | ((b & 0x55555555u) << 1);\n        b = ((b & 0xccccccccu) >> 2) | ((b & 0x33333333u) << 2);\n        b = ((b & 0xf0f0f0f0u) >> 4) | ((b & 0x0f0f0f0fu) << 4);\n        b = ((b & 0xff00ff00u) >> 8) | ((b & 0x00ff00ffu) << 8);\n        b = ((b >> 16) | (b << 16)) >> (32 - m);\n        if (b > a) {\n            std::complex<double> tmp = output[a];\n            output[a] = output[b];\n            output[b] = tmp;\n        }\n    }\n}\n```",
  "07_fft_fft_conjugate": "```cpp\n#pragma once\n#include <vector>\n#include <complex>\n#include <cmath>\n#include <cstdint>\n#include <omp.h>\n\nconstexpr double PI = 3.14159265358979323846264338328L;\n\nvoid fftConjugate(std::vector<std::complex<double>>& x)\n{\n    const size_t N = x.size();\n    if (N <= 1) {\n        if (N == 1) {\n            // Conjugate the single element\n            x[0].imag(-x[0].imag());\n        }\n        return;\n    }\n\n    std::complex<double>* data = x.data();\n\n    /* ---------- FFT (radix\u20112, decimation\u2011in\u2011time) ---------- */\n    size_t k = N;                      // current group size * 2\n    double thetaT = PI / static_cast<double>(N);\n    double phi_re = std::cos(thetaT);  // \u03c6\u2080 = cos(\u03c0/N) + i\u00b7(\u2013sin(\u03c0/N))\n    double phi_im = -std::sin(thetaT);\n\n    // Storage for twiddle factors of the current stage\n    std::vector<double> tw_re;\n    std::vector<double> tw_im;\n\n    while (k > 1) {\n        const size_t n = k;   // block size of the current stage\n        k >>= 1;              // half the block size for the next stage\n\n        /* \u03c6 = \u03c6\u00b2  (twiddle\u2011factor base for this stage) */\n        double tmp_re = phi_re * phi_re - phi_im * phi_im;\n        double tmp_im = phi_re * phi_im + phi_im * phi_re;\n        phi_re = tmp_re;\n        phi_im = tmp_im;\n\n        /* Build array of twiddle factors \u03c6\u2070, \u03c6\u00b9, \u2026, \u03c6\u207f\u207b\u00b9 (n=k) */\n        if (tw_re.size() < k) {\n            tw_re.resize(k);\n            tw_im.resize(k);\n        }\n        tw_re[0] = 1.0;\n        tw_im[0] = 0.0;\n        for (size_t i = 1; i < k; ++i) {\n            double r = tw_re[i - 1];\n            double im = tw_im[i - 1];\n            tw_re[i] = r * phi_re - im * phi_im;\n            tw_im[i] = r * phi_im + im * phi_re;\n        }\n\n        /* Parallel butterflies: one thread per group */\n        #pragma omp parallel for schedule(static)\n        for (size_t l = 0; l < k; ++l) {\n            const double T_re = tw_re[l];\n            const double T_im = tw_im[l];\n            for (size_t a = l; a < N; a += n) {\n                size_t b = a + k;\n\n                const double a_re = data[a].real();\n                const double a_im = data[a].imag();\n                const double b_re = data[b].real();\n                const double b_im = data[b].imag();\n\n                /* t = a - b */\n                const double t_re = a_re - b_re;\n                const double t_im = a_im - b_im;\n\n                /* a = a + b */\n                const double new_a_re = a_re + b_re;\n                const double new_a_im = a_im + b_im;\n\n                /* b = t * T */\n                const double new_b_re = t_re * T_re - t_im * T_im;\n                const double new_b_im = t_re * T_im + t_im * T_re;\n\n                data[a].real(new_a_re);\n                data[a].imag(new_a_im);\n                data[b].real(new_b_re);\n                data[b].imag(new_b_im);\n            }\n        }\n    }\n\n    /* ---------- Bit reversal (decimation) ---------- */\n    unsigned int m = 0;\n    {\n        unsigned int tmp = static_cast<unsigned int>(N);\n        while (tmp > 1) {\n            tmp >>= 1;\n            ++m;\n        }\n    }\n\n    #pragma omp parallel for schedule(static)\n    for (size_t idx = 0; idx < N; ++idx) {\n        uint32_t a = static_cast<uint32_t>(idx);\n        uint32_t b = a;\n        b = ((b & 0xaaaaaaaaU) >> 1) | ((b & 0x55555555U) << 1);\n        b = ((b & 0xccccccccU) >> 2) | ((b & 0x33333333U) << 2);\n        b = ((b & 0xf0f0f0f0U) >> 4) | ((b & 0x0f0f0f0fU) << 4);\n        b = ((b & 0xff00ff00U) >> 8) | ((b & 0x00ff00ffU) << 8);\n        b = ((b >> 16) | (b << 16)) >> (32 - m);\n        if (b > a) {\n            const std::complex<double> tmp = data[a];\n            data[a] = data[b];\n            data[b] = tmp;\n        }\n    }\n\n    /* ---------- Conjugation ---------- */\n    #pragma omp parallel for schedule(static)\n    for (size_t i = 0; i < N; ++i) {\n        data[i].imag(-data[i].imag());\n    }\n}\n```",
  "50_stencil_xor_kernel": "```cpp\n#pragma once\n#include <vector>\n#ifdef _OPENMP\n#include <omp.h>\n#endif\n\ninline void cellsXOR(std::vector<int> const& input, std::vector<int> &output, size_t N) {\n    if (N == 0) return;\n\n    const int *inp = input.data();\n    int *out   = output.data();\n    const size_t n = N;\n\n    #pragma omp parallel for schedule(static)\n    for (size_t i = 0; i < n; ++i) {\n        const size_t rowBase = i * n;\n        if (i == 0) { // Top row \u2013 no above neighbor\n            /* leftmost cell (j==0) */\n            size_t idx = rowBase;\n            int cnt = 0;\n            if (n > 1) cnt += (inp[idx + 1] == 1);\n            if (n > 1) cnt += (inp[idx + n] == 1);\n            out[idx] = (cnt == 1) ? 1 : 0;\n\n            /* interior columns j = 1 .. n-2 */\n            for (size_t j = 1; j + 1 < n; ++j) {\n                idx = rowBase + j;\n                int cntInt = (inp[idx - 1] == 1) + (inp[idx + 1] == 1) + (inp[idx + n] == 1);\n                out[idx] = (cntInt == 1) ? 1 : 0;\n            }\n\n            /* rightmost cell (j==n-1) */\n            if (n > 1) {\n                idx = rowBase + n - 1;\n                int cnt = 0;\n                if (n > 1) cnt += (inp[idx - 1] == 1);\n                if (n > 1) cnt += (inp[idx + n] == 1);\n                out[idx] = (cnt == 1) ? 1 : 0;\n            }\n        } else if (i == n - 1) { // Bottom row \u2013 no below neighbor\n            /* leftmost cell */\n            size_t idx = rowBase;\n            int cnt = 0;\n            if (n > 1) cnt += (inp[idx - n] == 1);\n            if (n > 1) cnt += (inp[idx + 1] == 1);\n            out[idx] = (cnt == 1) ? 1 : 0;\n\n            /* interior columns */\n            for (size_t j = 1; j + 1 < n; ++j) {\n                idx = rowBase + j;\n                int cntInt = (inp[idx - 1] == 1) + (inp[idx + 1] == 1) + (inp[idx - n] == 1);\n                out[idx] = (cntInt == 1) ? 1 : 0;\n            }\n\n            /* rightmost cell */\n            if (n > 1) {\n                idx = rowBase + n - 1;\n                int cnt = 0;\n                if (n > 1) cnt += (inp[idx - 1] == 1);\n                if (n > 1) cnt += (inp[idx - n] == 1);\n                out[idx] = (cnt == 1) ? 1 : 0;\n            }\n        } else { // Interior rows \u2013 have both above and below neighbors\n            /* leftmost cell */\n            size_t idx = rowBase;\n            int cnt = (inp[idx - n] == 1) + (inp[idx + n] == 1) + (inp[idx + 1] == 1);\n            out[idx] = (cnt == 1) ? 1 : 0;\n\n            /* interior columns */\n            for (size_t j = 1; j + 1 < n; ++j) {\n                idx = rowBase + j;\n                int cntInt = (inp[idx - n] == 1) + (inp[idx + n] == 1) +\n                             (inp[idx - 1] == 1) + (inp[idx + 1] == 1);\n                out[idx] = (cntInt == 1) ? 1 : 0;\n            }\n\n            /* rightmost cell */\n            if (n > 1) {\n                idx = rowBase + n - 1;\n                int cnt = (inp[idx - n] == 1) + (inp[idx + n] == 1) + (inp[idx - 1] == 1);\n                out[idx] = (cnt == 1) ? 1 : 0;\n            }\n        }\n    }\n}\n```\n",
  "43_sort_sort_an_array_of_structs_by_key": "```cpp\n#pragma once\n#include <vector>\n#include <algorithm>\n#ifdef _OPENMP\n#include <omp.h>\n#endif\n\nstruct Result {\n    int startTime;\n    int duration;\n    float value;\n};\n\ninline void sortByStartTime(std::vector<Result> &results) {\n    const size_t n = results.size();\n    if (n <= 1) return;\n\n    /* Initial block size for individual sorting. */\n    constexpr size_t BLOCK_SIZE = 8192;\n    const size_t blockSize = std::min<size_t>(BLOCK_SIZE, n);\n    const size_t numBlocks = (n + blockSize - 1) / blockSize;\n\n    /* Parallel sort of each block. */\n#pragma omp parallel for schedule(static)\n    for (size_t block = 0; block < numBlocks; ++block) {\n        const size_t start = block * blockSize;\n        const size_t end   = std::min(start + blockSize, n);\n        std::sort(results.begin() + start, results.begin() + end,\n                  [](const Result &a, const Result &b) noexcept { return a.startTime < b.startTime; });\n    }\n\n    /* Bottom\u2011up parallel merge. */\n    std::vector<Result> buffer;\n    buffer.resize(n);\n\n    std::vector<Result>* src = &results;\n    std::vector<Result>* dst = &buffer;\n    size_t width = blockSize;\n\n    while (width < n) {\n        size_t maxRight = 0;\n\n        /* Merge all non\u2011overlapping pairs of runs of length 'width'. */\n#pragma omp parallel for schedule(static) reduction(max : maxRight)\n        for (size_t left = 0; left + width < n; left += width * 2) {\n            const size_t mid   = left + width;\n            const size_t right = std::min(left + width * 2, n);\n\n            const auto &from = *src;\n            auto &to = *dst;\n\n            size_t i = left, j = mid, k = left;\n            while (i < mid && j < right) {\n                if (from[i].startTime < from[j].startTime) {\n                    to[k++] = from[i++];\n                } else {\n                    to[k++] = from[j++];\n                }\n            }\n            while (i < mid) to[k++] = from[i++];\n            while (j < right) to[k++] = from[j++];\n\n            if (right > maxRight) maxRight = right;\n        }\n\n        /* Copy any remaining elements that were not part of a merge. */\n        if (maxRight < n) {\n            for (size_t idx = maxRight; idx < n; ++idx) {\n                (*dst)[idx] = (*src)[idx];\n            }\n        }\n\n        /* Prepare for next pass. */\n        std::swap(src, dst);\n        width <<= 1;\n    }\n\n    /* If the final sorted data is in the buffer, copy it back. */\n    if (src != &results) {\n#pragma omp parallel for schedule(static)\n        for (size_t i = 0; i < n; ++i) {\n            results[i] = (*src)[i];\n        }\n    }\n}\n```"
}