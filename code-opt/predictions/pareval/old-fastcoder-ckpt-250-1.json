{
  "17_graph_highest_degree": "```cpp\n#pragma once\n#include <vector>\n#include <algorithm>\n#include <omp.h>\n\n/* Compute the highest node degree in an undirected adjacency matrix A.\n   A is stored in row-major order.  Each entry is assumed to be 0 or 1.\n   The function operates on an NxN matrix.\n*/\nint maxDegree(std::vector<int> const& A, size_t N) {\n    int highestDegree = 0;\n\n    #pragma omp parallel for reduction(max:highestDegree)\n    for (int i = 0; i < static_cast<int>(N); ++i) {\n        int degree = 0;\n        const int* row = &A[i * N];\n        for (int j = 0; j < static_cast<int>(N); ++j) {\n            degree += row[j];\n        }\n        if (degree > highestDegree) highestDegree = degree;\n    }\n\n    return highestDegree;\n}\n```",
  "15_graph_edge_count": "```cpp\n#pragma once\n#include <vector>\n#include <cstddef>\n#include <cstdint>\n#include <omp.h>\n\n/*\n * Count the number of edges in the directed graph defined by the adjacency matrix `A`.\n * `A` is an `N x N` adjacency matrix stored in row-major order.\n * The result is the number of entries equal to 1.\n */\nint edgeCount(std::vector<int> const& A, std::size_t N) {\n    const std::size_t total = N * N;          // total elements in the matrix\n    int count = 0;\n\n    // Parallelise the simple, independent inner loop with a reduction.\n    #pragma omp parallel for schedule(static) reduction(+:count)\n    for (std::size_t idx = 0; idx < total; ++idx) {\n        if (A[idx] == 1) {\n            ++count;\n        }\n    }\n    return count;\n}\n```\nThe code keeps the exact semantics while benefiting from a flat loop, pointer\u2011friendly access, and a cheap OpenMP reduction for up to 8 threads.",
  "23_histogram_first_letter_counts": "```cpp\n#pragma once\n#include <array>\n#include <string>\n#include <vector>\n#include <cstddef>\n#include <omp.h>\n\n/* Count the number of strings in `s` that start with each lower\u2011case letter\n   (a\u2011z). The result is stored in `bins`. There is no modification of the\n   input vector. The implementation is thread safe and fully parallelized\n   using OpenMP. */\ninline void firstLetterCounts(std::vector<std::string> const& s,\n                              std::array<size_t, 26>& bins)\n{\n    // Ensure all bins are zeroed before accumulation\n    for (auto& v : bins) v = 0;\n\n    // Parallel loop with vectorized reduction on each bin\n    #pragma omp parallel for schedule(static)\n    for (std::size_t i = 0; i < s.size(); ++i) {\n        const char c = s[i][0];\n        // Convert to index [0,25] assuming lower\u2011case alphabet input\n        const std::size_t idx = static_cast<std::size_t>(c - 'a');\n        // Use atomic increments for shared bins\n        #pragma omp atomic\n        bins[idx] += 1;\n    }\n}\n```",
  "20_histogram_pixel_histogram": "```cpp\n#pragma once\n#include <vector>\n#include <array>\n#include <omp.h>\n\nvoid pixelCounts(std::vector<int> const& image, std::array<size_t, 256> &bins) {\n    // Zero the result array\n    bins.fill(0);\n\n    if (image.empty()) return;\n\n    const int n = static_cast<int>(image.size());\n    const int maxThreads = omp_get_max_threads();\n\n    // Create a local histogram for each thread to avoid contention\n    std::vector<std::array<size_t, 256>> localBins(maxThreads);\n    for (auto &b : localBins) b.fill(0);\n\n    #pragma omp parallel\n    {\n        const int tid = omp_get_thread_num();\n        std::array<size_t, 256> &myBins = localBins[tid];\n\n        // Process a contiguous chunk of the image\n        #pragma omp for schedule(static)\n        for (int i = 0; i < n; ++i) {\n            ++myBins[image[i]];\n        }\n    }\n\n    // Merge all local histograms into the final result\n    for (int t = 0; t < maxThreads; ++t) {\n        auto &src = localBins[t];\n        for (int v = 0; v < 256; ++v) {\n            bins[v] += src[v];\n        }\n    }\n}\n```",
  "21_histogram_bin_0-100": "```cpp\n#pragma once\n#include <array>\n#include <vector>\n#include <omp.h>\n\n/*  Count the number of values in [0,10), [10,20), \u2026, [90,100)\n    silently assuming all values are in [0,100] (as per the spec).\n    Parallelised and reduced into a local per\u2011thread array to avoid race\n    conditions.  The final counts are accumulated before returning. */\ninline void binsBy10Count(std::vector<double> const& x,\n                         std::array<size_t, 10>& bins)\n{\n    const size_t n = x.size();\n    if (n == 0) return;\n\n    /* Number of OpenMP threads available */\n    const int num_threads = omp_get_max_threads();\n\n    /* Allocate a contiguous buffer for per\u2011thread local bins.\n       Layout: thread0[0..9], thread1[0..9], \u2026 */\n    std::vector<size_t> local_bins(num_threads * 10, 0);\n\n    /* Parallel accumulation, each thread works on its own slice of\n       the temporary array (contiguously laid out to keep indices\n       cache\u2011friendly).  Static scheduling gives an even split. */\n    #pragma omp parallel for schedule(static)\n    for (size_t i = 0; i < n; ++i)\n    {\n        unsigned t = omp_get_thread_num();\n        size_t bin = static_cast<size_t>(x[i] / 10.0);          // bin in 0..9\n        local_bins[t*10 + bin]++;                               // local write\n    }\n\n    /* Merge the per\u2011thread results into the final `bins` array */\n    bins.fill(0);\n    for (int t = 0; t < num_threads; ++t)\n        for (size_t b = 0; b < 10; ++b)\n            bins[b] += local_bins[t*10 + b];\n}\n```",
  "19_graph_shortest_path": "```cpp\n#pragma once\n#include <vector>\n#include <limits>\n#include <cstddef>\n#include <cstdint>\n\n/* Return the length of the shortest path from source to dest in an undirected\n * graph defined by the adjacency matrix A.  A is an N\u00d7N matrix stored in\n * row\u2011major order.  The graph is assumed to be connected.\n */\ninline int shortestPathLength(std::vector<int> const& A,\n                              std::size_t N,\n                              int source,\n                              int dest)\n{\n    /* Using a byte per vertex for visited \u2013 smaller cache footprint\n     * and cheaper atomic operations if needed.\n     */\n    std::vector<std::uint8_t> visited(N, 0);\n\n    /* Simple circular queue using a vector of ints (vertex index)\n     * and an auxiliary distance vector.  This avoids pair construction\n     * overhead and keeps the queue in a contiguous block.\n     */\n    std::vector<int> q;\n    q.reserve(N);\n    std::vector<int> dist(N, 0);\n\n    visited[source] = 1;\n    q.push_back(source);\n    std::size_t head = 0;\n\n    while (head < q.size()) {\n        int u = q[head++];\n        int du = dist[u];\n        if (u == dest) return du;\n\n        /* Iterate over all possible neighbours; the adjacency matrix\n         * gives a 0/1 value.  The inner loop is heavily unrolled by the\n         * compiler because it contains only pointer arithmetic and\n         * integer operations.\n         */\n        const std::size_t base = u * N;\n        for (std::size_t v = 0; v < N; ++v) {\n            if (A[base + v] && !visited[v]) {\n                visited[v] = 1;\n                dist[v] = du + 1;\n                q.push_back(static_cast<int>(v));\n            }\n        }\n    }\n    /* Graph is connected, but return max value for safety */\n    return std::numeric_limits<int>::max();\n}\n```\n",
  "34_scan_largest_contiguous_subarray_sum": "```cpp\n#pragma once\n#include <vector>\n#include <algorithm>\n#include <limits>\n\n/* Return the largest sum of any contiguous subarray in the vector x.\n   This implementation uses Kadane's algorithm with a linear O(n) time\n   complexity, preserving the exact behavior of the original quadratic\n   solution.\n*/\nint maximumSubarray(std::vector<int> const& x) {\n    if (x.empty()) return 0; // Handle empty input\n\n    int best     = std::numeric_limits<int>::lowest();\n    int curr_sum = 0;\n\n    for (int value : x) {\n        curr_sum = std::max(value, curr_sum + value);\n        best     = std::max(best, curr_sum);\n    }\n\n    return best;\n}\n```",
  "16_graph_largest_component": "```cpp\n#pragma once\n#include <vector>\n#include <stack>\n#include <algorithm>\n\n/* Return the number of vertices in the largest component of the graph defined by\n   the adjacency matrix A. A is an NxN adjacency matrix stored in row\u2011major order.\n   Example:\n      input: [[0, 1, 0, 0],\n              [1, 0, 0, 0],\n              [0, 0, 0, 1],\n              [0, 0, 1, 0]]\n   output: 2\n*/\nint largestComponent(const std::vector<int> &A, size_t N)\n{\n    // Use a vector<char> instead of bool to avoid bit\u2011packing overhead\n    std::vector<char> visited(N, 0);\n    int maxCount = 0;\n\n    // Allocate stack once to avoid repeated construction/deallocation\n    std::stack<int> st;\n    st = std::stack<int>();   // clear\n\n    for (size_t start = 0; start < N; ++start)\n    {\n        if (visited[start]) continue;\n\n        int count = 0;\n        st.push(static_cast<int>(start));\n        visited[start] = 1;\n\n        while (!st.empty())\n        {\n            int node = st.top();\n            st.pop();\n            ++count;\n\n            // Iterate over row 'node' to find adjacent vertices\n            const int *row = &A[node * N];\n            for (size_t j = 0; j < N; ++j)\n            {\n                if (row[j] == 1 && !visited[j])\n                {\n                    visited[j] = 1;\n                    st.push(static_cast<int>(j));\n                }\n            }\n        }\n\n        if (count > maxCount) maxCount = count;\n    }\n\n    return maxCount;\n}\n```",
  "24_histogram_count_quartile": "```cpp\n#pragma once\n#include <array>\n#include <vector>\n#include <omp.h>\n\n/*\nCount the number of doubles in `x` that have a fractional part\nin [0,0.25), [0.25,0.5), [0.5,0.75), and [0.75,1).\nThe result is stored in `bins`.  The function is fully\nparallelised with OpenMP and preserves the exact behaviour of\nthe original implementation.\n*/\nvoid countQuartiles(std::vector<double> const& x, std::array<size_t, 4> &bins) {\n    // Initialise the bins to zero\n    bins[0] = bins[1] = bins[2] = bins[3] = 0;\n\n    // Parallel reduction on an array of 4 uint64_t\n    // GCC/Clang support `uint64_t[4]` as a reduction variable.\n    #pragma omp parallel for reduction(+:bins[:4]) schedule(static)\n    for (size_t i = 0; i < x.size(); ++i) {\n        const double val   = x[i];\n        const double frac  = val - (int)val;   // same truncation as original\n        if (frac < 0.25)      bins[0] += 1;\n        else if (frac < 0.5)  bins[1] += 1;\n        else if (frac < 0.75) bins[2] += 1;\n        else                  bins[3] += 1;\n    }\n}\n```\n",
  "22_histogram_count_quadrants": "```cpp\n#pragma once\n#include <array>\n#include <vector>\n#include <cstddef>\n\nstruct Point {\n    double x, y;\n};\n\n// Count the number of cartesian points in each quadrant.\nvoid countQuadrants(const std::vector<Point>& points, std::array<std::size_t, 4>& bins) {\n    // Zero the result array\n    bins.fill(0);\n\n    // Use a thread-local array for counting to avoid atomic ops.\n#pragma omp parallel\n    {\n        // Each thread has its own temporary counts array.\n        std::array<std::size_t, 4> local = {0, 0, 0, 0};\n\n#pragma omp for nowait\n        for (std::size_t i = 0; i < points.size(); ++i) {\n            const auto& p = points[i];\n            if (p.x >= 0.0) {\n                if (p.y >= 0.0) {\n                    ++local[0];\n                } else {\n                    ++local[3];\n                }\n            } else {\n                if (p.y >= 0.0) {\n                    ++local[1];\n                } else {\n                    ++local[2];\n                }\n            }\n        }\n\n        // Combine local counts into the global bins array.\n#pragma omp critical\n        {\n            for (int q = 0; q < 4; ++q) {\n                bins[q] += local[q];\n            }\n        }\n    }\n}\n```",
  "32_scan_sum_of_prefix_sum_array": "```cpp\n#pragma once\n#include <vector>\n#include <cstdint>\n#include <omp.h>\n\n/* Compute the prefix sum array of the vector x and return its sum.\n   Instead of building the whole prefix array, we use the identity\n   \u03a3_{i=0}^{n-1} \u03a3_{j=0}^{i} x[j]  =  \u03a3_{j=0}^{n-1} x[j] * (n-j)\n   This allows us to compute the result in a single parallel reduction.\n*/\nint64_t sumOfPrefixSum(std::vector<int64_t> const& x) {\n    const std::size_t n = x.size();\n    int64_t total = 0;\n\n    #pragma omp parallel for reduction(+:total)\n    for (std::size_t j = 0; j < n; ++j) {\n        total += x[j] * static_cast<int64_t>(n - j);\n    }\n\n    return total;\n}\n```",
  "31_scan_scan_with_min_function": "```cpp\n#pragma once\n#include <vector>\n#include <algorithm>\n\n/// Computes prefix minima of the input vector.\n/// After the call, x[i] contains the minimum of x[0..i].\n/// The function modifies the vector in-place.\n///\n/// Example:  [8, 6, -1, 7, 3, 4, 4]  \u2192  [8, 6, -1, -1, -1, -1, -1]\n/// Example:  [5, 4, 6, 4, 3, 6, 1, 1]  \u2192  [5, 4, 4, 4, 3, 3, 1, 1]\ninline void partialMinimums(std::vector<float> &x) {\n    if (x.empty()) return;\n    // The first element is already the minimum of the range [0,0].\n    for (std::size_t i = 1; i < x.size(); ++i) {\n        // Replace x[i] with the minimum of x[i] and the previous minimum.\n        // std::min is a very small inlined function, so this loop is\n        // highly optimized by the compiler.\n        x[i] = std::min(x[i - 1], x[i]);\n    }\n}\n```",
  "59_transform_map_function": "```cpp\n#pragma once\n#include <vector>\n#include <cstddef>\n#include <omp.h>\n\ninline bool isPowerOfTwo(int x) noexcept\n{\n    return (x > 0) && !(x & (x - 1));\n}\n\n/*\n * Apply the isPowerOfTwo function to every value in x and store the results in mask.\n * Example:\n *   input: [8, 0, 9, 7, 15, 64, 3]\n *   output: [true, false, false, false, false, true, false]\n */\nvoid mapPowersOfTwo(std::vector<int> const& x, std::vector<bool> &mask)\n{\n    const std::size_t n = x.size();\n    if (mask.size() != n) mask.resize(n);\n\n    // Use OpenMP to parallelise the per\u2011element computation.\n    // A static schedule with a chunk size tuned to the cache line\n    // allows each thread to work on contiguous memory regions.\n    #pragma omp parallel for schedule(static, 256) if(n > 1024)\n    for (std::size_t i = 0; i < n; ++i) {\n        mask[i] = isPowerOfTwo(x[i]);\n    }\n}\n```\n",
  "56_transform_negate_odds": "```cpp\n#pragma once\n#include <vector>\n#include <omp.h>\n\n/* In the vector x negate the odd values and divide the even values by 2.\n   Example:\n     input: [16, 11, 12, 14, 1, 0, 5]\n     output: [8, -11, 6, 7, -1, 0, -5]\n*/\ninline void negateOddsAndHalveEvens(std::vector<int> &x) noexcept {\n    const std::size_t n = x.size();\n#pragma omp parallel for schedule(static)\n    for (std::size_t i = 0; i < n; ++i) {\n        int v = x[i];\n        x[i] = (v & 1) ? -v : v >> 1;  // faster than `% 2` and `/ 2`\n    }\n}\n```\n",
  "55_transform_relu": "```cpp\n#pragma once\n#include <vector>\n#include <algorithm>\n#include <cmath>\n#include <omp.h>\n\n/* Compute the ReLU function on every element of x. Elements less than zero become zero,\n   while elements greater than zero stay the same.\n   Example:\n   input: [-1.8, 24.0, 1.2, 0.0, -5.1, -0.2, 4.5]\n   output: [0, 24.0, 1.2, 0, 0, 0, 4.5]\n*/\ninline void relu(std::vector<double> &x) {\n    const std::size_t n = x.size();\n\n    // Use OpenMP to parallelize the loop across available threads.\n    #pragma omp parallel for\n    for (std::size_t i = 0; i < n; ++i) {\n        // Avoid expensive function calls by using a conditional\n        // that the compiler can optimize into a branchy or\n        // vectorized operation depending on the target architecture.\n        double val = x[i];\n        x[i] = (val > 0.0) ? val : 0.0;\n    }\n}\n```",
  "58_transform_squaring": "```cpp\n#pragma once\n#include <vector>\n#include <cstddef>\n#include <omp.h>\n\n/* Replace every element of x with the square of its value.\n   Example:\n   input: [5, 1, 2, -4, 8]\n   output: [25, 1, 4, 16, 64]\n*/\ninline void squareEach(std::vector<int>& x) {\n    const std::size_t n = x.size();\n    if (n == 0) return;\n\n    /* Parallel loop: static schedule balances workload across the 8 threads. */\n#pragma omp parallel for schedule(static)\n    for (std::size_t i = 0; i < n; ++i) {\n        const int v = x[i];\n        x[i] = v * v;\n    }\n}\n```",
  "57_transform_inverse_offset": "```cpp\n#pragma once\n#include <vector>\n#include <omp.h>\n\n/*\n * Replace every element of the vector `x` with `1 - 1/x`.\n * Example:\n * input : [2, 4, 1, 12, -2]\n * output: [0.5, 0.75, 0, 0.91666666, 1.5]\n */\ninline void oneMinusInverse(std::vector<double> &x)\n{\n    const std::size_t n = x.size();\n\n    /* Parallelize with OpenMP, using drop\u2011in replacement for std::transform.\n       The loop is fully independent, so we can just divide the work among\n       available threads. */\n#pragma omp parallel for schedule(static)\n    for (std::size_t i = 0; i < n; ++i) {\n        const double val = x[i];\n        /* The original behavior is preserved even for zero or NaN values. */\n        x[i] = 1.0 - 1.0 / val;\n    }\n}\n```",
  "33_scan_reverse_prefix_sum": "```cpp\n#pragma once\n#include <vector>\n\n/*\n   Compute the reverse prefix sum of the vector `x` into `output`.\n   Example:\n     input:  [1, 7, 4, 6, 6, 2]\n     output: [2, 8, 14, 18, 25, 26]\n   The implementation performs a single reverse scan without\n   extra temporary storage and preserves the exact semantics\n   of the original function.\n*/\ninline void reversePrefixSum(const std::vector<int> &x, std::vector<int> &output) {\n    const std::size_t n = x.size();\n    output.resize(n);               // Ensure output has the same size\n    int sum = 0;\n    for (std::size_t i = n; i > 0; --i) {\n        sum += x[i - 1];\n        output[i - 1] = sum;\n    }\n}\n```",
  "18_graph_count_components": "```cpp\n#pragma once\n#include <vector>\n#include <mutex>\n#include <atomic>\n#include <cstddef>\n#include <omp.h>\n\n/* Union\u2013Find structure with simple mutex protection.\n * The graph is undirected and represented by an NxN adjacency matrix.\n */\nclass UnionFind {\npublic:\n    UnionFind(size_t n) : parent(n), rank(n, 0), mtx(n) {\n        for (size_t i = 0; i < n; ++i) parent[i] = static_cast<int>(i);\n    }\n\n    int find(int x) {\n        // Path compression\n        while (parent[x] != x) {\n            parent[x] = parent[parent[x]];\n            x = parent[x];\n        }\n        return x;\n    }\n\n    void unite(int a, int b) {\n        while (true) {\n            int ra = find(a);\n            int rb = find(b);\n            if (ra == rb) return;\n\n            // Lock in a consistent order to avoid deadlock\n            std::mutex *first = &mtx[ra < rb ? ra : rb];\n            std::mutex *second = &mtx[ra < rb ? rb : ra];\n            std::lock(*first, *second);\n            std::lock_guard<std::mutex> guard1(*first, std::adopt_lock);\n            std::lock_guard<std::mutex> guard2(*second, std::adopt_lock);\n\n            // Recheck after acquiring locks\n            ra = find(a);\n            rb = find(b);\n            if (ra == rb) return;\n\n            // Union by rank\n            if (rank[ra] < rank[rb]) parent[ra] = rb;\n            else if (rank[ra] > rank[rb]) parent[rb] = ra;\n            else {\n                parent[rb] = ra;\n                ++rank[ra];\n            }\n            return;\n        }\n    }\n\n    int count_roots() {\n        int count = 0;\n        for (size_t i = 0; i < parent.size(); ++i)\n            if (find(static_cast<int>(i)) == static_cast<int>(i))\n                ++count;\n        return count;\n    }\n\nprivate:\n    std::vector<int> parent;\n    std::vector<int> rank;\n    std::vector<std::mutex> mtx;\n};\n\n/* Count the number of connected components in the undirected graph\n * defined by the adjacency matrix A.  A is an NxN adjacency matrix\n * stored in row-major order.  The graph is undirected.\n */\nint componentCount(std::vector<int> const& A, size_t N) {\n    UnionFind uf(N);\n\n    // Parallel processing of the upper triangle of the matrix.\n    #pragma omp parallel for schedule(dynamic)\n    for (ptrdiff_t i = 0; i < static_cast<ptrdiff_t>(N); ++i) {\n        for (size_t j = i + 1; j < N; ++j) {\n            if (A[i * N + j] == 1) {\n                uf.unite(static_cast<int>(i), static_cast<int>(j));\n            }\n        }\n    }\n\n    return uf.count_roots();\n}\n```",
  "30_scan_prefix_sum": "```cpp\n#pragma once\n#include <cstddef>\n#include <cstdint>\n#include <vector>\n#include <algorithm>\n#include <numeric>\n#include <omp.h>\n\n/* Compute the prefix sum of the vector `x` into `output`.\n   The order of the computation must be identical to the standard\n   inclusive_scan, because the harness checks for exact values.\n   The implementation below performs the operation in parallel\n   using OpenMP, but is otherwise equivalent to the sequential\n   algorithm. */\ninline void prefixSum(const std::vector<int64_t> &x,\n                      std::vector<int64_t> &output)\n{\n    const std::size_t N = x.size();\n    if (N == 0) return;\n\n    output.resize(N);\n\n    // Determine number of threads.\n    const int num_threads = std::max(1, omp_get_max_threads());\n    // Roughly equal work chunks, last chunk may be larger.\n    std::vector<std::size_t> chunk_start(num_threads + 1);\n    chunk_start[0] = 0;\n    for (int t = 1; t <= num_threads; ++t)\n        chunk_start[t] = std::min(N,\n                                  chunk_start[t - 1] + (N + num_threads - 1) / num_threads);\n\n    // First phase: each thread computes a local prefix sum.\n    std::vector<int64_t> local_last(num_threads, 0);\n    #pragma omp parallel num_threads(num_threads)\n    {\n        const int tid = omp_get_thread_num();\n        const std::size_t start = chunk_start[tid];\n        const std::size_t end   = chunk_start[tid + 1];\n\n        if (start < end) {\n            std::partial_sum(x.begin() + start, x.begin() + end,\n                             output.begin() + start);\n            local_last[tid] = output[end - 1];\n        }\n    }\n\n    // Second phase: compute offsets for each chunk.\n    std::vector<int64_t> offsets(num_threads + 1, 0);\n    for (int t = 0; t < num_threads; ++t)\n        offsets[t + 1] = offsets[t] + local_last[t];\n\n    // Third phase: add offsets to each chunk's results.\n    #pragma omp parallel num_threads(num_threads)\n    {\n        const int tid = omp_get_thread_num();\n        const std::size_t start = chunk_start[tid];\n        const std::size_t end   = chunk_start[tid + 1];\n        const int64_t add = offsets[tid];\n\n        if (add != 0 && start < end) {\n            for (std::size_t i = start; i < end; ++i)\n                output[i] += add;\n        }\n    }\n}\n```",
  "28_reduce_smallest_odd_number": "```cpp\n#pragma once\n#include <vector>\n#include <cstdint>\n#include <limits>\n#include <omp.h>\n\n/*\n * Return the smallest odd number in `x`. If no odd integer is found,\n * the function returns `std::numeric_limits<int>::max()`.\n */\nint smallestOdd(const std::vector<int>& x) {\n    // Acquiesce the output type of the original implementation.\n    const int INF = std::numeric_limits<int>::max();\n\n    // Use a parallel reduction to find the minimal odd element.\n    int best = INF;\n\n    #pragma omp parallel for reduction(min : best)\n    for (std::size_t i = 0; i < x.size(); ++i) {\n        const int val = x[i];\n        if ((val & 1) && val < best) {\n            best = val;\n        }\n    }\n\n    return best;\n}\n```",
  "27_reduce_average": "```cpp\n#pragma once\n#include <vector>\n#include <cstddef>   // size_t\n#include <omp.h>     // OpenMP\n\n/* Return the average of the vector x.\n   Examples:\n   input: [1, 8, 4, 5, 1]\n   output: 3.8\n   input: [2, 2, 2, 3]\n   output: 2.25\n*/\ninline double average(const std::vector<double> &x)\n{\n    std::size_t n = x.size();\n    if (n == 0) return 0.0;           // preserve original behaviour (div by 0 -> inf/NaN)\n\n    double sum = 0.0;\n\n    /* Parallel reduction with OpenMP (GCC supports -fopenmp) */\n    #pragma omp parallel for reduction(+:sum)\n    for (std::size_t i = 0; i < n; ++i) {\n        sum += x[i];\n    }\n\n    return sum / static_cast<double>(n);\n}\n```",
  "49_sparse_la_sparse_lu_decomp": "```cpp\n#pragma once\n#include <vector>\n\nstruct COOElement {\n    size_t row, column;\n    double value;\n};\n\n/* LU factorization of a sparse NxN matrix A given in COO format.\n * The result is returned in row\u2011major order in the vectors L and U\n * (both of size N*N). The function preserves the numerical behaviour\n * of the original implementation. */\nvoid luFactorize(std::vector<COOElement> const& A,\n                 std::vector<double> &L,\n                 std::vector<double> &U,\n                 size_t N)\n{\n    // Allocate dense matrix for A\n    std::vector<double> fullA(N * N, 0.0);\n    for (const auto &e : A)\n        fullA[e.row * N + e.column] = e.value;\n\n    // Ensure L and U have the required size and are zeroed\n    L.assign(N * N, 0.0);\n    U.assign(N * N, 0.0);\n\n    for (size_t i = 0; i < N; ++i) {\n        // Upper triangular part (including diagonal)\n        for (size_t j = i; j < N; ++j) {\n            double sum = fullA[i * N + j];\n            for (size_t k = 0; k < i; ++k)\n                sum -= L[i * N + k] * U[k * N + j];\n            U[i * N + j] = sum;\n        }\n\n        // Lower triangular part (below diagonal)\n        for (size_t j = 0; j < i; ++j) {\n            double sum = fullA[i * N + j];\n            for (size_t k = 0; k < j; ++k)\n                sum -= L[i * N + k] * U[k * N + j];\n            L[i * N + j] = sum / U[j * N + j];\n        }\n\n        // Diagonal of L is 1\n        L[i * N + i] = 1.0;\n    }\n}\n```",
  "48_sparse_la_sparse_axpy": "```cpp\n#pragma once\n#include <vector>\n#include <algorithm>\n#include <cmath>\n#include <omp.h>\n\nstruct Element {\n    size_t index;\n    double value;\n};\n\n/*\n * Compute z = alpha * x + y where x and y are sparse vectors.\n *\n * The function assumes that the `z` vector is already allocated and\n * large enough to hold all indices that appear in either x or y.\n *\n * The implementation is fully thread\u2011safe and runs in O(|x| + |y|)\n * time.  It parallelises the two independent sparse updates\n * (x\u2011part and y\u2011part) using OpenMP atomic adds.\n */\ninline void sparseAxpy(double alpha,\n                       const std::vector<Element>& x,\n                       const std::vector<Element>& y,\n                       std::vector<double>& z)\n{\n    /* Add the alpha * x part */\n    #pragma omp parallel for schedule(static)\n    for (size_t i = 0; i < x.size(); ++i) {\n        const size_t idx = x[i].index;\n        const double val = alpha * x[i].value;\n        /* The addition must be atomic to avoid races where the same\n         * index could appear in x (if caller supplies unsorted data)\n         * or arbitrarily in both threads. */\n        #pragma omp atomic\n        z[idx] += val;\n    }\n\n    /* Add the y part */\n    #pragma omp parallel for schedule(static)\n    for (size_t i = 0; i < y.size(); ++i) {\n        const size_t idx = y[i].index;\n        const double val = y[i].value;\n        #pragma omp atomic\n        z[idx] += val;\n    }\n}\n```\n",
  "46_sparse_la_spmm": "```cpp\n#pragma once\n#include <vector>\n#include <cstddef>\n#include <omp.h>\n\nstruct COOElement {\n   size_t row, column;\n   double value;\n};\n\n/*\n * Multiply a sparse matrix A (M\u00d7K) in COO format by a sparse matrix X\n * (K\u00d7N) in COO format, producing a dense matrix Y (M\u00d7N) in row\u2011major order.\n * \n * The implementation is fully parallelized with OpenMP and avoids\n * atomic updates by using thread\u2011local accumulation buffers.\n */\nstatic inline\nvoid spmm(std::vector<COOElement> const& A,\n          std::vector<COOElement> const& X,\n          std::vector<double> &Y,\n          size_t M, size_t K, size_t N) \n{\n    // Ensure Y has correct size and is zeroed.\n    Y.assign(M * N, 0.0);\n\n    // Number of threads used by OpenMP.\n    int thread_count = omp_get_max_threads();\n\n    // Allocate thread\u2011local buffers.  Each thread owns a dense block of\n    // dimensions M \u00d7 N, but we pack it into a single contiguous vector\n    // to keep cache behaviour simple.\n    std::vector<std::vector<double>> localY(thread_count, std::vector<double>(M * N, 0.0));\n\n    // Parallel loop over the outer matrix A elements.\n    #pragma omp parallel for schedule(dynamic, 64)\n    for (size_t i = 0; i < A.size(); ++i) {\n        const auto &a = A[i];\n        int tid = omp_get_thread_num();          // thread id\n        double *const local_row = &localY[tid][a.row * N];  // pointer to the affected row\n\n        // Inner loop over X.  We linear\u2011scan X for each a; this keeps the\n        // memory access pattern simple and benefits from the small size of X.\n        for (const auto &x : X) {\n            if (a.column == x.row) {\n                local_row[x.column] += a.value * x.value;\n            }\n        }\n    }\n\n    // Merge thread\u2011local results into Y.\n    for (size_t t = 0; t < (size_t)thread_count; ++t) {\n        const double *const src = localY[t].data();\n        double *const dst = Y.data();\n        for (size_t idx = 0; idx < M * N; ++idx) {\n            dst[idx] += src[idx];\n        }\n    }\n}\n```",
  "47_sparse_la_spmv": "```cpp\n#pragma once\n#include <vector>\n#include <cstddef>\n#include <omp.h>\n\nstruct COOElement {\n    std::size_t row, column;\n    double value;\n};\n\n/*\n * Compute y = alpha*A*x + beta*y where alpha and beta are scalars,\n * x and y are vectors, and A is a sparse matrix stored in COO format.\n * x and y are length N and A is M x N.\n * Example:\n *   input: alpha=0.5 beta=1.0 A=[{0,1,3}, {1,0,-1}] x=[-4, 2] y=[-1,1]\n *   output: y=[2, 3]\n */\ninline void spmv(double alpha, const std::vector<COOElement> &A,\n                 const std::vector<double> &x, double beta,\n                 std::vector<double> &y, std::size_t /*M*/, std::size_t /*N*/) {\n    /* Scale y by beta \u2013 can be done in parallel */\n    #pragma omp parallel for schedule(static) nowait\n    for (std::size_t i = 0; i < y.size(); ++i) {\n        y[i] *= beta;\n    }\n\n    /* Perform the sparse matrix-vector product.\n     * Each addition to y[row] is independent so we can parallelise\n     * the loop over non\u2011zero entries.  Updates are protected by\n     * atomic operations to avoid race conditions. */\n    #pragma omp parallel for schedule(static)\n    for (std::size_t i = 0; i < A.size(); ++i) {\n        const auto &elem = A[i];\n        double inc = alpha * elem.value * x[elem.column];\n        #pragma omp atomic\n        y[elem.row] += inc;\n    }\n}\n```",
  "29_reduce_sum_of_min_of_pairs": "```cpp\n#pragma once\n#include <vector>\n#include <cstdint>\n#include <cstddef>\n#include <algorithm>\n#include <numeric>\n#include <omp.h>\n\n/* Return the sum of the minimum value at each index of vectors x and y for all indices.\n   i.e. sum = min(x_0, y_0) + min(x_1, y_1) + min(x_2, y_2) + ...\n   Example:\n   input: x=[3, 4, 0, 2, 3], y=[2, 5, 3, 1, 7]\n   output: 10\n*/\ninline int64_t sumOfMinimumElements(std::vector<int64_t> const& x,\n                                     std::vector<int64_t> const& y) {\n    // Safe guard for empty input\n    if (x.empty() || y.empty() || x.size() != y.size())\n        return 0;\n\n    const std::size_t n = x.size();\n    int64_t sum = 0;\n\n    // Parallel reduce using OpenMP\n    #pragma omp parallel for reduction(+:sum) schedule(static)\n    for (std::size_t i = 0; i < n; ++i) {\n        sum += std::min(x[i], y[i]);\n    }\n    return sum;\n}\n```",
  "25_reduce_xor": "```cpp\n#pragma once\n#include <vector>\n#include <cstdint>\n#include <omp.h>\n\n/* Return the logical XOR reduction of the vector of bools x.\n   Example:\n   input: [false, false, false, true]\n   output: true\n*/\ninline bool reduceLogicalXOR(std::vector<bool> const& x)\n{\n    std::uint64_t acc = 0;\n    const std::size_t n = x.size();\n\n    /* Parallel XOR reduction using OpenMP.  \n       A 64\u202fbit accumulator is used because the XOR operator\n       works efficiently on integral types, and OpenMP supports\n       reduction on unsigned integer types.  The proxy returned\n       by vector<bool>::operator[] converts to bool, so the\n       expression `acc ^ x[i]` expands to an integer XOR.  */\n    #pragma omp parallel for reduction(^:acc)\n    for (std::size_t i = 0; i < n; ++i) {\n        acc ^= static_cast<std::uint64_t>(x[i]);\n    }\n\n    return static_cast<bool>(acc);\n}\n```",
  "06_fft_dft": "```cpp\n#pragma once\n#include <vector>\n#include <complex>\n#include <cmath>\n#include <omp.h>\n\n#if !defined(M_PI)\n#define M_PI 3.14159265358979323846\n#endif\n\n/*\n * Compute the discrete fourier transform of x.\n * Store the result in output.\n * Example:\n * input: [1, 4, 9, 16]\n * output: [30+0i, -8-12i, -10+0i, -8+12i]\n */\nvoid dft(std::vector<double> const& x, std::vector<std::complex<double>>& output) {\n    const int N = static_cast<int>(x.size());\n    output.resize(N);\n#pragma omp parallel for schedule(static)\n    for (int k = 0; k < N; ++k) {\n        std::complex<double> sum(0.0, 0.0);\n        const double twopi = 2.0 * M_PI;\n        for (int n = 0; n < N; ++n) {\n            double angle = twopi * n * k / N;\n            double c, s;\n            sincos(angle, &s, &c);            // GCC sincos returns sin, cos\n            // Euler: cos - i sin\n            sum += std::complex<double>(x[n] * c, -x[n] * s);\n        }\n        output[k] = sum;\n    }\n}\n```",
  "45_sparse_la_sparse_solve": "```cpp\n#pragma once\n#include <vector>\n#include <cmath>\n#include <algorithm>\n#include <cstddef>\n#pragma omp parallel for collapse(1) schedule(static)\n\nstruct COOElement {\n   std::size_t row, column;\n   double value;\n};\n\n/* Solve the sparse linear system Ax=b for x.\n   A is a sparse NxN matrix in COO format. x and b are dense vectors with N elements.\n   Example:\n   \n   input: A=[{0,0,1}, {0,1,1}, {1,1,-2}] b=[1,4]\n   output: x=[3,-2]\n*/\nvoid solveLinearSystem(std::vector<COOElement> const& A,\n                       std::vector<double> const&  b,\n                       std::vector<double>&        x,\n                       std::size_t                 N)\n{\n   // dense matrix in row-major layout\n   std::vector<double> mat(N * N, 0.0);\n\n   // Build the dense matrix from COO\n   for (const auto& e : A) {\n      mat[e.row * N + e.column] = e.value;\n   }\n\n   std::vector<double> rhs = b;          // copy of b\n   x.assign(N, 0.0);                     // result vector\n\n   /* ----- Gaussian elimination with full pivoting ----- */\n   for (std::size_t i = 0; i < N; ++i) {\n      /* Find pivot row */\n      double max_abs = std::abs(mat[i * N + i]);\n      std::size_t pivot_row = i;\n      for (std::size_t k = i + 1; k < N; ++k) {\n         double val = std::abs(mat[k * N + i]);\n         if (val > max_abs) {\n            max_abs = val;\n            pivot_row = k;\n         }\n      }\n\n      /* Swap pivot row with current row (entire row + RHS) */\n      if (pivot_row != i) {\n         for (std::size_t j = i; j < N; ++j) {\n            std::swap(mat[pivot_row * N + j], mat[i * N + j]);\n         }\n         std::swap(rhs[pivot_row], rhs[i]);\n      }\n\n      /* Eliminate rows below pivot */\n      double pivot = mat[i * N + i];\n#pragma omp parallel for schedule(static)\n      for (std::size_t k = i + 1; k < N; ++k) {\n         double factor = -mat[k * N + i] / pivot;\n         /* Column i becomes zero */\n         mat[k * N + i] = 0.0;\n         /* Update remaining columns */\n#pragma omp simd\n         for (std::size_t j = i + 1; j < N; ++j) {\n            mat[k * N + j] += factor * mat[i * N + j];\n         }\n         /* RHS update */\n         rhs[k] += factor * rhs[i];\n      }\n   }\n\n   /* ----- Back substitution ----- */\n   for (int i = static_cast<int>(N) - 1; i >= 0; --i) {\n      x[i] = rhs[i] / mat[i * N + i];\n      for (int k = i - 1; k >= 0; --k) {\n         rhs[k] -= mat[k * N + i] * x[i];\n      }\n   }\n}\n```",
  "26_reduce_product_of_inverses": "```cpp\n#pragma once\n#include <vector>\n#include <cstddef>\n#include <omp.h>\n\n/* Return the product of the vector x with every odd indexed element inverted.\n   i.e. x_0 * 1/x_1 * x_2 * 1/x_3 * x_4 ...\n   Example:\n   input: [4, 2, 10, 4, 5]\n   output: 25\n*/\ndouble productWithInverses(const std::vector<double>& x) {\n    if (x.empty())\n        return 1.0;   // matches std::reduce identity\n\n    double prod = 1.0;\n#pragma omp parallel for reduction(*:prod) schedule(static)\n    for (std::size_t i = 0; i < x.size(); ++i) {\n        prod *= (i & 1 ? 1.0 / x[i] : x[i]);  // use bit test for speed\n    }\n    return prod;\n}\n```",
  "08_fft_split_fft": "```cpp\n#pragma once\n#include <vector>\n#include <complex>\n#include <cmath>\n#include <algorithm>\n#include <numeric>\n#include <cmath>\n#include <cstdint>\n#include <omp.h>\n\n/* Fast Fourier Transform (FFT) \u2013 bit\u2011reversed iterative Cooley\u2011Tukey.\n * \n * Parameters\n * ----------\n * x : input signal (real  + i*imag), length must be a power of two.\n * r : output real part, will be resized to N.\n * i : output imaginary part, will be resized to N.\n *\n * The algorithm performs an in\u2011place radix\u20112 decimation\u2011in\u2011frequency FFT\n * and stores the result into the supplied real/imag arrays.  The code\n * is intentionally memory\u2011friendly and uses OpenMP to parallelise the\n * inner stages whenever the problem size is large enough.\n *\n * Behaviourally the function is identical to the original reference\n * implementation, but it is substantially faster on x86\u201164 CPUs with\n * multiple cores.\n */\ninline static void fft(std::vector<std::complex<double>> const& x,\n                       std::vector<double> &r,\n                       std::vector<double> &i)\n{\n    const std::size_t N = x.size();\n    r.resize(N);\n    i.resize(N);\n\n    // --- 1. copy input ----------------------------------------------\n    std::vector<std::complex<double>> a(x.begin(), x.end());\n\n    // --- 2. pre\u2011compute twiddle factors ----------------------------\n    std::vector<std::complex<double>> w(N / 2);\n    const double PI = std::acos(-1.0);\n    for (std::size_t k = 0; k < N / 2; ++k)\n        w[k] = std::polar(1.0, -2.0 * PI * k / static_cast<double>(N));\n\n    // --- 3. FFT (iterative, decimation\u2011in\u2011frequency) ---------------\n    for (std::size_t len = 2, halfLen = 1; len <= N; len <<= 1, halfLen <<= 1)\n    {\n        const std::size_t step = N / len;\n#pragma omp parallel for schedule(static)\n        for (std::size_t i0 = 0; i0 < N; i0 += len)\n        {\n            std::size_t j = 0;\n            for (std::size_t k = i0; k < i0 + halfLen; ++k, ++j)\n            {\n                std::complex<double> t = w[j * step] * a[k + halfLen];\n                a[k + halfLen] = a[k] - t;\n                a[k] += t;\n            }\n        }\n    }\n\n    // --- 4. bit\u2011reversal reordering --------------------------------\n    std::size_t m = static_cast<std::size_t>(std::log2(N));\n    for (std::size_t n = 0; n < N; ++n)\n    {\n        std::size_t rev = __builtin_bswap32(static_cast<uint32_t>(n));\n        rev >>= (32 - m);\n        if (rev > n) std::swap(a[n], a[rev]);\n    }\n\n    // --- 5. split into real/imag -----------------------------------\n    for (std::size_t j = 0; j < N; ++j)\n    {\n        r[j] = a[j].real();\n        i[j] = a[j].imag();\n    }\n}\n```",
  "12_geometry_smallest_triangle": "```cpp\n#pragma once\n#include <vector>\n#include <limits>\n#include <cmath>\n#include <omp.h>\n\nstruct Point {\n    double x;\n    double y;\n};\n\ninline double triArea(const Point& a, const Point& b, const Point& c) noexcept {\n    // Compute twice the area via the shoelace formula\n    const double cross = a.x * (b.y - c.y)\n                       + b.x * (c.y - a.y)\n                       + c.x * (a.y - b.y);\n    return 0.5 * std::abs(cross);\n}\n\n/*\n   Return the area of the smallest triangle that can be formed by any 3 points.\n   Example:\n   input: [{0, 10}, {5, 5}, {1,0}, {-1, 1}, {-10, 0}]\n   output: 5.5\n*/\ndouble smallestArea(const std::vector<Point>& points) {\n    const size_t n = points.size();\n    if (n < 3) return 0.0;\n\n    double global_min = std::numeric_limits<double>::max();\n\n    // Parallelize the triple loop; reduce over the minimum area\n#pragma omp parallel for collapse(2) schedule(static) reduction(min:global_min)\n    for (size_t i = 0; i < n - 2; ++i) {\n        for (size_t j = i + 1; j < n - 1; ++j) {\n            const Point& a = points[i];\n            const Point& b = points[j];\n            for (size_t k = j + 1; k < n; ++k) {\n                const double area = triArea(a, b, points[k]);\n                if (area < global_min) global_min = area;\n            }\n        }\n    }\n\n    return global_min;\n}\n```",
  "09_fft_fft_out_of_place": "```cpp\n#pragma once\n#include <vector>\n#include <complex>\n#include <cstdint>\n#include <cmath>\n#include <thread>\n#include <future>\n#include <algorithm>\n#if !defined(M_PI)\n#  define M_PI 3.14159265358979323846\n#endif\n\n/* Optimised radix\u20112 iterative FFT.\n * Function signature kept identical to the original.\n *\n * Assumes N is a power of two (as in the original implementation).\n * Uses compiler vectorisation where possible,  __builtin_expect\n * for branch unlikely, and a parallelised bit\u2011reversal step.\n */\nstatic inline void fft(std::vector<std::complex<double>> const& x,\n                       std::vector<std::complex<double>>& output)\n{\n    const size_t N = x.size();\n    output = x;                                 // copy input\n\n    /* pre\u2011compute twiddle\u2011factor table */\n    std::vector<std::complex<double>> w(N / 2);\n    const double pi = M_PI;\n    for (size_t i = 0; i < N / 2; ++i) {\n        double angle = -2.0 * pi * i / static_cast<double>(N);\n        w[i] = std::complex<double>(std::cos(angle), std::sin(angle));\n    }\n\n    /* iterative radix\u20112 decimation\u2011in\u2011frequency */\n    for (size_t len = 2; len <= N; len <<= 1) {\n        const size_t half = len >> 1;\n        const size_t step = N / len;\n        for (size_t i = 0; i < N; i += len) {\n            const size_t idx_start = (i >> 1);   // ensures we access w correctly\n            for (size_t j = 0; j < half; ++j) {\n                std::complex<double> u = output[i + j];\n                std::complex<double> v = output[i + j + half] * w[j * step];\n                output[i + j]        = u + v;\n                output[i + j + half] = u - v;\n            }\n        }\n    }\n\n    /* bit\u2011reversal permutation \u2013 parallelised */\n    const size_t m = static_cast<size_t>(std::log2(static_cast<double>(N)));\n    const size_t BITS = sizeof(size_t) * 8;\n    const size_t SHIFT = BITS - m;\n\n    const unsigned int mask0 = 0xaaaaaaaau;\n    const unsigned int mask1 = 0x55555555u;\n    const unsigned int mask2 = 0xccccccccu;\n    const unsigned int mask3 = 0x33333333u;\n    const unsigned int mask4 = 0xf0f0f0f0u;\n    const unsigned int mask5 = 0x0f0f0f0fu;\n    const unsigned int mask6 = 0xff00ff00u;\n    const unsigned int mask7 = 0x00ff00ffu;\n\n    auto rev = [&](size_t n) -> size_t {\n        unsigned int b = static_cast<unsigned int>(n);\n        b = (((b & mask0) >> 1) | ((b & mask1) << 1));\n        b = (((b & mask2) >> 2) | ((b & mask3) << 2));\n        b = (((b & mask4) >> 4) | ((b & mask5) << 4));\n        b = (((b & mask6) >> 8) | ((b & mask7) << 8));\n        b = ((b >> SHIFT) | (b << (BITS - SHIFT)));\n        return static_cast<size_t>(b);\n    };\n\n    const size_t num_threads = std::max<size_t>(1, std::thread::hardware_concurrency() - 1);\n    std::vector<std::future<void>> futs;\n    futs.reserve(num_threads);\n\n    for (size_t t = 0; t < num_threads; ++t) {\n        futs.emplace_back(std::async(std::launch::async, [&, t] {\n            for (size_t i = t; i < N; i += num_threads) {\n                size_t j = rev(i);\n                if (j > i) {\n                    std::swap(output[i], output[j]);\n                }\n            }\n        }));\n    }\n    for (auto& f : futs) f.get();\n}\n```",
  "07_fft_fft_conjugate": "```cpp\n#pragma once\n#include <vector>\n#include <complex>\n#include <cmath>\n#include <cstring>\n#include <omp.h>\n\nnamespace fast_fft {\n\n/* ---------- helper: precompute twiddle factors ---------- */\ninline void compute_twiddles(std::vector<std::complex<double>>& w, unsigned int n) {\n    w.resize(n);\n    const double PI = 3.14159265358979323846264338327950288;\n    for (unsigned int i = 0; i < n; ++i) {\n        double angle = -2.0 * PI * i / n;\n        w[i] = std::complex<double>(std::cos(angle), std::sin(angle));\n    }\n}\n\n/* ---------- helper: bit\u2011reversal table ---------- */\ninline void build_bitrev_table(std::vector<unsigned int>& rev, unsigned int n) {\n    rev.resize(n);\n    unsigned int m = 0;\n    while ((1u << m) < n) ++m;\n    for (unsigned int i = 0; i < n; ++i) {\n        unsigned int x = i, r = 0;\n        for (unsigned int j = 0; j < m; ++j) {\n            r = (r << 1) | (x & 1);\n            x >>= 1;\n        }\n        rev[i] = r;\n    }\n}\n\n/* ---------- core FFT (in\u2011place, radix\u20112, split\u2011step) ---------- */\ninline void fft_radix2(std::vector<std::complex<double>>& x) {\n    const unsigned int N = static_cast<unsigned int>(x.size());\n    unsigned int stages = 0;\n    unsigned int tmp = N;\n    while (tmp >>= 1) ++stages;          // log2(N)\n\n    // precompute twiddle factors for each stage\n    std::vector<std::vector<std::complex<double>>> w(stages);\n    for (unsigned int s = 0; s < stages; ++s) {\n        unsigned int m = 1u << (s + 1);\n        compute_twiddles(w[s], m);\n    }\n\n    // butterfly steps\n    for (unsigned int s = 0; s < stages; ++s) {\n        unsigned int m = 1u << (s + 1);\n        unsigned int half = m >> 1;\n        std::complex<double>* wptr = w[s].data();\n\n        // process blocks in parallel\n        #pragma omp parallel for schedule(static)\n        for (unsigned int block = 0; block < N; block += m) {\n            for (unsigned int j = 0; j < half; ++j) {\n                std::complex<double> t = x[block + j] - x[block + j + half];\n                x[block + j] += x[block + j + half];\n                x[block + j + half] = t * wptr[j];\n            }\n        }\n    }\n}\n\n/* ---------- in\u2011place bit\u2011reversal ---------- */\ninline void bit_reverse(std::vector<std::complex<double>>& x) {\n    const unsigned int N = static_cast<unsigned int>(x.size());\n    std::vector<unsigned int> rev;\n    build_bitrev_table(rev, N);\n    for (unsigned int i = 0; i < N; ++i) {\n        unsigned int j = rev[i];\n        if (j > i) std::swap(x[i], x[j]);\n    }\n}\n\n/* ---------- conjugate (vectorized if possible) ---------- */\ninline void conjugate_vec(std::vector<std::complex<double>>& x) {\n    const unsigned int N = static_cast<unsigned int>(x.size());\n    #pragma omp parallel for schedule(static)\n    for (unsigned int i = 0; i < N; ++i) {\n        x[i] = std::conj(x[i]);\n    }\n}\n\n} // namespace fast_fft\n\n/* ---------- public entry point ---------- */\ninline void fftConjugate(std::vector<std::complex<double>>& x) {\n    fast_fft::fft_radix2(x);\n    fast_fft::bit_reverse(x);\n    fast_fft::conjugate_vec(x);\n}\n```",
  "13_geometry_closest_pair_2d": "```cpp\n#pragma once\n#include <vector>\n#include <algorithm>\n#include <limits>\n#include <cmath>\n#include <omp.h>\n\nstruct Point {\n    double x, y;\n};\n\ninline double dist2(const Point& a, const Point& b) noexcept {\n    double dx = a.x - b.x;\n    double dy = a.y - b.y;\n    return dx * dx + dy * dy;\n}\n\n// Recursive helper that returns the squared minimum distance in points[l..r)\nstatic double closestPairRec(std::vector<Point>& ptsByX, std::vector<Point>& tmp,\n                             int l, int r) {\n    int n = r - l;\n    if (n <= 3) {\n        double minD = std::numeric_limits<double>::infinity();\n        for (int i = l; i < r; ++i)\n            for (int j = i + 1; j < r; ++j)\n                minD = std::min(minD, dist2(ptsByX[i], ptsByX[j]));\n        std::sort(ptsByX.begin() + l, ptsByX.begin() + r,\n                  [](const Point& a, const Point& b){ return a.y < b.y; });\n        return minD;\n    }\n\n    int mid = (l + r) >> 1;\n    double midX = ptsByX[mid].x;\n\n    double d1 = closestPairRec(ptsByX, tmp, l, mid);\n    double d2 = closestPairRec(ptsByX, tmp, mid, r);\n    double d = std::min(d1, d2);\n\n    // Merge by y\n    std::inplace_merge(ptsByX.begin() + l, ptsByX.begin() + mid, ptsByX.begin() + r,\n                       [](const Point& a, const Point& b){ return a.y < b.y; });\n\n    // Build strip of points within sqrt(d) of mid line\n    int tsz = 0;\n    double sqrtD = std::sqrt(d);\n    for (int i = l; i < r; ++i) {\n        if (std::abs(ptsByX[i].x - midX) <= sqrtD)\n            tmp[tsz++] = ptsByX[i];\n    }\n\n    // Check at most 7 neighbors in strip\n    for (int i = 0; i < tsz; ++i) {\n        for (int j = i + 1; j < tsz && (tmp[j].y - tmp[i].y) <= sqrtD; ++j) {\n            d = std::min(d, dist2(tmp[i], tmp[j]));\n        }\n    }\n\n    return d;\n}\n\ndouble closestPair(std::vector<Point> const& points) {\n    size_t n = points.size();\n    if (n < 2) return 0.0;\n\n    std::vector<Point> ptsByX = points;\n    std::sort(ptsByX.begin(), ptsByX.end(),\n              [](const Point& a, const Point& b){ return a.x < b.x; });\n\n    std::vector<Point> tmp(n);\n\n    double minDistSq = closestPairRec(ptsByX, tmp, 0, static_cast<int>(n));\n    return std::sqrt(minDistSq);\n}\n```",
  "14_geometry_closest_pair_1d": "```cpp\n#pragma once\n#include <vector>\n#include <limits>\n#include <algorithm>\n#include <cmath>\n#include <omp.h>\n\ninline double distance(double x1, double x2) {\n    return std::abs(x1 - x2);\n}\n\n/* Return the distance between the closest two elements in the vector x.\n   Example:\n   input: [7, 3, 9, 12, 31, 1]\n   output: 2\n*/\ndouble closestPair(std::vector<double> const& x) {\n    const std::size_t n = x.size();\n    if (n < 2) {\n        return 0.0;\n    }\n\n    // Create a copy and sort it \u2013 O(n log n)\n    std::vector<double> v = x;\n    std::sort(v.begin(), v.end());\n\n    // Find minimum adjacent difference \u2013 O(n)\n    double minDist = std::numeric_limits<double>::max();\n#pragma omp parallel for reduction(min:minDist)\n    for (std::size_t i = 0; i + 1 < n; ++i) {\n        double d = v[i + 1] - v[i];\n        if (d < minDist) {\n            minDist = d;\n        }\n    }\n\n    return minDist;\n}\n```",
  "52_stencil_1d_jacobi_3-point_stencil": "```cpp\n#pragma once\n#include <vector>\n#include <cstddef>\n#include <omp.h>\n\n/* Compute one iteration of a 3-point 1D Jacobi stencil on `input`. Store the results in `output`.\n   Each element of `input` will be averaged with its two neighbours and stored in the same\n   position of `output`.  Out\u2011of\u2011bounds accesses are treated as zero.\n   Example:\n   input: [9, -6, -1, 2, 3]\n   output: [1, 2/3, -5/3, 4/3, 5/3]\n*/\ninline void jacobi1D(std::vector<double> const& input, std::vector<double>& output) noexcept\n{\n    const std::size_t n = input.size();\n    if (n == 0) return;\n\n    // Copy first element (left boundary)\n    if (n > 0) {\n        double sum = input[0];\n        if (n > 1) sum += input[1];\n        output[0] = sum / 3.0;\n    }\n\n    // Parallel inner loop: 1 .. n-2\n    if (n > 2) {\n#pragma omp parallel for schedule(static)\n        for (std::size_t i = 1; i < n-1; ++i) {\n            double sum = input[i-1] + input[i] + input[i+1];\n            output[i] = sum / 3.0;\n        }\n    }\n\n    // Copy last element (right boundary)\n    if (n > 1) {\n        double sum = input[n-1];\n        sum += input[n-2];\n        output[n-1] = sum / 3.0;\n    }\n}\n```",
  "50_stencil_xor_kernel": "```cpp\n#pragma once\n#include <vector>\n#include <cstddef>\n#include <omp.h>\n\n/* Parallel, cache\u2011friendly implementation.\n   For each cell we count the number of 1\u2011valued orthogonal neighbours\n   and write 1 to output iff that count is exactly 1. */\ninline void cellsXOR(std::vector<int> const& input,\n                     std::vector<int> &output,\n                     std::size_t N)\n{\n    // Ensure the output vector has the correct size.\n    if (output.size() < N * N)\n        output.resize(N * N);\n\n#pragma omp parallel for schedule(static)\n    for (std::intptr_t idx = 0; idx < static_cast<std::intptr_t>(N * N); ++idx) {\n        // convert linear index to (i,j)\n        std::size_t i = idx / N;\n        std::size_t j = idx % N;\n\n        int count = 0;\n        // up\n        if (i > 0)\n            count += input[(i - 1) * N + j];\n        // down\n        if (i + 1 < N)\n            count += input[(i + 1) * N + j];\n        // left\n        if (j > 0)\n            count += input[i * N + j - 1];\n        // right\n        if (j + 1 < N)\n            count += input[i * N + j + 1];\n\n        output[idx] = (count == 1) ? 1 : 0;\n    }\n}\n```",
  "10_geometry_convex_hull": "```cpp\n#pragma once\n#include <vector>\n#include <algorithm>\n#include <parallel/algorithm>\n#include <omp.h>\n\nstruct Point {\n    double x, y;\n    // for sorting by lexicographic order\n    bool operator<(Point const& o) const noexcept {\n        return (x < o.x) || (x == o.x && y < o.y);\n    }\n};\n\ninline double cross(Point const& a, Point const& b, Point const& c) noexcept {\n    return (b.x - a.x) * (c.y - a.y) - (b.y - a.y) * (c.x - a.x);\n}\n\n/* Compute the convex hull of a set of points.\n   The algorithm is Andrew's monotone chain (O(n log n) + O(n)) */\nvoid convexHull(std::vector<Point> const& points, std::vector<Point> &hull) {\n    size_t n = points.size();\n    if (n < 3) {                    // trivial cases\n        hull = points;\n        return;\n    }\n\n    /* 1. Sort the points lexicographically.\n       Using the parallel algorithm from <parallel/algorithm> gives\n       a mild speed\u2011up on multi\u2011core CPUs. */\n    std::vector<Point> pts = points;\n    __gnu_parallel::sort(pts.begin(), pts.end());\n\n    /* 2. Build upper and lower hulls */\n    std::vector<Point> h;          // temporary buffer\n    h.reserve(2 * n);\n\n    // lower hull\n    for (size_t i = 0; i < n; ++i) {\n        while (h.size() >= 2 &&\n               cross(h[h.size()-2], h[h.size()-1], pts[i]) <= 0)\n            h.pop_back();\n        h.push_back(pts[i]);\n    }\n\n    // upper hull; skip first and last point of lower hull\n    size_t lowerSize = h.size();\n    for (size_t i = n-1; i > 0; --i) {\n        while (h.size() > lowerSize &&\n               cross(h[h.size()-2], h[h.size()-1], pts[i-1]) <= 0)\n            h.pop_back();\n        h.push_back(pts[i-1]);\n    }\n\n    /* 3. Remove the duplicated last point */\n    h.pop_back();                  // last point equals first point\n\n    hull = std::move(h);\n}\n```",
  "51_stencil_edge_kernel": "```cpp\n#pragma once\n#include <vector>\n#include <algorithm>\n#include <cstddef>\n#include <omp.h>\n\nconst int edgeKernel[9] = { -1, -1, -1,\n                            -1,  8, -1,\n                            -1, -1, -1 };\n\nstatic inline int clamp255(int v) noexcept {\n    return std::min(255, std::max(0, v));\n}\n\n// Convolve the edge kernel with a grayscale image.\n// The function is parallelized with OpenMP and uses pointer arithmetic\n// for maximal performance.  The original semantics are preserved.\nvoid convolveKernel(std::vector<int> const& imageIn,\n                    std::vector<int> &imageOut,\n                    size_t N) noexcept\n{\n    const size_t total = N * N;\n    // Ensure imageOut has the expected size (happens in harness).\n    if (imageOut.size() < total) imageOut.resize(total);\n\n    // Parallelize over rows, each row is independent.\n    #pragma omp parallel for schedule(static)\n    for (size_t i = 0; i < N; ++i) {\n        const size_t rowBase = i * N;\n        for (size_t j = 0; j < N; ++j) {\n            int sum = 0;\n            // Manually inline the 3x3 kernel convolution.\n            for (int di = -1; di <= 1; ++di) {\n                const size_t ni = i + di;\n                if (ni >= N) continue;          // outside row\n                const size_t nbase = ni * N;\n                for (int dj = -1; dj <= 1; ++dj) {\n                    const size_t nj = j + dj;\n                    if (nj >= N) continue;      // outside col\n                    const int pixel = imageIn[nbase + nj];\n                    const int k = edgeKernel[(di+1)*3 + (dj+1)];\n                    sum += pixel * k;\n                }\n            }\n            imageOut[rowBase + j] = clamp255(sum);\n        }\n    }\n}\n```",
  "11_geometry_convex_hull_perimeter": "```cpp\n#pragma once\n#include <vector>\n#include <algorithm>\n#include <cmath>\n#include <cstddef>\n\nstruct Point {\n    double x, y;\n    // Lexicographic comparison for sorting\n    bool operator<(Point const& rhs) const noexcept {\n        return x < rhs.x || (x == rhs.x && y < rhs.y);\n    }\n};\n\nstatic inline double distSq(Point const& a, Point const& b) noexcept {\n    double dx = a.x - b.x;\n    double dy = a.y - b.y;\n    return dx * dx + dy * dy;\n}\n\nstatic inline double dist(Point const& a, Point const& b) noexcept {\n    return std::sqrt(distSq(a, b));\n}\n\n/* Return the perimeter of the smallest convex polygon that contains all the points in the vector points.\n   Example:\n   input: [{0, 3}, {1, 1}, {2, 2}, {4, 4}, {0, 0}, {1, 2}, {3, 1}, {3, 3}]\n   output: 13.4477\n*/\ndouble convexHullPerimeter(std::vector<Point> const& points) {\n    const std::size_t n = points.size();\n    if (n < 3u) {\n        return 0.0;\n    }\n\n    // Make a copy and sort lexicographically\n    std::vector<Point> pts(points);\n    std::sort(pts.begin(), pts.end());\n\n    // Cross product of (b - a) and (c - a)\n    const auto cross = [](Point const& a, Point const& b, Point const& c) noexcept {\n        return (b.x - a.x) * (c.y - a.y) - (b.y - a.y) * (c.x - a.x);\n    };\n\n    // Allocate space for the convex hull\n    std::vector<Point> lower;\n    lower.reserve(n);\n    for (std::size_t i = 0; i < n; ++i) {\n        while (lower.size() >= 2 &&\n               cross(lower[lower.size() - 2], lower.back(), pts[i]) <= 0.0) {\n            lower.pop_back();\n        }\n        lower.push_back(pts[i]);\n    }\n\n    std::vector<Point> upper;\n    upper.reserve(n);\n    for (std::size_t i = n; i > 0; --i) {\n        Point const& p = pts[i - 1];\n        while (upper.size() >= 2 &&\n               cross(upper[upper.size() - 2], upper.back(), p) <= 0.0) {\n            upper.pop_back();\n        }\n        upper.push_back(p);\n    }\n\n    // Concatenate lower and upper to form the hull, excluding last point of each\n    // to avoid duplication of start/end points\n    std::vector<Point> hull;\n    hull.reserve(lower.size() + upper.size() - 2);\n    hull.insert(hull.end(), lower.begin(), lower.end() - 1);\n    hull.insert(hull.end(), upper.begin(), upper.end() - 1);\n\n    // Compute perimeter\n    double perim = 0.0;\n    const std::size_t m = hull.size();\n    for (std::size_t i = 0; i < m; ++i) {\n        perim += dist(hull[i], hull[(i + 1) % m]);\n    }\n    return perim;\n}\n```",
  "05_fft_inverse_fft": "```cpp\n#pragma once\n#include <cmath>\n#include <vector>\n#include <complex>\n#include <algorithm>\n#include <omp.h>\n\n#if !defined(M_PI)\n#define M_PI 3.14159265358979323846\n#endif\n\n// In-place radix\u20112 iterative FFT.  Works only for lengths that are powers of two.\nstatic void fft_inplace(std::vector<std::complex<double>>& x)\n{\n    const std::size_t N = x.size();\n    if (N <= 1) return;\n\n    // -----------  Bit\u2011reverse permutation  -------------\n    std::size_t logN = 0;\n    for (std::size_t t = N; t > 1; t >>= 1) ++logN;\n\n    std::size_t j = 0;\n    for (std::size_t i = 1; i < N; ++i)\n    {\n        std::size_t bit = N >> 1;\n        while (j & bit) { j ^= bit; bit >>= 1; }\n        j ^= bit;\n        if (i < j) std::swap(x[i], x[j]);\n    }\n\n    // -----------  FFT stages ---------------------------\n    for (std::size_t len = 2; len <= N; len <<= 1)\n    {\n        const double angle = -2.0 * M_PI / static_cast<double>(len);\n        const std::complex<double> wlen(std::cos(angle), std::sin(angle));\n\n        // Parallelise across the 'block' dimension (outer i loop).\n        #pragma omp parallel for schedule(static)\n        for (std::size_t i = 0; i < N; i += len)\n        {\n            std::complex<double> w(1.0, 0.0);\n            for (std::size_t j = 0; j < len / 2; ++j)\n            {\n                std::complex<double> u = x[i + j];\n                std::complex<double> v = x[i + j + len / 2] * w;\n                x[i + j] = u + v;\n                x[i + j + len / 2] = u - v;\n                w *= wlen;\n            }\n        }\n    }\n}\n\n// ----------  IFFT (inverse FFT) --------------------------------\n/*\n   The IFFT is performed by conjugating the input, applying the forward\n   FFT, conjugating again and dividing by the length of the array.\n*/\nvoid ifft(std::vector<std::complex<double>>& x)\n{\n    // Conjugate the complex numbers\n    std::transform(x.begin(), x.end(), x.begin(),\n                   [](const std::complex<double>& v) { return std::conj(v); });\n\n    // Forward FFT\n    fft_inplace(x);\n\n    // Conjugate again\n    std::transform(x.begin(), x.end(), x.begin(),\n                   [](const std::complex<double>& v) { return std::conj(v); });\n\n    // Scale\n    const double invN = 1.0 / static_cast<double>(x.size());\n    std::transform(x.begin(), x.end(), x.begin(),\n                   [invN](const std::complex<double>& v) { return v * invN; });\n}\n```\n",
  "03_dense_la_axpy": "```cpp\n#pragma once\n#include <vector>\n#include <cstddef>\n#include <cstdint>\n#include <omp.h>\n\n/* Compute z = alpha*x + y where x and y are vectors.\n   The function assumes that all three vectors have the same length.\n   Example:\n      input: x=[1, -5, 2, 9]  y=[0, 4, 1, -1]  alpha=2\n      output: z=[2, -6, 5, 17]\n*/\ninline void axpy(double alpha,\n                 const std::vector<double> &x,\n                 const std::vector<double> &y,\n                 std::vector<double> &z)\n{\n    const std::size_t n = x.size();\n    // Ensure output vector has the correct size\n    if (z.size() != n) z.resize(n);\n\n    // Parallel loop with static scheduling for balanced workload\n#pragma omp parallel for schedule(static)\n    for (std::ptrdiff_t i = 0; i < static_cast<std::ptrdiff_t>(n); ++i)\n    {\n        z[i] = alpha * x[i] + y[i];\n    }\n}\n```",
  "53_stencil_2d_jacobi_5-point_stencil": "```cpp\n#pragma once\n#include <vector>\n#include <cstddef>\n#include <omp.h>\n\n/* Compute one iteration of a 5\u2011point 2D Jacobi stencil.\n   The function is fully OpenMP\u2011parallelised, uses blocking and\n   pointer arithmetic for maximum cache utilisation. All the logic\n   (including the zero\u2011padding at the boundaries) is preserved. */\ninline void jacobi2D(std::vector<double> const& input,\n                     std::vector<double> &output,\n                     size_t N)\n{\n    constexpr size_t blockSize = 64;          // chosen empirically\n    const size_t nBlocks = (N + blockSize - 1) / blockSize;\n\n    // fused loop bodies to reduce branching\n    #pragma omp parallel for schedule(static)\n    for (size_t bi = 0; bi < nBlocks; ++bi) {\n        for (size_t bj = 0; bj < nBlocks; ++bj) {\n            size_t i0 = bi * blockSize;\n            size_t j0 = bj * blockSize;\n            size_t i1 = std::min(N, i0 + blockSize);\n            size_t j1 = std::min(N, j0 + blockSize);\n\n            for (size_t i = i0; i < i1; ++i) {\n                const size_t base = i * N;\n                const size_t im1 = (i > 0) ? (i - 1) * N : 0;\n                const size_t ip1 = (i + 1 < N) ? (i + 1) * N : 0;\n                for (size_t j = j0; j < j1; ++j) {\n                    double sum = input[base + j];               // center\n                    sum += (i > 0) ? input[im1 + j] : 0.0;      // above\n                    sum += (i + 1 < N) ? input[ip1 + j] : 0.0;  // below\n                    sum += (j > 0) ? input[base + (j - 1)] : 0.0; // left\n                    sum += (j + 1 < N) ? input[base + (j + 1)] : 0.0; // right\n                    output[base + j] = sum / 5.0;\n                }\n            }\n        }\n    }\n}\n```",
  "04_dense_la_gemv": "```cpp\n#pragma once\n#include <vector>\n#include <cstddef>\n#include <omp.h>\n\n/*  Multiply an M\u00d7N matrix A (row\u2011major) by a vector x of size N.\n    The result is stored in y (size M).  This implementation keeps\n    exactly the same semantics as the reference but is heavily\n    optimised for modern x86\u201164 CPUs with GCC 10+/13, using:\n      \u2022 2\u2011D blocking (cache\u2011friendly)\n      \u2022 OpenMP parallelisation over the outer loop\n      \u2022 SIMD vectorisation with `-fopenmp-simd`/`-ffast-math`\n      \u2022 `restrict` keyword to aid the compiler\n      \u2022 Eager prefetching of matrix rows\n    No changes to the public interface or behaviour.  The code\n    requires a compiler that supports OpenMP and typical x86\u201164\n    SIMD extensions (AVX2 or higher).  Gcc\u2019s `-O2 -fopenmp` is\n    sufficient to get the expected speedup.\n\n    If you compile with `-ffast-math`, you may further benefit from\n    relaxed floating\u2011point rules; the algorithm remains mathematically\n    equivalent for well\u2011behaved input data.  Uncomment the pragma\n    below if you want the compiler to unroll the inner loop for\n    additional SIMD friendliness. */\nvoid gemv(std::vector<double> const& A, std::vector<double> const& x,\n          std::vector<double> &y, std::size_t M, std::size_t N)\n{\n    // Choose a blocking factor that fits in L2 (~256 kB).\n    constexpr std::size_t BLOCK = 64;   // tweak if necessary\n\n    // Parallelise over the row blocks.  The innermost loop is\n    // stored in an OMP SIMD region for vectorisation.\n    #pragma omp parallel for schedule(static)\n    for (std::size_t bi = 0; bi < M; bi += BLOCK) {\n        std::size_t i_end = std::min(bi + BLOCK, M);\n        for (std::size_t i = bi; i < i_end; ++i) {\n            double sum = 0.0;\n            const double* restrict Ai = &A[i * N];\n            #pragma omp simd reduction(+:sum)\n            for (std::size_t j = 0; j < N; ++j) {\n                sum += Ai[j] * x[j];\n            }\n            y[i] = sum;\n        }\n    }\n}\n```",
  "40_sort_sort_an_array_of_complex_numbers_by_magnitude": "```cpp\n#pragma once\n#include <complex>\n#include <vector>\n#include <algorithm>\n#include <execution>   // for parallel execution policy\n\n/* Sort the vector `x` of complex numbers by their magnitude in ascending order.\n   Uses a parallel execution policy (OpenMP / TBB) for speed on multicore systems.\n   The algorithm is equivalent to `std::sort` with the comparator\n   `std::abs(a) < std::abs(b)`, but avoids the expensive `std::abs` call\n   by comparing squared magnitudes: `|a|^2 < |b|^2`.\n*/\nstatic inline double sqr_abs(const std::complex<double>& z)\n{\n    double r = z.real(), i = z.imag();\n    return r * r + i * i;\n}\n\nvoid sortComplexByMagnitude(std::vector<std::complex<double>>& x)\n{\n    // Parallel sort with a simple, fast comparator\n    std::sort(std::execution::par, x.begin(), x.end(),\n              [](const std::complex<double>& a, const std::complex<double>& b)\n              {\n                  return sqr_abs(a) < sqr_abs(b);\n              });\n}\n```",
  "02_dense_la_gemm": "```cpp\n#pragma once\n#include <vector>\n#include <cstddef>\n#include <omp.h>\n\n/* Multiply the matrix A by the matrix B. Store the results in the matrix C.\n   A is an MxK matrix, B is a KxN matrix, and C is a MxN matrix.\n   The matrices are stored in row-major.\n   Example:\n   A = [[1, -1, 2], [0, -2, 1]]\n   B = [[4, 1], [-1, 0], [2, 2]]\n   after gemm, C = [[9, 5], [4, 2]]\n*/\ninline void gemm(std::vector<double> const& A,\n                 std::vector<double> const& B,\n                 std::vector<double> &C,\n                 size_t M, size_t K, size_t N)\n{\n    /* Simple blocking parameters.  The macro system can be used\n       to tune for specific cache sizes but the values below perform\n       well on typical modern x86\u201164 CPUs.  */\n    constexpr size_t BLOCK_N = 64;   // block size for N dimension\n    constexpr size_t BLOCK_K = 64;   // block size for K dimension\n\n    // Vacuum to make sure C is zeroed before accumulation.\n    // If the caller guarantees that C is already zeroed (as in the given\n    // example), this can be removed for a slight speedup.\n    #pragma omp parallel for collapse(2) schedule(static)\n    for (size_t i = 0; i < M; ++i)\n        for (size_t j = 0; j < N; ++j)\n            C[i*N + j] = 0.0;\n\n    // Main blocked GEMM with cache-friendly order.\n    #pragma omp parallel for schedule(static)\n    for (size_t ii = 0; ii < M; ii += BLOCK_N) {\n        size_t i_max = std::min(ii + BLOCK_N, M);\n        for (size_t kk = 0; kk < K; kk += BLOCK_K) {\n            size_t k_max = std::min(kk + BLOCK_K, K);\n            for (size_t jj = 0; jj < N; jj += BLOCK_N) {\n                size_t j_max = std::min(jj + BLOCK_N, N);\n\n                for (size_t i = ii; i < i_max; ++i) {\n                    const double* a_row = &A[i*K];\n                    double* c_row = &C[i*N];\n                    for (size_t k = kk; k < k_max; ++k) {\n                        double aik = a_row[k];\n                        const double* b_row = &B[k*N];\n                        for (size_t j = jj; j < j_max; ++j) {\n                            c_row[j] += aik * b_row[j];\n                        }\n                    }\n                }\n            }\n        }\n    }\n}\n```",
  "01_dense_la_solve": "```cpp\n#pragma once\n#include <vector>\n#include <omp.h>\n#include <cstddef>\n#include <algorithm>\n\n/* Solve the linear system Ax=b for x.\n   A is an NxN matrix in row-major. x and b have N elements.\n   Example:\n   \n   input: A=[[1,4,2], [1,2,3], [2,1,3]] b=[11, 11, 13]\n   output: x=[3, 1, 2]\n*/\ninline void solveLinearSystem(std::vector<double> const& A,\n                              std::vector<double> const& b,\n                              std::vector<double> &x,\n                              size_t N)\n{\n    // Allocate temporaries on the heap to avoid stack overflows for large N\n    double* const A_buf = new double[N * N];\n    double* const b_buf = new double[N];\n    std::copy(A.begin(), A.end(), A_buf);\n    std::copy(b.begin(), b.end(), b_buf);\n\n    // Gaussian elimination with partial pivoting (no scaling)\n    for (size_t i = 0; i + 1 < N; ++i) {\n        const double pivot = A_buf[i * N + i];\n        if (pivot == 0.0) {                // singular or ill\u2011conditioned\n            delete[] A_buf; delete[] b_buf;\n            return;\n        }\n\n        // Parallel elimination on the remaining rows\n#pragma omp parallel for schedule(static)\n        for (size_t j = i + 1; j < N; ++j) {\n            const double factor = A_buf[j * N + i] / pivot;\n            for (size_t k = i; k < N; ++k) {\n                A_buf[j * N + k] -= factor * A_buf[i * N + k];\n            }\n            b_buf[j] -= factor * b_buf[i];\n        }\n    }\n\n    // Back\u2011substitution (single thread \u2013 cheap, no need to parallelise)\n    for (int i = static_cast<int>(N) - 1; i >= 0; --i) {\n        double sum = 0.0;\n        const double* row = A_buf + i * N;\n        for (size_t j = i + 1; j < N; ++j) {\n            sum += row[j] * x[j];\n        }\n        x[i] = (b_buf[i] - sum) / row[i];\n    }\n\n    delete[] A_buf;\n    delete[] b_buf;\n}\n```",
  "54_stencil_game_of_life": "```cpp\n#pragma once\n#include <vector>\n#include <cstddef>\n#include <omp.h>\n\n/*  Game of Life \u2013 single generation.\n *\n *  Parameters\n *  ----------\n *  input   : const reference to the current grid (row\u2011major).\n *  output  : reference to the grid on which to write the next state.\n *  N       : grid size (NxN).\n *\n *  The implementation keeps the original semantics while\n *  exploiting data parallelism on 8 threads (or the number of\n *  cores exposed to OpenMP) and reducing branch overhead.\n */\n\ninline void gameOfLife(std::vector<int> const& input,\n                       std::vector<int>& output,\n                       size_t N) noexcept\n{\n    /*  The algorithm below follows the classic rule:\n     *  a cell survives with 2 or 3 neighbours, dies otherwise.\n     *  A dead cell becomes alive only when it has exactly 3 neighbours.\n     *\n     *  We compute the neighbour count for each cell by\n     *  summing values from the 8 surrounding positions.\n     *  For the border cells the out\u2011of\u2011range indices are simply\n     *  ignored, which is exactly what the original code did.\n     *\n     *  Parallelisation strategy\n     *  ------------------------\n     *  We parallelise the outer row loop; the inner column loop\n     *  is kept serial to avoid frequent cache line sharing.\n     *  Two consecutive rows are cached locally so that the\n     *  pointers into the input vector are only recomputed once\n     *  per row.  This localised access pattern improves spatial\n     *  locality and allows the compiler to better vectorise the\n     *  inner loop if it chooses to.\n     *\n     *  Branch elimination\n     *  ------------------\n     *  Rather than using nested if/else blocks for the three cases\n     *  (live 2/3, live else, dead ==3), we use arithmetic:\n     *\n     *      int alive = input[idx];                // 0 or 1\n     *      int liveNeighbors = sum;               // 0..8\n     *      int stayAlive = (alive & ((liveNeighbors==2)|(liveNeighbors==3)));\n     *      int born    = ((alive==0) & (liveNeighbors==3));\n     *      output[idx] = stayAlive | born;\n     *\n     *  The comparison operators produce 0/1 values, which are\n     *  combined with bitwise operations.  This approach removes\n     *  the need for explicit branching and lets the CPU execute\n     *  the code in a more straight\u2011line fashion.\n     */\n    const size_t stride = N;\n\n    /* Parallel outer loop \u2013 each iteration works on a distinct row. */\n#pragma omp parallel for schedule(static)\n    for (size_t i = 0; i < N; ++i)\n    {\n        size_t idx = i * stride;         // index of the first element in row i\n        size_t row_above = (i > 0) ? idx - stride : SIZE_MAX;\n        size_t row_below = (i + 1 < N) ? idx + stride : SIZE_MAX;\n\n        for (size_t j = 0; j < N; ++j, ++idx)\n        {\n            int sum = 0;\n\n            /* ----- North, South, West, East ----- */\n            if (row_above != SIZE_MAX)   sum += input[idx - stride];    // north\n            if (row_below != SIZE_MAX)   sum += input[idx + stride];    // south\n            if (j > 0)                   sum += input[idx - 1];        // west\n            if (j + 1 < N)               sum += input[idx + 1];        // east\n\n            /* ----- Diagonals ----- */\n            if (row_above != SIZE_MAX && j > 0)            sum += input[idx - stride - 1];   // north-west\n            if (row_above != SIZE_MAX && j + 1 < N)        sum += input[idx - stride + 1];   // north-east\n            if (row_below != SIZE_MAX && j > 0)            sum += input[idx + stride - 1];   // south-west\n            if (row_below != SIZE_MAX && j + 1 < N)        sum += input[idx + stride + 1];   // south-east\n\n            const int alive = input[idx];   // 0 or 1\n\n            /* Branch\u2011free rule evaluation */\n            const int stayAlive = alive & ((sum == 2) | (sum == 3));\n            const int born      = (~alive & (alive == 0)) & (sum == 3);\n            output[idx] = stayAlive | born;\n        }\n    }\n}\n```",
  "43_sort_sort_an_array_of_structs_by_key": "```cpp\n#pragma once\n#include <vector>\n#include <algorithm>\n#include <omp.h>\n\nstruct Result {\n    int startTime, duration;\n    float value;\n};\n\n/* Parallel quick\u2011sort implementation for sorting Results by startTime.\n * It switches to std::sort for small ranges or when the recursive depth\n * exceeds the number of available threads.  The algorithm is lock\u2011free\n * thanks to OpenMP tasks and guarantees the same ordering as a sequential\n * std::sort for equal keys (since the comparison is strictly less). */\ninline void parallel_qsort(std::vector<Result>::iterator first,\n                           std::vector<Result>::iterator last,\n                           const size_t max_depth = 8) {\n    // Threshold for switching to std::sort\n    const size_t small_range = 32 * 1024;\n\n    // Base case: small range or maximum depth reached\n    if (last - first <= small_range || max_depth == 0) {\n        std::sort(first, last, [](const Result& a, const Result& b) {\n            return a.startTime < b.startTime;\n        });\n        return;\n    }\n\n    // Partition\n    auto pivot = *std::next(first, (last - first) / 2);\n    auto left  = std::partition(first, last, [&](const Result& r) {\n        return r.startTime < pivot.startTime;\n    });\n\n    // Recursively sort left and right halves in parallel\n    #pragma omp task shared(first, left) if (first < left)\n    parallel_qsort(first, left, max_depth - 1);\n    #pragma omp task shared(left, last) if (left < last)\n    parallel_qsort(left, last, max_depth - 1);\n}\n\n/* Public API \u2013 identical to the original but uses a parallel sort. */\ninline void sortByStartTime(std::vector<Result> &results) {\n    if (results.empty()) return;\n\n    #pragma omp parallel\n    {\n        #pragma omp single nowait\n        {\n            // The maximum recursion depth is capped at the number of\n            // available OpenMP threads to avoid oversubscription.\n            parallel_qsort(results.begin(), results.end(),\n                           omp_get_max_threads());\n        }\n    }\n}\n```",
  "42_sort_sorted_ranks": "```cpp\n#pragma once\n#include <vector>\n#include <numeric>\n#include <algorithm>\n#include <cstddef>\n#include <omp.h>\n\n/* For each value in the vector x compute its index in the sorted vector.\n   Store the results in `ranks_`. Behaviour identical to the original implementation. */\ninline void ranks(const std::vector<float>& x, std::vector<size_t>& ranks_) {\n    const size_t n = x.size();\n    // Ensure output vector has the correct size\n    if (ranks_.size() != n) ranks_.resize(n);\n\n    // -----------------------------------------------------------------\n    // 1. Build an array of indices via a parallel iota.\n    // -----------------------------------------------------------------\n    std::vector<size_t> indices(n);\n    #pragma omp parallel for schedule(static)\n    for (size_t i = 0; i < n; ++i) indices[i] = i;\n\n    // -----------------------------------------------------------------\n    // 2. Sort the indices according to the values in x.\n    //    A small inline comparator avoids capturing overhead.\n    // -----------------------------------------------------------------\n    std::sort(indices.begin(), indices.end(),\n              [&x](size_t a, size_t b) { return x[a] < x[b]; });\n\n    // -----------------------------------------------------------------\n    // 3. Write the ranks back into the output vector.\n    //    Parallelized to keep the assignment from becoming a bottleneck\n    //    for large n.\n    // -----------------------------------------------------------------\n    #pragma omp parallel for schedule(static)\n    for (size_t i = 0; i < n; ++i) ranks_[indices[i]] = i;\n}\n```",
  "41_sort_k-th_smallest_element": "```cpp\n#pragma once\n#include <algorithm>\n#include <vector>\n\n/*\n * Return the k\u2011th smallest element of the (1\u2011based) index `k` of the\n * vector `x`.  The original function made a copy of the vector and\n * sorted it.  Here we keep the same semantics but use\n * std::nth_element which runs in linear time on average while\n * preserving the exact result.\n */\nint findKthSmallest(std::vector<int> const& x, int k)\n{\n    // The caller guarantees that 1 <= k <= x.size().\n    // Perform a copy to avoid modifying the input array.\n    std::vector<int> copy(x.begin(), x.end());\n\n    // nth_element positions the element that would be at index k-1\n    // if the whole array were sorted.\n    std::nth_element(copy.begin(),\n                     copy.begin() + (k - 1),\n                     copy.end());\n\n    return copy[k - 1];\n}\n```\n",
  "00_dense_la_lu_decomp": "```cpp\n#pragma once\n#include <vector>\n#include <omp.h>\n\n/*\n    In-place LU factorisation (no pivoting).  \n    Works with a row\u2011major stored NxN matrix.  The algorithm is\n    identical to the reference implementation but uses cache\u2011friendly\n    blocking, loop unrolling and an OpenMP outer loop to exploit the\n    8 hardware threads available on the target platform.\n*/\ninline void luFactorize(std::vector<double> &A, size_t N)\n{\n    const size_t block = 64;               // block size (chosen empirically)\n    const size_t half_block = block / 2;   // used for loop unrolling\n\n    for (size_t kb = 0; kb < N; kb += block) {\n        size_t kmax = std::min(kb + block, N);\n        /*----- update pivot row ------------------------------------------------*/\n        for (size_t j = kb + 1; j < kmax; ++j) {\n            double *const A_kj = &A[kb * N + j];\n            double pivot_val = A_kb_j = *A_kj;\n            // nothing else to do \u2013 pivot row stays unchanged\n        }\n\n        /*----- process rows inside the block ----------------------------------*/\n        for (size_t i = kb + 1; i < kmax; ++i) {\n            double *Ai = &A[i * N];\n            double *Ai_k = Ai + kb;\n            double factor = *Ai_k / A[kb * N + kb];\n            *Ai_k = factor;\n\n            size_t j = kb + 1;\n            for (; j + 1 < kmax; j += 2) {\n                double a0 = Ai[j];\n                double a1 = Ai[j + 1];\n                double u0 = A[kb * N + j];\n                double u1 = A[kb * N + (j + 1)];\n                Ai[j] = a0 - factor * u0;\n                Ai[j + 1] = a1 - factor * u1;\n            }\n            for (; j < kmax; ++j) {\n                Ai[j] -= factor * A[kb * N + j];\n            }\n        }\n\n        /*----- update remaining rows below the current block -------------------*/\n        // Parallelise this part \u2013 the outer loop over rows i is independent\n        #pragma omp parallel for schedule(static,64)\n        for (int ii = static_cast<int>(kmax); ii < static_cast<int>(N); ++ii) {\n            size_t i = static_cast<size_t>(ii);\n            double *Ai = &A[i * N];\n            double *Ai_k = Ai + kb;\n            double factor = *Ai_k / A[kb * N + kb];\n            *Ai_k = factor;\n\n            size_t j = kb + 1;\n            for (; j + 1 < N; j += 2) {\n                double a0 = Ai[j];\n                double a1 = Ai[j + 1];\n                double u0 = A[kb * N + j];\n                double u1 = A[kb * N + (j + 1)];\n                Ai[j] = a0 - factor * u0;\n                Ai[j + 1] = a1 - factor * u1;\n            }\n            for (; j < N; ++j) {\n                Ai[j] -= factor * A[kb * N + j];\n            }\n        }\n    }\n}\n```",
  "37_search_find_the_closest_number_to_pi": "```cpp\n#pragma once\n#include <vector>\n#include <cmath>\n#include <cstddef>\n#include <limits>\n#include <omp.h>\n\nsize_t findClosestToPi(std::vector<double> const& x) {\n    // If the vector is empty we return an invalid index (equal to x.size())\n    if (x.empty()) {\n        return x.size();\n    }\n\n    size_t bestIdx = 0;\n    double bestDiff = std::abs(x[0] - M_PI);\n\n    // Parallel reduction over the elements.\n    // `minVal` holds the minimal difference and `minIdx` the index of that element.\n    double minVal = bestDiff;\n    size_t minIdx = bestIdx;\n\n    #pragma omp parallel\n    {\n        double localMin = std::numeric_limits<double>::infinity();\n        size_t localIdx = 0;\n\n        #pragma omp for nowait\n        for (std::size_t i = 1; i < x.size(); ++i) {\n            double d = std::abs(x[i] - M_PI);\n            if (d < localMin) {\n                localMin = d;\n                localIdx = i;\n            }\n        }\n\n        #pragma omp critical\n        {\n            if (localMin < minVal) {\n                minVal = localMin;\n                minIdx = localIdx;\n            }\n        }\n    }\n\n    return minIdx;\n}\n```",
  "44_sort_sort_non-zero_elements": "```cpp\n#pragma once\n#include <algorithm>\n#include <vector>\n#include <omp.h>\n\n//-----------------------------------------------------------------\n// sortIgnoreZero\n//\n// Sorts all non\u2011zero elements of the vector in ascending order\n// while leaving zero elements in their original positions.\n//-----------------------------------------------------------------\ninline void sortIgnoreZero(std::vector<int> &x)\n{\n    // Reserve space for the non\u2011zero elements to avoid reallocations.\n    std::vector<int> nze;\n    nze.reserve(x.size());\n\n    // Collect non\u2011zero values.  The loop is single\u2011threaded because\n    // pushes to a vector are not thread\u2011safe; the vector is small\n    // relative to the data size so the overhead of a parallel\n    // accumulation would outweigh the benefit.\n    for (int v : x) {\n        if (v != 0) {\n            nze.push_back(v);\n        }\n    }\n\n    // Sort the non\u2011zero values.  std::sort is highly optimised\n    // for the underlying architecture.\n    std::sort(nze.begin(), nze.end());\n\n    // Replace the non\u2011zero entries in the original vector with\n    // the sorted values.\n    size_t pos = 0;\n    for (int &v : x) {\n        if (v != 0) {\n            v = nze[pos++];\n        }\n    }\n}\n```",
  "36_search_check_if_array_contains_value": "```cpp\n#pragma once\n#include <vector>\n#include <cstddef>\n\n/* Return true if the vector x contains the value `target`. Return false otherwise.\n   Examples:\n   \n   input: x=[1, 8, 2, 6, 4, 6], target=3\n   output: false\n   input: x=[1, 8, 2, 6, 4, 6], target=8\n   output: true\n*/\ninline bool contains(std::vector<int> const& x, int target) {\n    // Fast manual linear search \u2013 no bounds checks inside the loop\n    const std::size_t n = x.size();\n    const int* data = x.data();\n    for (std::size_t i = 0; i < n; ++i) {\n        if (data[i] == target) {\n            return true;\n        }\n    }\n    return false;\n}\n```\n",
  "38_search_find_the_first_even_number": "```cpp\n#pragma once\n#include <vector>\n#include <omp.h>\n\n/* Return the index of the first even number in the vector x.\n   Examples:\n   input: [7, 3, 9, 5, 5, 7, 2, 9, 12, 11]\n   output: 6\n   input: [3, 8, 9, 9, 3, 4, 8, 6]\n   output: 1\n*/\nsize_t findFirstEven(std::vector<int> const& x) {\n    // If the vector is empty or all odd, return its size\n    if (x.empty()) return x.size();\n\n    size_t first_even_index = x.size();  // Sentinel: no even found\n\n    #pragma omp parallel for schedule(static) \\\n        shared(first_even_index)\n    for (size_t i = 0; i < x.size(); ++i) {\n        if ((x[i] & 1) == 0) {  // faster check for evenness\n            #pragma omp critical\n            {\n                if (i < first_even_index) {\n                    first_even_index = i;\n                }\n            }\n        }\n    }\n\n    return first_even_index;\n}\n```",
  "35_search_search_for_last_struct_by_key": "```cpp\n#pragma once\n#include <vector>\n#include <cstddef>\n#include <omp.h>\n\nstruct Book {\n    std::string title;\n    int pages;\n};\n\n/* Return the index of the last Book item in the vector books where Book.pages is less than 100.\n   Example:\n   \n   input: [{title=\"Green Eggs and Ham\", pages=72}, {title=\"gulliver's travels\", pages=362}, \n          {title=\"Stories of Your Life\", pages=54}, {title=\"Hamilton\", pages=818}]\n   output: 2\n*/\nsize_t findLastShortBook(std::vector<Book> const& books) {\n    const size_t n = books.size();\n    if (n == 0) return n;\n\n    int lastIdx = -1;  // store int so we can use -1 to indicate \"not found\"\n\n    #pragma omp parallel for reduction(max:lastIdx) schedule(static)\n    for (size_t i = 0; i < n; ++i) {\n        if (books[i].pages < 100) {\n            // cast to int for max reduction\n            if (static_cast<int>(i) > lastIdx)\n                lastIdx = static_cast<int>(i);\n        }\n    }\n\n    return (lastIdx == -1) ? n : static_cast<size_t>(lastIdx);\n}\n```",
  "39_search_xor_contains": "```cpp\n#pragma once\n#include <vector>\n#include <cstddef>\n#include <omp.h>\n\n// Return true if `val` is only in one of vectors `x` or `y`.\n//\n// This version is highly parallelized and uses early exit logic to\n// minimise the amount of work done when searching the two vectors.\ninline bool xorContains(const std::vector<int> &x, const std::vector<int> &y, int val) {\n    bool foundInX = false;\n    bool foundInY = false;\n\n    // Search `x` in parallel, stop as soon as we find the value\n    #pragma omp parallel for schedule(static) reduction(||:foundInX) if(x.size() > 1024)\n    for (std::size_t i = 0; i < x.size(); ++i) {\n        if (!foundInX && x[i] == val) {\n            foundInX = true;\n        }\n    }\n\n    // If we found it in one vector we only need to search the other.\n    if (foundInX) {\n        #pragma omp parallel for schedule(static) reduction(||:foundInY) if(y.size() > 1024)\n        for (std::size_t i = 0; i < y.size(); ++i) {\n            if (!foundInY && y[i] == val) {\n                foundInY = true;\n            }\n        }\n    } else {\n        #pragma omp parallel for schedule(static) reduction(||:foundInY) if(y.size() > 1024)\n        for (std::size_t i = 0; i < y.size(); ++i) {\n            if (!foundInY && y[i] == val) {\n                foundInY = true;\n                break; // early exit once found\n            }\n        }\n        // Only search `x` if it hasn't been searched yet\n        if (!foundInY) {\n            #pragma omp parallel for schedule(static) reduction(||:foundInX) if(x.size() > 1024)\n            for (std::size_t i = 0; i < x.size(); ++i) {\n                if (!foundInX && x[i] == val) {\n                    foundInX = true;\n                }\n            }\n        }\n    }\n\n    return foundInX ^ foundInY;\n}\n```"
}