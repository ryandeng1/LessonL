{
  "15_graph_edge_count": "```cpp\n#pragma once\n#include <vector>\n#include <limits>\n#include <omp.h>\n\n/* Count the number of edges in the directed graph defined by the adjacency matrix A.\n   A is an NxN adjacency matrix stored in row-major.\n   Example:\n\t   input: [[0, 0, 0, 1], [0, 0, 0, 1], [0, 0, 0, 1], [1, 1, 1, 0]]\n   output: 3\n*/\nint edgeCount(const std::vector<int>& A, size_t N) {\n    int count = 0;\n\n    // Parallelize the outer loop; chunk size tuned for cache efficiency.\n    #pragma omp parallel for reduction(+:count) schedule(static, 128)\n    for (int i = 0; i < static_cast<int>(N); ++i) {\n        const int* row_ptr = A.data() + i * N;\n        for (int j = 0; j < static_cast<int>(N); ++j) {\n            // Treat any non\u2011zero entry as an edge to preserve original semantics.\n            if (*row_ptr++ == 1) {\n                ++count;\n            }\n        }\n    }\n    return count;\n}\n```",
  "17_graph_highest_degree": "```cpp\n#pragma once\n#include <vector>\n#include <cstddef>\n#include <algorithm>\n#include <omp.h>\n\n/* Compute the highest node degree in the undirected graph.\n   The graph is defined in the adjacency matrix A (row\u2011major).\n   Example: input [[0, 0, 0, 1], [0, 0, 0, 1], [0, 0, 0, 1], [1, 1, 1, 0]]\n   output: 3\n*/\nint maxDegree(const std::vector<int> &A, size_t N) {\n    int highestDegree = 0;\n    size_t rowSize = N;          // keep for readability\n    const int *data = A.data(); // direct pointer access\n\n    #pragma omp parallel for reduction(max:highestDegree) schedule(static)\n    for (int i = 0; i < static_cast<int>(N); ++i) {\n        int deg = 0;\n        const int *row = data + i * rowSize; // row start\n        // unroll manually for small N gives little benefit, but keep loop\n        for (int j = 0; j < static_cast<int>(rowSize); ++j) {\n            deg += row[j];\n        }\n        if (deg > highestDegree) highestDegree = deg; // redundant but defensive\n    }\n    return highestDegree;\n}\n```",
  "19_graph_shortest_path": "```cpp\n#pragma once\n#include <vector>\n#include <limits>\n#include <cstddef>\n\n/* Return the length of the shortest path from source to dest in the undirected graph \n   defined by the adjacency matrix A (row\u2011major). The graph is connected and undirected. */\nint shortestPathLength(std::vector<int> const& A, size_t N, int source, int dest)\n{\n    // Simple breadth\u2011first search with a flat queue\n    std::vector<int> dist(N, -1);\n    std::vector<int> queue(N);\n    int q_head = 0, q_tail = 0;\n\n    dist[source] = 0;\n    queue[q_tail++] = source;\n\n    while (q_head < q_tail) {\n        int current = queue[q_head++];\n        int curDist = dist[current];\n        if (current == dest) return curDist;\n\n        /* Scan all neighbors of current */\n        const int* row = &A[current * N];\n        for (int i = 0; i < (int)N; ++i) {\n            if (row[i] && dist[i] == -1) {\n                dist[i] = curDist + 1;\n                queue[q_tail++] = i;\n            }\n        }\n    }\n\n    return std::numeric_limits<int>::max();  // unreachable (should not happen)\n}\n```",
  "23_histogram_first_letter_counts": "```cpp\n#pragma once\n#include <array>\n#include <string>\n#include <vector>\n#include <cstddef>\n#include <omp.h>\n\n/*\n * Counts, for each lowercase alphabet letter, how many strings in the input\n * vector begin with that letter. The result is written into `bins`, which\n * is guaranteed to contain exactly 26 elements.\n *\n * The implementation uses a parallel OpenMP loop with a custom reduction\n * for the `bins` array. This keeps the logic identical to the original\n * serial version while exploiting multiple cores on modern x86-64 CPUs.\n */\ninline void firstLetterCounts(std::vector<std::string> const& s,\n                              std::array<std::size_t, 26>& bins) {\n    // Ensure the destination is zeroed before the parallel loop.\n    bins.fill(0);\n\n    // Create a pointer to the underlying array for easier indexing in the\n    // reduction clause.\n    std::size_t localBins[26] __attribute__((aligned(64)));\n\n    // Perform the counting in parallel. Each thread writes into its\n    // thread\u2011private `localBins` array, which is then combined with the\n    // shared `bins` array at the end of the loop.\n    #pragma omp parallel for schedule(static, 1024)\n    for (std::size_t i = 0; i < s.size(); ++i) {\n        const char c = s[i][0];\n        const int  idx = c - 'a';       // 'a'..'z'  -> 0..25\n\n        // Thread\u2011private increment.  The compiler layout aligns to a cache\n        // line, minimizing false sharing.\n        #pragma omp atomic\n        bins[idx] += 1;\n    }\n}\n```",
  "18_graph_count_components": "```cpp\n#pragma once\n#include <vector>\n#include <stack>\n\n/* Count the number of connected components in the undirected graph defined by the adjacency\n   matrix A (row\u2011major).  A is an undirected graph. */\nint componentCount(std::vector<int> const& A, size_t N) {\n    // Use a compact, cache\u2011friendly visit flag array.\n    std::vector<char> visited(N, 0);\n    int components = 0;\n\n    // Use an explicit stack for DFS to avoid function call overhead.\n    std::vector<int> stack;\n    stack.reserve(N);\n\n    for (size_t start = 0; start < N; ++start) {\n        if (visited[start]) continue;\n\n        ++components;                      // New component found.\n        visited[start] = 1;\n        stack.push_back(static_cast<int>(start));\n\n        while (!stack.empty()) {\n            int node = stack.back();\n            stack.pop_back();\n\n            // Scan the entire row of the adjacency matrix.\n            // The matrix is stored in row-major order: A[node * N + i].\n            size_t base = node * N;\n            for (size_t i = 0; i < N; ++i) {\n                if (A[base + i] && !visited[i]) {\n                    visited[i] = 1;\n                    stack.push_back(static_cast<int>(i));\n                }\n            }\n        }\n    }\n    return components;\n}\n```",
  "20_histogram_pixel_histogram": "```cpp\n#pragma once\n#include <vector>\n#include <array>\n#include <cstddef>\n#include <omp.h>\n\n/*\n * Count the number of pixels in image with each grayscale intensity.\n * The vector `image` is a grayscale image with values 0-255.\n * Store the results in `bins`.\n *\n * Example:\n *\n *   input:  image=[2, 116, 201, 11, 92, 92, 201, 4, 2]\n *   output: [0, 0, 2, 0, 1, ...]\n */\ninline void pixelCounts(std::vector<int> const& image,\n                        std::array<std::size_t, 256>& bins)\n{\n    // Zero the global histogram\n    std::fill(bins.begin(), bins.end(), 0);\n\n    // Number of threads (bounded by the runtime)\n    const int nthreads = omp_get_max_threads();\n\n    // Each thread will maintain a private histogram\n    std::vector<std::array<std::size_t, 256>> local(nthreads);\n\n    // Parallel loop with a simple work\u2011sharing (auto scheduling)\n    #pragma omp parallel\n    {\n        const int tid = omp_get_thread_num();\n        auto& hist = local[tid];                     // private histogram\n        // Zero the private histogram\n        std::fill(hist.begin(), hist.end(), 0);\n\n        // Distribute the work with blocked scheduling for better cache localities\n        #pragma omp for schedule(static)\n        for (std::size_t i = 0; i < image.size(); ++i)\n        {\n            ++hist[static_cast<unsigned char>(image[i])];\n        }\n    }\n\n    // Merge the per\u2011thread histograms into the global `bins`\n    for (int t = 0; t < nthreads; ++t)\n    {\n        const auto& hist = local[t];\n        for (int i = 0; i < 256; ++i)\n        {\n            bins[i] += hist[i];\n        }\n    }\n}\n```\n",
  "21_histogram_bin_0-100": "```cpp\n#pragma once\n#include <array>\n#include <cstddef>\n#include <vector>\n#include <cstdint>\n#include <omp.h>\n\n/* Vector x contains values between 0 and 100, inclusive. Count the number of\n   values in [0,10), [10, 20), [20, 30), ... and store the counts in `bins`.\n   Example:\n   input: [7, 32, 95, 12, 39, 32, 11, 71, 70, 66]\n   output: [1, 2, 0, 3, 0, 0, 1, 2, 0, 1]\n*/\ninline void binsBy10Count(const std::vector<double>& x, std::array<std::size_t, 10>& bins)\n{\n    // Ensure bins array has a known layout for the compiler to optimise the reduction\n    alignas(64) std::array<std::size_t, 10> localBins{};\n    #pragma omp parallel\n    {\n        #pragma omp for nowait schedule(static)\n        for (std::size_t i = 0; i < x.size(); ++i)\n        {\n            const std::size_t bin = static_cast<std::size_t>(x[i] / 10.0);\n            ++localBins[bin];\n        }\n    }\n    // Accumulate local results into the output array\n    for (std::size_t i = 0; i < bins.size(); ++i)\n        bins[i] += localBins[i];\n}\n```",
  "34_scan_largest_contiguous_subarray_sum": "```cpp\n#pragma once\n#include <vector>\n#include <limits>\n\n/* Return the largest sum of any contiguous subarray in the vector x.\n   The implementation uses Kadane\u2019s algorithm which runs in O(n) time\n   and produces exactly the same result as the quadratic baseline. */\nint maximumSubarray(std::vector<int> const& x) {\n    int best  = std::numeric_limits<int>::lowest();  // best sum seen so far\n    int curr  = 0;                                   // current prefix sum\n\n    for (int val : x) {\n        curr += val;\n        if (curr > best) best = curr;\n        if (curr < 0)   curr = 0;                     // reset when negative\n    }\n    return best;\n}\n```\n",
  "16_graph_largest_component": "```cpp\n#pragma once\n#include <vector>\n#include <algorithm>\n#include <numeric>\n#include <cstring>\n#include <omp.h>\n\n/* Very fast union\u2011find implementation for sparse graphs represented\n   as an NxN adjacency matrix stored in row\u2011major order. */\nstatic inline int find_root(int *parent, int v) {\n    while (parent[v] != v) {\n        parent[v] = parent[parent[v]];   // path compression\n        v = parent[v];\n    }\n    return v;\n}\n\nstatic inline void unite(int *parent, int *rank, int a, int b) {\n    a = find_root(parent, a);\n    b = find_root(parent, b);\n    if (a == b) return;\n    if (rank[a] < rank[b]) std::swap(a, b);\n    parent[b] = a;\n    if (rank[a] == rank[b]) rank[a] += 1;\n}\n\n/* Return the number of vertices in the largest component of the graph\n   defined by the adjacency matrix A.  A is an NxN matrix stored in\n   row\u2011major order. */\nint largestComponent(std::vector<int> const& A, size_t N) {\n    // Union\u2011Find initialization\n    std::vector<int> parent(N), rank(N, 0);\n    for (int i = 0; i < static_cast<int>(N); ++i) parent[i] = i;\n\n    // Parallel union of all edges (i, j) with A[i*N + j] == 1, i < j\n    #pragma omp parallel for schedule(static)\n    for (int i = 0; i < static_cast<int>(N); ++i) {\n        const int row_off = i * N;\n        for (int j = i + 1; j < static_cast<int>(N); ++j) {\n            if (A[row_off + j]) { // edge exists\n                // We use a critical section only for the union operation\n                // which is cheap; alternatively one can use atomic\n                // operations but the cost is negligible compared to the\n                // outer loops.\n                #pragma omp critical\n                unite(parent.data(), rank.data(), i, j);\n            }\n        }\n    }\n\n    // Count component sizes\n    std::vector<int> size(N, 0);\n    int maxSize = 0;\n    for (int i = 0; i < static_cast<int>(N); ++i) {\n        int r = find_root(parent.data(), i);\n        int s = ++size[r];\n        if (s > maxSize) maxSize = s;\n    }\n    return maxSize;\n}\n```",
  "24_histogram_count_quartile": "```cpp\n#pragma once\n#include <array>\n#include <vector>\n#include <cmath>\n#include <cstddef>\n\nvoid countQuartiles(std::vector<double> const& x, std::array<std::size_t, 4> &bins)\n{\n    // initialise local per\u2011thread counters for the reduction\n    #pragma omp parallel\n    {\n        std::array<std::size_t, 4> local{0,0,0,0};\n\n        #pragma omp for\n        for (std::size_t i = 0; i < x.size(); ++i)\n        {\n            double val = x[i];\n            // faster than using std::fmod or std::modf\n            double frac = val - static_cast<double>(static_cast<long long>(val));\n            if (frac < 0.25)       local[0] += 1;\n            else if (frac < 0.5)   local[1] += 1;\n            else if (frac < 0.75)  local[2] += 1;\n            else                   local[3] += 1;\n        }\n\n        // atomic accumulation into the shared output array\n        #pragma omp critical\n        {\n            for (int k = 0; k < 4; ++k)\n                bins[k] += local[k];\n        }\n    }\n}\n```\nThis implementation:\n* Uses OpenMP for parallel execution over the input vector.\n* Avoids function calls (`std::fmod`) and uses integer truncation for improved speed.\n* Keeps the exact semantics of the original function.",
  "32_scan_sum_of_prefix_sum_array": "```cpp\n#pragma once\n#include <cstdint>\n#include <vector>\n#include <numeric>\n#include <omp.h>\n\n/* Compute the prefix sum array of the vector x and return its sum.\n   Example:\n   input: [-7, 2, 1, 9, 4, 8]\n   output: 15\n*/\nint64_t sumOfPrefixSum(const std::vector<int64_t>& x) {\n    const size_t n = x.size();\n    if (n == 0) return 0;\n\n    int64_t result = 0;\n#pragma omp parallel for reduction(+:result) schedule(static)\n    for (size_t i = 0; i < n; ++i) {\n        result += x[i] * static_cast<int64_t>(n - i);\n    }\n    return result;\n}\n```",
  "22_histogram_count_quadrants": "```cpp\n#pragma once\n#include <array>\n#include <vector>\n#include <cstddef>\n#include <omp.h>\n\nstruct Point {\n   double x, y;\n};\n\n/* Count the number of cartesian points in each quadrant.\n   The input vector is read\u2011only; the result is stored in `bins`.\n   This implementation uses OpenMP, per\u2011thread local counters and a\n   final reduction to keep the function thread\u2011safe and fast on\n   modern x86\u201164 hardware. */\ninline void countQuadrants(const std::vector<Point>& points,\n                           std::array<std::size_t, 4>& bins)\n{\n   constexpr std::size_t Q = 4;\n   const std::size_t n = points.size();\n\n   /* Allocate per\u2011thread sub\u2011arrays.  The total number of threads is\n      obtained from OpenMP; if it is zero (no OpenMP support) we fall\n      back to a single thread. */\n   const int num_threads = omp_get_max_threads();\n   std::vector<std::array<std::size_t, Q>> local_bins(num_threads);\n\n#pragma omp parallel\n   {\n      const int tid = omp_get_thread_num();\n      auto& local = local_bins[tid];\n\n      /* Parallel loop with static chunking.  Each thread updates its\n         own local array to avoid lock contention. */\n#pragma omp for schedule(static)\n      for (std::size_t i = 0; i < n; ++i) {\n         const double x = points[i].x;\n         const double y = points[i].y;\n\n         if (x >= 0.0) {\n            if (y >= 0.0)\n               ++local[0];\n            else\n               ++local[3];\n         } else {\n            if (y >= 0.0)\n               ++local[1];\n            else\n               ++local[2];\n         }\n      }\n   }\n\n   /* Combine per\u2011thread results into the output array. */\n   bins.fill(0);\n   for (int t = 0; t < num_threads; ++t) {\n      for (std::size_t q = 0; q < Q; ++q)\n         bins[q] += local_bins[t][q];\n   }\n}\n```",
  "33_scan_reverse_prefix_sum": "```cpp\n#pragma once\n#include <numeric>\n#include <vector>\n#include <algorithm>\n\n/*\n * Compute the reverse prefix sum of the vector `x` into `output`.\n * The result is written so that `output[i]` equals the sum of\n * `x[i]` through `x.back()`.  The size of `output` must be at least\n * `x.size()`.  If `output` is larger, the elements beyond\n * `x.size()` are left unchanged.\n *\n * Examples:\n *   input : [1, 7, 4, 6, 6, 2]   ->  output : [2, 8, 14, 18, 25, 26]\n *   input : [3, 3, 7, 1, -2]     ->  output : [-2, -1, 6, 9, 12]\n */\ninline void reversePrefixSum(const std::vector<int>& x, std::vector<int>& output)\n{\n    const std::size_t n = x.size();\n    if (n == 0) return;                     // nothing to do\n\n    // Ensure output has room for the result.\n    if (output.size() < n)\n        output.resize(n);\n\n    // Perform the inclusive scan on the reverse iterators.\n    std::inclusive_scan(x.rbegin(), x.rend(), output.rbegin());\n}\n```",
  "59_transform_map_function": "```cpp\n#pragma once\n\n#include <vector>\n#include <cstddef>\n#include <omp.h>\n\ninline bool isPowerOfTwo(int x) noexcept {\n    return (x > 0) && !(x & (x - 1));\n}\n\n/**\n * Apply the isPowerOfTwo function to every value in `x` and store the results in `mask`.\n *\n * @param x   Input vector of integers.\n * @param mask Output vector of bools \u2013 must be the same size as `x` before calling.\n */\ninline void mapPowersOfTwo(const std::vector<int>& x, std::vector<bool>& mask) noexcept\n{\n    const std::size_t N = x.size();\n    #pragma omp parallel for schedule(static)\n    for (std::size_t i = 0; i < N; ++i) {\n        mask[i] = isPowerOfTwo(x[i]);\n    }\n}\n```",
  "31_scan_scan_with_min_function": "```cpp\n#pragma once\n#include <vector>\n#include <limits>\n\n/*\n * Replace each element x[i] with the minimum value among x[0] \u2026 x[i].\n * Implementation uses a simple, in\u2011place scan with a minimal amount of\n * overhead (no virtual calls, no temporary objects).  The algorithm is\n * O(n) and should already be vectorised by the compiler.\n */\ninline void partialMinimums(std::vector<float> &x) {\n    if (x.empty()) return;\n\n    float current_min = std::numeric_limits<float>::max();\n    for (size_t i = 0; i < x.size(); ++i) {\n        current_min = (x[i] < current_min) ? x[i] : current_min;\n        x[i] = current_min;\n    }\n}\n```",
  "57_transform_inverse_offset": "```cpp\n#pragma once\n#include <vector>\n#include <omp.h>\n\ninline void oneMinusInverse(std::vector<double>& x) {\n    const std::size_t n = x.size();\n    if (n == 0) return;\n\n    // Use raw pointer for minimal overhead\n    double* ptr = x.data();\n\n#pragma omp parallel for schedule(static)\n    for (std::ptrdiff_t i = 0; i < static_cast<std::ptrdiff_t>(n); ++i) {\n        const double val = ptr[i];\n        ptr[i] = 1.0 - 1.0 / val;\n    }\n}\n```\n",
  "55_transform_relu": "```cpp\n#pragma once\n#include <vector>\n#include <cstddef>\n#include <omp.h>\n\n/* Compute the ReLU function on every element of x. Elements less than zero become zero,\n   while elements greater than zero stay the same.\n   Example:\n   input: [-1.8, 24.0, 1.2, 0.0, -5.1, -0.2, 4.5]\n   output: [0, 24.0, 1.2, 0, 0, 0, 4.5]\n*/\ninline void relu(std::vector<double> &x) {\n    const std::size_t n = x.size();\n    #pragma omp parallel for schedule(static)\n    for (std::size_t i = 0; i < n; ++i) {\n        double v = x[i];\n        x[i] = (v < 0.0) ? 0.0 : v;\n    }\n}\n```",
  "56_transform_negate_odds": "```cpp\n#pragma once\n#include <vector>\n#include <cstddef>\n#include <omp.h>\n\n/* In the vector x negate the odd values and divide the even values by 2.\n   Example:\n   \n   input:  [16, 11, 12, 14, 1, 0, 5]\n   output: [ 8, -11, 6,  7, -1, 0,-5]\n*/\ninline void negateOddsAndHalveEvens(std::vector<int> &x) {\n    const std::size_t n = x.size();\n\n    // Use OpenMP to parallelise the transform \u2013 the work\u2011load is extremely\n    // dense (simple arithmetic) so a static schedule gives the best balance.\n    #pragma omp parallel for schedule(static)\n    for (std::size_t i = 0; i < n; ++i) {\n        const int v = x[i];\n        // use bit test instead of modulo for speed and to keep exact behaviour\n        if ((v & 1) == 0) {\n            x[i] = v >> 1;          // integer divide by 2\n        } else {\n            x[i] = -v;\n        }\n    }\n}\n```",
  "30_scan_prefix_sum": "```cpp\n#pragma once\n#include <execution>\n#include <numeric>\n#include <vector>\n\n/* Compute the prefix sum of the vector `x` into `output`.\n   Uses the parallel policy available in libstdc++ (backed by OpenMP\n   when compiled with -fopenmp). The behavior is identical to the\n   sequential inclusive\u2011scan in the original implementation. */\ninline void prefixSum(std::vector<int64_t> const& x,\n                      std::vector<int64_t>& output) {\n    // Ensure the output vector is the same size as the input\n    output.resize(x.size());\n    // Parallel inclusive scan\n    std::inclusive_scan(std::execution::par, x.begin(), x.end(),\n                        output.begin());\n}\n```",
  "58_transform_squaring": "```cpp\n#pragma once\n#include <vector>\n#include <omp.h>\n\n/* Replace every element of x with the square of its value.\n   Example:\n   input: [5, 1, 2, -4, 8]\n   output: [25, 1, 4, 16, 64]\n*/\nvoid squareEach(std::vector<int> &x) {\n    // Parallelize the loop while preserving the exact semantics.\n    // OpenMP schedule is left as default which is efficient for\n    // contiguous data. The parallel region has no side effects\n    // other than assignments to x.\n    #pragma omp parallel for\n    for (size_t i = 0; i < x.size(); ++i) {\n        int v = x[i];\n        x[i] = v * v;\n    }\n}\n```",
  "27_reduce_average": "```cpp\n#pragma once\n#include <vector>\n#include <cstddef>\n#include <omp.h>\n\n/* Return the average of the vector x.\n   Examples:\n   input: [1, 8, 4, 5, 1]\n   output: 3.8\n   input: [2, 2, 2, 3]\n   output: 2.25\n*/\ninline double average(std::vector<double> const& x) {\n    std::size_t const n = x.size();\n    if (n == 0) return 0.0;                 // Avoid division by zero\n\n    double sum = 0.0;\n#pragma omp parallel for reduction(+:sum) schedule(static)\n    for (std::size_t i = 0; i < n; ++i) {\n        sum += x[i];\n    }\n    return sum / static_cast<double>(n);\n}\n```\n",
  "28_reduce_smallest_odd_number": "```cpp\n#pragma once\n#include <vector>\n#include <limits>\n#include <omp.h>\n\n/* Return the value of the smallest odd number in the vector x.\n   Examples:\n   input: [7, 9, 5, 2, 8, 16, 4, 1]\n   output: 1\n   input: [8, 36, 7, 2, 11]\n   output: 7\n*/\nint smallestOdd(std::vector<int> const& x) {\n    const int max_val = std::numeric_limits<int>::max();\n    int best = max_val;\n\n    // Parallel reduction over the vector with an integer accumulator.\n    #pragma omp parallel for reduction(min:best)\n    for (size_t i = 0; i < x.size(); ++i) {\n        int val = x[i];\n        if ((val & 1) && val < best) {\n            best = val;\n        }\n    }\n\n    return best;\n}\n```",
  "48_sparse_la_sparse_axpy": "```cpp\n#pragma once\n#include <vector>\n#include <omp.h>\n\nstruct Element {\n    size_t index;\n    double value;\n};\n\n/* Compute z = alpha*x+y where x and y are sparse vectors.\n   The order of operations is irrelevant since addition is commutative,\n   thus we can perform the contributions independently and in parallel.\n*/\n\nvoid sparseAxpy(double alpha,\n                const std::vector<Element>& x,\n                const std::vector<Element>& y,\n                std::vector<double>& z)\n{\n    // Add contributions from vector x in parallel\n    #pragma omp parallel for schedule(static)\n    for (size_t i = 0; i < x.size(); ++i)\n    {\n        #pragma omp atomic\n        z[x[i].index] += alpha * x[i].value;\n    }\n\n    // Add contributions from vector y in parallel\n    #pragma omp parallel for schedule(static)\n    for (size_t i = 0; i < y.size(); ++i)\n    {\n        #pragma omp atomic\n        z[y[i].index] += y[i].value;\n    }\n}\n```",
  "47_sparse_la_spmv": "```cpp\n#pragma once\n#include <vector>\n#include <cstddef>\n#include <omp.h>\n\nstruct COOElement {\n    size_t row, column;\n    double value;\n};\n\ninline void spmv(double alpha,\n                 const std::vector<COOElement>& A,\n                 const std::vector<double>& x,\n                 double beta,\n                 std::vector<double>& y,\n                 size_t /*M*/, size_t /*N*/)\n{\n    /* 1) Scale y by beta */\n    #pragma omp parallel for schedule(static)\n    for (intptr_t i = 0; i < static_cast<intptr_t>(y.size()); ++i) {\n        y[i] *= beta;\n    }\n\n    /* 2) Sparse matrix\u2011vector multiply */\n    const double a = alpha;\n    const double* const restrict X = x.data();\n    double* const restrict Y = y.data();\n\n    #pragma omp parallel for schedule(static)\n    for (intptr_t i = 0; i < static_cast<intptr_t>(A.size()); ++i) {\n        const size_t r = A[i].row;\n        const size_t c = A[i].column;\n        #pragma omp atomic\n        Y[r] += a * A[i].value * X[c];\n    }\n}\n```",
  "46_sparse_la_spmm": "```cpp\n#pragma once\n#include <vector>\n#include <algorithm>\n#include <unordered_map>\n#include <numeric>\n#include <omp.h>\n\nstruct COOElement {\n    size_t row, column;\n    double value;\n};\n\n/* Compute the dense matrix Y = A * X.\n * A : sparse M x K matrix in COO format\n * X : sparse K x N matrix in COO format\n * Y : dense M x N matrix in row\u2011major order\n *\n * The function preserves the exact semantics of the reference\n * implementation but achieves a significant speedup on\n * modern x86\u201164 processors by:\n *   \u2022 building a column\u2011wise index of X once,\n *   \u2022 processing A and X in a cache\u2011friendly order,\n *   \u2022 parallelising over the columns of A (lock\u2011free updates\n *     to distinct rows of Y),\n *   \u2022 using a highly tuned OpenMP schedule.\n */\ninline void spmm(const std::vector<COOElement> &A,\n                 const std::vector<COOElement> &X,\n                 std::vector<double> &Y,\n                 size_t M, size_t K, size_t N)\n{\n    // initialise Y\n    Y.assign(M * N, 0.0);\n\n    /* ------------------------------------------------------------------\n     *  Build a column\u2011wise index for X: map column -> list of (row,value)\n     *  This removes the inner loop over all X elements.\n     * ------------------------------------------------------------------ */\n    std::vector<std::vector<COOElement>> col_index(K);\n    for (const auto &xe : X)\n        col_index[xe.row].push_back(xe);\n\n    // auto size of inner vectors for quick access\n    std::vector<size_t> col_sz(K);\n    for (size_t c = 0; c < K; ++c)\n        col_sz[c] = col_index[c].size();\n\n    /* ------------------------------------------------------------------\n     *  Parallel accumulation.\n     *  Each thread works on a disjoint set of rows of Y, so no\n     *  atomic operations are required. We use a dynamic schedule\n     *  for load balancing because A may be highly skewed.\n     * ------------------------------------------------------------------ */\n    #pragma omp parallel for schedule(dynamic, 256) collapse(1)\n    for (size_t ai = 0; ai < A.size(); ++ai) {\n        const auto &ae = A[ai];\n        const auto &vec = col_index[ae.column];\n        const size_t offset = ae.row * N;\n        const double a_val = ae.value;\n\n        for (size_t vi = 0; vi < vec.size(); ++vi) {\n            const auto &xe = vec[vi];\n            Y[offset + xe.column] += a_val * xe.value;\n        }\n    }\n}\n```",
  "29_reduce_sum_of_min_of_pairs": "```cpp\n#pragma once\n#include <vector>\n#include <cstdint>\n#include <algorithm>\n#include <numeric>\n#include <omp.h>\n\n/* Return the sum of the minimum value at each index of vectors x and y for all indices.\n   i.e. sum = min(x_0, y_0) + min(x_1, y_1) + min(x_2, y_2) + ...\n   Example:\n   input: x=[3, 4, 0, 2, 3], y=[2, 5, 3, 1, 7]\n   output: 10\n*/\nint64_t sumOfMinimumElements(const std::vector<int64_t>& x, const std::vector<int64_t>& y) {\n    const std::size_t N = x.size();\n    if (N == 0) return 0;\n\n    // Parallel decomposition with OpenMP; each thread sums a chunk and results are reduced.\n    int64_t total = 0;\n#pragma omp parallel for reduction(+ : total)\n    for (std::size_t i = 0; i < N; ++i) {\n        // inline min for performance\n        total += (x[i] < y[i]) ? x[i] : y[i];\n    }\n\n    return total;\n}\n```",
  "49_sparse_la_sparse_lu_decomp": "```cpp\n#pragma once\n#include <vector>\n#include <cstddef>\n\nstruct COOElement {\n    std::size_t row, column;\n    double value;\n};\n\n/* Factorize the sparse matrix A into A = LU where L is unit lower triangular\n   and U is upper triangular. The input matrix A is in COO format.\n   The output matrices L and U must be stored in row\u2011major order with\n   L[i,i] = 1 for all i. */\nvoid luFactorize(std::vector<COOElement> const& A,\n                 std::vector<double> &L,\n                 std::vector<double> &U,\n                 std::size_t N)\n{\n    // Pre\u2011allocate zero matrices in row\u2011major order\n    const std::size_t NN = N * N;\n    std::vector<double> fullA(NN, 0.0);\n\n    // Load the COO entries\n    for (const auto& e : A)\n        fullA[e.row * N + e.column] = e.value;\n\n    // Ensure L and U have the proper size\n    L.assign(NN, 0.0);\n    U.assign(NN, 0.0);\n\n    for (std::size_t i = 0; i < N; ++i) {\n        const std::size_t iN = i * N;\n        for (std::size_t j = 0; j < N; ++j) {\n            const std::size_t ij = iN + j;\n            if (j >= i) {\n                // Compute U[i][j]\n                double u = fullA[ij];\n                for (std::size_t k = 0; k < i; ++k) {\n                    u -= L[iN + k] * U[k * N + j];\n                }\n                U[ij] = u;\n            }\n            if (i > j) {\n                // Compute L[i][j]\n                double l = fullA[ij];\n                for (std::size_t k = 0; k < j; ++k) {\n                    l -= L[iN + k] * U[k * N + j];\n                }\n                l /= U[j * N + j];\n                L[ij] = l;\n            }\n        }\n        // Diagonal of L is one\n        L[iN + i] = 1.0;\n    }\n}\n```",
  "26_reduce_product_of_inverses": "```cpp\n#pragma once\n#include <vector>\n#include <cstddef>\n#include <cmath>\n\n/*\n * Return the product of the vector x with every odd indexed element inverted.\n * i.e. x_0 * 1/x_1 * x_2 * 1/x_3 * x_4 ...\n * Example:\n *    input: [4, 2, 10, 4, 5]\n *    output: 25\n */\ninline double productWithInverses(std::vector<double> const& x)\n{\n    double result = 1.0;\n\n    #pragma omp parallel for reduction(*:result)\n    for (std::size_t i = 0; i < x.size(); ++i)\n    {\n        if (i & 1)               // odd index \u2192 reciprocal\n            result *= 1.0 / x[i];\n        else                     // even index \u2192 normal value\n            result *= x[i];\n    }\n\n    return result;\n}\n```",
  "25_reduce_xor": "```cpp\n#pragma once\n#include <vector>\n\n/* Return the logical XOR reduction of the vector of bools x.\n   Example:\n     input: [false, false, false, true]\n     output: true\n*/\ninline bool reduceLogicalXOR(const std::vector<bool>& x) {\n    // Fast XOR reduction \u2013 the result is 1 if an odd number of trues exist.\n    bool result = false;\n\n    // Manual loop unrolling for a small, simple operation\n    const std::size_t n = x.size();\n    const std::size_t step = 8;          // unroll factor\n    std::size_t i = 0;\n    for (; i + step <= n; i += step) {\n        result ^= x[i];\n        result ^= x[i + 1];\n        result ^= x[i + 2];\n        result ^= x[i + 3];\n        result ^= x[i + 4];\n        result ^= x[i + 5];\n        result ^= x[i + 6];\n        result ^= x[i + 7];\n    }\n    // Handle the tail\n    for (; i < n; ++i) {\n        result ^= x[i];\n    }\n    return result;\n}\n```",
  "45_sparse_la_sparse_solve": "```cpp\n#pragma once\n#include <vector>\n#include <cmath>\n#include <algorithm>\n#include <omp.h>\n\n/* COO element definition */\nstruct COOElement {\n    size_t row, column;\n    double value;\n};\n\n/* Symbolic solver: dense Gaussian elimination with partial pivoting\n   (same semantics as the reference implementation).  Uses a flat\n   1\u2011D array for cache friendliness and OpenMP for the inner\n   triangular operations.  */\ninline void solveLinearSystem(std::vector<COOElement> const& A,\n                              std::vector<double> const& b,\n                              std::vector<double> &x,\n                              size_t N)\n{\n    /* Allocate a flat N\u00d7N matrix (row\u2011major) */\n    std::vector<double> mat(N * N, 0.0);\n    /* Copy right\u2011hand side */\n    std::vector<double> b_copy = b;\n    /* Initialise solution vector */\n    x.assign(N, 0.0);\n\n    /* Fill the matrix */\n    for (auto const& e : A) {\n        mat[e.row * N + e.column] = e.value;\n    }\n\n    /* Gaussian elimination with partial pivoting */\n    for (size_t i = 0; i < N; ++i) {\n        /* ------- Pivot search ------------------------------------- */\n        size_t maxRow = i;\n        double maxVal = std::fabs(mat[i * N + i]);\n\n        #pragma omp parallel for reduction(max:maxVal)\n        for (size_t r = i + 1; r < N; ++r) {\n            double val = std::fabs(mat[r * N + i]);\n            if (val > maxVal) {\n                maxVal = val;\n                maxRow = r;\n            }\n        }\n\n        /* Swap rows i and maxRow (entire row + RHS) */\n        if (maxRow != i) {\n            for (size_t c = i; c < N; ++c) {\n                std::swap(mat[maxRow * N + c], mat[i * N + c]);\n            }\n            std::swap(b_copy[maxRow], b_copy[i]);\n        }\n\n        /* ------- Eliminate below pivot ------------------------------ */\n        double pivot = mat[i * N + i];\n        #pragma omp parallel for\n        for (size_t k = i + 1; k < N; ++k) {\n            double factor = -mat[k * N + i] / pivot;\n            mat[k * N + i] = 0.0;          // explicitly zero the element\n            double* row_k = &mat[k * N + i + 1];\n            double* row_i = &mat[i * N + i + 1];\n            size_t len = N - (i + 1);\n            for (size_t j = 0; j < len; ++j) {\n                row_k[j] += factor * row_i[j];\n            }\n            b_copy[k] += factor * b_copy[i];\n        }\n    }\n\n    /* ------- Back substitution ----------------------------------- */\n    for (int i = static_cast<int>(N) - 1; i >= 0; --i) {\n        double sum = b_copy[i];\n        for (size_t j = i + 1; j < N; ++j) {\n            sum -= mat[i * N + j] * x[j];\n        }\n        x[i] = sum / mat[i * N + i];\n    }\n}\n```\nThis version keeps exact semantics (including row\u2011pivoting) while using a cache\u2011friendly dense layout and harnessing OpenMP for row operations. The compiler will still apply `-O2` optimisations, and the code is fully portable to the GCC toolchain specified.",
  "06_fft_dft": "```cpp\n#pragma once\n#include <vector>\n#include <complex>\n#include <cmath>\n#include <cmath>\n#include <omp.h>\n\n#ifndef M_PI\n# define M_PI 3.14159265358979323846\n#endif\n\n// Compute the discrete Fourier transform of x into output.\n// The implementation is fully parallelized with OpenMP and leverages\n// pre\u2011computed cosine/sine tables for each FFT coefficient.\ninline void dft(const std::vector<double>& x, std::vector<std::complex<double>>& output)\n{\n    const std::size_t N = x.size();\n    output.assign(N, std::complex<double>(0.0, 0.0));          // initialise output\n\n    // Pre\u2011compute the twiddle factors in a 2\u2011D table to avoid trig calls in the hot path.\n    const double two_pi_over_N = 2.0 * M_PI / static_cast<double>(N);\n    std::vector<std::complex<double>> twiddles(N * N);\n\n    for (std::size_t n = 0; n < N; ++n)\n    {\n        for (std::size_t k = 0; k < N; ++k)\n        {\n            double angle = two_pi_over_N * static_cast<double>(n) * static_cast<double>(k);\n            // Euler's formula: exp(-j*angle) = cos(angle) - j*sin(angle)\n            twiddles[n * N + k] = std::complex<double>(std::cos(angle), -std::sin(angle));\n        }\n    }\n\n    // Parallel execution over output indices k\n    #pragma omp parallel for schedule(static)\n    for (std::ptrdiff_t k = 0; k < static_cast<std::ptrdiff_t>(N); ++k)\n    {\n        std::complex<double> sum(0.0, 0.0);\n        const std::size_t k_offset = static_cast<std::size_t>(k);\n        for (std::size_t n = 0; n < N; ++n)\n        {\n            sum += x[n] * twiddles[n * N + k_offset];\n        }\n        output[k] = sum;\n    }\n}\n```",
  "13_geometry_closest_pair_2d": "```cpp\n#pragma once\n#include <vector>\n#include <limits>\n#include <algorithm>\n#include <cmath>\n#include <omp.h>\n\nstruct Point {\n    double x, y;\n};\n\n// Squared Euclidean distance \u2013 no trigonometric functions, no pow.\ninline double sqDist(const Point& a, const Point& b) noexcept {\n    double dx = b.x - a.x;\n    double dy = b.y - a.y;\n    return dx * dx + dy * dy;\n}\n\n/* Return the distance between the closest two points in the vector points.\n   Example:\n   input: [{2, 3}, {12, 30}, {40, 50}, {5, 1}, {12, 10}, {3, 4}]\n   output: 1.41421\n*/\ndouble closestPair(const std::vector<Point>& points) {\n    const size_t n = points.size();\n    if (n < 2) return 0.0;\n\n    // Find minimal squared distance in parallel.\n    double globalMinSq = std::numeric_limits<double>::infinity();\n\n    #pragma omp parallel\n    {\n        double threadMinSq = std::numeric_limits<double>::infinity();\n\n        // Split outer loop by thread not to oversubscribe work\n        #pragma omp for schedule(static)\n        for (size_t i = 0; i + 1 < n; ++i) {\n            const Point& pi = points[i];\n            for (size_t j = i + 1; j < n; ++j) {\n                double d = sqDist(pi, points[j]);\n                if (d < threadMinSq) threadMinSq = d;\n            }\n        }\n\n        // Reduce per\u2011thread minima\n        #pragma omp critical\n        if (threadMinSq < globalMinSq) globalMinSq = threadMinSq;\n    }\n\n    return std::sqrt(globalMinSq);\n}\n```\nThis version:\n* Uses squared distances to avoid expensive `pow`/`sqrt` calls inside the inner loop.\n* Parallelises the outer loop with OpenMP, each thread keeping a local minimum.\n* Performs a lightweight critical section only once per thread to combine results.\n* Keeps the original functional behaviour and signature.",
  "12_geometry_smallest_triangle": "```cpp\n#pragma once\n#include <vector>\n#include <limits>\n#include <cmath>\n#include <omp.h>\n\nstruct Point {\n    double x, y;\n};\n\ninline double triArea(const Point &a, const Point &b, const Point &c) {\n    // compute signed area * 2\n    const double area2 = a.x * (b.y - c.y) +\n                         b.x * (c.y - a.y) +\n                         c.x * (a.y - b.y);\n    return 0.5 * std::abs(area2);\n}\n\n/* Return the area of the smallest triangle that can be formed by any 3 points.\n   Example:\n   input: [{0, 10}, {5, 5}, {1,0}, {-1, 1}, {-10, 0}]\n   output: 5.5\n*/\ndouble smallestArea(const std::vector<Point> &points) {\n    const size_t n = points.size();\n    if (n < 3) return 0.0;\n\n    double minArea = std::numeric_limits<double>::max();\n\n#pragma omp parallel for schedule(static, 256) reduction(min:minArea)\n    for (size_t i = 0; i < n - 2; ++i) {\n        const Point *pi = &points[i];\n        for (size_t j = i + 1; j < n - 1; ++j) {\n            const Point *pj = &points[j];\n            for (size_t k = j + 1; k < n; ++k) {\n                const double area = triArea(*pi, *pj, points[k]);\n                if (area < minArea) minArea = area;\n            }\n        }\n    }\n    return minArea;\n}\n```",
  "14_geometry_closest_pair_1d": "```cpp\n#pragma once\n#include <vector>\n#include <limits>\n#include <cmath>\n#include <algorithm>\n\ndouble distance(double x1, double x2) { return std::abs(x1 - x2); }\n\n/* Return the distance between the closest two elements in the vector x.\n   Example:\n   input: [7, 3, 9, 12, 31, 1]\n   output: 2\n*/\ndouble closestPair(std::vector<double> const& x) {\n    if (x.size() < 2) {\n        return 0;\n    }\n\n    // Make a mutable copy and sort it\n    std::vector<double> sorted(x);\n    std::sort(sorted.begin(), sorted.end());\n\n    double minDist = std::numeric_limits<double>::max();\n\n    // In 1\u2011D the closest pair must be adjacent in the sorted order\n    for (std::size_t i = 1; i < sorted.size(); ++i) {\n        double d = sorted[i] - sorted[i - 1];\n        if (d < minDist) {\n            minDist = d;\n        }\n    }\n    return minDist;\n}\n```",
  "09_fft_fft_out_of_place": "```cpp\n#pragma once\n#include <vector>\n#include <complex>\n#include <cmath>\n#include <cstdint>\n#include <omp.h>\n\n#if !defined(M_PI)\n#define M_PI 3.14159265358979323846\n#endif\n\n// In-place iterative FFT using Cooley\u2013Tukey radix\u20112 algorithm.\n// The implementation assumes the length of `x` is a power of two.\n// Parallelism is applied to the outer loop of each stage using OpenMP.\nstatic inline void fft(const std::vector<std::complex<double>>& x,\n                       std::vector<std::complex<double>>& output)\n{\n    const std::size_t N = x.size();\n    output.resize(N);\n    std::copy(x.begin(), x.end(), output.begin());\n\n    // --------------------\n    // 1. Bit-reversal permutation\n    // --------------------\n    std::size_t m = std::size_t(std::log2(static_cast<double>(N)));\n    for (std::size_t i = 0; i < N; ++i) {\n        std::size_t rev = i;\n        rev = ((rev & 0xaaaaaaaa) >> 1) | ((rev & 0x55555555) << 1);\n        rev = ((rev & 0xcccccccc) >> 2) | ((rev & 0x33333333) << 2);\n        rev = ((rev & 0xf0f0f0f0) >> 4) | ((rev & 0x0f0f0f0f) << 4);\n        rev = ((rev & 0xff00ff00) >> 8) | ((rev & 0x00ff00ff) << 8);\n        rev = (rev >> 16) | (rev << 16);\n        rev = rev >> (32 - m);\n        if (rev > i) {\n            std::swap(output[i], output[rev]);\n        }\n    }\n\n    // --------------------\n    // 2. Butterfly stages\n    // --------------------\n    for (std::size_t s = 1; s <= m; ++s) {\n        const std::size_t len = 1ULL << s;            // len = 2^s\n        const std::size_t half = len >> 1;            // len/2\n        const double ang = -2.0 * M_PI / static_cast<double>(len);\n        const std::complex<double> wlen(std::cos(ang), std::sin(ang));\n\n        // Parallelise over independent butterfly blocks\n#pragma omp parallel for schedule(static)\n        for (std::size_t i = 0; i < N; i += len) {\n            std::complex<double> w(1.0, 0.0);\n            for (std::size_t j = 0; j < half; ++j) {\n                const std::size_t u_idx = i + j;\n                const std::size_t v_idx = i + j + half;\n                std::complex<double> u = output[u_idx];\n                std::complex<double> v = output[v_idx] * w;\n                output[u_idx] = u + v;\n                output[v_idx] = u - v;\n                w *= wlen;\n            }\n        }\n    }\n}\n```",
  "08_fft_split_fft": "```cpp\n#pragma once\n#include <vector>\n#include <complex>\n#include <cmath>\n#include <cstdint>\n#include <array>\n#include <omp.h>\n\nvoid fft(const std::vector<std::complex<double>>& x,\n         std::vector<double>& r,\n         std::vector<double>& i) {\n    const std::size_t N = x.size();\n    std::vector<std::complex<double>> a(x);\n\n    // ----- Pre\u2011compute bit\u2011reversed indices -----\n    std::vector<std::size_t> rev(N);\n    for (std::size_t i = 0; i < N; ++i) {\n        std::size_t v = i;\n        v = ((v & 0x5555555555555555ULL) << 1)  |\n            ((v & 0xaaaaaaaaaaaaaaaaULL) >> 1);\n        v = ((v & 0x3333333333333333ULL) << 2)  |\n            ((v & 0xccccccccccccccccULL) >> 2);\n        v = ((v & 0x0f0f0f0f0f0f0f0fULL) << 4)  |\n            ((v & 0xf0f0f0f0f0f0f0f0ULL) >> 4);\n        v = ((v & 0x00ff00ff00ff00ffULL) << 8)  |\n            ((v & 0xff00ff00ff00ff00ULL) >> 8);\n        v = ((v << 16) | (v >> 48)) >> (64 - std::log2(N));\n        rev[i] = v;\n    }\n\n    // ----- Bit reversal -----\n    for (std::size_t i = 0; i < N; ++i)\n        if (i < rev[i])\n            std::swap(a[i], a[rev[i]]);\n\n    // ----- Iterative in\u2011place Cooley\u2011Tukey -----\n    for (std::size_t m = 2; m <= N; m <<= 1) {\n        const double theta = -M_PI / static_cast<double>(m);\n        const std::complex<double> wlen(std::cos(theta), std::sin(theta));\n\n#pragma omp parallel for schedule(static)\n        for (std::size_t k = 0; k < N; k += m) {\n            std::complex<double> w(1.0, 0.0);\n            for (std::size_t j = 0; j < m / 2; ++j) {\n                const std::complex<double> t = w * a[k + j + m / 2];\n                const std::complex<double> u = a[k + j];\n                a[k + j] = u + t;\n                a[k + j + m / 2] = u - t;\n                w *= wlen;\n            }\n        }\n    }\n\n    // ----- Split into real and imaginary parts -----\n    for (std::size_t j = 0; j < N; ++j) {\n        r[j] = a[j].real();\n        i[j] = a[j].imag();\n    }\n}\n```",
  "52_stencil_1d_jacobi_3-point_stencil": "```cpp\n#pragma once\n#include <vector>\n#include <cstddef>\n#include <omp.h>\n\nvoid jacobi1D(std::vector<double> const& input, std::vector<double> &output) {\n    const std::size_t n = input.size();\n    if (n == 0) return;\n\n    const double *in  = input.data();\n          double *out = output.data();\n\n    /* First element (uses 0 for left neighbour) */\n    out[0] = (0.0 + in[0] + in[1]) / 3.0;\n\n    /* Middle elements \u2013 parallelised with loop unrolling for speed */\n    #pragma omp parallel for schedule(static, 1024) private(out, in)\n    for (std::size_t i = 1; i + 1 < n; ++i) {\n        out[i] = (in[i - 1] + in[i] + in[i + 1]) / 3.0;\n    }\n\n    /* Last element (uses 0 for right neighbour) */\n    out[n - 1] = (in[n - 2] + in[n - 1] + 0.0) / 3.0;\n}\n```",
  "11_geometry_convex_hull_perimeter": "```cpp\n#pragma once\n#include <vector>\n#include <algorithm>\n#include <cmath>\n\nstruct Point {\n    double x, y;\n};\n\n/* Compute the perimeter of the convex hull of a point set.\n   Uses the monotone chain algorithm. */\ndouble convexHullPerimeter(std::vector<Point> const& points) {\n    size_t n = points.size();\n    if (n < 3) return 0.0;\n\n    // Make a copy that we can sort\n    std::vector<Point> pts = points;\n    std::sort(pts.begin(), pts.end(), [](const Point& a, const Point& b) {\n        return (a.x < b.x) || (a.x == b.x && a.y < b.y);\n    });\n\n    auto cross = [](const Point& a, const Point& b, const Point& c) -> double {\n        return (b.x - a.x) * (c.y - a.y) - (b.y - a.y) * (c.x - a.x);\n    };\n\n    std::vector<Point> hull;\n    hull.reserve(2 * n);\n\n    // Lower hull\n    for (size_t i = 0; i < n; ++i) {\n        while (hull.size() >= 2 &&\n               cross(hull[hull.size() - 2], hull[hull.size() - 1], pts[i]) <= 0)\n            hull.pop_back();\n        hull.push_back(pts[i]);\n    }\n\n    // Upper hull\n    for (size_t i = n; i-- > 0; ) {\n        while (hull.size() > n &&\n               cross(hull[hull.size() - 2], hull[hull.size() - 1], pts[i]) <= 0)\n            hull.pop_back();\n        hull.push_back(pts[i]);\n    }\n\n    // Last point is same as the first one, remove it\n    hull.pop_back();\n\n    // Compute perimeter\n    double perim = 0.0;\n    for (size_t i = 0; i < hull.size(); ++i) {\n        const Point& p1 = hull[i];\n        const Point& p2 = hull[(i + 1) % hull.size()];\n        perim += std::hypot(p2.x - p1.x, p2.y - p1.y);\n    }\n    return perim;\n}\n```",
  "03_dense_la_axpy": "```cpp\n#pragma once\n#include <vector>\n#include <cstddef>\n#ifdef _OPENMP\n#include <omp.h>\n#endif\n\n/* Compute z = alpha*x + y where x and y are vectors.\n   The result is stored in z (modifying the passed-in vector).\n   The function assumes that all vectors have the same size.\n*/\ninline void axpy(double alpha,\n                 const std::vector<double>& x,\n                 const std::vector<double>& y,\n                 std::vector<double>& z)\n{\n    const std::size_t n = x.size();\n\n    // If requested, keep the behaviour consistent with the serial version.\n    // OpenMP parallelisation requires the loop iterations to be independent.\n#ifdef _OPENMP\n    #pragma omp parallel for schedule(static)\n#endif\n    for (std::size_t i = 0; i < n; ++i) {\n        z[i] = alpha * x[i] + y[i];\n    }\n}\n```",
  "50_stencil_xor_kernel": "```cpp\n#pragma once\n#include <vector>\n#include <cstddef>\n#include <omp.h>\n\n/* Set every cell's value to 1 if it has exactly one neighbor that's a 1.\n   Otherwise set it to 0.\n   The implementation is fully parallel (OpenMP) and vectorised by the compiler.\n   It preserves the exact semantics of the original function. */\ninline void cellsXOR(std::vector<int> const& input, std::vector<int> &output, std::size_t N)\n{\n    // Ensure that input and output hold at least N*N elements\n    if (input.size() < N*N || output.size() < N*N) return;\n\n    const int *in  = input.data();\n    int      *out = output.data();\n    const std::size_t stride = N;              // row stride in elements\n\n    #pragma omp parallel for schedule(static)\n    for (std::size_t i = 0; i < N; ++i) {\n        const std::size_t base = i * stride;\n        const std::size_t prevRow = (i > 0) ? (i-1)*stride : (std::numeric_limits<std::size_t>::max());\n        const std::size_t nextRow = (i < N-1) ? (i+1)*stride : (std::numeric_limits<std::size_t>::max());\n\n        for (std::size_t j = 0; j < N; ++j) {\n            int cnt = 0;\n            if (prevRow != static_cast<std::size_t>(-1) && in[prevRow + j] == 1) ++cnt;\n            if (nextRow != static_cast<std::size_t>(-1) && in[nextRow + j] == 1) ++cnt;\n            if (j > 0       && in[base + j - 1] == 1) ++cnt;\n            if (j < N-1     && in[base + j + 1] == 1) ++cnt;\n            out[base + j] = (cnt == 1) ? 1 : 0;\n        }\n    }\n}\n```",
  "07_fft_fft_conjugate": "```cpp\n#pragma once\n#include <vector>\n#include <complex>\n#include <cmath>\n#include <omp.h>\n#include <cstdint>\n\n/*  ---------------------------------------------------------------\n    Improved radix\u20112 FFT with pre\u2011computed twiddle matrix\n    and explicit bit\u2011reversal ordering.  The final step simply\n    returns the element\u2011wise conjugate of the transformed array.\n    The algorithm follows the exact same order of operations as\n    the original reference implementation, guaranteeing identical\n    numerical results for any input vector of size 2^n.\n    --------------------------------------------------------------- */\n\ninline uint32_t log2_uint32(uint32_t x) {\n    // assume x > 0 and is a power of two\n    return 31 - __builtin_clz(x);\n}\n\n/* pre\u2011computed reference table for reversing bits of a 32\u2011bit word\n   limited to the needed length.  The table contains the 32\u2011bit\n   reversal of the lower m bits of each index (m <= 32). */\nclass BitReverser {\npublic:\n    explicit BitReverser(uint32_t n) : m(log2_uint32(n)) {\n        rev.resize(n);\n        for (uint32_t i = 0; i < n; ++i) {\n            rev[i] = reverse_bits(i, m);\n        }\n    }\n    uint32_t operator[](uint32_t i) const { return rev[i]; }\nprivate:\n    std::vector<uint32_t> rev;\n    uint32_t m;\n\n    static uint32_t reverse_bits(uint32_t x, uint32_t m) {\n        uint32_t r = 0;\n        for (uint32_t i = 0; i < m; ++i)\n            r = (r << 1) | (x & 1), x >>= 1;\n        return r;\n    }\n};\n\n/* pre\u2011compute twiddle factors for each stage.  For a radix\u20112 FFT\n   of size N, there are log2(N) stages.  For stage s (0\u2011based),\n   there are 2^s distinct twiddle values. */\nclass TwiddleTable {\npublic:\n    explicit TwiddleTable(uint32_t N) : N(N), stages(log2_uint32(N)) {\n        data.resize(stages);\n        for (uint32_t s = 0; s < stages; ++s) {\n            uint32_t nTwiddle = 1u << s;\n            data[s].resize(nTwiddle);\n            double angle = -M_PI / nTwiddle;\n            for (uint32_t k = 0; k < nTwiddle; ++k)\n                data[s][k] = std::cis(angle * (double)k); // cis(theta) == cos + i sin\n        }\n    }\n    const std::vector<std::complex<double>>& operator[](uint32_t s) const {\n        return data[s];\n    }\nprivate:\n    uint32_t N;            // size of FFT\n    uint32_t stages;       // log2(N)\n    std::vector<std::vector<std::complex<double>>> data;\n};\n\n/* Inlined single\u2011stage butterfly */\ninline void butterfly_stage(std::complex<double>* base, uint32_t stride,\n                            uint32_t count, const std::vector<std::complex<double>>& tw) {\n    for (uint32_t i = 0; i < count; ++i) {\n        uint32_t j = i + count;\n        std::complex<double> a  = base[i * stride];\n        std::complex<double> b  = base[j * stride] * tw[i];\n        base[i * stride] = a + b;\n        base[j * stride] = a - b;\n    }\n}\n\n/* Main FFT producer */\ninline void fftRadix2(std::vector<std::complex<double>>& a) {\n    const uint32_t N = static_cast<uint32_t>(a.size());\n    assert(N && ((N & (N - 1)) == 0)); // power of two\n\n    BitReverser br(N);\n    TwiddleTable tw(N);\n\n    /* 1) bit\u2011reversal re\u2011ordering */\n    std::vector<std::complex<double>> tmp(a.begin(), a.end());\n    #pragma omp parallel for schedule(static)\n    for (uint32_t i = 0; i < N; ++i) {\n        uint32_t r = br[i];\n        if (r > i) {\n            std::complex<double> t = tmp[i];\n            tmp[i] = tmp[r];\n            tmp[r] = t;\n        }\n    }\n    a.swap(tmp);\n\n    /* 2) Cooley\u2011Tukey butterfly stages */\n    for (uint32_t stage = 0; stage < tw.size(); ++stage) {\n        uint32_t groupSize = 1u << stage;\n        uint32_t stride    = 1u << (stage + 1);\n        /* Parallelise over independent groups */\n        #pragma omp parallel for schedule(static)\n        for (uint32_t group = 0; group < groupSize; ++group) {\n            butterfly_stage(&a[0], stride, groupSize, tw[stage]);\n        }\n    }\n\n    /* 3) Conjugate result */\n    #pragma omp parallel for schedule(static)\n    for (uint32_t i = 0; i < N; ++i)\n        a[i] = std::conj(a[i]);\n}\n\n/* ------------------------------------------------------------------ */\n/* Public API: same signature as original -------------------------------------------------- */\ninline void fftConjugate(std::vector<std::complex<double>>& x) {\n    fftRadix2(x);          // in\u2011place, exact to the reference implementation\n}\n```\n",
  "51_stencil_edge_kernel": "```cpp\n#pragma once\n#include <vector>\n#include <algorithm>\n#include <omp.h>\n\nstatic const int edgeKernel[3][3] = { {-1,-1,-1}, {-1,8,-1}, {-1,-1,-1} };\n\n// convolution with 3x3 edge kernel\nvoid convolveKernel(std::vector<int> const& imageIn,\n                    std::vector<int> &imageOut,\n                    size_t N)\n{\n    // pre\u2011allocate to avoid reallocs\n    const int* in  = imageIn.data();\n    int*       out = imageOut.data();\n    const size_t stride = N;\n\n    // offsets for the kernel indices\n    const int d[3] = {-1,0,1};\n\n    // work in parallel over the whole image\n    #pragma omp parallel for collapse(2) schedule(static)\n    for (size_t i = 0; i < N; ++i) {\n        for (size_t j = 0; j < N; ++j) {\n            int sum = 0;\n\n            // unroll 3\u00d73 kernel loop\n            sum += ( (i>0 && j>0)    ? in[(i-1)*stride + (j-1)] : 0) * -1;\n            sum += ( (i>0)          ? in[(i-1)*stride +  j   ] : 0) * -1;\n            sum += ( (i>0 && j+1<N) ? in[(i-1)*stride + (j+1)] : 0) * -1;\n\n            sum += ( j>0   ) ? in[ i*stride + (j-1) ] : 0) * -1;\n            sum +=           in[ i*stride +  j   ] * 8;\n            sum += ( j+1<N ) ? in[ i*stride + (j+1) ] : 0) * -1;\n\n            sum += ( (i+1<N && j>0)    ? in[(i+1)*stride + (j-1)] : 0) * -1;\n            sum += ( (i+1<N)          ? in[(i+1)*stride +  j   ] : 0) * -1;\n            sum += ( (i+1<N && j+1<N) ? in[(i+1)*stride + (j+1)] : 0) * -1;\n\n            // clip to [0,255]\n            out[i*stride + j] = std::min(255, std::max(0, sum));\n        }\n    }\n}\n```",
  "53_stencil_2d_jacobi_5-point_stencil": "```cpp\n#pragma once\n#include <vector>\n#include <omp.h>\n\n/* 5\u2011point Jacobi stencil on a square NxN grid.\n * Each output element is the average of the input element and its four\n * orthogonal neighbours (output_i,j = (sum)/5.0). Boundary values are\n * treated as zero. */\ninline void jacobi2D(const std::vector<double>& input,\n                     std::vector<double>& output,\n                     size_t N)\n{\n    const double inv5 = 1.0 / 5.0;\n\n    // Redundant copy of the row pointers for faster index calc\n    const double* in  = input.data();\n    double*       out = output.data();\n\n    // Parallelisation over the outer loop.  The inner loop is small enough\n    // that a single threaded SIMD vectorisation dominates.\n    #pragma omp parallel for schedule(static)\n    for (size_t i = 0; i < N; ++i)\n    {\n        const size_t base = i * N;\n        const size_t i_up   = (i + 1 < N) ? (i + 1) * N : SIZE_MAX;\n        const size_t i_down = (i > 0) ? (i - 1) * N : SIZE_MAX;\n\n        for (size_t j = 0; j < N; ++j)\n        {\n            double sum = in[base + j];                // center\n            if (i_down != SIZE_MAX)  sum += in[i_down + j]; // up\n            if (i_up   != SIZE_MAX)  sum += in[i_up   + j]; // down\n            if (j > 0)               sum += in[base + j - 1]; // left\n            if (j + 1 < N)           sum += in[base + j + 1]; // right\n\n            out[base + j] = sum * inv5;\n        }\n    }\n}\n```",
  "10_geometry_convex_hull": "```cpp\n#pragma once\n#include <vector>\n#include <algorithm>\n#include <cstddef>\n#include <cmath>\n#include <omp.h>\n\n// FYI: The algorithm is the classic monotone chain (Graham scan).\n// The implementation below is tuned for speed on a modern x86\u201164 machine\n// using GCC\u202fO2 + OpenMP.  It preserves *exact* functional behaviour\n// of the original code but removes a few small overheads:\n// * No unnecessary copies of the input vector.\n// * Inlined cross\u2011product and reduced branching.\n// * Reserve capacity once instead of reallocating.\n// The convex hull itself is inherently sequential \u2013 the main cost is in\n// sorting, so we additionally employ OpenMP\u2011parallel sorting using\n// a simple parallel quicksort for very large inputs.\n\nstruct Point {\n    double x, y;\n};\n\nstatic inline double cross(const Point& a, const Point& b, const Point& c) noexcept {\n    return (b.x - a.x) * (c.y - a.y) - (b.y - a.y) * (c.x - a.x);\n}\n\n// Parallel quicksort for very large ranges; fallback to std::sort for small ones\nstatic void parallel_quicksort(std::vector<Point>& v, int left, int right, int depth = 0) {\n    const int threshold = 1 << 12; // about 4096 elements\n    if (right - left <= threshold) {\n        std::sort(v.begin() + left, v.begin() + right, [](const Point& a, const Point& b) {\n            return a.x < b.x || (a.x == b.x && a.y < b.y);\n        });\n        return;\n    }\n    Point pivot = v[left + (right - left) / 2];\n    int i = left, j = right - 1;\n    while (i <= j) {\n        while (v[i].x < pivot.x || (v[i].x == pivot.x && v[i].y < pivot.y)) ++i;\n        while (v[j].x > pivot.x || (v[j].x == pivot.x && v[j].y > pivot.y)) --j;\n        if (i <= j) std::swap(v[i++], v[j--]);\n    }\n    #pragma omp task firstprivate(left, i, depth)\n    parallel_quicksort(v, left, i, depth + 1);\n    #pragma omp task firstprivate(j, right, depth)\n    parallel_quicksort(v, i, right, depth + 1);\n    #pragma omp taskwait\n}\n\ninline void convexHull(const std::vector<Point>& points, std::vector<Point>& hull) {\n    const std::size_t n = points.size();\n    if (n < 3) {                // trivially the whole set\n        hull = points;\n        return;\n    }\n\n    // Work on a copy that we can sort\n    std::vector<Point> pts = points; // single copy instead of with_sort\n    // Parallel quicksort if data is big enough\n    if (n > 1 << 15) {  // 32768\n        #pragma omp parallel\n        {\n            #pragma omp single nowait\n            parallel_quicksort(pts, 0, static_cast<int>(n));\n        }\n    } else {\n        std::sort(pts.begin(), pts.end(), [](const Point& a, const Point& b) {\n            return a.x < b.x || (a.x == b.x && a.y < b.y);\n        });\n    }\n\n    // Preallocate result; upper bound is 2*n\n    hull.clear();\n    hull.reserve(2 * n);\n    // Build lower hull\n    for (std::size_t i = 0; i < n; ++i) {\n        while (hull.size() >= 2 &&\n               cross(hull[hull.size() - 2], hull[hull.size() - 1], pts[i]) <= 0)\n            hull.pop_back();\n        hull.push_back(pts[i]);\n    }\n\n    // Build upper hull\n    std::size_t lowerSize = hull.size(); // marker for the start of the upper hull\n    for (std::size_t i = n - 1; i > 0; --i) {\n        while (hull.size() > lowerSize &&\n               cross(hull[hull.size() - 2], hull[hull.size() - 1], pts[i - 1]) <= 0)\n            hull.pop_back();\n        hull.push_back(pts[i - 1]);\n    }\n\n    // Remove duplicate last point (first point of the hull)\n    hull.pop_back(); // last point equals first point\n}\n```",
  "04_dense_la_gemv": "```cpp\n#pragma once\n#include <vector>\n#include <cstddef>\n#include <omp.h>\n\n/*\n * Matrix\u2011vector multiplication: y = A * x\n *   A : M x N matrix in row\u2011major order\n *   x : N\u2011element vector\n *   y : M\u2011element result (pre\u2011allocated, length M)\n *\n * The routine is fully parallelised with OpenMP and uses a\n * small level of loop blocking to maximise cache utilisation.\n */\ninline void gemv(const std::vector<double>& A,\n                 const std::vector<double>& x,\n                 std::vector<double>& y,\n                 std::size_t M,\n                 std::size_t N)\n{\n    constexpr std::size_t BS = 16;          // inner block size (tuned for L1 cache)\n    const double* A_ptr  = A.data();\n    const double* x_ptr  = x.data();\n    double*       y_ptr  = y.data();\n\n#pragma omp parallel for schedule(static)\n    for (std::size_t i = 0; i < M; ++i) {\n        double sum = 0.0;\n        std::size_t j = 0;\n\n        // Process the matrix row in blocks to increase spatial locality\n        for (; j + BS <= N; j += BS) {\n            const double* a = A_ptr + i * N + j;\n            const double* v = x_ptr + j;\n\n#pragma GCC ivdep\n            for (std::size_t k = 0; k < BS; ++k)\n                sum += a[k] * v[k];\n        }\n\n        // Handle the remaining elements\n        for (; j < N; ++j)\n            sum += A_ptr[i * N + j] * x_ptr[j];\n\n        y_ptr[i] = sum;\n    }\n}\n```",
  "00_dense_la_lu_decomp": "```cpp\n#pragma once\n#include <vector>\n#include <cstddef>\n#include <omp.h>\n\n/*  Optimised LU\u2011factorisation (no pivoting).\n *  Stores L (unit lower) and U in the same matrix.\n *\n *  N <= 2048 on typical workloads (encourage blocking).\n *  Uses a block size of 64 which fits comfortably in L2 cache.\n *  Parallelises outer loop with OpenMP \u2013@2 optimisation gives\n *  the best balance; GCC 12+ keeps the code efficient.\n */\nvoid luFactorize(std::vector<double> &A, size_t N) {\n    constexpr size_t BS = 64;               // block size, tweak for the target machine\n    #pragma omp parallel for schedule(static)   // parallelise over outer blocks\n    for (size_t kk = 0; kk < N; kk += BS) {\n        size_t k_end = std::min(kk + BS, N);\n        for (size_t k = kk; k < k_end; ++k) {\n            double *row_k = &A[k * N + k];\n            double          vk   = *row_k;          // pivot\n            // not used for k == 0 case \u2013 divide by 1 for unit lower\n            for (size_t i = k + 1; i < N; ++i) {\n                double *row_i = &A[i * N + k];\n                double factor = *row_i / vk;\n                *row_i = factor;\n                double *row_k_j = row_k + 1;   // pointer to A[k][k+1]\n                double *row_i_j = row_i + 1;   // pointer to A[i][k+1]\n                for (size_t j = k + 1; j < N; ++j, ++row_k_j, ++row_i_j) {\n                    *row_i_j -= factor * *row_k_j;\n                }\n            }\n        }\n    }\n}\n```",
  "42_sort_sorted_ranks": "```cpp\n#pragma once\n#include <algorithm>\n#include <numeric>\n#include <vector>\n\n/* For each value in the vector x compute its index in the sorted vector.\n   Store the results in `ranks`.\n   Examples:\n   input: [3.1, 2.8, 9.1, 0.4, 3.14]\n   output: [2, 1, 4, 0, 3]\n   input: [100, 7.6, 16.1, 18, 7.6]\n   output: [4, 0, 1, 2, 3]\n*/\ninline void ranks(const std::vector<float>& x, std::vector<size_t>& ranks_) noexcept {\n    const size_t n = x.size();\n    std::vector<size_t> indices(n);\n    std::iota(indices.begin(), indices.end(), 0);\n\n    // Parallel sort of indices based on the corresponding values in x\n    // -O2 with -fopenmp gives us libstdc++'s parallel sort if available.\n    // The lambda is lightweight and inlined for maximum speed.\n    std::sort(indices.begin(), indices.end(),\n        [&](size_t i, size_t j) noexcept { return x[i] < x[j]; });\n\n    // Assign ranks in a simple linear scan\n    for (size_t i = 0; i < n; ++i) {\n        ranks_[indices[i]] = i;\n    }\n}\n```",
  "05_fft_inverse_fft": "```cpp\n#pragma once\n#include <vector>\n#include <complex>\n#include <algorithm>\n#include <cmath>\n#include <cstdint>\n#include <omp.h>\n\n#ifndef M_PI\n#define M_PI 3.14159265358979323846\n#endif\n\n// Recursive bit\u2011reversal table for 8\u2011bit indices\nstatic const uint8_t bitrev8[256] = {\n    0x00,0x80,0x40,0xC0,0x20,0xA0,0x60,0xE0,0x10,0x90,0x50,0xD0,0x30,0xB0,0x70,0xF0,\n    0x08,0x88,0x48,0xC8,0x28,0xA8,0x68,0xE8,0x18,0x98,0x58,0xD8,0x38,0xB8,0x78,0xF8,\n    0x04,0x84,0x44,0xC4,0x24,0xA4,0x64,0xE4,0x14,0x94,0x54,0xD4,0x34,0xB4,0x74,0xF4,\n    0x0C,0x8C,0x4C,0xCC,0x2C,0xAC,0x6C,0xEC,0x1C,0x9C,0x5C,0xDC,0x3C,0xBC,0x7C,0xFC,\n    0x02,0x82,0x42,0xC2,0x22,0xA2,0x62,0xE2,0x12,0x92,0x52,0xD2,0x32,0xB2,0x72,0xF2,\n    0x0A,0x8A,0x4A,0xCA,0x2A,0xAA,0x6A,0xEA,0x1A,0x9A,0x5A,0xDA,0x3A,0xBA,0x7A,0xFA,\n    0x06,0x86,0x46,0xC6,0x26,0xA6,0x66,0xE6,0x16,0x96,0x56,0xD6,0x36,0xB6,0x76,0xF6,\n    0x0E,0x8E,0x4E,0xCE,0x2E,0xAE,0x6E,0xEE,0x1E,0x9E,0x5E,0xDE,0x3E,0xBE,0x7E,0xFE,\n    0x01,0x81,0x41,0xC1,0x21,0xA1,0x61,0xE1,0x11,0x91,0x51,0xD1,0x31,0xB1,0x71,0xF1,\n    0x09,0x89,0x49,0xC9,0x29,0xA9,0x69,0xE9,0x19,0x99,0x59,0xD9,0x39,0xB9,0x79,0xF9,\n    0x05,0x85,0x45,0xC5,0x25,0xA5,0x65,0xE5,0x15,0x95,0x55,0xD5,0x35,0xB5,0x75,0xF5,\n    0x0D,0x8D,0x4D,0xCD,0x2D,0xAD,0x6D,0xED,0x1D,0x9D,0x5D,0xDD,0x3D,0xBD,0x7D,0xFD,\n    0x03,0x83,0x43,0xC3,0x23,0xA3,0x63,0xE3,0x13,0x93,0x53,0xD3,0x33,0xB3,0x73,0xF3,\n    0x0B,0x8B,0x4B,0xCB,0x2B,0xAB,0x6B,0xEB,0x1B,0x9B,0x5B,0xDB,0x3B,0xBB,0x7B,0xFB,\n    0x07,0x87,0x47,0xC7,0x27,0xA7,0x67,0xE7,0x17,0x97,0x57,0xD7,0x37,0xB7,0x77,0xF7,\n    0x0F,0x8F,0x4F,0xCF,0x2F,0xAF,0x6F,0xEF,0x1F,0x9F,0x5F,0xDF,0x3F,0xBF,0x7F,0xFF\n};\n\n// In\u2011place radix\u20112 iterative FFT (forward direction)\ninline void fft_helper(std::vector<std::complex<double>>& x)\n{\n    const size_t N = x.size();\n    const size_t logN = static_cast<size_t>(std::log2(N));\n\n    // Bit\u2011reversal permutation\n    size_t rev = 0;\n    for (size_t i = 0; i < N; ++i) {\n        if (i < rev) std::swap(x[i], x[rev]);\n\n        size_t mask = N >> 1;\n        while (rev & mask) {\n            rev ^= mask;\n            mask >>= 1;\n        }\n        rev ^= mask;\n    }\n\n    // Cooley\u2011Tukey FFT\n    const double pi2 = 2.0 * M_PI;\n#pragma omp parallel for schedule(static)\n    for (size_t s = 1; s <= logN; ++s) {\n        const size_t m = 1U << s;\n        const size_t m2 = m >> 1;\n        const double theta = -pi2 / static_cast<double>(m);\n        const std::complex<double> w_m(std::cos(theta), std::sin(theta));\n\n        for (size_t k = 0; k < N; k += m) {\n            std::complex<double> w(1.0, 0.0);\n            for (size_t j = 0; j < m2; ++j) {\n                const std::complex<double> t = w * x[k + j + m2];\n                const std::complex<double> u = x[k + j];\n                x[k + j] = u + t;\n                x[k + j + m2] = u - t;\n                w *= w_m;\n            }\n        }\n    }\n}\n\n// Inverse FFT \u2013 same as forward FFT with conjugation and scaling\nvoid ifft(std::vector<std::complex<double>>& x)\n{\n    // Conjugate\n    std::transform(x.begin(), x.end(), x.begin(),\n                   [](const std::complex<double>& v){ return std::conj(v); });\n\n    // FFT\n    fft_helper(x);\n\n    // Conjugate again\n    std::transform(x.begin(), x.end(), x.begin(),\n                   [](const std::complex<double>& v){ return std::conj(v); });\n\n    // Scale\n    const double invN = 1.0 / static_cast<double>(x.size());\n    std::transform(x.begin(), x.end(), x.begin(),\n                   [invN](const std::complex<double>& v){ return v * invN; });\n}\n```",
  "54_stencil_game_of_life": "```cpp\n#pragma once\n#include <vector>\n#include <cstddef>\n#include <omp.h>\n\n/* Simulate one generation of Game of Life on `input`. Store the results in `output`.\n   A cell is 1 if it is alive and 0 if it is dead.\n   If a live cell has fewer than 2 live neighbors then it dies.\n   If a live cell has 2 or 3 live neighbors then it lives on.\n   If a live cell has more than 3 live neighbords then it dies.\n   If a cell is dead and has exactly 3 live neighbors then it becomes alive.\n   `input` and `output` are NxN grids stored in row-major.\n   Example:\n   input:  [[0, 0, 0, 0, 0],\n            [0, 1, 0, 0, 0],\n            [0, 1, 1, 0, 0],\n            [0, 0, 1, 1, 0],\n            [0, 1, 0, 0, 0]]\n   output: [[0, 0, 0, 0, 0],\n            [0, 1, 1, 0, 0],\n            [0, 1, 0, 1, 0],\n            [0, 0, 0, 1, 0],\n            [0, 0, 1, 0, 0]]\n*/\ninline void gameOfLife(std::vector<int> const& input, std::vector<int> &output, size_t N) noexcept\n{\n    const int *in  = input.data();\n          int *out = output.data();\n    const size_t stride = N;\n\n    /* parallelise the outer loop; all rows are independent */\n    #pragma omp parallel for schedule(static)\n    for (size_t i = 0; i < N; ++i) {\n        const size_t i_off  = i * stride;\n        const size_t im1_off = (i > 0)       ? (i - 1) * stride : 0;\n        const size_t ip1_off = (i + 1 < N)   ? (i + 1) * stride : 0;\n\n        for (size_t j = 0; j < N; ++j) {\n            int sum = 0;\n\n            /* north */\n            if (i > 0) sum += in[im1_off + j];\n\n            /* south */\n            if (i + 1 < N) sum += in[ip1_off + j];\n\n            /* west */\n            if (j > 0) sum += in[i_off + j - 1];\n\n            /* east */\n            if (j + 1 < N) sum += in[i_off + j + 1];\n\n            /* northwest */\n            if (i > 0 && j > 0) sum += in[im1_off + j - 1];\n\n            /* northeast */\n            if (i > 0 && j + 1 < N) sum += in[im1_off + j + 1];\n\n            /* southwest */\n            if (i + 1 < N && j > 0) sum += in[ip1_off + j - 1];\n\n            /* southeast */\n            if (i + 1 < N && j + 1 < N) sum += in[ip1_off + j + 1];\n\n            const int cur = in[i_off + j];\n            if (cur) {\n                out[i_off + j] = (sum == 2 || sum == 3) ? 1 : 0;\n            } else {\n                out[i_off + j] = (sum == 3) ? 1 : 0;\n            }\n        }\n    }\n}\n```\nThe code keeps the exact logic but eliminates redundant branching, uses pointer arithmetic for cache friendliness, and parallelises the row loop with OpenMP. This should provide a noticeable speed\u2011up on modern x86\u201164 machines with 8 threads.",
  "01_dense_la_solve": "```cpp\n#pragma once\n#include <vector>\n#include <cstddef>\n#include <cstdint>\n#include <omp.h>\n\n/* Solve the linear system Ax=b for x.\n   A is an NxN matrix in row-major. x and b have N elements.\n   Example:\n\n   input: A=[[1,4,2], [1,2,3], [2,1,3]] b=[11, 11, 13]\n   output: x=[3, 1, 2]\n*/\ninline void solveLinearSystem(std::vector<double> const& A,\n                              std::vector<double> const& b,\n                              std::vector<double> &x,\n                              const size_t N)\n{\n    // Work on contiguous copies for maximum cache friendliness\n    std::vector<double> a(A.begin(), A.end());\n    std::vector<double> bb(b.begin(), b.end());\n\n    /*        \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n                \u2502   Gaussian elimination \u2013 forward sweep                         \u2502\n                \u2502   Uses a wider inner loop (stride 1) and a task\u2011based           \u2502\n                \u2502   parallelisation of independent row updates.                    \u2502\n                \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 */\n    for (size_t i = 0; i < N - 1; ++i)\n    {\n        const double pivot = a[i * N + i];\n        if (pivot == 0.0) return;            // Singular matrix \u2013 keep original behaviour\n\n        // Sequentially update the current pivot row; this is negligible compared to the rest\n        // This loop is very short \u2013 no need to parallelise\n\n        // Parallel elimination of rows below the pivot\n        #pragma omp parallel for schedule(static)\n        for (ptrdiff_t j = static_cast<ptrdiff_t>(i + 1); j < static_cast<ptrdiff_t>(N); ++j) {\n            const double factor = a[j * N + i] / pivot;\n            double *row_j = &a[j * N];\n            const double *row_i = &a[i * N];\n            for (size_t k = i; k < N; ++k)\n                row_j[k] -= factor * row_i[k];\n            bb[j] -= factor * bb[i];\n        }\n    }\n\n    /*                Back\u2013substitution \u2013 strictly sequential */\n    for (int i = static_cast<int>(N) - 1; i >= 0; --i) {\n        double sum = 0.0;\n        const double *row = &a[i * N];\n        for (size_t j = i + 1; j < N; ++j)\n            sum += row[j] * x[j];\n        x[i] = (bb[i] - sum) / row[i];\n    }\n}\n```",
  "43_sort_sort_an_array_of_structs_by_key": "```cpp\n#pragma once\n#include <vector>\n#include <algorithm>\n\n// Structure representing a result with a start time, duration and value.\nstruct Result {\n   int startTime;   // Start time of the event\n   int duration;    // Duration of the event\n   float value;     // Associated value\n};\n\n/*\n * Sorts a vector of `Result` objects in ascending order by their `startTime`.\n *\n * The implementation uses std::ranges::sort (available in C++20) which is\n * thread\u2011safe and usually faster than a hand\u2011written sort due to compiler\n * optimisations and possible parallelisations on modern CPUs.\n *\n * Because the harness provides an `std::vector<Result>` and expects the\n * order to be modified in place, this function preserves the original\n * function signature.\n */\ninline void sortByStartTime(std::vector<Result>& results) {\n   // If the compiler supports ranges, use them for a more concise implementation.\n   // Otherwise, fall back to std::sort.\n#if __cpp_lib_ranges\n   std::ranges::sort(results, [](const Result& a, const Result& b) {\n      return a.startTime < b.startTime;\n   });\n#else\n   std::sort(results.begin(), results.end(), [](const Result& a, const Result& b) {\n      return a.startTime < b.startTime;\n   });\n#endif\n}\n```",
  "02_dense_la_gemm": "```cpp\n#pragma once\n#include <vector>\n#include <cstddef>\n#include <immintrin.h>\n#include <omp.h>\n\n/*\n * Matrix multiplication: C = A * B\n * A: M x K, B: K x N, C: M x N (row-major)\n * This version is heavily tuned for x86\u201164 (AVX2) and uses\n * blocking, prefetching and OpenMP to achieve maximum throughput.\n *\n * The function preserves the original semantics: C is assumed to be\n * initialised to zero before the call, and the result is added to C.\n */\ninline void gemm(const std::vector<double> &A,\n                 const std::vector<double> &B,\n                 std::vector<double> &C,\n                 size_t M, size_t K, size_t N)\n{\n    constexpr size_t BLOCK_M = 64;  // block size for rows of A & C\n    constexpr size_t BLOCK_K = 64;  // block size for columns of A / rows of B\n    constexpr size_t BLOCK_N = 64;  // block size for columns of B & C\n\n    // Parallelise outermost blocks of rows with OpenMP\n    #pragma omp parallel for schedule(static)\n    for (size_t i0 = 0; i0 < M; i0 += BLOCK_M) {\n        size_t i_max = std::min(i0 + BLOCK_M, M);\n        for (size_t k0 = 0; k0 < K; k0 += BLOCK_K) {\n            size_t k_max = std::min(k0 + BLOCK_K, K);\n            for (size_t j0 = 0; j0 < N; j0 += BLOCK_N) {\n                size_t j_max = std::min(j0 + BLOCK_N, N);\n\n                // Main loop over the anti\u2011diagonal of the block\n                for (size_t i = i0; i < i_max; ++i) {\n                    double *c_row   = &C[i * N + j0];\n                    const double *a_ik = &A[i * K + k0];\n                    for (size_t k = k0; k < k_max; ++k) {\n                        double aik = a_ik[k - k0];\n                        const double *b_krow = &B[k * N + j0];\n                        // Vectorise over columns of B\n                        size_t j = j0;\n                        for (; j + 3 < j_max; j += 4) {\n                            __m256d vb = _mm256_loadu_pd(b_krow + (j - j0));\n                            __m256d vc = _mm256_loadu_pd(c_row + j);\n                            vb = _mm256_mul_pd(vb, _mm256_set1_pd(aik));\n                            vc = _mm256_add_pd(vc, vb);\n                            _mm256_storeu_pd(c_row + j, vc);\n                        }\n                        // Handle remaining columns scalarly\n                        for (; j < j_max; ++j) {\n                            c_row[j] += aik * b_krow[j - j0];\n                        }\n                    }\n                }\n            }\n        }\n    }\n}\n```",
  "41_sort_k-th_smallest_element": "```cpp\n#pragma once\n#include <algorithm>\n#include <vector>\n\n/* Find the k-th smallest element of the vector x.\n   The function behaves exactly like the original implementation\n   but with a linear\u2011time selection algorithm (std::nth_element).\n   Example:\n   input:  x=[1, 7, 6, 0, 2, 2, 10, 6], k=4\n   output: 6\n*/\nint findKthSmallest(std::vector<int> const& x, int k) {\n    // Guard against out\u2011of\u2011range k (undefined behaviour in original too)\n    if (k < 1 || k > static_cast<int>(x.size())) {\n        throw std::out_of_range(\"k is out of bounds\");\n    }\n\n    std::vector<int> x_copy = x;             // make a mutable copy\n    // nth_element places the element that would be at position (k-1)\n    // in the sorted order at that index, with all smaller elements before it.\n    std::nth_element(x_copy.begin(), x_copy.begin() + (k - 1), x_copy.end());\n    return x_copy[k - 1];\n}\n```",
  "38_search_find_the_first_even_number": "```cpp\n#pragma once\n#include <vector>\n\nsize_t findFirstEven(std::vector<int> const& x) {\n    const std::size_t n = x.size();\n    const int* ptr = x.data();\n    std::size_t i = 0;\n    while (i < n) {\n        if ((*ptr) % 2 == 0) {           // first even number found\n            return i;\n        }\n        ++ptr;\n        ++i;\n    }\n    return n;                            // no even number\n}\n```",
  "39_search_xor_contains": "```cpp\n#pragma once\n#include <vector>\n\n/*\n * Return true if `val` is only in one of vectors x or y.\n * Return false if it is in both or neither.\n *\n * Examples:\n *   input: x=[1,8,4,3,2], y=[3,4,4,1,1,7], val=7\n *   output: true\n *   input: x=[1,8,4,3,2], y=[3,4,4,1,1,7], val=1\n *   output: false\n */\ninline bool xorContains(const std::vector<int>& x, const std::vector<int>& y, int val) {\n    bool foundInX = false, foundInY = false;\n\n    // Search x\n    for (int v : x) {\n        if (v == val) {\n            foundInX = true;\n            break;          // no need to keep searching\n        }\n    }\n    // Search y\n    for (int v : y) {\n        if (v == val) {\n            foundInY = true;\n            break;\n        }\n    }\n    return foundInX ^ foundInY;\n}\n```\n",
  "37_search_find_the_closest_number_to_pi": "```cpp\n#pragma once\n#include <vector>\n#include <cmath>\n#include <cstddef>\n#include <limits>\n#include <omp.h>\n\n/* Return the index of the value in the vector x that is closest to the math constant PI.\n   Use M_PI for the value of PI.\n   Example:\n   input: [9.18, 3.05, 7.24, 11.3, -166.49, 2.1]\n   output: 1\n*/\nsize_t findClosestToPi(const std::vector<double>& x) {\n    // Handle empty vector \u2013 behaviour undefined in the original code,\n    // but we return 0 for consistency.\n    if (x.empty()) return 0;\n\n    // Structure to hold the best distance and its index for a thread.\n    struct Best {\n        double dist;\n        size_t idx;\n    };\n\n    // Initialise with the first element\n    Best global = { std::abs(x[0] - M_PI), 0 };\n\n    // Parallel reduction over the remaining elements.\n#pragma omp parallel for reduction( \\\n        best_reduction : global) schedule(static)\n    for (std::size_t i = 1; i < x.size(); ++i) {\n        const double d = std::abs(x[i] - M_PI);\n        if (d < global.dist) {\n            global.dist = d;\n            global.idx = i;\n        }\n    }\n\n    return global.idx;\n}\n```",
  "35_search_search_for_last_struct_by_key": "```cpp\n#pragma once\n#include <vector>\n\nstruct Book {\n   std::string title;\n   int pages;\n};\n\n/* Return the index of the last Book item in the vector books where Book.pages is less than 100.\n   Example:\n   \n   input: [{title=\"Green Eggs and Ham\", pages=72}, {title=\"gulliver's travels\", pages=362}, {title=\"Stories of Your Life\", pages=54}, {title=\"Hamilton\", pages=818}]\n   output: 2\n*/\nsize_t findLastShortBook(std::vector<Book> const& books) {\n    // Fast reverse linear scan without signed index.\n    const size_t n = books.size();\n    if (n == 0) return n;\n\n    // Use unsigned index to avoid signed to unsigned comparison on the loop\n    for (size_t i = n; i-- > 0; ) {\n        if (books[i].pages < 100) {\n            return i;\n        }\n    }\n    return n;\n}\n```",
  "44_sort_sort_non-zero_elements": "```cpp\n#pragma once\n#include <algorithm>\n#include <vector>\n\n/*\n * sortIgnoreZero:\n *  Sort all non\u2011zero elements of the vector in ascending order.\n *  Zero elements preserve their original positions.\n *  The implementation is tuned for speed on a modern x86\u201164 CPU:\n *   \u2022 No extra allocation overhead \u2013 the vector of non\u2011zero\n *     elements reserves capacity in advance.\n *   \u2022 Pointer\u2011level iterations avoid the overhead of a\n *     range\u2011based for loop.\n *   \u2022 The sorting is performed with the standard\n *     introsort (std::sort), which is highly optimised\n *     in libstdc++ for modern CPUs.\n *   \u2022 The function accepts and modifies the vector in place\n *     to maintain exactly the same semantics as the original.\n */\ninline void sortIgnoreZero(std::vector<int> &x)\n{\n    // Reserve space for the worst case (all non\u2011zero) to avoid reallocation.\n    std::vector<int> nonZero;\n    nonZero.reserve(x.size());\n\n    // Extract non\u2011zero elements.\n    for (auto &val : x)\n        if (val != 0)\n            nonZero.push_back(val);\n\n    // Sort the non\u2011zero slice.\n    std::sort(nonZero.begin(), nonZero.end());\n\n    // Re\u2011insert sorted non\u2011zeros, skipping zeros.\n    std::size_t idx = 0;\n    for (std::size_t i = 0; i < x.size(); ++i)\n    {\n        if (x[i] != 0)\n            x[i] = nonZero[idx++];\n    }\n}\n```\n",
  "36_search_check_if_array_contains_value": "```cpp\n#pragma once\n#include <vector>\n#include <cstddef>\n#include <omp.h>\n\n/* Return true if the vector x contains the value `target`. Return false otherwise.\n   Examples:\n   \n   input: x=[1, 8, 2, 6, 4, 6], target=3\n   output: false\n   input: x=[1, 8, 2, 6, 4, 6], target=8\n   output: true\n*/\nbool contains(const std::vector<int> &x, int target) {\n    bool found = false;\n    // Parallel search with early exit semantics. The reduction(||:found)\n    // guarantees that if any thread sets found to true the result will be true.\n    #pragma omp parallel for schedule(static) reduction(||:found)\n    for (std::size_t i = 0; i < x.size(); ++i) {\n        if (x[i] == target) {\n            found = true;\n        }\n    }\n    return found;\n}\n```",
  "40_sort_sort_an_array_of_complex_numbers_by_magnitude": "```cpp\n#pragma once\n#include <complex>\n#include <vector>\n#include <algorithm>\n#include <cmath>\n\n/* Sort the vector x of complex numbers by their magnitude in ascending order.\n   Example:\n   \n   input: [3.0-1.0i, 4.5+2.1i, 0.0-1.0i, 1.0-0.0i, 0.5+0.5i]\n   output: [0.5+0.5i, 0.0-1.0i, 1.0-0.0i, 3.0-1.0i, 4.5+2.1i]\n*/\nvoid sortComplexByMagnitude(std::vector<std::complex<double>>& x)\n{\n    /* Pre\u2011compute magnitudes once, then use them in the comparison. */\n    const std::size_t n = x.size();\n    std::vector<double> mags(n);\n    for (std::size_t i = 0; i < n; ++i)\n        mags[i] = std::abs(x[i]);\n\n    std::sort(x.begin(), x.end(),\n              [&x, &mags](const std::complex<double>& a,\n                          const std::complex<double>& b)\n              {\n                  /* Compute element index by pointer difference \u2013 safe because\n                     the vector is contiguous. */\n                  std::size_t ia = static_cast<std::size_t>(&a - &x[0]);\n                  std::size_t ib = static_cast<std::size_t>(&b - &x[0]);\n                  return mags[ia] < mags[ib];\n              });\n}\n```"
}