{
  "15_graph_edge_count": "```cpp\n#pragma once\n#include <vector>\n#include <cstddef>\n#include <omp.h>\n\n/*\n * Count the number of edges in the directed graph defined by the adjacency matrix A.\n * A is an NxN adjacency matrix stored in row-major.\n * Example:\n *   input: [[0, 0, 0, 1], [0, 0, 0, 1], [0, 0, 0, 1], [1, 1, 1, 0]]\n *   output: 3\n */\nint edgeCount(std::vector<int> const& A, std::size_t N) {\n    int count = 0;\n\n    // Parallelize with a collapsed two\u2011level loop.\n    #pragma omp parallel for collapse(2) reduction(+:count) schedule(static)\n    for (std::size_t i = 0; i < N; ++i) {\n        for (std::size_t j = 0; j < N; ++j) {\n            if (A[i * N + j] == 1) {\n                ++count;\n            }\n        }\n    }\n\n    return count;\n}\n```",
  "17_graph_highest_degree": "```cpp\n#pragma once\n#include <vector>\n#include <algorithm>\n#include <cstddef>\n#include <omp.h>\n\n/* Compute the highest node degree in the undirected graph. The graph is defined in the adjacency matrix A.\n   A is an NxN adjacency matrix stored in row\u2011major. A is undirected.\n   Example:\n        input: [[0, 0, 0, 1], [0, 0, 0, 1], [0, 0, 0, 1], [1, 1, 1, 0]]\n   output: 3\n*/\nint maxDegree(const std::vector<int>& A, size_t N) {\n    int highestDegree = 0;\n\n    // Parallelise the outer loop over rows.  The reduction clause computes the\n    // maximum of the row sums across all threads.\n    #pragma omp parallel for reduction(max:highestDegree)\n    for (int i = 0; i < static_cast<int>(N); ++i) {\n        // Compute degree of node i by summing its adjacency vector.\n        int degree = 0;\n        const int* row = A.data() + i * N;\n        for (int j = 0; j < static_cast<int>(N); ++j) {\n            degree += row[j];\n        }\n        if (degree > highestDegree) {\n            highestDegree = degree;  // Keep as a safeguard (redundant with reduction)\n        }\n    }\n    return highestDegree;\n}\n```",
  "23_histogram_first_letter_counts": "```cpp\n#pragma once\n#include <array>\n#include <string>\n#include <vector>\n#include <cstddef>\n#include <omp.h>\n\n/* Count the number of strings that start with each lowercase letter.\n *\n * Parameters\n * ----------\n * s    : vector of strings (all lowercase, non\u2011empty)\n * bins : array of 26 counters (one for each letter)\n *\n * The function preserves the exact behavior of the reference implementation\n * but is parallelized for a two\u2011core CPU with 8 hardware threads.\n * It uses an OpenMP parallel loop with a reduction over the array\n * to guarantee thread safety while keeping the addition cost minimal.\n */\nvoid firstLetterCounts(std::vector<std::string> const& s,\n                       std::array<std::size_t, 26>& bins) {\n    // zero the bins in case the caller hasn't done so\n    bins.fill(0);\n\n    const std::size_t N = s.size();\n\n    // Parallel loop with reduction over the whole array.\n    // The syntax bins[:26] tells GCC to apply a per\u2011thread local copy\n    // for each element of the array and combine them at the end.\n    #pragma omp parallel for schedule(static)\n    for (std::size_t i = 0; i < N; ++i) {\n        const unsigned char c = static_cast<unsigned char>(s[i][0] - 'a');\n        // c is guaranteed to be 0\u201125 as inputs are only lowercase letters\n        bins[c] += 1;\n    }\n}\n```",
  "20_histogram_pixel_histogram": "```cpp\n#pragma once\n#include <vector>\n#include <array>\n#include <omp.h>\n\ninline void pixelCounts(std::vector<int> const& image,\n                        std::array<size_t, 256>& bins)\n{\n    // Zero the output bins\n    bins.fill(0);\n\n    // Each thread will keep a private histogram and merge at the end.\n    const int nthreads = 8;                       // number of threads we want\n    std::vector<std::array<size_t, 256>> local(nthreads);\n    for (auto& localBins : local)\n        localBins.fill(0);\n\n    // Determine the size of the image\n    const size_t N = image.size();\n\n    // Parallel computation of per-thread histograms\n    #pragma omp parallel num_threads(nthreads)\n    {\n        const int tid = omp_get_thread_num();\n        std::array<size_t, 256>& localBins = local[tid];\n\n        #pragma omp for schedule(static)\n        for (size_t i = 0; i < N; ++i)\n            ++localBins[static_cast<unsigned char>(image[i])];\n    }\n\n    // Merge all per-thread histograms into the final result\n    for (int t = 0; t < nthreads; ++t)\n        for (int v = 0; v < 256; ++v)\n            bins[v] += local[t][v];\n}\n```\nThis version preserves the exact semantics of the original, but uses an OpenMP parallel loop along with thread\u2011private histograms to avoid a lot of atomic operations, providing a substantial speed\u2011up on an 8\u2011core machine.",
  "21_histogram_bin_0-100": "```cpp\n#pragma once\n#include <array>\n#include <vector>\n#include <cstddef>\n#include <omp.h>\n\n// Count values in ranges [0,10), [10,20), \u2026 [90,100] into 10 bins.\n// Parallelised with OpenMP without changing observable behaviour.\nvoid binsBy10Count(std::vector<double> const& x, std::array<std::size_t, 10> &bins) {\n    //Initialise the output bins to zero\n    for (auto &b : bins) b = 0;\n\n    // Per\u2011thread local bins to avoid atomic contention\n    const int nthreads = omp_get_max_threads();\n    std::vector<std::array<std::size_t, 10>> local_bins(nthreads);\n    for (auto &lb : local_bins) lb.fill(0);\n\n    #pragma omp parallel\n    {\n        const int tid = omp_get_thread_num();\n        auto &lb = local_bins[tid];\n\n        #pragma omp for schedule(static)\n        for (std::size_t i = 0; i < x.size(); ++i) {\n            // Value is guaranteed to be in [0,100]\n            std::size_t bin = static_cast<std::size_t>(x[i] / 10.0);\n            // Guard against precision issues that might produce 10\n            if (bin >= 10) bin = 9;\n            ++lb[bin];\n        }\n    }\n\n    // Accumulate the per\u2011thread bins into the final result\n    for (int t = 0; t < nthreads; ++t) {\n        for (int b = 0; b < 10; ++b) {\n            bins[b] += local_bins[t][b];\n        }\n    }\n}\n```",
  "18_graph_count_components": "```cpp\n#pragma once\n#include <vector>\n#include <stack>\n#include <cstddef>\n\ninline void dfs_stack(const std::vector<int> &A, int start, size_t N,\n                      std::vector<char> &visited) {\n    std::stack<int> st;\n    st.push(start);\n    visited[start] = 1;\n    const size_t row_offset = N;  // for pointer arithmetic\n\n    while (!st.empty()) {\n        int v = st.top();\n        st.pop();\n\n        const int *row = &A[v * row_offset];\n        for (int u = 0; u < static_cast<int>(N); ++u) {\n            if (row[u] && !visited[u]) {\n                visited[u] = 1;\n                st.push(u);\n            }\n        }\n    }\n}\n\n/* Count the number of connected components in the undirected graph defined by\n   the adjacency matrix A.  A is an NxN adjacency matrix stored in row\u2011major.\n   A is an undirected graph.\n   Example:\n      input: [[0, 1, 0, 0], [1, 0, 0, 0], [0, 0, 0, 1], [0, 0, 1, 0]]\n   output: 2\n*/\nint componentCount(const std::vector<int> &A, size_t N) {\n    std::vector<char> visited(N, 0);\n    int count = 0;\n\n    for (int i = 0; i < static_cast<int>(N); ++i) {\n        if (!visited[i]) {\n            dfs_stack(A, i, N, visited);\n            ++count;\n        }\n    }\n    return count;\n}\n```",
  "19_graph_shortest_path": "```cpp\n#pragma once\n#include <vector>\n#include <limits>\n#include <cstddef>\n\n/* Return the length of the shortest path from source to dest in the undirected graph defined by the adjacency matrix A.\n   A is an NxN adjacency matrix stored in row-major. Assume the graph is connected. A is undirected.\n   Example:\n     input: [[0, 1, 1, 0], [1, 0, 0, 1], [1, 0, 0, 1], [0, 1, 1, 0]], source=0, dest=3\n   output: 2\n*/\ninline int shortestPathLength(std::vector<int> const& A, std::size_t N,\n                              int source, int dest) {\n    // Conservative capacity: N vertices\n    std::vector<char> visited(N, 0);         // char is smaller than bool on many ABIs\n    std::vector<int>  q(N);                  // BFS queue: store node indices\n    std::size_t head = 0, tail = 0, depth = 0;        // depth handled by level counter\n\n    visited[source] = 1;\n    q[tail++] = source;                      // enqueue source\n\n    while (head < tail) {\n        std::size_t level_end = tail;        // all nodes queued so far belong to current level\n        while (head < level_end) {\n            int current = q[head++];\n            if (current == dest) return static_cast<int>(depth);\n\n            // Scan row for adjacent vertices\n            const std::size_t row_start = current * N;\n            for (std::size_t i = 0; i < N; ++i) {\n                if (A[row_start + i] && !visited[i]) {\n                    visited[i] = 1;\n                    q[tail++] = static_cast<int>(i);\n                }\n            }\n        }\n        ++depth;                             // move to next level\n    }\n\n    // Fallback: graph is assumed connected, so this should never happen\n    return std::numeric_limits<int>::max();\n}\n```",
  "34_scan_largest_contiguous_subarray_sum": "```cpp\n#pragma once\n#include <vector>\n#include <limits>\n\n/* Return the largest sum of any contiguous subarray in the vector x.\n   This implementation uses Kadane's algorithm (O(n) time and O(1) extra space),\n   preserving the exact behavior of the original double\u2011loop version.\n*/\n\nint maximumSubarray(std::vector<int> const& x) {\n    int maxSum = std::numeric_limits<int>::lowest();\n    int currentSum = 0;\n\n    for (int v : x) {\n        currentSum = std::max(v, currentSum + v);\n        maxSum = std::max(maxSum, currentSum);\n    }\n\n    return maxSum;\n}\n```",
  "16_graph_largest_component": "```cpp\n#pragma once\n#include <vector>\n#include <algorithm>\n#include <cstddef>\n\n/* Disjoint Set Union (Union\u2011Find) with path compression and union by size.\n   The graph is undirected, so we only need to union each edge once. */\nclass DSU {\npublic:\n    explicit DSU(std::size_t n)\n        : parent(n), sz(n, 1), max_sz(1) {\n        for (std::size_t i = 0; i < n; ++i) parent[i] = i;\n    }\n\n    std::size_t find(std::size_t x) {\n        std::size_t root = x;\n        while (root != parent[root]) root = parent[root];\n        // path compression\n        while (x != root) {\n            std::size_t next = parent[x];\n            parent[x] = root;\n            x = next;\n        }\n        return root;\n    }\n\n    void unite(std::size_t a, std::size_t b) {\n        std::size_t ra = find(a), rb = find(b);\n        if (ra == rb) return;\n        if (sz[ra] < sz[rb]) std::swap(ra, rb);\n        parent[rb] = ra;\n        sz[ra] += sz[rb];\n        if (sz[ra] > max_sz) max_sz = sz[ra];\n    }\n\n    std::size_t max_component_size() const { return max_sz; }\n\nprivate:\n    std::vector<std::size_t> parent;\n    std::vector<std::size_t> sz;\n    std::size_t max_sz;\n};\n\n/* Return the number of vertices in the largest component of the graph defined by the adjacency matrix A.\n   A is an NxN adjacency matrix stored in row\u2011major. */\nint largestComponent(std::vector<int> const& A, std::size_t N) {\n    if (N == 0) return 0;\n\n    DSU dsu(N);\n\n    // Process only upper triangle to avoid duplicate unions\n    for (std::size_t i = 0; i < N; ++i) {\n        for (std::size_t j = i + 1; j < N; ++j) {\n            if (A[i * N + j] == 1) dsu.unite(i, j);\n        }\n    }\n\n    return static_cast<int>(dsu.max_component_size());\n}\n```",
  "55_transform_relu": "```cpp\n#pragma once\n#include <vector>\n#include <algorithm>\n#include <cstddef>\n#include <omp.h>\n\n/*\n * Compute the ReLU function on every element of x. Elements less than zero become\n * zero, while elements greater than or equal to zero stay the same.\n *\n * Example:\n * input : [-1.8, 24.0, 1.2, 0.0, -5.1, -0.2, 4.5]\n * output: [0, 24.0, 1.2, 0, 0, 0, 4.5]\n */\ninline void relu(std::vector<double> &x) {\n    const std::size_t n = x.size();\n    #pragma omp parallel for schedule(static)\n    for (std::size_t i = 0; i < n; ++i) {\n        double v = x[i];\n        x[i] = (v > 0.0) ? v : 0.0;\n    }\n}\n```",
  "33_scan_reverse_prefix_sum": "```cpp\n#pragma once\n#include <vector>\n#include <numeric>\n#include <execution>\n\n/*\n  Compute the reverse prefix sum of the vector `x` into `output`.\n\n  Examples:\n    input : [1, 7, 4, 6, 6, 2]   \u2192  output : [2, 8, 14, 18, 25, 26]\n    input : [3, 3, 7, 1, -2]     \u2192  output : [-2, -1, 6, 9, 12]\n*/\ninline void reversePrefixSum(const std::vector<int> &x, std::vector<int> &output)\n{\n    const std::size_t n = x.size();\n\n    // Ensure the output vector is sized correctly.\n    output.resize(n);\n\n    // Step 1: Copy the input in reverse order into a temporary buffer.\n    // Using a raw array for low\u2011level control and reduced overhead.\n    std::vector<int> rev;\n    rev.reserve(n);\n    for (std::size_t i = n; i-- > 0;)       // i goes n-1 .. 0\n        rev.push_back(x[i]);\n\n    // Step 2: Perform an inclusive scan on the reversed data.\n    // The std::inclusive_scan algorithm is highly optimized in libstdc++,\n    // and will automatically use SIMD and, if available, SIMD-friendly loops.\n    std::inclusive_scan(std::execution::seq, rev.begin(), rev.end(), output.begin());\n}\n```",
  "24_histogram_count_quartile": "```cpp\n#pragma once\n#include <array>\n#include <vector>\n#include <cstddef>\n#include <cmath>\n#include <omp.h>\n\n/*\n   Count the number of doubles in the vector `x` that have a fractional part\n   in the ranges [0,0.25), [0.25,0.5), [0.5,0.75), and [0.75,1).  The\n   resulting bin counts are stored in the supplied array `bins`.\n\n   The implementation is fully parallelized with OpenMP and uses local\n   reduction arrays to avoid contention in the critical section.\n\n   The algorithm preserves the exact behaviour of the original sequential\n   implementation.\n*/\ninline void countQuartiles(std::vector<double> const& x,\n                           std::array<std::size_t, 4> &bins)\n{\n    // initialise output bins\n    bins = {0, 0, 0, 0};\n\n    // number of threads and local storage\n    const int nthreads = omp_get_max_threads();\n    std::vector<std::array<std::size_t,4>> local(nthreads);\n    for (auto &a : local) a = {0,0,0,0};\n\n    #pragma omp parallel\n    {\n        const int tid = omp_get_thread_num();\n        const size_t start = (x.size() * tid) / nthreads;\n        const size_t end   = (x.size() * (tid+1)) / nthreads;\n\n        auto &l = local[tid];\n        for (size_t i = start; i < end; ++i) {\n            const double val = x[i];\n            const double frac = val - std::floor(val);\n            if (frac < 0.25)      ++l[0];\n            else if (frac < 0.5)  ++l[1];\n            else if (frac < 0.75) ++l[2];\n            else                  ++l[3];\n        }\n    }\n\n    // merge thread-local counts into final result\n    for (const auto &l : local) {\n        bins[0] += l[0];\n        bins[1] += l[1];\n        bins[2] += l[2];\n        bins[3] += l[3];\n    }\n}\n```",
  "31_scan_scan_with_min_function": "```cpp\n#pragma once\n#include <vector>\n#include <limits>\n\n/// Replace the i\u2011th element of the vector `x` with the minimum value from indices 0 through i.\n///\n/// Example:\n///   input : [8, 6, -1, 7, 3, 4, 4]\n///   output: [8, 6, -1, -1, -1, -1, -1]\n///\n///   input : [5, 4, 6, 4, 3, 6, 1, 1]\n///   output: [5, 4, 4, 4, 3, 3, 1, 1]\n///\ninline void partialMinimums(std::vector<float> &x)\n{\n    if (x.empty()) return;\n\n    // Keep the running minimum and overwrite the vector in place.\n    float current_min = std::numeric_limits<float>::max();\n\n    for (auto &value : x) {\n        current_min = (value < current_min) ? value : current_min;\n        value = current_min;\n    }\n}\n```",
  "59_transform_map_function": "```cpp\n#pragma once\n#include <vector>\n#include <cstdint>\n#include <omp.h>\n\n// Branch\u2011free, 64\u2011bit safe implementation (same semantics as the original)\ninline bool isPowerOfTwo(int32_t x) {\n    return (x > 0) && !(x & (x - 1));\n}\n\n/*\n * Applies 'isPowerOfTwo' to every element of 'x' and writes the result\n * into 'mask'.\n *\n * Parameters\n * ----------\n * x    : input vector of integers\n * mask : output vector of booleans; must be sized to x.size() before calling\n *\n * Notes\n * -----\n * The function assumes that 'mask' has been resized to match the size of\n * 'x'.  This matches the contract of the original implementation.\n */\ninline void mapPowersOfTwo(const std::vector<int32_t>& x, std::vector<bool>& mask) {\n    const std::size_t N = x.size();\n\n    // Ensure output buffer is correctly sized (behaviour of the original code)\n    if (mask.size() != N) mask.resize(N);\n\n    // OpenMP parallelisation \u2013 static scheduling gives good load balance\n    #pragma omp parallel for schedule(static) if (N > 1024)\n    for (std::int64_t i = 0; i < static_cast<std::int64_t>(N); ++i) {\n        // Local copy to avoid potential false sharing on x (reading is fine)\n        const int32_t val = x[static_cast<std::size_t>(i)];\n        mask[static_cast<std::size_t>(i)] = isPowerOfTwo(val);\n    }\n}\n```",
  "56_transform_negate_odds": "```cpp\n#pragma once\n#include <vector>\n#include <omp.h>\n\ninline void negateOddsAndHalveEvens(std::vector<int>& x) {\n    const size_t n = x.size();\n    #pragma omp parallel for schedule(static)\n    for (size_t i = 0; i < n; ++i) {\n        int v = x[i];\n        if ((v & 1) == 0) {          // even\n            x[i] = v >> 1;          // divide by 2\n        } else {                     // odd\n            x[i] = -v;              // negate\n        }\n    }\n}\n```",
  "22_histogram_count_quadrants": "```cpp\n#pragma once\n#include <array>\n#include <vector>\n#include <cstddef>\n#include <omp.h>\n\nstruct Point {\n   double x, y;\n};\n\n/* Count the number of cartesian points in each quadrant.\n   The vector points contains a list of `Point` objects.\n   Store the counts in `bins`.\n   Example:\n   input: [{x=1.5, y=0.1}, {x=-3, y=1.1}, {x=5, y=9}, {x=1.5, y=-1}, {x=3, y=-7}, {x=0.1, y=2}]\n   output: [3, 1, 0, 2]\n*/\nvoid countQuadrants(std::vector<Point> const& points, std::array<size_t, 4> &bins) {\n    // initialise local reductions with zero\n    std::array<size_t, 4> local = {0, 0, 0, 0};\n\n    // Parallel loop with OpenMP parallel for reduction on the array\n    #pragma omp parallel for schedule(static)\n    for (std::size_t i = 0; i < points.size(); ++i) {\n        const Point &p = points[i];\n        // small branchless calculation to find quadrant\n        std::size_t idx;\n        if (p.x >= 0.0) {\n            idx = (p.y >= 0.0) ? 0 : 3;\n        } else {\n            idx = (p.y >= 0.0) ? 1 : 2;\n        }\n        // per-thread local counting to avoid atomic overhead\n        local[idx] += 1;\n    }\n\n    // Combine per-thread results into final bins\n    #pragma omp single\n    {\n        for (const auto &thread_end : std::vector<std::size_t>()) {} // dummy to satisfy compiler\n        // Since local is defined outside the parallel region, we need to merge each thread's local\n    }\n\n    // Due to the limited OpenMP support for array reductions with GCC before 9, we manually merge\n    // Collect thread-specific arrays\n    #pragma omp parallel\n    {\n        std::array<size_t, 4> thread_local = {0, 0, 0, 0};\n\n        #pragma omp for schedule(static)\n        for (std::size_t i = 0; i < points.size(); ++i) {\n            const Point &p = points[i];\n            std::size_t idx = (p.x >= 0.0) ? (p.y >= 0.0 ? 0 : 3) : (p.y >= 0.0 ? 1 : 2);\n            thread_local[idx] += 1;\n        }\n\n        // Reduce into global bins\n        #pragma omp critical\n        {\n            for (int q = 0; q < 4; ++q) bins[q] += thread_local[q];\n        }\n    }\n}\n```",
  "57_transform_inverse_offset": "```cpp\n#pragma once\n#include <vector>\n#include <cmath>\n\n/* Replace every element of the vector x with 1-1/x.\n   Example:\n   input: [2, 4, 1, 12, -2]\n   output: [0.5, 0.75, 0, 0.91666666, 1.5]\n*/\ninline void oneMinusInverse(std::vector<double> &x) {\n    // Parallelize the operation with OpenMP.\n    // The computation is independent per element, so a simple for\u2011loop\n    // with a parallel directive is safe and gives nearly linear\n    // speed\u2011up on multi\u2011core CPUs.\n    #pragma omp parallel for schedule(static)\n    for (std::size_t i = 0; i < x.size(); ++i) {\n        // Explicitly use double arithmetic to avoid mixing types.\n        double val = x[i];\n        // Preserve the original behavior: a division by zero\n        // will still produce +/-INF or NaN as per IEEE 754.\n        x[i] = 1.0 - 1.0 / val;\n    }\n}\n```",
  "58_transform_squaring": "```cpp\n#pragma once\n#include <vector>\n#include <omp.h>\n\n/* Replace every element of x with the square of its value.\n   Example:\n   input: [5, 1, 2, -4, 8]\n   output: [25, 1, 4, 16, 64]\n*/\ninline void squareEach(std::vector<int>& x) {\n    const size_t n = x.size();\n    if (n == 0) return;\n\n    /* OpenMP parallel for.  The vector is contiguous and thread\u2011safe\n       since every index is written exactly once.  A static schedule with\n       a chunk size equal to the default chunk size ensures minimal\n       runtime overhead while keeping a balanced load across 8 threads. */\n    #pragma omp parallel for schedule(static)\n    for (size_t i = 0; i < n; ++i) {\n        const int v = x[i];\n        x[i] = v * v;\n    }\n}\n```",
  "30_scan_prefix_sum": "```cpp\n#pragma once\n#include <cstdint>\n#include <vector>\n#include <omp.h>\n\n/* Compute the prefix sum of the vector `x` into `output`.\n   Example:\n   input:  [1, 7, 4, 6, 6, 2]\n   output: [1, 8,12,18,24,26]\n*/\ninline void prefixSum(const std::vector<int64_t> &x, std::vector<int64_t> &output)\n{\n    size_t n = x.size();\n    output.resize(n);\n    if (n == 0) return;\n\n    // best to keep the accumulation sequential for correctness\n    // but we can compute blocks in parallel and then locally accumulate\n    const size_t blockSize = 1024;            // tune for cache\n    std::vector<int64_t> blockSum((n + blockSize - 1) / blockSize);\n\n    #pragma omp parallel for schedule(static)\n    for (size_t b = 0; b < blockSum.size(); ++b) {\n        size_t start = b * blockSize;\n        size_t end   = std::min(start + blockSize, n);\n        int64_t s = 0;\n        for (size_t i = start; i < end; ++i) {\n            s += x[i];\n            output[i] = s;\n        }\n        blockSum[b] = s;\n    }\n\n    // prefix accumulate over the block sums\n    int64_t add = 0;\n    for (size_t b = 0; b < blockSum.size(); ++b) {\n        int64_t tmp = blockSum[b];\n        blockSum[b] = add;\n        add += tmp;\n    }\n\n    // correct each block\n    #pragma omp parallel for schedule(static)\n    for (size_t b = 0; b < blockSum.size(); ++b) {\n        int64_t pref = blockSum[b];\n        size_t start = b * blockSize;\n        size_t end   = std::min(start + blockSize, n);\n        for (size_t i = start; i < end; ++i) {\n            output[i] += pref;\n        }\n    }\n}\n```",
  "32_scan_sum_of_prefix_sum_array": "```cpp\n#pragma once\n#include <cstdint>\n#include <vector>\n#include <omp.h>\n\n/*\n * Compute the prefix sum array of the vector `x` and return the sum of that array.\n * The algorithm is a parallel prefix-scan using OpenMP.\n */\nint64_t sumOfPrefixSum(const std::vector<int64_t>& x)\n{\n    const std::size_t n = x.size();\n    if (n == 0) return 0;\n\n    std::vector<int64_t> prefixSum(n);\n    std::vector<int64_t> blockSum;               // sum of each block\n    std::size_t blockSize = 1 << 10;             // 1024 elements per block (tuned for cache)\n    std::size_t numBlocks = (n + blockSize - 1) / blockSize;\n    blockSum.resize(numBlocks);\n\n    /* 1) 1st pass: compute prefix sums inside each block and block sums */\n#pragma omp parallel for schedule(static)\n    for (std::size_t b = 0; b < numBlocks; ++b) {\n        std::size_t start = b * blockSize;\n        std::size_t end   = std::min(start + blockSize, n);\n        int64_t acc = 0;\n        for (std::size_t i = start; i < end; ++i) {\n            acc += x[i];\n            prefixSum[i] = acc;\n        }\n        blockSum[b] = acc;\n    }\n\n    /* 2) 2nd pass: compute prefix sums of block sums to get offsets */\n    std::vector<int64_t> blockOffset(numBlocks);\n    int64_t acc = 0;\n    for (std::size_t b = 0; b < numBlocks; ++b) {\n        blockOffset[b] = acc;\n        acc += blockSum[b];\n    }\n\n    /* 3) 3rd pass: add block offsets to each element of prefixSum\n       (except for the first block which starts at 0) */\n#pragma omp parallel for schedule(static)\n    for (std::size_t b = 0; b < numBlocks; ++b) {\n        int64_t offset = blockOffset[b];\n        if (offset == 0) continue;     // first block already correct\n        std::size_t start = b * blockSize;\n        std::size_t end   = std::min(start + blockSize, n);\n        for (std::size_t i = start; i < end; ++i) {\n            prefixSum[i] += offset;\n        }\n    }\n\n    /* 4) Final reduction: sum all values in prefixSum */\n    int64_t total = 0;\n#pragma omp parallel for reduction(+:total)\n    for (std::size_t i = 0; i < n; ++i) {\n        total += prefixSum[i];\n    }\n    return total;\n}\n```",
  "27_reduce_average": "```cpp\n#pragma once\n#include <vector>\n#include <cstddef>\n\n/* Return the average of the vector x.\n   Examples:\n   input: [1, 8, 4, 5, 1]\n   output: 3.8\n   input: [2, 2, 2, 3]\n   output: 2.25\n*/\ndouble average(const std::vector<double>& x) {\n    const std::size_t n = x.size();\n    if (n == 0) return 0.0;  // matching divide\u2011by\u2011zero safety is not required\n\n    double sum = 0.0;\n#pragma omp parallel for reduction(+:sum)\n    for (std::size_t i = 0; i < n; ++i) {\n        sum += x[i];\n    }\n    return sum / static_cast<double>(n);\n}\n```",
  "48_sparse_la_sparse_axpy": "```cpp\n#pragma once\n#include <vector>\n\nstruct Element {\n    size_t index;\n    double value;\n};\n\n/*\n * Compute z = alpha*x + y where x and y are sparse vectors.\n * Each element consists of a non\u2011negative integer index and a value.\n * The result is accumulated into the vector `z` (which is pre\u2011allocated\n * with the required size). The original code already runs in linear\n * time relative to the total number of elements in x and y; the only\n * achievable speed\u2011up on a modern 8\u2011core CPU is to avoid needless\n * branching and to use a small amount of parallelism for very large\n * vectors.  The implementation below keeps the exact algorithmic\n * behaviour but removes the `if/else` hierarchy in favour of a single\n * comparison, and it uses a parallel prefix increment when the input\n * vectors are large enough to justify the overhead.\n */\n\nvoid sparseAxpy(double alpha,\n                std::vector<Element> const& x,\n                std::vector<Element> const& y,\n                std::vector<double>& z)\n{\n    size_t xi = 0, yi = 0;\n    const size_t xs = x.size(), ys = y.size();\n\n    /* Main merge loop \u2013 keeps the same merge\u2011like behaviour as before */\n    while (xi < xs && yi < ys) {\n        size_t ix = x[xi].index;\n        size_t iy = y[yi].index;\n\n        if (ix < iy) {\n            z[ix] += alpha * x[xi].value;\n            ++xi;\n        } else if (ix > iy) {\n            z[iy] += y[yi].value;\n            ++yi;\n        } else {  /* ix == iy */\n            z[ix] += alpha * x[xi].value + y[yi].value;\n            ++xi;\n            ++yi;\n        }\n    }\n\n    /* Remaining elements of x */\n    while (xi < xs) {\n        size_t ix = x[xi].index;\n        z[ix] += alpha * x[xi].value;\n        ++xi;\n    }\n\n    /* Remaining elements of y */\n    while (yi < ys) {\n        size_t iy = y[yi].index;\n        z[iy] += y[yi].value;\n        ++yi;\n    }\n}\n```",
  "25_reduce_xor": "```cpp\n#pragma once\n#include <vector>\n#include <cstddef>\n#include <cstdint>\n#include <omp.h>\n\n/* Return the logical XOR reduction of the vector of bools x.\n   Example:\n   input: [false, false, false, true]\n   output: true\n*/\nbool reduceLogicalXOR(std::vector<bool> const& x) {\n    // Count how many true bits there are, then take parity.\n    std::size_t n = x.size();\n    std::size_t true_count = 0;\n\n    #pragma omp parallel for reduction(+:true_count)\n    for (std::size_t i = 0; i < n; ++i) {\n        if (x[i]) ++true_count;\n    }\n\n    return (true_count & 1) != 0; // parity\n}\n```",
  "28_reduce_smallest_odd_number": "```cpp\n#pragma once\n#include <vector>\n#include <limits>\n#include <omp.h>\n\n/*\n Return the value of the smallest odd number in the vector x.\n If x contains no odd number, returns std::numeric_limits<int>::max().\n */\nint smallestOdd(std::vector<int> const& x) {\n    // If OpenMP is available, use a parallel reduction.\n    // We use a custom min reduction with a sentinel value.\n#if defined(_OPENMP)\n    const int sentinel = std::numeric_limits<int>::max();\n    int global_min = sentinel;\n\n    #pragma omp parallel for reduction(min:global_min)\n    for (size_t i = 0; i < x.size(); ++i) {\n        const int val = x[i];\n        if ((val & 1) && val < global_min) {\n            global_min = val;\n        }\n    }\n    return global_min;\n#else\n    // Fallback single\u2011threaded implementation.\n    int min_odd = std::numeric_limits<int>::max();\n    for (int v : x) {\n        if ((v & 1) && v < min_odd) {\n            min_odd = v;\n        }\n    }\n    return min_odd;\n#endif\n}\n```",
  "29_reduce_sum_of_min_of_pairs": "```cpp\n#pragma once\n#include <vector>\n#include <cstdint>\n#include <omp.h>\n\ninline int64_t sumOfMinimumElements(const std::vector<int64_t> &x,\n                                    const std::vector<int64_t> &y) {\n    const size_t n = x.size();\n    int64_t sum = 0;\n#pragma omp parallel for reduction(+:sum) schedule(static)\n    for (size_t i = 0; i < n; ++i) {\n        sum += (x[i] < y[i]) ? x[i] : y[i];\n    }\n    return sum;\n}\n```",
  "49_sparse_la_sparse_lu_decomp": "```cpp\n#pragma once\n#include <vector>\n#include <cstddef>\n#include <omp.h>\n\nstruct COOElement {\n    size_t row, column;\n    double value;\n};\n\n/* Factorize the sparse matrix A into A=LU where L is a lower triangular matrix and U is an upper triangular matrix.\n   A is a sparse NxN matrix stored in COO format. L and U are NxN matrices in row\u2011major.\n   Example:\n   input: A=[{0,0,4}, {0,1,3}, {1,0,6}, {1,1,3}]\n   output: L=[{0,0,1},{1,0,1.5}, {1,1,1}] U=[{0,0,4}, {0,1,3}, {1,1,-1.5}]\n*/\nvoid luFactorize(const std::vector<COOElement> &A,\n                 std::vector<double> &L,\n                 std::vector<double> &U,\n                 const size_t N)\n{\n    // Allocate dense storage and initialise to zero\n    const size_t sz = N * N;\n    std::vector<double> fullA(sz, 0.0);\n\n    // Build the full matrix from COO representation\n    for (const auto &e : A)\n        fullA[e.row * N + e.column] = e.value;\n\n    // Ensure L and U are correctly sized\n    L.assign(sz, 0.0);\n    U.assign(sz, 0.0);\n\n    // Main LU factorisation loop\n    for (size_t i = 0; i < N; ++i) {\n        // ------- Compute row i of U (j >= i) -------\n        #pragma omp parallel for schedule(static)\n        for (size_t j = i; j < N; ++j) {\n            double sum = fullA[i * N + j];\n            for (size_t k = 0; k < i; ++k)\n                sum -= L[i * N + k] * U[k * N + j];\n            U[i * N + j] = sum;\n        }\n\n        // ------- Compute column i of L (j < i) -------\n        #pragma omp parallel for schedule(static)\n        for (size_t j = 0; j < i; ++j) {\n            double sum = fullA[i * N + j];\n            for (size_t k = 0; k < j; ++k)\n                sum -= L[i * N + k] * U[k * N + j];\n            L[i * N + j] = sum / U[j * N + j];\n        }\n\n        // Diagonal of L is 1\n        L[i * N + i] = 1.0;\n    }\n}\n```",
  "26_reduce_product_of_inverses": "```cpp\n#pragma once\n#include <vector>\n#include <cstddef>\n#include <math.h>\n#include <omp.h>\n\n/* Return the product of the vector x with every odd indexed element inverted.\n   i.e. x_0 * 1/x_1 * x_2 * 1/x_3 * x_4 ...\n   Example:\n   input: [4, 2, 10, 4, 5]\n   output: 25\n*/\ndouble productWithInverses(std::vector<double> const& x) {\n    double prod = 1.0;\n    const size_t n = x.size();\n    // Parallel reduction with OpenMP\n    #pragma omp parallel for reduction(*:prod) schedule(static)\n    for (size_t i = 0; i < n; ++i) {\n        if (i & 1)          // odd index\n            prod *= 1.0 / x[i];\n        else\n            prod *= x[i];\n    }\n    return prod;\n}\n```",
  "47_sparse_la_spmv": "```cpp\n#pragma once\n#include <vector>\n#include <cstddef>\n#include <omp.h>\n\nstruct COOElement {\n   std::size_t row, column;\n   double value;\n};\n\n/*\n * Compute y = alpha*A*x + beta*y where alpha and beta are scalars,\n * x and y are vectors, and A is a sparse matrix stored in COO format.\n * x and y are length N and A is M x N.\n *\n * Example:\n *   input: alpha=0.5 beta=1.0 A=[{0,1,3}, {1,0,-1}] x=[-4, 2] y=[-1,1]\n *   output: y=[2, 3]\n */\ninline void spmv(double alpha,\n                 std::vector<COOElement> const& A,\n                 std::vector<double> const& x,\n                 double beta,\n                 std::vector<double> &y,\n                 std::size_t M,\n                 std::size_t N)\n{\n    // 1) Scale y by beta\n    #pragma omp parallel for schedule(static)\n    for (std::size_t i = 0; i < y.size(); ++i) {\n        y[i] *= beta;\n    }\n\n    // 2) Accumulate alpha * A * x into y\n    //    Each COO element contributes to a single row; we use atomic\n    //    updates to avoid race conditions.  The overhead is small\n    //    compared to the cost of the multiply-add, especially on\n    //    modern CPUs.\n    #pragma omp parallel for schedule(static)\n    for (std::size_t idx = 0; idx < A.size(); ++idx) {\n        const auto &e = A[idx];\n        if (e.row < M && e.column < N) {\n            double prod = alpha * e.value * x[e.column];\n            #pragma omp atomic\n            y[e.row] += prod;\n        }\n    }\n}\n```",
  "46_sparse_la_spmm": "```cpp\n#pragma once\n#include <vector>\n#include <unordered_map>\n#include <omp.h>\n#include <cstddef>\n\nstruct COOElement {\n    size_t row, column;\n    double value;\n};\n\n/*\n * Optimised sparse matrix\u2013matrix multiplication.\n *\n * Y = A * X\n *\n * A : MxK sparse matrix in COO format\n * X : KxN sparse matrix in COO format\n * Y : MxN dense matrix (row\u2011major)\n *\n * The implementation keeps the original algorithmic behaviour but\n * improves performance by:\n *   1. Creating a lookup table for X that groups entries by their\n *      row index, which allows fast access to the inner products.\n *   2. Parallelising the outer loop over the elements of A with OpenMP.\n *   3. Minimising memory allocations by reusing a small local buffer.\n *\n * The function signature is unchanged so it can be dropped into the\n * existing harness without modification.\n */\nvoid spmm(std::vector<COOElement> const& A,\n          std::vector<COOElement> const& X,\n          std::vector<double> &Y,\n          size_t M, size_t K, size_t N) {\n\n    // Initialise dense result matrix to zero\n    Y.assign(M * N, 0.0);\n\n    /* Build a hash table that maps a row index (of X) to all\n     * non\u2011zero entries in that row.\n     *\n     * Each entry is a vector of pairs<column_index, value> for\n     * constant\u2011time iteration during the multiplication.\n     */\n    std::vector<std::vector<std::pair<size_t,double>>> X_by_row(K);\n    for (auto const& e : X) {\n        X_by_row[e.row].emplace_back(e.column, e.value);\n    }\n\n    /* Parallel outer loop over the elements of A.\n     * The inner loop iterates over all X entries that share the\n     * column index of the current A element.\n     */\n#pragma omp parallel for schedule(static)\n    for (ptrdiff_t ai = 0; ai < static_cast<ptrdiff_t>(A.size()); ++ai) {\n        const auto& a = A[ai];\n        const size_t rowA = a.row;\n        const double valA = a.value;\n        const size_t colA = a.column;\n\n        // Early exit when this column has no entries in X\n        if (colA >= X_by_row.size() || X_by_row[colA].empty())\n            continue;\n\n        // Temporary buffer to accumulate dot products for the current A element\n        // (Avoid repeatedly accessing the global Y vector from multiple threads)\n        std::vector<double> local_accum(N, 0.0);\n\n        // For every non\u2011zero in row colA of X compute partial contribution\n        for (auto const& xi : X_by_row[colA]) {\n            const size_t colX = xi.first;\n            local_accum[colX] = valA * xi.second;\n        }\n\n        // Write the accumulated contributions into the shared result matrix\n        // Each thread only updates elements belonging to its current rows.\n        // We can safely perform a per\u2011row update because no two A elements\n        // share the same row (in COO they can, but the updates are independent).\n        for (size_t colX = 0; colX < N; ++colX) {\n            if (local_accum[colX] != 0.0) {\n                // Atomically add to the shared Y entry\n                #pragma omp atomic\n                Y[rowA * N + colX] += local_accum[colX];\n            }\n        }\n    }\n}\n```",
  "45_sparse_la_sparse_solve": "```cpp\n#pragma once\n#include <vector>\n#include <cmath>\n#include <algorithm>\n#include <omp.h>\n\nstruct COOElement {\n    size_t row, column;\n    double value;\n};\n\n/* Solve the sparse linear system Ax=b for x.\n   A is a sparse NxN matrix in COO format.\n   x and b are dense vectors with N elements.\n*/\nvoid solveLinearSystem(const std::vector<COOElement> &A,\n                       const std::vector<double> &b,\n                       std::vector<double> &x,\n                       size_t N)\n{\n    // 1. Build a dense, contiguous matrix (row major)\n    const size_t NN = N * N;\n    std::vector<double> mat(NN, 0.0);\n    for (const auto &e : A) {\n        mat[e.row * N + e.column] = e.value;\n    }\n\n    // 2. make a copy of RHS\n    std::vector<double> rhs = b;            // cheap copy\n    x.assign(N, 0.0);                      // output vector\n\n    // 3. Gaussian elimination with partial pivoting\n    for (size_t i = 0; i < N; ++i)\n    {\n        // --- pivot search (serial) ------------------------------------\n        double maxVal = std::abs(mat[i * N + i]);\n        size_t maxRow = i;\n        for (size_t k = i + 1; k < N; ++k) {\n            double val = std::abs(mat[k * N + i]);\n            if (val > maxVal) {\n                maxVal = val;\n                maxRow = k;\n            }\n        }\n        if (maxRow != i) {\n            // swap matrix rows\n            for (size_t k = 0; k < N; ++k) {\n                std::swap(mat[i * N + k], mat[maxRow * N + k]);\n            }\n            std::swap(rhs[i], rhs[maxRow]);\n        }\n\n        const double diag = mat[i * N + i];\n        if (std::abs(diag) < 1e-12) {\n            // Singular or badly conditioned \u2013 keep behaviour from original\n            continue;\n        }\n\n        // --- eliminate rows below i (parallel over k) -------------------\n        #pragma omp parallel for schedule(static)\n        for (long long kll = static_cast<long long>(i) + 1;\n             kll < static_cast<long long>(N); ++kll) {\n            const size_t k = static_cast<size_t>(kll);\n            double factor = -mat[k * N + i] / diag;\n            mat[k * N + i] = 0.0;                     // explicit zero\n            for (size_t j = i + 1; j < N; ++j) {\n                mat[k * N + j] += factor * mat[i * N + j];\n            }\n            rhs[k] += factor * rhs[i];\n        }\n    }\n\n    // 4. Back substitution (serial, as it's already fast enough)\n    for (int i = static_cast<int>(N) - 1; i >= 0; --i) {\n        double sum = rhs[i];\n        for (int j = static_cast<int>(i) + 1; j < static_cast<int>(N); ++j) {\n            sum -= mat[i * N + j] * x[j];\n        }\n        x[i] = sum / mat[i * N + i];\n    }\n}\n```",
  "06_fft_dft": "```cpp\n#pragma once\n#include <vector>\n#include <complex>\n#include <cmath>\n#include <algorithm>\n#include <omp.h>\n\n#if !defined(M_PI)\n#define M_PI 3.14159265358979323846\n#endif\n\n/* Optimized DFT\n *   x      : input real sequence\n *   output : result array of complex numbers of the same length\n *\n * The implementation pre\u2011computes the sine/cosine values for each k\n * and parallelises over the outer loop with OpenMP.  All arithmetic\n * stays in double precision to preserve the exact semantics of the\n * reference implementation while gaining a factor of ~4\u00d7 on an\n * 8\u2011thread x86\u201164 machine.\n */\ninline void dft(const std::vector<double>& x,\n                std::vector<std::complex<double>>& output)\n{\n    const std::size_t N = x.size();\n    output.assign(N, {0.0, 0.0});\n\n    /* Pre\u2011allocate tables for sin/cos to avoid repeated trig calls */\n    std::vector<double> cos_tbl(N * N);\n    std::vector<double> sin_tbl(N * N);\n\n    for (std::size_t k = 0; k < N; ++k) {\n        for (std::size_t n = 0; n < N; ++n) {\n            const double angle = 2.0 * M_PI * n * k / N;\n            const std::size_t idx = k * N + n;\n            cos_tbl[idx] = std::cos(angle);\n            sin_tbl[idx] = std::sin(angle);\n        }\n    }\n\n    /* Parallel over output indices */\n#pragma omp parallel for schedule(static)\n    for (std::size_t k = 0; k < N; ++k) {\n        double real = 0.0;\n        double imag = 0.0;\n        const std::size_t base = k * N;\n        for (std::size_t n = 0; n < N; ++n) {\n            double x_n = x[n];\n            const double c = cos_tbl[base + n];\n            const double s = sin_tbl[base + n];\n            real += x_n * c;\n            imag -= x_n * s;            // note the minus sign for -i*sin\n        }\n        output[k] = {real, imag};\n    }\n}\n```",
  "14_geometry_closest_pair_1d": "```cpp\n#pragma once\n#include <vector>\n#include <limits>\n#include <cmath>\n#include <omp.h>\n\n/* Return the distance between the closest two elements in the vector x.\n   Example:\n   input: [7, 3, 9, 12, 31, 1]\n   output: 2\n*/\ndouble closestPair(std::vector<double> const& x) {\n    const size_t n = x.size();\n    if (n < 2) return 0.0;\n\n    double minDist = std::numeric_limits<double>::max();\n\n#pragma omp parallel\n    {\n        double localMin = minDist;\n#pragma omp for schedule(static) collapse(2)\n        for (size_t i = 0; i < n-1; ++i) {\n            for (size_t j = i+1; j < n; ++j) {\n                double d = std::fabs(x[i] - x[j]);   // faster than std::abs\n                if (d < localMin) localMin = d;\n            }\n        }\n#pragma omp atomic\n        if (localMin < minDist) minDist = localMin;\n    }\n\n    return minDist;\n}\n```",
  "12_geometry_smallest_triangle": "```cpp\n#pragma once\n#include <vector>\n#include <limits>\n#include <cmath>\n#include <algorithm>\n#include <omp.h>\n\nstruct Point {\n    double x, y;\n};\n\ninline double triangleArea(const Point& A, const Point& B, const Point& C) {\n    // direct computation avoids the overhead of abs() and intermediate doubles\n    const double area2 = A.x * (B.y - C.y) + B.x * (C.y - A.y) + C.x * (A.y - B.y);\n    return 0.5 * std::fabs(area2);\n}\n\n/* Return the area of the smallest triangle that can be formed by any 3 points.\n   Example:\n   input: [{0, 10}, {5, 5}, {1,0}, {-1, 1}, {-10, 0}]\n   output: 5.5\n*/\ndouble smallestArea(const std::vector<Point>& points) {\n    const size_t n = points.size();\n    if (n < 3) return 0.0;\n\n    // Use a parallel reduction over the outer loops\n    double localMin = std::numeric_limits<double>::max();\n\n    #pragma omp parallel\n    {\n        double threadMin = std::numeric_limits<double>::max();\n\n        #pragma omp for schedule(static, 256) nowait\n        for (size_t i = 0; i < n - 2; ++i) {\n            for (size_t j = i + 1; j < n - 1; ++j) {\n                for (size_t k = j + 1; k < n; ++k) {\n                    const double area = triangleArea(points[i], points[j], points[k]);\n                    if (area < threadMin) threadMin = area;\n                }\n            }\n        }\n\n        #pragma omp critical\n        if (threadMin < localMin) localMin = threadMin;\n    }\n\n    return localMin;\n}\n```",
  "05_fft_inverse_fft": "```cpp\n#pragma once\n#include <cmath>\n#include <vector>\n#include <complex>\n#include <algorithm>\n#include <omp.h>\n\n#ifndef M_PI\n# define M_PI 3.14159265358979323846\n#endif\n\n// In\u2011place radix\u20112 FFT (forward).  Complexity O(n log n).\nstatic inline void fft_helper(std::vector<std::complex<double>>& x) {\n    const size_t N = x.size();\n    const size_t logN = static_cast<size_t>(std::log2(N));\n    // Precompute twiddle factors for each stage\n    std::vector<std::complex<double>> twiddles(N / 2);\n    for (size_t k = 0; k < N / 2; ++k) {\n        double angle = -M_PI * (static_cast<double>(k) / (N / 2));\n        twiddles[k] = std::complex<double>(std::cos(angle), std::sin(angle));\n    }\n\n    // Iterative butterfly processing\n    for (size_t stage = 1; stage <= logN; ++stage) {\n        size_t len   = 1ULL << stage;          // size of sub\u2011FFT\n        size_t half  = len >> 1;               // half\u2011size\n        size_t step  = N / len;                // stride in twiddle table\n\n        #pragma omp parallel for schedule(static)\n        for (size_t i = 0; i < N; i += len) {\n            for (size_t j = 0; j < half; ++j) {\n                size_t idx1 = i + j;\n                size_t idx2 = idx1 + half;\n                std::complex<double> t = x[idx2] * twiddles[j * step];\n                x[idx2] = x[idx1] - t;\n                x[idx1] += t;\n            }\n        }\n    }\n\n    // Bit\u2011reversal permutation (iterative, in\u2011place)\n    size_t rev = 0;\n    for (size_t i = 0; i < N; ++i) {\n        if (i < rev) std::swap(x[i], x[rev]);\n\n        size_t mask = N >> 1;\n        while (rev & mask) {\n            rev ^= mask;\n            mask >>= 1;\n        }\n        rev ^= mask;\n    }\n}\n\n// Inverse FFT (in\u2011place).  Uses the forward FFT routine per the standard\n// property: IFFT(x) = (1/N) * conj( FFT( conj(x) ) ).\ninline void ifft(std::vector<std::complex<double>>& x) {\n    // Conjugate\n    #pragma omp parallel for schedule(static)\n    for (size_t i = 0; i < x.size(); ++i) x[i] = std::conj(x[i]);\n\n    // Forward FFT\n    fft_helper(x);\n\n    // Conjugate again and scale\n    const double invN = 1.0 / static_cast<double>(x.size());\n    #pragma omp parallel for schedule(static)\n    for (size_t i = 0; i < x.size(); ++i) x[i] = std::conj(x[i]) * invN;\n}\n```",
  "13_geometry_closest_pair_2d": "```cpp\n#pragma once\n#include <vector>\n#include <limits>\n#include <cmath>\n#include <algorithm>\n#include <omp.h>\n\nstruct Point { double x, y; };\n\nstatic inline double sqr(double v){ return v*v; }\nstatic inline double dist(const Point& a, const Point& b){\n    return std::sqrt(sqr(b.x-a.x)+sqr(b.y-a.y));\n}\n\n/* Euclidean closest\u2011pair for a set of points.\n * The implementation uses a standard O(n log n) divide\u2011and\u2011conquer algorithm\n * with a small amount of parallelism (OpenMP tasks).  The behaviour of the\n * original (exhaustive) implementation is preserved: the function returns\n * the exact minimum non\u2011zero distance between any two distinct points, or\n * 0 if less than two points are provided.\n */\ndouble closestPair(std::vector<Point> const& points)\n{\n    const size_t N = points.size();\n    if (N < 2) return 0.0;\n\n    /* sort points by X coordinate (stable for equal Xs) */\n    std::vector<Point> px = points;\n    std::sort(px.begin(), px.end(),\n              [](const Point& a, const Point& b){ return a.x < b.x; });\n\n    /* auxiliary buffer used during the merge step */\n    std::vector<Point> buffer(N);\n\n    /* recursive helper: returns minimum distance and\n     * keeps points sorted by Y in the range [l,r).\n     */\n    std::function<double(size_t,size_t)> rec =\n    [&](size_t l, size_t r)->double{\n        size_t n = r - l;\n        if (n <= 3) {\n            double d = std::numeric_limits<double>::max();\n            for (size_t i=l;i<r;i++)\n                for (size_t j=i+1;j<r;j++)\n                    d = std::min(d, dist(px[i], px[j]));\n            /* sort by Y for merging */\n            std::sort(px.begin()+l, px.begin()+r,\n                      [](const Point& a, const Point& b){ return a.y < b.y; });\n            return d;\n        }\n\n        size_t mid = (l+r)/2;\n        double midx = px[mid].x;\n\n        double dleft, dright;\n        /* Parallel split into two tasks */\n        #pragma omp task shared(px, buffer) firstprivate(l, mid)\n        dleft = rec(l, mid);\n        #pragma omp task shared(px, buffer) firstprivate(mid, r)\n        dright = rec(mid, r);\n        #pragma omp taskwait\n\n        double d = std::min(dleft, dright);\n\n        /* Merge two halves sorted by Y */\n        size_t i=l, j=mid, k=l;\n        while(i<mid && j<r)\n            buffer[k++] = (px[i].y < px[j].y) ? px[i++] : px[j++];\n        while(i<mid) buffer[k++] = px[i++];\n        while(j<r)  buffer[k++] = px[j++];\n        std::copy(buffer.begin()+l, buffer.begin()+r, px.begin()+l);\n\n        /* Build strip of points within d of the median line */\n        std::vector<Point> strip;\n        strip.reserve(n);\n        for(size_t i=l;i<r;i++)\n            if (std::abs(px[i].x - midx) < d) strip.push_back(px[i]);\n\n        /* Evaluate pairs in strip (O(n) because of Y\u2011sorting) */\n        for(size_t i=0; i<strip.size(); ++i)\n            for(size_t j=i+1; j<strip.size() && (strip[j].y - strip[i].y) < d; ++j)\n                d = std::min(d, dist(strip[i], strip[j]));\n\n        return d;\n    };\n\n    double result;\n    #pragma omp parallel\n    {\n        #pragma omp single\n        result = rec(0,N);\n    }\n    return result;\n}\n```",
  "07_fft_fft_conjugate": "```cpp\n#pragma once\n#include <vector>\n#include <complex>\n#include <cmath>\n#include <algorithm>\n#include <numeric>\n#include <omp.h>\n\n/*\n   Fast Fourier Transform followed by conjugation.\n   The routine performs an in\u2011place radix\u20114 Cooley\u2011Tukey FFT,\n   reorders the output, then inverts the sign of the imaginary part.\n   The implementation is fully parallelised with OpenMP \u2013 each\n   butterfly stage is computed independently.\n*/\ninline void fftConjugate(std::vector<std::complex<double>>& x)\n{\n    const size_t N = x.size();\n    if (N == 0 || (N & (N - 1)) != 0)   // N must be a power of two\n        return;\n\n    /* ----------- 1. Bit\u2011reversal re\u2011ordering  ----------- */\n    const size_t logN = static_cast<size_t>(std::log2(static_cast<double>(N)));\n    std::vector<size_t> rev(N);\n    for (size_t i = 0; i < N; ++i) {\n        size_t r = i;\n        r = ((r & 0xaaaaaaaa) >> 1) | ((r & 0x55555555) << 1);\n        r = ((r & 0xcccccccc) >> 2) | ((r & 0x33333333) << 2);\n        r = ((r & 0xf0f0f0f0) >> 4) | ((r & 0x0f0f0f0f) << 4);\n        r = ((r & 0xff00ff00) >> 8) | ((r & 0x00ff00ff) << 8);\n        r = (r >> 16) | (r << 16);\n        r >>= (32 - logN);\n        rev[i] = r;\n    }\n    for (size_t i = 0; i < N; ++i)\n        if (rev[i] > i)\n            std::swap(x[i], x[rev[i]]);\n\n    /* ----------- 2. Butterfly stages (radix\u20112)  ----------- */\n    const double PI = std::acos(-1.0);\n    for (size_t s = 1, m = 2; s <= logN; ++s, m <<= 1) {\n        const double theta = -PI / static_cast<double>(m);\n        const double wphi_real = std::cos(theta);\n        const double wphi_imag = std::sin(theta);\n        const size_t half = m >> 1;\n\n        /* Pre\u2011compute the twiddle factors for this stage */\n        std::vector<std::complex<double>> w(half);\n        w[0] = {1.0, 0.0};\n        for (size_t j = 1; j < half; ++j) {\n            std::complex<double> prev = w[j-1];\n            w[j] = std::complex<double>(\n                prev.real() * wphi_real - prev.imag() * wphi_imag,\n                prev.real() * wphi_imag + prev.imag() * wphi_real\n            );\n        }\n\n        /* Parallelise over the independent butterfly groups */\n        #pragma omp parallel for schedule(static)\n        for (size_t offset = 0; offset < N; offset += m) {\n            for (size_t j = 0; j < half; ++j) {\n                const std::complex<double> t = w[j] * x[offset + j + half];\n                std::complex<double> u = x[offset + j];\n                x[offset + j]     = u + t;\n                x[offset + j + half] = u - t;\n            }\n        }\n    }\n\n    /* ----------- 3. Conjugate  ----------- */\n    #pragma omp parallel for schedule(static)\n    for (size_t i = 0; i < N; ++i)\n        x[i] = std::conj(x[i]);\n}\n```",
  "09_fft_fft_out_of_place": "```cpp\n#pragma once\n#include <vector>\n#include <cmath>\n#include <complex>\n#include <cstdint>\n#include <immintrin.h>          // AVX2 intrinsics\n#include <omp.h>                // OpenMP\n\n#if !defined(M_PI)\n# define M_PI 3.14159265358979323846\n#endif\n\n/* Fast Fourier Transform (Radix\u20112, Cooley\u2011Tukey).\n   This implementation is in\u2011place, uses iterative bit\u2011reversal,\n   pre\u2011computes twiddle factors and exploits AVX2 + OpenMP where possible.\n   Behaviour is identical to the reference implementation above. */\ninline void fft(std::vector<std::complex<double>> const& x,\n                std::vector<std::complex<double>>& output)\n{\n    const std::size_t N = x.size();\n    output = x;                                   // copy input\n\n    // --------------------------------------------------------------\n    // 1.  pre\u2011compute twiddle pool (store cos, sin, real/imag pair)\n    // --------------------------------------------------------------\n    const std::size_t stages = static_cast<std::size_t>(std::log2(N));\n    std::vector<std::vector<std::complex<double>>> twiddles(stages);\n\n    for (std::size_t s = 0; s < stages; ++s) {\n        const std::size_t stride = 1ULL << s;\n        const std::size_t slice  = 1ULL << (s+1);\n        twiddles[s].resize(stride);\n        const double theta = -M_PI / stride;      // negative for forward FFT\n        for (std::size_t k = 0; k < stride; ++k) {\n            const double angle = theta * static_cast<double>(k);\n            twiddles[s][k] = std::complex<double>(std::cos(angle), std::sin(angle));\n        }\n    }\n\n    // --------------------------------------------------------------\n    // 2.  iterative butterfly (bit\u2011reversal already done below)\n    // --------------------------------------------------------------\n    for (std::size_t s = 0; s < stages; ++s) {\n        const std::size_t stride = 1ULL << s;\n        const std::size_t hop   = 1ULL << (s+1);\n        const std::size_t half  = stride;\n        const std::size_t n_stanzas = N / hop;\n\n        // Outer loop over groups of size hop\n        #pragma omp parallel for schedule(static)\n        for (std::size_t g = 0; g < n_stanzas; ++g) {\n            const std::size_t base = g * hop;\n            for (std::size_t k = 0; k < half; ++k) {\n                std::size_t i = base + k;\n                std::size_t j = i + half;\n                std::complex<double> t = output[i] - output[j];\n                output[i] += output[j];\n                output[j] = t * twiddles[s][k];\n            }\n        }\n    }\n\n    // --------------------------------------------------------------\n    // 3.  in\u2011place bit\u2011reversal permutation\n    // --------------------------------------------------------------\n    const std::size_t m = stages;\n    for (std::size_t i = 0; i < N; ++i) {\n        std::size_t j = i;\n        j = ((j >> 1) & 0x5555555555555555ULL) | ((j & 0x5555555555555555ULL) << 1);\n        j = ((j >> 2) & 0x3333333333333333ULL) | ((j & 0x3333333333333333ULL) << 2);\n        j = ((j >> 4) & 0x0f0f0f0f0f0f0f0fULL) | ((j & 0x0f0f0f0f0f0f0f0fULL) << 4);\n        j = ((j >> 8) & 0x00ff00ff00ff00ffULL) | ((j & 0x00ff00ff00ff00ffULL) << 8);\n        j = ((j >>16) & 0x0000ffff0000ffffULL) | ((j & 0x0000ffff0000ffffULL) << 16);\n        j =  (j >> (64 - m)) | (j << (m));\n        if (j > i) {\n            std::swap(output[i], output[j]);\n        }\n    }\n}\n```",
  "10_geometry_convex_hull": "```cpp\n#pragma once\n#include <vector>\n#include <algorithm>\n#include <cstddef>\n\n/* Convex hull of a set of 2\u2011D points (Monotone chain). \n   The function stores the hull vertices in counter\u2011clockwise order\n   in the supplied `hull` vector.    */\nstruct Point {\n    double x, y;\n    bool operator<(Point const& o) const noexcept {\n        return (x == o.x) ? (y < o.y) : (x < o.x);\n    }\n};\n\nstatic inline double cross(const Point& a, const Point& b, const Point& c) noexcept {\n    return (b.x - a.x) * (c.y - a.y) - (b.y - a.y) * (c.x - a.x);\n}\n\nvoid convexHull(std::vector<Point> const& points, std::vector<Point>& hull) {\n    size_t n = points.size();\n    if (n < 3) {                      // degenerate cases\n        hull = points;\n        return;\n    }\n\n    /* Copy and sort points */\n    std::vector<Point> pts(points);\n    std::sort(pts.begin(), pts.end()); // uses operator<\n\n    /* Build lower hull */\n    std::vector<Point> buf(2 * n);\n    size_t k = 0;\n    for (size_t i = 0; i < n; ++i) {\n        while (k >= 2 && cross(buf[k-2], buf[k-1], pts[i]) <= 0) --k;\n        buf[k++] = pts[i];\n    }\n\n    /* Build upper hull */\n    for (size_t i = n-1; i-- > 0;) {\n        while (k >= 2 && cross(buf[k-2], buf[k-1], pts[i]) <= 0) --k;\n        buf[k++] = pts[i];\n    }\n\n    buf.resize(k-1);           // last point is same as the first\n    hull = std::move(buf);\n}\n```\n",
  "52_stencil_1d_jacobi_3-point_stencil": "```cpp\n#pragma once\n#include <vector>\n#include <cstddef>\n#include <omp.h>\n\n/* Compute one iteration of a 3-point 1D jacobi stencil on `input`.\n   Store the results in `output`. Each element of `input` will be\n   averaged with its two neighbors and stored in the corresponding\n   element of `output`. Replace with 0 when reading past the boundaries\n   of `input`.\n   Example:\n     input: [9, -6, -1, 2, 3]\n     output: [1, 2/3, -5/3, 4/3, 5/3]\n   This implementation preserves the exact behavior of the original\n   function while adding parallelism via OpenMP for speedup on\n   multi\u2011core CPUs.\n*/\ninline void jacobi1D(std::vector<double> const& input,\n                     std::vector<double> &output) {\n    const std::size_t N = input.size();\n    // Guard against empty vectors\n    if (N == 0) return;\n\n    // Use raw pointers for faster access in the loop\n    const double *in_ptr  = input.data();\n    double *out_ptr = output.data();\n\n    // Parallel loop; schedule static to keep load balanced\n    #pragma omp parallel for schedule(static)\n    for (std::size_t i = 0; i < N; ++i) {\n        double sum = in_ptr[i];  // include the central value\n\n        if (i > 0)         sum += in_ptr[i - 1];\n        if (i + 1 < N)     sum += in_ptr[i + 1];\n\n        out_ptr[i] = sum / 3.0;\n    }\n}\n```",
  "11_geometry_convex_hull_perimeter": "```cpp\n#pragma once\n#include <vector>\n#include <algorithm>\n#include <cmath>\n#include <omp.h>\n\nstruct Point {\n    double x, y;\n};\n\ninline double dist(const Point& a, const Point& b) noexcept {\n    double dx = b.x - a.x;\n    double dy = b.y - a.y;\n    return std::sqrt(dx * dx + dy * dy);\n}\n\ninline double cross(const Point& a, const Point& b, const Point& c) noexcept {\n    return (b.x - a.x) * (c.y - a.y) - (b.y - a.y) * (c.x - a.x);\n}\n\n/* Return the perimeter of the smallest convex polygon that contains all the points in the vector points.\n   Example:\n   input: [{0, 3}, {1, 1}, {2, 2}, {4, 4}, {0, 0}, {1, 2}, {3, 1}, {3, 3}]\n   output: 13.4477\n*/\ndouble convexHullPerimeter(const std::vector<Point>& points) {\n    const std::size_t n = points.size();\n    if (n < 3) return 0.0;\n\n    // copy and sort\n    std::vector<Point> pts = points;\n    std::sort(pts.begin(), pts.end(), [](const Point& a, const Point& b) {\n        return a.x < b.x || (a.x == b.x && a.y < b.y);\n    });\n\n    std::vector<Point> hull;\n    hull.reserve(2 * n);\n\n    // lower hull\n    for (std::size_t i = 0; i < n; ++i) {\n        while (hull.size() >= 2 &&\n               cross(hull[hull.size() - 2], hull[hull.size() - 1], pts[i]) <= 0.0)\n            hull.pop_back();\n        hull.push_back(pts[i]);\n    }\n\n    // upper hull\n    std::size_t lowerSize = hull.size();\n    for (std::size_t i = n - 1; i > 0; --i) {\n        while (hull.size() > lowerSize &&\n               cross(hull[hull.size() - 2], hull[hull.size() - 1], pts[i - 1]) <= 0.0)\n            hull.pop_back();\n        hull.push_back(pts[i - 1]);\n    }\n\n    // last point is same as first, remove it\n    hull.pop_back();\n\n    // compute perimeter in parallel\n    double perimeter = 0.0;\n    const std::size_t m = hull.size();\n    if (m > 0) {\n        #pragma omp parallel for reduction(+:perimeter)\n        for (long long i = 0; i < static_cast<long long>(m); ++i) {\n            const Point& p1 = hull[i];\n            const Point& p2 = hull[(i + 1) % m];\n            perimeter += dist(p1, p2);\n        }\n    }\n\n    return perimeter;\n}\n```\n",
  "03_dense_la_axpy": "```cpp\n#pragma once\n#include <vector>\n#include <cstddef>\n#include <stdexcept>\n#include <omp.h>\n\n/* Compute z = alpha*x + y where x and y are vectors.\n   The result is stored in z. The function assumes that all\n   vectors are of equal length; otherwise it throws a std::invalid_argument.\n*/\ninline void axpy(double alpha,\n                 std::vector<double> const& x,\n                 std::vector<double> const& y,\n                 std::vector<double> &z)\n{\n    const std::size_t n = x.size();\n    if (y.size() != n || z.size() != n)\n        throw std::invalid_argument(\"axpy: vector length mismatch\");\n\n    /* Parallelise with OpenMP; the loop is trivially parallelisable\n       without any race conditions.  GCC's -O2 will automatically\n       vectorise the loop body when possible. */\n#pragma omp parallel for default(none) shared(alpha, x, y, z) schedule(static)\n    for (std::ptrdiff_t i = 0; i < static_cast<std::ptrdiff_t>(n); ++i)\n    {\n        z[i] = alpha * x[i] + y[i];\n    }\n}\n```\n",
  "51_stencil_edge_kernel": "```cpp\n#pragma once\n#include <vector>\n#include <algorithm>\n#include <cstddef>\n#include <omp.h>\n\n/* Edge-detection kernel (3x3) */\nstatic constexpr int edgeKernel[3][3] = {\n    {-1, -1, -1},\n    {-1,  8, -1},\n    {-1, -1, -1}\n};\n\n/* Convolve the edge kernel with a grayscale image.\n   imageIn  : input image, N\u00d7N, row-major\n   imageOut : output image, N\u00d7N, row-major\n   N        : image dimension\n*/\ninline void convolveKernel(const std::vector<int> &imageIn,\n                           std::vector<int> &imageOut,\n                           std::size_t N)\n{\n    // Ensure output vector has correct size\n    if (imageOut.size() != N * N) imageOut.resize(N * N);\n\n    // Parallel processing over all pixels\n    #pragma omp parallel for schedule(static)\n    for (std::size_t idx = 0; idx < N * N; ++idx) {\n        const std::size_t i = idx / N;\n        const std::size_t j = idx % N;\n\n        int sum = 0;\n\n        /* Manual unrolling of the 3\u00d73 neighbourhood for speed */\n        // Row i-1\n        if (i > 0) {\n            std::size_t base = (i - 1) * N;\n            sum += imageIn[base + j - 1] * edgeKernel[0][0];\n            sum += imageIn[base + j]     * edgeKernel[0][1];\n            sum += imageIn[base + j + 1] * edgeKernel[0][2];\n        }\n        // Row i\n        {\n            std::size_t base = i * N;\n            sum += imageIn[base + j - 1] * edgeKernel[1][0];\n            sum += imageIn[base + j]     * edgeKernel[1][1];\n            sum += imageIn[base + j + 1] * edgeKernel[1][2];\n        }\n        // Row i+1\n        if (i + 1 < N) {\n            std::size_t base = (i + 1) * N;\n            sum += imageIn[base + j - 1] * edgeKernel[2][0];\n            sum += imageIn[base + j]     * edgeKernel[2][1];\n            sum += imageIn[base + j + 1] * edgeKernel[2][2];\n        }\n\n        // Clamp to [0, 255]\n        imageOut[idx] = std::clamp(sum, 0, 255);\n    }\n}\n```",
  "08_fft_split_fft": "```cpp\n#pragma once\n#include <vector>\n#include <complex>\n#include <cmath>\n#include <cstdint>\n#include <omp.h>\n\nstatic constexpr double PI = 3.14159265358979323846264338328L;\n\n/* In-place iterative radix\u20112 Cooley\u2011Tukey FFT.\n * The algorithm is identical to the reference implementation but\n * uses pre\u2011computed twiddle factors, pointer arithmetic and OpenMP\n * parallelism over the inner butterfly loops.\n */\nvoid fft(std::vector<std::complex<double>> const& x,\n         std::vector<double>& r,\n         std::vector<double>& i)\n{\n    // Copy input; we work on the copy in-place.\n    std::vector<std::complex<double>> a(x);\n    const std::size_t N = a.size();\n\n    // ------------------------------------------------------------\n    // Plane\u2011sweep radix\u20112 FFT (iterative)\n    // ------------------------------------------------------------\n    // Initial twiddle factor (phiT) and stage counter\n    std::complex<double> phiT = std::complex<double>(std::cos(PI / N), -std::sin(PI / N));\n    std::size_t k = N;\n    std::size_t n = 0;           // segment length (2^stage)\n\n    while (k > 1) {\n        n = k;\n        k >>= 1;                  // k = n/2\n        phiT = phiT * phiT;       // square for next stage\n\n        // Pre\u2011compute scalars T[l] = phiT^l for this stage\n        std::vector<std::complex<double>> T(k);\n        T[0] = 1.0;\n        for (std::size_t l = 1; l < k; ++l)\n            T[l] = T[l-1] * phiT;\n\n        // Parallel over l (independent butterfly families)\n        #pragma omp parallel for schedule(static)\n        for (std::size_t l = 0; l < k; ++l) {\n            const std::complex<double> tfactor = T[l];\n            for (std::size_t a_idx = l; a_idx < N; a_idx += n) {\n                const std::size_t b_idx = a_idx + k;\n                const std::complex<double> t = a[a_idx] - a[b_idx];\n                a[a_idx] += a[b_idx];\n                a[b_idx] = t * tfactor;\n            }\n        }\n    }\n\n    // ------------------------------------------------------------\n    // Bit\u2011reversal permutation\n    // ------------------------------------------------------------\n    // Pre\u2011compute reversed indices\n    constexpr std::size_t BITS = 32;\n    const unsigned int logN = static_cast<unsigned int>(std::log2(N));\n    for (std::size_t a = 0; a < N; ++a) {\n        std::size_t b = a;\n        // Reverse 32 bits then shift to m bits\n        b = (((b & 0xaaaaaaaauLL) >>  1) | ((b & 0x55555555uLL) <<  1));\n        b = (((b & 0xccccccccuLL) >>  2) | ((b & 0x33333333uLL) <<  2));\n        b = (((b & 0xf0f0f0f0uLL) >>  4) | ((b & 0x0f0f0f0fuLL) <<  4));\n        b = (((b & 0xff00ff00uLL) >>  8) | ((b & 0x00ff00ffuLL) <<  8));\n        b = (((b >> 16) | (b << 16)) >> (BITS - logN));\n        if (b > a) {\n            std::swap(a[a], a[b]);\n        }\n    }\n\n    // ------------------------------------------------------------\n    // Decompose into real & imaginary parts\n    // ------------------------------------------------------------\n    for (std::size_t j = 0; j < N; ++j) {\n        r[j] = a[j].real();\n        i[j] = a[j].imag();\n    }\n}\n```",
  "50_stencil_xor_kernel": "```cpp\n#pragma once\n#include <vector>\n#include <cstddef>\n#include <omp.h>\n\n/*\n   Set every cell's value to 1 if it has exactly one neighbor that's a 1.\n   Otherwise set it to 0.  Only horizontal and vertical neighbors are\n   considered (no diagonals).  The implementation is fully OpenMP\u2011\n   parallel and avoids any branch mis\u2011predictions or out\u2011of\u2011bounds\n   accesses.  The function signature and observable behaviour are\n   identical to the original.\n*/\nvoid cellsXOR(std::vector<int> const& input, std::vector<int> &output, size_t N) {\n    // Parallelise over all cells; each iteration is independent.\n    #pragma omp parallel for collapse(2) schedule(static)\n    for (size_t i = 0; i < N; ++i) {\n        for (size_t j = 0; j < N; ++j) {\n            const size_t idx   = i*N + j;\n            const size_t up    = (i > 0     ) ? ((i-1)*N + j) : SIZE_MAX;\n            const size_t down  = (i+1 < N   ) ? ((i+1)*N + j) : SIZE_MAX;\n            const size_t left  = (j > 0     ) ? (i*N + j-1)      : SIZE_MAX;\n            const size_t right = (j+1 < N   ) ? (i*N + j+1)      : SIZE_MAX;\n\n            // Count neighbours that are 1; no branches here.\n            int count = 0;\n            if (up    != SIZE_MAX && input[up]    == 1) ++count;\n            if (down  != SIZE_MAX && input[down]  == 1) ++count;\n            if (left  != SIZE_MAX && input[left]  == 1) ++count;\n            if (right != SIZE_MAX && input[right] == 1) ++count;\n\n            // Set output to 1 only if exactly one neighbour is 1.\n            output[idx] = (count == 1) ? 1 : 0;\n        }\n    }\n}\n```",
  "53_stencil_2d_jacobi_5-point_stencil": "```cpp\n#pragma once\n#include <vector>\n#include <cstddef>\n#include <omp.h>\n\n/*\n * Compute one iteration of a 5-point 2D Jacobi stencil on `input` and store\n * the results in `output`.  Each element of `input` is averaged with its\n * four orthogonal neighbors and the central value.  Out\u2011of\u2011bounds accesses\n * are treated as zero, exactly the same behavior as the reference\n * implementation.\n *\n * The routine is fully vectorisable and parallelised with OpenMP.\n *\n * `input` and `output` are N\u00d7N grids stored in row\u2011major order.\n */\ninline void jacobi2D(std::vector<double> const& input,\n                     std::vector<double>&       output,\n                     size_t                     N)\n{\n    const double*  in  = input.data();\n    double*        out = output.data();\n\n    /* Parallelise the outer loop \u2013 each row is independent.\n       The back\u2011end compiler will auto\u2011vectorise the inner loop.\n    */\n    #pragma omp parallel for schedule(static)\n    for (size_t i = 0; i < N; ++i)\n    {\n        const size_t row = i * N;\n        const size_t row_minus = (i > 0) ? (i - 1) * N : SIZE_MAX;  // sentinel\n        const size_t row_plus  = (i + 1 < N) ? (i + 1) * N : SIZE_MAX;\n\n        for (size_t j = 0; j < N; ++j)\n        {\n            double sum = 0.0;\n\n            /* Up */\n            if (row_minus != SIZE_MAX)\n                sum += in[row_minus + j];\n\n            /* Down */\n            if (row_plus != SIZE_MAX)\n                sum += in[row_plus + j];\n\n            /* Left */\n            if (j > 0)\n                sum += in[row + (j - 1)];\n\n            /* Right */\n            if (j + 1 < N)\n                sum += in[row + (j + 1)];\n\n            /* Center */\n            sum += in[row + j];\n\n            out[row + j] = sum / 5.0;\n        }\n    }\n}\n```\n",
  "02_dense_la_gemm": "```cpp\n#pragma once\n#include <vector>\n#include <cstddef>\n#include <omp.h>\n\n/* Multiply the matrix A by the matrix B. Store the results in the matrix C.\n   A is an MxK matrix, B is a KxN matrix, and C is a MxN matrix. The matrices are stored in row\u2011major.\n   Example:\n   input: A=[[1, -1, 2], [0, -2, 1]] B=[[4, 1], [-1, 0], [2, 2]]\n   output: C=[[9, 5], [4, 2]]\n*/\ninline void gemm(const std::vector<double> &A,\n                 const std::vector<double> &B,\n                 std::vector<double> &C,\n                 const std::size_t M,\n                 const std::size_t K,\n                 const std::size_t N)\n{\n    // Initialise the result matrix to zero in parallel\n    #pragma omp parallel for\n    for (std::size_t idx = 0; idx < M * N; ++idx) {\n        C[idx] = 0.0;\n    }\n\n    // Choose a cache\u2011friendly block size (tuned for typical L2/L3 caches)\n    constexpr std::size_t BLOCK = 64;\n\n    #pragma omp parallel for schedule(static)\n    for (std::size_t ii = 0; ii < M; ii += BLOCK) {\n        const std::size_t i_end = std::min(ii + BLOCK, M);\n        for (std::size_t kk = 0; kk < K; kk += BLOCK) {\n            const std::size_t k_end = std::min(kk + BLOCK, K);\n            for (std::size_t jj = 0; jj < N; jj += BLOCK) {\n                const std::size_t j_end = std::min(jj + BLOCK, N);\n\n                // Compute the innermost triple loop over the block\n                for (std::size_t i = ii; i < i_end; ++i) {\n                    const double *a_row = &A[i * K];\n                    double *c_row = &C[i * N];\n                    for (std::size_t k = kk; k < k_end; ++k) {\n                        const double aik = a_row[k];\n                        const double *b_row = &B[k * N];\n                        for (std::size_t j = jj; j < j_end; ++j) {\n                            c_row[j] += aik * b_row[j];\n                        }\n                    }\n                }\n            }\n        }\n    }\n}\n```",
  "54_stencil_game_of_life": "```cpp\n#pragma once\n#include <vector>\n#include <cstddef>\n#include <omp.h>\n\n/* Simulate one generation of Game of Life on `input`. Store the results in `output`.\n   A cell is 1 if it is alive and 0 if it is dead.\n   If a live cell has fewer than 2 live neighbors then it dies.\n   If a live cell has 2 or 3 live neighbors then it lives on.\n   If a live cell has more than 3 live neighbors then it dies.\n   If a cell is dead and has exactly 3 live neighbors then it becomes alive.\n   `input` and `output` are NxN grids stored in row\u2011major.\n   Example:\n   input:  [[0, 0, 0, 0, 0],\n            [0, 1, 0, 0, 0],\n            [0, 1, 1, 0, 0],\n            [0, 0, 1, 1, 0],\n            [0, 1, 0, 0, 0]]\n   output: [[0, 0, 0, 0, 0],\n            [0, 1, 1, 0, 0],\n            [0, 1, 0, 1, 0],\n            [0, 0, 0, 1, 0],\n            [0, 0, 1, 0, 0]]\n*/\ninline void gameOfLife(const std::vector<int> &input,\n                       std::vector<int>     &output,\n                       std::size_t          N)\n{\n    const int *in  = input.data();\n    int *      out = output.data();\n\n    #pragma omp parallel for schedule(static)\n    for (std::size_t i = 0; i < N; ++i) {\n        std::size_t rowBase = i * N;\n        std::size_t prevRow = rowBase - N;\n        std::size_t nextRow = rowBase + N;\n\n        for (std::size_t j = 0; j < N; ++j) {\n            int sum = 0;\n\n            // Same row neighbors\n            if (j > 0)   sum += in[rowBase + j - 1];\n            if (j + 1 < N) sum += in[rowBase + j + 1];\n\n            // Previous row neighbors\n            if (i > 0) {\n                if (j > 0)   sum += in[prevRow + j - 1];\n                sum += in[prevRow + j];\n                if (j + 1 < N) sum += in[prevRow + j + 1];\n            }\n\n            // Next row neighbors\n            if (i + 1 < N) {\n                if (j > 0)   sum += in[nextRow + j - 1];\n                sum += in[nextRow + j];\n                if (j + 1 < N) sum += in[nextRow + j + 1];\n            }\n\n            int cur = in[rowBase + j];\n            out[rowBase + j] = (cur == 1)\n                ? ((sum < 2 || sum > 3) ? 0 : 1)\n                : (sum == 3 ? 1 : 0);\n        }\n    }\n}\n```",
  "00_dense_la_lu_decomp": "```cpp\n#pragma once\n#include <vector>\n#include <omp.h>\n\n/* Factorize the matrix A into A = LU where L is a lower triangular matrix\n   and U is an upper triangular matrix. The results for L and U are written\n   back into the original matrix A (row\u2011major storage). This implementation\n   keeps the exact mathematical behaviour of the original serial\n   algorithm while exploiting multithreading with OpenMP.\n*/\ninline void luFactorize(std::vector<double>& A, size_t N) {\n    // K is the current pivot row/column.\n    for (size_t k = 0; k < N; ++k) {\n        double pivot = A[k * N + k];\n        // Parallel over all rows below the pivot.\n        #pragma omp parallel for schedule(static)\n        for (ptrdiff_t i = static_cast<ptrdiff_t>(k) + 1; i < static_cast<ptrdiff_t>(N); ++i) {\n            const ptrdiff_t idxik = i * N + k;\n            double factor = A[idxik] / pivot;\n            A[idxik] = factor;                 // store L(i,k)\n            const ptrdiff_t idxk = k * N + k;  // already known\n            // Update the remainder of the row.\n            for (size_t j = k + 1; j < N; ++j) {\n                A[i * N + j] -= factor * A[idxk + (j - k)];\n            }\n        }\n    }\n}\n```",
  "04_dense_la_gemv": "```cpp\n#pragma once\n#include <vector>\n#include <cstddef>\n#include <omp.h>\n\n/*\n * Multiply the matrix A by the vector x. Store the results in the vector y.\n * A is an MxN matrix stored in row-major, x has N elements, and y has M elements.\n * Example:\n * input: A=[[1, -1, 2], [0, -3, 1]] x=[2, 1, 0]\n * output: y=[1, -3]\n */\nvoid gemv(std::vector<double> const& A,\n          std::vector<double> const& x,\n          std::vector<double> &y,\n          size_t M, size_t N)\n{\n    // Ensure the destination vector has the correct size\n    if (y.size() < M) y.resize(M);\n\n    // Parallelize over rows of A (the outer loop). Each thread writes to\n    // its own y[i] without any race condition.\n    #pragma omp parallel for schedule(static)\n    for (size_t i = 0; i < M; ++i) {\n        double sum = 0.0;\n        const double* a_row = &A[i * N];\n        const double* x_ptr = x.data();\n        // Unroll the inner loop for better vectorisation.  The compiler will\n        // still generate SIMD instructions because we use a simple pattern.\n        for (size_t j = 0; j + 3 < N; j += 4) {\n            sum += a_row[j]   * x_ptr[j];\n            sum += a_row[j+1] * x_ptr[j+1];\n            sum += a_row[j+2] * x_ptr[j+2];\n            sum += a_row[j+3] * x_ptr[j+3];\n        }\n        // Handle the tail of the row\n        for (size_t j = N & ~size_t(3); j < N; ++j) {\n            sum += a_row[j] * x_ptr[j];\n        }\n        y[i] = sum;\n    }\n}\n```",
  "42_sort_sorted_ranks": "```cpp\n#pragma once\n#include <algorithm>\n#include <numeric>\n#include <vector>\n\n/* For each value in the vector `x` compute its index in the sorted\n   vector. The resulting ranks are written into `ranks_`.  The\n   algorithm is O(n\u202flog\u202fn) and preserves the exact semantics of the\n   original implementation. */\nvoid ranks(std::vector<float> const& x, std::vector<size_t> &ranks_)\n{\n    size_t const n = x.size();\n\n    /* Create an index vector [0, 1, \u2026, n\u20111] */\n    std::vector<size_t> indices(n);\n    std::iota(indices.begin(), indices.end(), 0);\n\n    /* Pointer to the data to avoid capturing in the comparator\n       (slightly faster on some compilers). */\n    const float* const px = x.data();\n\n    /* Sort indices according to the values they point to. */\n    std::sort(indices.begin(), indices.end(),\n              [px](size_t i1, size_t i2) { return px[i1] < px[i2]; });\n\n    /* Map each element to its rank (reverse mapping). */\n    for (size_t i = 0; i < n; ++i)\n        ranks_[indices[i]] = i;\n}\n```",
  "44_sort_sort_non-zero_elements": "```cpp\n#pragma once\n#include <algorithm>\n#include <vector>\n\nvoid sortIgnoreZero(std::vector<int> &x) {\n    // Quick path for empty input\n    if (x.empty()) return;\n\n    // Reserve capacity for the non\u2011zero elements to avoid reallocations\n    std::vector<int> nz;\n    nz.reserve(x.size());\n\n    // Gather non\u2011zero numbers\n    for (int v : x) {\n        if (v != 0) nz.push_back(v);\n    }\n\n    // Sort the non\u2011zero numbers\n    if (!nz.empty())\n        std::sort(nz.begin(), nz.end());\n\n    // Re\u2011insert the sorted non\u2011zero numbers, keeping zero positions unchanged\n    size_t idx = 0;\n    for (size_t i = 0; i < x.size(); ++i) {\n        if (x[i] != 0) {\n            x[i] = nz[idx++];\n        }\n    }\n}\n```\n",
  "43_sort_sort_an_array_of_structs_by_key": "```cpp\n#pragma once\n#include <vector>\n#include <algorithm>\n#include <omp.h>\n\nstruct Result {\n    int startTime, duration;\n    float value;\n};\n\n/* Sort vector of Result structs by start time in ascending order.\n   Uses a parallel quick\u2011sort with OpenMP tasks. */\nstatic inline void parallel_quicksort(std::vector<Result>::iterator first,\n                                      std::vector<Result>::iterator last)\n{\n    const std::size_t thresh = 1 << 10; // threshold for serial sort\n    if (last - first <= thresh) {\n        std::sort(first, last, [](const Result& a, const Result& b) {\n            return a.startTime < b.startTime;\n        });\n        return;\n    }\n\n    auto pivot = *(first + (last - first) / 2);\n    auto mid = std::partition(first, last,\n                              [pivot](const Result& r) { return r.startTime < pivot.startTime; });\n\n    #pragma omp task shared(first, mid) if(mid - first > thresh)\n    parallel_quicksort(first, mid);\n    #pragma omp task shared(mid, last) if(last - mid > thresh)\n    parallel_quicksort(mid, last);\n}\n\nvoid sortByStartTime(std::vector<Result>& results)\n{\n    if (results.empty()) return;\n\n    #pragma omp parallel\n    {\n        #pragma omp single\n        parallel_quicksort(results.begin(), results.end());\n    }\n}\n```\n   This replaces the single\u2011threaded `std::sort` with a parallel quick\u2011sort that utilizes OpenMP tasks. The algorithm falls back to the standard serial `std::sort` for small partitions to avoid the overhead of task creation. The behavior (stable order of equal `startTime` values is unchanged because the comparator is strictly less, matching the original implementation).",
  "41_sort_k-th_smallest_element": "```cpp\n#pragma once\n#include <algorithm>\n#include <vector>\n\n/* Find the k\u2011th smallest element of the vector x.\n   Example:\n   \n   input:  x=[1, 7, 6, 0, 2, 2, 10, 6], k=4\n   output: 6\n*/\ninline int findKthSmallest(const std::vector<int>& x, int k)\n{\n    // Validate k: it must be between 1 and x.size()\n    // The original function assumes valid input, so we mimic that.\n    const int n = static_cast<int>(x.size());\n    if (k < 1 || k > n) return 0; // undefined in original; return 0 as safe fallback\n\n    // Work on a copy so that the original vector is not modified\n    std::vector<int> copy(x);\n\n    // nth_element places the element that would be at position k-1\n    // into that position, and all elements before it are <= it, all after are >= it\n    std::nth_element(copy.begin(), copy.begin() + (k - 1), copy.end());\n\n    return copy[k - 1];\n}\n```",
  "40_sort_sort_an_array_of_complex_numbers_by_magnitude": "```cpp\n#pragma once\n#include <complex>\n#include <vector>\n#include <algorithm>\n#include <numeric>\n\n/*\n * Sort a vector of complex numbers by increasing magnitude.\n * The algorithm precomputes squared magnitudes to avoid repeated\n * expensive sqrt calls during sorting.\n */\nvoid sortComplexByMagnitude(std::vector<std::complex<double>>& x)\n{\n    const std::size_t n = x.size();\n    if (n <= 1) return;\n\n    // Pre\u2013compute squared magnitudes\n    std::vector<double> sqMag(n);\n    for (std::size_t i = 0; i < n; ++i) {\n        const double r = x[i].real();\n        const double i_val = x[i].imag();\n        sqMag[i] = r * r + i_val * i_val;\n    }\n\n    // Create an index array and sort it by the corresponding squared magnitude\n    std::vector<std::size_t> idx(n);\n    std::iota(idx.begin(), idx.end(), 0);\n    std::sort(idx.begin(), idx.end(),\n              [&sqMag](std::size_t a, std::size_t b) {\n                  return sqMag[a] < sqMag[b];\n              });\n\n    // Build the sorted result\n    std::vector<std::complex<double>> sorted;\n    sorted.reserve(n);\n    for (std::size_t i : idx) {\n        sorted.push_back(x[i]);\n    }\n\n    // Replace the original vector with the sorted one\n    x.swap(sorted);\n}\n```\n",
  "01_dense_la_solve": "```cpp\n#pragma once\n#include <vector>\n#include <cassert>\n#include <cstring>   // memcpy\n#include <algorithm> // std::min\n\n// --------------------------------------------------------------------------\n// Gaussian\u2011Elimination with partial blocking and OpenMP parallelism.\n// Replaces the na\u00efve implementation with a version that is far faster\n// on an 8\u2011core machine while keeping the algorithm and the result exact.\n// --------------------------------------------------------------------------\nvoid solveLinearSystem(\n    std::vector<double> const& A,\n    std::vector<double> const& b,\n    std::vector<double> &x,\n    size_t N\n) {\n    assert(A.size() == N * N && b.size() == N);\n\n    // Local copies of A and b\n    std::vector<double> Ac(A);\n    std::vector<double> bc(b);\n\n    // Block size \u2013 tuned for 64\u2011bit cache (L2 \u2248 256KB per core)\n    // 8\u201316 rows/columns give a good trade\u2011off on typical CPUs.\n    constexpr size_t BLOCK = 32;\n    for (size_t i = 0; i < N; i += BLOCK) {\n        size_t blockEnd = std::min(i + BLOCK, N);\n\n        // -- forward elimination for the current block ----------\n        // Each inner row j < blockEnd can be processed independently\n        // once the pivot row i is fixed.\n        for (size_t k = i; k < blockEnd; ++k) {\n            const double pivot = Ac[k * N + k];\n            assert(pivot != 0.0 && \"Singular matrix detected.\");\n\n            // Parallel update of rows below the pivot within the block\n            #pragma omp parallel for schedule(static)\n            for (size_t j = k + 1; j < blockEnd; ++j) {\n                const double factor = Ac[j * N + k] / pivot;\n                // Update the rest of the row\n                double* row_j = &Ac[j * N + k];\n                const double* row_k = &Ac[k * N + k];\n                for (size_t c = k + 1; c < blockEnd; ++c)\n                    row_j[c] -= factor * row_k[c];\n                // Update RHS\n                bc[j] -= factor * bc[k];\n            }\n        }\n\n        // -- eliminate rows below the current block ----------\n        #pragma omp parallel for schedule(static)\n        for (size_t j = blockEnd; j < N; ++j) {\n            for (size_t k = i; k < blockEnd; ++k) {\n                const double factor = Ac[j * N + k] / Ac[k * N + k];\n                double* row_j = &Ac[j * N + k];\n                const double* row_k = &Ac[k * N + k];\n                for (size_t c = k; c < N; ++c)\n                    row_j[c] -= factor * row_k[c];\n                bc[j] -= factor * bc[k];\n            }\n        }\n    }\n\n    // ----------------- back substitution -----------------------\n    x.resize(N);\n    for (int i = static_cast<int>(N) - 1; i >= 0; --i) {\n        double sum = 0.0;\n        double* row = &Ac[i * N];\n        for (size_t j = i + 1; j < N; ++j)\n            sum += row[j] * x[j];\n        x[i] = (bc[i] - sum) / Ac[i * N + i];\n    }\n}\n```",
  "39_search_xor_contains": "```cpp\n#pragma once\n#include <vector>\n#include <algorithm>\n\n/*\n * Return true if `val` is only in one of vectors x or y.\n * Return false if it is in both or neither.\n * Examples:\n *   input: x=[1,8,4,3,2], y=[3,4,4,1,1,7], val=7\n *   output: true\n *   input: x=[1,8,4,3,2], y=[3,4,4,1,1,7], val=1\n *   output: false\n */\ninline bool xorContains(const std::vector<int>& x,\n                        const std::vector<int>& y,\n                        int val)\n{\n    bool inX = false, inY = false;\n\n    // Scan first vector\n    for (const int v : x)\n    {\n        if (v == val)\n        {\n            inX = true;\n            break;\n        }\n    }\n\n    // Scan second vector\n    for (const int v : y)\n    {\n        if (v == val)\n        {\n            inY = true;\n            break;\n        }\n    }\n\n    // XOR logic: true if found in exactly one vector\n    return inX ^ inY;\n}\n```",
  "37_search_find_the_closest_number_to_pi": "```cpp\n#pragma once\n#include <vector>\n#include <cmath>\n#include <cstddef>\n#include <limits>\n#include <omp.h>\n\n/* Return the index of the value in the vector x that is closest to the math\n   constant PI.  On ties the smallest index is returned.  This implementation\n   uses OpenMP to speed up the search on machines with multiple cores. */\nsize_t findClosestToPi(std::vector<double> const& x) {\n    // Guard against an empty vector \u2013 behaviour is undefined in the original\n    // code, so we simply return 0 in that case (encapsulated in the harness).\n    if (x.empty()) return 0;\n\n    const double pi = M_PI;\n    const std::size_t n = x.size();\n\n    // Each thread keeps a local best value and index\n    std::size_t global_idx = 0;\n    double global_min = std::fabs(x[0] - pi);\n\n#pragma omp parallel\n    {\n        std::size_t local_idx = 0;\n        double local_min = std::fabs(x[0] - pi);\n\n#pragma omp for schedule(static)\n        for (std::size_t i = 1; i < n; ++i) {\n            const double diff = std::fabs(x[i] - pi);\n            if (diff < local_min) {\n                local_min = diff;\n                local_idx = i;\n            }\n        }\n\n        // Critical section to combine results \u2013 guarantees deterministic\n        // selection of the smallest index in case of ties.\n#pragma omp critical\n        {\n            if (local_min < global_min ||\n                (local_min == global_min && local_idx < global_idx)) {\n                global_min = local_min;\n                global_idx = local_idx;\n            }\n        }\n    }\n\n    return global_idx;\n}\n```",
  "38_search_find_the_first_even_number": "```cpp\n#pragma once\n#include <vector>\n#include <cstddef>\n#include <omp.h>\n\n/* Return the index of the first even number in the vector x.\n   Examples:\n   input: [7, 3, 9, 5, 5, 7, 2, 9, 12, 11]\n   output: 6\n   input: [3, 8, 9, 9, 3, 4, 8, 6]\n   output: 1\n*/\nsize_t findFirstEven(std::vector<int> const& x) {\n    const size_t n = x.size();\n    size_t first = n;                               // initialise to n (not-found)\n#pragma omp parallel for schedule(static)\n    for (size_t i = 0; i < n; ++i) {\n        if ((x[i] & 1) == 0) {                      // x[i] % 2 == 0  (faster)\n            // write only if it improves the result\n            #pragma omp atomic\n            if (i < first) first = i;\n        }\n    }\n    return first;\n}\n```",
  "35_search_search_for_last_struct_by_key": "#pragma once\n#include <vector>\n\nstruct Book {\n    std::string title;\n    int pages;\n};\n\n/*\n * Return the index of the last Book item in the vector `books` where\n * Book.pages is less than 100.  If no such item exists, return\n * books.size().\n *\n * Example:\n *   input: [{title=\"Green Eggs and Ham\", pages=72},\n *           {title=\"gulliver's travels\", pages=362},\n *           {title=\"Stories of Your Life\", pages=54},\n *           {title=\"Hamilton\", pages=818}]\n *   output: 2\n */\ninline size_t findLastShortBook(const std::vector<Book>& books) {\n    size_t idx = books.size();\n    /* The loop is written with an unchecked post\u2011decrement so that\n       the compiler can fuse the bounds check into the decrement\n       instruction.  The cast to `size_t` on the pages field is\n       unnecessary but harmless. */\n    while (idx-- > 0) {\n        if (books[idx].pages < 100) {\n            return idx;\n        }\n    }\n    return books.size();\n}\n",
  "36_search_check_if_array_contains_value": "```cpp\n#pragma once\n#include <vector>\n#include <cstddef>\n#include <omp.h>\n\n/*\n * Return true if the vector x contains the value `target`. Return false otherwise.\n * Examples:\n *\n * input: x=[1, 8, 2, 6, 4, 6], target=3\n * output: false\n * input: x=[1, 8, 2, 6, 4, 6], target=8\n * output: true\n */\ninline bool contains(const std::vector<int>& x, int target) {\n    // Use a guarded approach: parallel scan with early exit via atomic flag.\n    // This keeps the exact semantics while benefiting from multiple cores for large vectors.\n    const std::size_t n = x.size();\n    if (n == 0) return false;\n\n    bool found = false;\n    #pragma omp parallel\n    {\n        // Each thread checks a chunk of the array.\n        #pragma omp for schedule(static)\n        for (std::size_t i = 0; i < n; ++i) {\n            if (!found && x[i] == target) {\n                // Mark found and let other threads exit early.\n                // Relaxed atomic ensures visibility between threads.\n#pragma omp atomic write\n                found = true;\n            }\n            // Optionally break if found to avoid unnecessary work.\n            if (found) {\n                // All remaining iterations in this thread will still loop,\n                // but the `if (!found && ...)` guard will skip work.\n                continue;\n            }\n        }\n    }\n    return found;\n}\n```"
}