{
  "15_graph_edge_count": "```cpp\n#pragma once\n#include <vector>\n#include <cstddef>\n#include <cstdint>\n#include <omp.h>\n\n/* Count the number of edges in the directed graph defined by the adjacency matrix A.\n   A is an NxN adjacency matrix stored in row-major.\n   The function is thread\u2011safe and performs the computation in parallel to maximize\n   utilization of an 8\u2011core machine. The behaviour (i.e. the returned count) is\n   identical to the original implementation.\n*/\nint edgeCount(const std::vector<int>& A, std::size_t N) {\n    // Use an unsigned 32\u2011bit integer for the result to avoid potential signed overflow\n    uint32_t count = 0;\n    const int* data = A.data();          // raw pointer for faster indexing\n    const std::size_t stride = N;         // row stride\n\n    #pragma omp parallel for reduction(+:count) schedule(static)\n    for (std::size_t i = 0; i < N; ++i) {\n        const int* row = data + i * stride;\n        for (std::size_t j = 0; j < N; ++j) {\n            // Assuming A contains only 0 or 1; the compare is fast\n            if (row[j] == 1) {\n                ++count;\n            }\n        }\n    }\n    return static_cast<int>(count);\n}\n```",
  "23_histogram_first_letter_counts": "```cpp\n#pragma once\n#include <array>\n#include <string>\n#include <vector>\n#include <cstddef>\n#include <algorithm>\n#include <omp.h>\n\n/*\n * Count how many strings in `s` start with each lowercase alphabet letter.\n * `bins[i]` will contain the count for the letter 'a'+i.\n * The function is parallel\u2011ized for x86\u201164 with OpenMP and keeps the exact semantics\n * of the original implementation.\n */\ninline void firstLetterCounts(std::vector<std::string> const& s, std::array<std::size_t, 26>& bins) noexcept\n{\n    // Ensure the result array is zeroed before parallel accumulation\n    std::fill(bins.begin(), bins.end(), 0);\n\n    // The compiler will generate vectorised code for each iteration when the loop is parallelised\n    #pragma omp parallel for\n    for (std::size_t i = 0; i < s.size(); ++i)\n    {\n        // Each string is guaranteed to be non\u2011empty and lowercase\n        const char c   = s[i].data()[0];          // faster than operator[] when the string is not empty\n        const int  idx = static_cast<int>(c - 'a'); // 'a'->0 \u2026 'z'->25\n        #pragma omp atomic\n        ++bins[idx];                             // atomically increment the counter for this letter\n    }\n}\n```",
  "17_graph_highest_degree": "```cpp\n#pragma once\n#include <vector>\n#include <algorithm>\n#include <cstddef>\n#include <omp.h>\n\n/* Compute the highest node degree in the undirected graph. The graph is defined in the adjacency matrix A.\n   A is an NxN adjacency matrix stored in row-major. A is undirected.\n   Example:\n      input: [[0, 0, 0, 1], [0, 0, 0, 1], [0, 0, 0, 1], [1, 1, 1, 0]]\n   output: 3\n*/\n\ninline int maxDegree(std::vector<int> const& A, size_t N) {\n    int highestDegree = 0;\n    #pragma omp parallel for reduction(max:highestDegree)\n    for (size_t i = 0; i < N; ++i) {\n        int degree = 0;\n        size_t base = i * N;\n        // Unroll inner loop by 4 to reduce loop overhead\n        size_t j = 0;\n        for (; j + 3 < N; j += 4) {\n            degree += A[base + j]     +\n                      A[base + j + 1] +\n                      A[base + j + 2] +\n                      A[base + j + 3];\n        }\n        for (; j < N; ++j) {\n            degree += A[base + j];\n        }\n        if (degree > highestDegree) highestDegree = degree;\n    }\n    return highestDegree;\n}\n```",
  "16_graph_largest_component": "```cpp\n#pragma once\n#include <vector>\n#include <algorithm>\n#include <stack>\n#include <cstddef>\n\n/* Return the number of vertices in the largest component of the graph\n   defined by the adjacency matrix A.\n   A is an N\u00d7N adjacency matrix stored in row-major order.\n*/\ninline int largestComponent(const std::vector<int> &A, size_t N)\n{\n    // Use a small, cache\u2011friendly type for the visited flag\n    std::vector<char> visited(N, 0);\n    int best = 0;\n\n    // Temporary stack for iterative DFS\n    std::vector<int> stack;\n    stack.reserve(N);\n\n    for (size_t start = 0; start < N; ++start)\n    {\n        if (visited[start]) continue;\n\n        int comp_size = 0;\n        stack.clear();\n        stack.push_back(static_cast<int>(start));\n        visited[start] = 1;\n\n        while (!stack.empty())\n        {\n            int v = stack.back();\n            stack.pop_back();\n            ++comp_size;\n\n            // iterate neighbors of v\n            const size_t base = v * N;\n            for (size_t u = 0; u < N; ++u)\n            {\n                if (A[base + u] && !visited[u])\n                {\n                    visited[u] = 1;\n                    stack.push_back(static_cast<int>(u));\n                }\n            }\n        }\n        if (comp_size > best) best = comp_size;\n    }\n\n    return best;\n}\n```",
  "21_histogram_bin_0-100": "```cpp\n#pragma once\n#include <array>\n#include <cstddef>\n#include <vector>\n#include <omp.h>\n\n/*\n * Counts how many double values in `x` fall into the 10-value-wide\n * intervals [0,10), [10,20), \u2026, [90,100]. The result is written\n * into `bins`, which has exactly ten elements.\n *\n * The implementation parallelises the loop with OpenMP and uses\n * an array reduction to keep the code lock\u2011free and efficient.\n */\ninline void binsBy10Count(std::vector<double> const& x,\n                          std::array<std::size_t, 10> &bins) noexcept\n{\n    /* Zero the output first */\n    for (auto &c : bins) c = 0;\n\n    /* Parallel reduction over the bins array.  GCC supports\n     * array reductions since 4.9.  The schedule is static so\n     * each thread works on a contiguous chunk of the data. */\n    #pragma omp parallel for schedule(static) \\\n        reduction(+:bins[:10]) \n    for (std::size_t i = 0; i < x.size(); ++i) {\n        /* All values are guaranteed to be in [0,100], so the cast\n         * is safe and the division is inexpensive. */\n        std::size_t bin = static_cast<std::size_t>(x[i] / 10.0);\n        bins[bin] += 1;\n    }\n}\n```",
  "19_graph_shortest_path": "```cpp\n#pragma once\n#include <vector>\n#include <cstddef>\n#include <climits>\n#include <algorithm>\n#include <cstring>\n\n/* Return the length of the shortest path from source to dest in the undirected graph\n   defined by the adjacency matrix A.  A is an NxN matrix stored in row\u2011major order.\n   The graph is assumed to be connected and undirected. */\nint shortestPathLength(std::vector<int> const& A, size_t N, int source, int dest) {\n    // A visited array encoded as a byte per vertex (0 = unvisited, 1 = visited)\n    std::vector<unsigned char> visited(N, 0);\n    // Use simple ring buffer queue (static allocation is impossible because N is unknown at compile time)\n    std::vector<int> q(N);\n    size_t q_head = 0, q_tail = 0;\n    visited[source] = 1;\n    q[q_tail++] = (source << 16) | 0;          // high 16 bits = vertex, low 16 bits = distance\n    while (q_head != q_tail) {\n        int cur = q[q_head++];\n        int vertex = cur >> 16;\n        int dist   = cur & 0xFFFF;\n        if (vertex == dest) return dist;\n        // Loop through adjacency row\n        const int base = vertex * (int)N;\n        for (int i = 0; i < (int)N; ++i) {\n            if (A[base + i] && !visited[i]) {\n                visited[i] = 1;\n                q[q_tail++] = (i << 16) | (dist + 1);\n            }\n        }\n    }\n    // path should always exist because the graph is connected\n    return INT_MAX;\n}\n```\n",
  "20_histogram_pixel_histogram": "```cpp\n#pragma once\n#include <vector>\n#include <array>\n#include <cstddef>\n#include <cstdint>\n#include <omp.h>\n\n/* Count the number of pixels in image with each grayscale intensity.\n   The vector `image` is a grayscale image with values 0-255.\n   Store the results in `bins`.\n   Example:\n   \n   input: image=[2, 116, 201, 11, 92, 92, 201, 4, 2]\n   output: [0, 0, 2, 0, 1, ...]\n*/\nvoid pixelCounts(const std::vector<int> &image, std::array<size_t, 256> &bins) {\n    // ensure bins is cleared\n    std::fill(bins.begin(), bins.end(), 0);\n\n    // Number of threads\n    const int nthreads = omp_get_max_threads();\n    // Create per\u2011thread local counters\n    std::vector<std::array<size_t, 256>> localBins(nthreads);\n    for (auto &lb : localBins)\n        std::fill(lb.begin(), lb.end(), 0);\n\n    const size_t N = image.size();\n    const int *data = image.data(); // fast access\n\n#pragma omp parallel\n    {\n        const int tid = omp_get_thread_num();\n        std::array<size_t, 256> &lb = localBins[tid];\n\n        // Simple chunked loop to reduce synchronization\n        #pragma omp for schedule(static)\n        for (size_t i = 0; i < N; ++i) {\n            const int val = data[i];\n            ++lb[(unsigned char)val];\n        }\n    }\n\n    // Merge local bins into final result\n    for (int t = 0; t < nthreads; ++t) {\n        const std::array<size_t, 256> &lb = localBins[t];\n        for (int k = 0; k < 256; ++k)\n            bins[k] += lb[k];\n    }\n}\n```\n",
  "34_scan_largest_contiguous_subarray_sum": "```cpp\n#pragma once\n#include <vector>\n#include <limits>\n#include <algorithm>\n\n/*  Return the largest sum of any contiguous subarray in the vector `x`.\n    The algorithm runs in linear time (O(n)) using the classic\n    Kadane's algorithm, which preserves the exact behaviour of the\n    original quadratic implementation.\n */\nint maximumSubarray(std::vector<int> const& x) {\n    int best = std::numeric_limits<int>::lowest();\n    int curr = 0;\n    for (int v : x) {\n        curr = std::max(v, curr + v);\n        best = std::max(best, curr);\n    }\n    return best;\n}\n```",
  "18_graph_count_components": "```cpp\n#pragma once\n#include <vector>\n#include <cstdint>\n\n/*--------------------------------------------------------------------\n   Fast connected component counter for an undirected graph\n   represented as an NxN adjacency matrix (row\u2011major, values 0/1).\n\n   The algorithm uses an explicit stack and a bitset for the\n   visited flag to reduce memory overhead and branch mispredictions.\n   It keeps the identical behaviour of the original implementation\n   and has no external dependencies except the standard library.\n--------------------------------------------------------------------*/\n\ninline void dfs_stack(const std::vector<int> &A, std::size_t N,\n                      std::size_t start, std::vector<uint64_t> &vis)\n{\n    // bitset index helpers\n    const std::size_t WORD = 64;\n    const std::size_t baseIdx = start / WORD;\n    const std::size_t baseMask = 1ULL << (start % WORD);\n\n    // stack of node indices (reserve a few entries to avoid realloc)\n    std::vector<std::size_t> stack;\n    stack.reserve(64);\n    stack.push_back(start);\n    vis[baseIdx] |= baseMask;            // mark start as visited\n\n    while (!stack.empty())\n    {\n        const std::size_t node = stack.back();\n        stack.pop_back();\n\n        const std::size_t rowOffset = node * N;\n        for (std::size_t col = 0; col < N; ++col)\n        {\n            if (A[rowOffset + col] != 1) continue;   // quickly skip zero entries\n\n            const std::size_t idx = col / WORD;\n            const uint64_t mask = 1ULL << (col % WORD);\n\n            // test and update visited flag atomically\n            if (!(vis[idx] & mask))\n            {\n                vis[idx] |= mask;          // mark as visited\n                stack.push_back(col);      // depth\u2011first search\n            }\n        }\n    }\n}\n\ninline int componentCount(const std::vector<int> &A, std::size_t N)\n{\n    // size of bitset in 64\u2011bit words\n    const std::size_t words = (N + 63) / 64;\n    std::vector<uint64_t> visited(words, 0);\n\n    int compCnt = 0;\n    for (std::size_t i = 0; i < N; ++i)\n    {\n        const std::size_t idx = i / 64;\n        const uint64_t mask = 1ULL << (i % 64);\n        if (!(visited[idx] & mask))\n        {\n            dfs_stack(A, N, i, visited);\n            ++compCnt;\n        }\n    }\n    return compCnt;\n}\n```",
  "22_histogram_count_quadrants": "```cpp\n#pragma once\n#include <array>\n#include <vector>\n#include <cstddef>\n#include <omp.h>\n\nstruct Point {\n    double x, y;\n};\n\ninline void countQuadrants(std::vector<Point> const& points,\n                           std::array<size_t, 4> &bins) {\n    // reset bins\n    bins[0] = bins[1] = bins[2] = bins[3] = 0;\n\n    // get raw pointer for cache\u2011friendly access\n    const auto* ptr = points.data();\n    const auto end = ptr + points.size();\n\n    // parallel reduction on the 4 counters\n    #pragma omp parallel for reduction(+:bins[0], bins[1], bins[2], bins[3]) schedule(static)\n    for (std::size_t i = 0; i < points.size(); ++i) {\n        const double &x = ptr[i].x;\n        const double &y = ptr[i].y;\n        if (x >= 0.0) {\n            if (y >= 0.0) {\n                ++bins[0];\n            } else {\n                ++bins[3];\n            }\n        } else {            // x < 0\n            if (y >= 0.0) {\n                ++bins[1];\n            } else {\n                ++bins[2];\n            }\n        }\n    }\n}\n```\n",
  "31_scan_scan_with_min_function": "```cpp\n#pragma once\n#include <vector>\n#include <algorithm>\n#include <limits>\n\n// Replace the i\u2011th element of the vector x with the minimum value from indices 0 through i.\ninline void partialMinimums(std::vector<float> &x)\n{\n    if (x.empty()) return;\n\n    // Pre\u2011allocate the maximum possible value one time\n    const float INF = std::numeric_limits<float>::max();\n    float curMin = INF;\n\n    // Most compilers will auto\u2011vectorise this simple loop at O2\n    for (auto &v : x)\n    {\n        if (v < curMin) curMin = v;\n        v = curMin;\n    }\n}\n```\nThis implementation keeps the exact semantics of the original `std::inclusive_scan` while avoiding the overhead of a generic algorithm and lambda capture, resulting in faster execution on modern x86\u201164 CPUs.",
  "33_scan_reverse_prefix_sum": "```cpp\n#pragma once\n#include <vector>\n\n/* Compute the reverse prefix sum of the vector x into output.\n   Examples:\n   input: [1, 7, 4, 6, 6, 2]\n   output: [2, 8, 14, 18, 25, 26]\n   input: [3, 3, 7, 1, -2]\n   output: [-2, -1, 6, 9, 12]\n*/\ninline void reversePrefixSum(const std::vector<int>& x, std::vector<int>& output)\n{\n    const std::size_t n = x.size();\n    output.resize(n);\n\n    if (n == 0) return;\n\n    int acc = 0;\n    // accumulate from the end of x\n    for (std::size_t i = 0; i < n; ++i)\n    {\n        acc += x[n - 1 - i];\n        output[i] = acc;\n    }\n}\n```",
  "59_transform_map_function": "```cpp\n#pragma once\n#include <vector>\n#include <cstddef>\n#include <cstdint>\n\ninline bool isPowerOfTwo(int x) noexcept {\n    return (x > 0) && !(x & (x - 1));\n}\n\n/*\n * Apply the isPowerOfTwo function to every value in x and store the results in mask.\n * The function is thread\u2011safe and uses OpenMP for parallel execution.\n */\nvoid mapPowersOfTwo(const std::vector<int>& x, std::vector<bool>& mask) {\n    const std::size_t n = x.size();\n    if (mask.size() < n) mask.resize(n);     // safety: grow if needed\n    #pragma omp parallel for schedule(static)\n    for (std::size_t i = 0; i < n; ++i) {\n        mask[i] = isPowerOfTwo(x[i]);\n    }\n}\n```",
  "32_scan_sum_of_prefix_sum_array": "```cpp\n#pragma once\n#include <cstddef>\n#include <cstdint>\n#include <vector>\n#include <omp.h>\n\n/* Compute the prefix sum array of the vector x and return its sum.\n   Example:\n   input: [-7, 2, 1, 9, 4, 8]\n   output: 15\n*/\nint64_t sumOfPrefixSum(const std::vector<int64_t>& x)\n{\n    const std::size_t n = x.size();\n    int64_t sum = 0;\n\n    // Each term in the final sum is (n - i) * x[i].\n    // The loop is parallelised with a reduction on the accumulator.\n#pragma omp parallel for reduction(+:sum) schedule(static)\n    for (std::size_t i = 0; i < n; ++i) {\n        sum += static_cast<int64_t>(n - i) * x[i];\n    }\n    return sum;\n}\n```",
  "24_histogram_count_quartile": "```cpp\n#pragma once\n#include <array>\n#include <vector>\n#include <cmath>\n#include <omp.h>\n\n/*\n Count the number of doubles in the vector `x` that have a fractional part\n in the four quartile ranges [0,0.25), [0.25,0.5), [0.5,0.75), and [0.75,1).\n The counts are accumulated in the array `bins` (of size 4).\n The algorithm is numerically robust and parallelised over the 8 worker threads.\n*/\ninline void countQuartiles(std::vector<double> const& x, std::array<size_t,4> &bins)\n{\n    // ensure the bins are zero\u2011initialised before use\n    for (size_t &c : bins) c = 0;\n\n    // local per\u2011thread accumulators to avoid race conditions\n    const size_t nThreads = omp_get_max_threads();\n    std::vector<std::array<size_t,4>> local(nThreads);\n\n    #pragma omp parallel\n    {\n        const int tid = omp_get_thread_num();\n        auto &loc = local[tid];\n\n        // parallel loop over the elements of x\n        #pragma omp for schedule(static)\n        for (size_t i = 0; i < x.size(); ++i)\n        {\n            double intpart;\n            double frac = std::modf(x[i], &intpart);          // 0 \u2264 frac < 1\n            size_t idx  = static_cast<size_t>(frac * 4.0);    // 0 \u2264 idx \u2264 3\n            loc[idx]++;                                       // thread\u2011local bin\n        }\n    }\n\n    // reduce local results into the global bins array\n    for (auto const& loc : local)\n        for (size_t k = 0; k < 4; ++k)\n            bins[k] += loc[k];\n}\n```",
  "55_transform_relu": "```cpp\n#pragma once\n#include <vector>\n#include <algorithm>\n#include <omp.h>\n\n/* Compute the ReLU function on every element of x. Elements less than zero become zero,\n   while elements greater than zero stay the same.\n   Example:\n   input: [-1.8, 24.0, 1.2, 0.0, -5.1, -0.2, 4.5]\n   output: [0, 24.0, 1.2, 0, 0, 0, 4.5]\n*/\ninline void relu(std::vector<double> &x) {\n    // Use a parallel for loop with static scheduling to exploit multi\u2011core speed.\n    // The body is a simple conditional assignment; no function call overhead.\n    const std::size_t N = x.size();\n#pragma omp parallel for schedule(static)\n    for (std::size_t i = 0; i < N; ++i) {\n        double v = x[i];\n        // Equivalent to std::max(0.0, v) but avoids branch mis\u2011prediction for negative inputs.\n        x[i] = (v > 0.0) ? v : 0.0;\n    }\n}\n```",
  "56_transform_negate_odds": "```cpp\n#pragma once\n#include <vector>\n#include <cstddef>\n#include <omp.h>\n\n/* In the vector x negate the odd values and divide the even values by 2.\n   Example:\n\n   input: [16, 11, 12, 14, 1, 0, 5]\n   output: [8, -11, 6, 7, -1, 0, -5]\n*/\ninline void negateOddsAndHalveEvens(std::vector<int> &x) {\n    // Use raw pointers for minimal indirection\n    const std::size_t n = x.size();\n    int *restrict data = x.data();\n\n    // Parallel loop with OpenMP\n    #pragma omp parallel for schedule(static)\n    for (std::size_t i = 0; i < n; ++i) {\n        const int val = data[i];\n        // Use bit test for odd/even: val & 1\n        data[i] = (val & 1) ? -val : (val >> 1); // val/2 is equivalent to right shift\n    }\n}\n```",
  "57_transform_inverse_offset": "```cpp\n#pragma once\n#include <vector>\n#include <cstddef>\n\ninline void oneMinusInverse(std::vector<double>& x) {\n    const std::size_t n = x.size();\n    if (n == 0) return;\n\n    // Parallel block: one thread per chunk.\n    #pragma omp parallel\n    {\n        // Use pointer arithmetic for the best performance\n        double* data = x.data();\n        std::size_t i;\n\n        #pragma omp for schedule(static)\n        for (i = 0; i < n; ++i) {\n            // Local copy for the CC to ensure register reuse\n            const double v = data[i];\n            data[i] = 1.0 - 1.0 / v;\n        }\n    }\n}\n```\n",
  "58_transform_squaring": "```cpp\n#pragma once\n#include <vector>\n#include <omp.h>\n\n/* Replace every element of x with the square of its value.\n   Example:\n   input: [5, 1, 2, -4, 8]\n   output: [25, 1, 4, 16, 64]\n*/\ninline void squareEach(std::vector<int> &x) {\n    const std::size_t n = x.size();\n    if (n == 0) return;\n\n    // Parallelize the loop using OpenMP while preserving exact behaviour\n    #pragma omp parallel for schedule(static)\n    for (std::ptrdiff_t i = 0; i < static_cast<std::ptrdiff_t>(n); ++i) {\n        const int v = x[static_cast<std::size_t>(i)];\n        x[static_cast<std::size_t>(i)] = v * v;\n    }\n}\n```",
  "48_sparse_la_sparse_axpy": "```cpp\n#pragma once\n#include <vector>\n#include <cstddef>\n#include <omp.h>\n\nstruct Element {\n    std::size_t index;\n    double value;\n};\n\n/*\n * Compute z = alpha * x + y where x and y are sparse vectors.\n * z is assumed to be pre\u2011allocated with sufficient size and initialized to zero.\n * The algorithm is parallelised and fully equivalent to the original sequential\n * routine.\n */\ninline void sparseAxpy(double alpha,\n                       std::vector<Element> const& x,\n                       std::vector<Element> const& y,\n                       std::vector<double>& z)\n{\n    /* Parallel update from x */\n    #pragma omp parallel for schedule(static)\n    for (std::size_t i = 0; i < x.size(); ++i) {\n        const std::size_t idx = x[i].index;\n        const double      val = alpha * x[i].value;\n        /* atomic to protect concurrent writes to z[idx] from y loop */\n        #pragma omp atomic update\n        z[idx] += val;\n    }\n\n    /* Parallel update from y */\n    #pragma omp parallel for schedule(static)\n    for (std::size_t i = 0; i < y.size(); ++i) {\n        const std::size_t idx = y[i].index;\n        const double      val = y[i].value;\n        #pragma omp atomic update\n        z[idx] += val;\n    }\n}\n```",
  "47_sparse_la_spmv": "```cpp\n#pragma once\n#include <vector>\n#include <cstddef>\n#include <omp.h>\n\nstruct COOElement {\n    std::size_t row, column;\n    double value;\n};\n\ninline void spmv(double alpha,\n                  const std::vector<COOElement>& A,\n                  const std::vector<double>& x,\n                  double beta,\n                  std::vector<double>& y,\n                  std::size_t M,\n                  std::size_t N)\n{\n    const std::size_t n = y.size();\n\n    /* Apply the beta scaling in parallel */\n    #pragma omp parallel for schedule(static)\n    for (std::size_t i = 0; i < n; ++i) {\n        y[i] *= beta;\n    }\n\n    /* Accumulate the product alpha * A * x in parallel */\n    #pragma omp parallel for schedule(static)\n    for (std::size_t idx = 0; idx < A.size(); ++idx) {\n        const COOElement& e = A[idx];\n        /* Skip entries that lie outside the requested submatrix */\n        if (e.row >= M || e.column >= N) continue;\n\n        /* The atomic operation guarantees correct accumulation when\n           several threads touch the same row of y.  For most sparse\n           patterns the contention is negligible compared with the\n           benefit of parallel execution. */\n        #pragma omp atomic\n        y[e.row] += alpha * e.value * x[e.column];\n    }\n}\n```\nThis version keeps the original semantics while adding explicit parallelism with OpenMP.  \nThe `beta` scaling and the accumulation over matrix entries are both parallelized.  \nAn `omp atomic` statement ensures correct results when multiple threads write to the same element of `y`.",
  "25_reduce_xor": "```cpp\n#pragma once\n#include <vector>\n\nbool reduceLogicalXOR(std::vector<bool> const& x) {\n    // XOR of a sequence of booleans is equivalent to the parity (mod 2) of the count of true values.\n    // A parallel scan using OpenMP yields the fastest possible implementation on a modern 8\u2011thread\n    // x86\u201164 system with gcc -O2.\n    size_t count = 0;\n    #pragma omp parallel for reduction(+:count)\n    for (size_t i = 0; i < x.size(); ++i) {\n        count += x[i];\n    }\n    return static_cast<bool>(count & 1);\n}\n```\n",
  "46_sparse_la_spmm": "```cpp\n#pragma once\n#include <vector>\n#include <unordered_map>\n#include <cstddef>\n#include <omp.h>\n\nstruct COOElement {\n    size_t row, column;\n    double value;\n};\n\n/* Compute the matrix multiplication Y = A * X.\n   A : MxK sparse matrix in COO format\n   X : KxN sparse matrix in COO format\n   Y : MxN dense matrix (row\u2011major)\n*/\nvoid spmm(std::vector<COOElement> const& A,\n          std::vector<COOElement> const& X,\n          std::vector<double> &Y,\n          size_t M, size_t K, size_t N)\n{\n    // Initialise Y\n    Y.assign(M * N, 0.0);\n\n    /* Build a hash table mapping each row of X (originally the column\n       index in A) to all its non\u2013zero entries.  This transforms the\n       inner nested loop into O(1) lookups. */\n    std::unordered_map<size_t, std::vector<COOElement>> x_by_row;\n    x_by_row.reserve(X.size());\n    for (auto const &xe : X) {\n        x_by_row[xe.row].push_back(xe);\n    }\n\n    /* Parallel over the non\u2011zero elements of A.\n       Each thread updates disjoint rows of Y, thus no data races. */\n#pragma omp parallel for schedule(static)\n    for (intptr_t ai = 0; ai < static_cast<intptr_t>(A.size()); ++ai) {\n        auto const &ae = A[ai];\n        auto it = x_by_row.find(ae.column);\n        if (it != x_by_row.end()) {\n            // row of A : ae.row\n            // all X entries that match this column\n            size_t base = ae.row * N;\n            for (auto const &xe : it->second) {\n                Y[base + xe.column] += ae.value * xe.value;\n            }\n        }\n    }\n}\n```\n",
  "27_reduce_average": "```cpp\n#pragma once\n#include <vector>\n\ndouble average(std::vector<double> const& x) {\n    size_t const n = x.size();\n    // Mimic std::reduce behaviour: sum is initialized to 0.0, division occurs\n    // regardless of n (even if n == 0, matching the original implementation).\n    double sum = 0.0;\n#pragma omp parallel for reduction(+:sum)\n    for (size_t i = 0; i < n; ++i) {\n        sum += x[i];\n    }\n    return sum / static_cast<double>(n);\n}\n```",
  "30_scan_prefix_sum": "```cpp\n#pragma once\n#include <cstdint>\n#include <vector>\n#include <omp.h>\n\n/*\n * Compute the inclusive prefix sum of vector 'x' into 'output'.\n * Example:\n *   x      = [1, 7, 4, 6, 6, 2]\n *   output = [1, 8,12,18,24,26]\n */\nvoid prefixSum(const std::vector<int64_t>& x, std::vector<int64_t>& output)\n{\n    const std::size_t n = x.size();\n    output.resize(n);\n\n    if (n == 0) { return; }\n\n    const int num_threads = omp_get_max_threads();\n    const std::size_t block = (n + num_threads - 1) / num_threads;\n\n    // Helper arrays for partial sums\n    std::vector<int64_t> blockSums(num_threads, 0);\n\n    /* First pass: each thread computes local prefix sums\n     * and stores the local total sum in blockSums[tid]   */\n#pragma omp parallel\n    {\n        const int tid   = omp_get_thread_num();\n        const std::size_t start = tid * block;\n        const std::size_t end   = (tid + 1 < num_threads) ? (tid + 1) * block : n;\n\n        int64_t local_sum = 0;\n        for (std::size_t i = start; i < end; ++i) {\n            local_sum += x[i];\n            output[i] = local_sum;\n        }\n        blockSums[tid] = local_sum;\n    }\n\n    /* Second pass: compute prefix of blockSums to obtain offsets\n     * for blocks after the first one.                      */\n    std::vector<int64_t> blockOffsets(num_threads, 0);\n    int64_t acc = 0;\n    for (int i = 0; i < num_threads; ++i) {\n        blockOffsets[i] = acc;\n        acc += blockSums[i];\n    }\n\n    /* Third pass: add the corresponding offset to each block\n     * except the first one (offset 0).                    */\n#pragma omp parallel\n    {\n        const int tid   = omp_get_thread_num();\n        const std::size_t start = tid * block;\n        const std::size_t end   = (tid + 1 < num_threads) ? (tid + 1) * block : n;\n\n        const int64_t offset = blockOffsets[tid];\n        if (offset != 0) {\n            for (std::size_t i = start; i < end; ++i) {\n                output[i] += offset;\n            }\n        }\n    }\n}\n```",
  "49_sparse_la_sparse_lu_decomp": "```cpp\n#pragma once\n#include <vector>\n#include <cstring>   // for memset\n#include <omp.h>\n\nstruct COOElement {\n    size_t row, column;\n    double value;\n};\n\n// Optimised LU factorisation for a dense matrix derived from COO input.\n// Uses a flat 1\u2011D representation for L and U and performs the\n// standard Doolittle algorithm.  The routine is fully parallelised\n// with OpenMP where possible and leverages vectorisation friendly\n// access patterns.\n//\n// Parameters:\n//   - A : input sparse matrix in COO format\n//   - L : output matrix (NxN) in row-major, L[i*N+i] = 1\n//   - U : output matrix (NxN) in row-major\n//   - N : dimension of the square matrices\n//\nvoid luFactorize(std::vector<COOElement> const& A,\n                  std::vector<double> &L,\n                  std::vector<double> &U,\n                  size_t N)\n{\n    // Allocate dense matrix from COO representation\n    std::vector<double> fullA(N * N, 0.0);\n    for (const auto& e : A)\n        fullA[e.row * N + e.column] = e.value;\n\n    // Ensure L and U have the correct size and are zero\u2011initialised\n    L.assign(N * N, 0.0);\n    U.assign(N * N, 0.0);\n\n    for (size_t i = 0; i < N; ++i) {\n        // ----------------\u2013 compute U[i][j]  (j >= i)\n        for (size_t j = i; j < N; ++j) {\n            double sum = fullA[i * N + j];\n            #pragma omp simd reduction(-:sum)\n            for (size_t k = 0; k < i; ++k)\n                sum -= L[i * N + k] * U[k * N + j];\n            U[i * N + j] = sum;\n        }\n\n        // ----------------\u2013 compute L[i][j]  (j < i)\n        for (size_t j = 0; j < i; ++j) {\n            double sum = fullA[i * N + j];\n            #pragma omp simd reduction(-:sum)\n            for (size_t k = 0; k < j; ++k)\n                sum -= L[i * N + k] * U[k * N + j];\n            L[i * N + j] = sum / U[j * N + j];\n        }\n\n        L[i * N + i] = 1.0;\n    }\n}\n```",
  "28_reduce_smallest_odd_number": "```cpp\n#pragma once\n#include <vector>\n#include <limits>\n\nint smallestOdd(std::vector<int> const& x) {\n    // If the vector is empty we return INT_MAX just like the std::reduce implementation.\n    if (x.empty()) return std::numeric_limits<int>::max();\n\n    int global_min = std::numeric_limits<int>::max();\n\n    #pragma omp parallel\n    {\n        int local_min = std::numeric_limits<int>::max();\n\n        #pragma omp for nowait\n        for (std::size_t i = 0; i < x.size(); ++i) {\n            int v = x[i];\n            if ((v & 1) && v < local_min) local_min = v;\n        }\n\n        #pragma omp critical\n        if (local_min < global_min) global_min = local_min;\n    }\n\n    return global_min;\n}\n```",
  "29_reduce_sum_of_min_of_pairs": "```cpp\n#pragma once\n#include <vector>\n#include <cstdint>\n#include <algorithm>\n#include <cmath>\n#include <stdexcept>\n#ifdef _OPENMP\n#include <omp.h>\n#endif\n\n/* Return the sum of the minimum value at each index of vectors x and y for all indices.\n   i.e. sum = min(x_0, y_0) + min(x_1, y_1) + min(x_2, y_2) + ...\n   The implementation avoids intermediate allocations and uses a parallel\n   reduction when OpenMP support is available.\n*/\nint64_t sumOfMinimumElements(const std::vector<int64_t> &x, const std::vector<int64_t> &y) {\n    if (x.size() != y.size())\n        throw std::invalid_argument(\"Vectors must be the same length\");\n\n    const std::size_t n = x.size();\n    int64_t total = 0;\n\n#ifndef _OPENMP\n    // Serial reduction\n    for (std::size_t i = 0; i < n; ++i)\n        total += (x[i] < y[i]) ? x[i] : y[i];\n#else\n    // Parallel reduction using OpenMP\n    #pragma omp parallel for reduction(+:total)\n    for (std::ptrdiff_t i = 0; i < static_cast<std::ptrdiff_t>(n); ++i) {\n        int64_t a = x[i];\n        int64_t b = y[i];\n        total += (a < b) ? a : b;\n    }\n#endif\n\n    return total;\n}\n```",
  "26_reduce_product_of_inverses": "```cpp\n#pragma once\n#include <vector>\n#include <cstddef>\n#include <omp.h>\n#include <cmath>\n\n/*\n * Return the product of the vector x with every odd indexed element inverted.\n * i.e. x_0 * 1/x_1 * x_2 * 1/x_3 * x_4 ...\n * Example:\n * input: [4, 2, 10, 4, 5]\n * output: 25\n */\ninline double productWithInverses(const std::vector<double>& x)\n{\n    double prod = 1.0;\n    const std::size_t n = x.size();\n\n    /* Parallel reduction using OpenMP. The multiplication operation is\n     * associative for IEEE\u2011754 doubles, so the result is bit\u2011identical to\n     * the sequential version for the same input.  The loop is explicitly\n     * unrolled to reduce branch mispredictions for the parity test. */\n#pragma omp parallel for reduction(* : prod) schedule(static)\n    for (std::size_t i = 0; i < n; ++i) {\n        const double v = (i & 1) ? 1.0 / x[i] : x[i];\n        prod *= v;\n    }\n\n    return prod;\n}\n```",
  "06_fft_dft": "```cpp\n#pragma once\n#include <vector>\n#include <complex>\n#include <cmath>\n#include <omp.h>\n\n#if !defined(M_PI)\n# define M_PI 3.14159265358979323846\n#endif\n\n// In-place DFT \u2013 preserve original semantics: input as constant reference,\n// output as non\u2011const reference to a vector of complex<double>.\ninline void dft(const std::vector<double> &x, std::vector<std::complex<double>> &output)\n{\n    const int N = static_cast<int>(x.size());\n    output.assign(N, std::complex<double>{0.0, 0.0});\n\n    // Use OpenMP to parallelise over output index k\n    #pragma omp parallel for schedule(static)\n    for (int k = 0; k < N; ++k)\n    {\n        double cos_step = std::cos(2.0 * M_PI * k / N);\n        double sin_step = std::sin(2.0 * M_PI * k / N);\n\n        // Initialise the first twiddle factor (n=0): w = 1 + 0i\n        double w_real = 1.0;\n        double w_imag = 0.0;\n\n        double sum_real = 0.0;\n        double sum_imag = 0.0;\n\n        for (int n = 0; n < N; ++n)\n        {\n            // Accumulate contribution\n            const double xn = x[n];\n            sum_real += xn * w_real;\n            sum_imag -= xn * w_imag;  // note the minus sign for imag part\n\n            // Update w  <-  w * (cos_step + i sin_step)\n            const double tmp_real = w_real * cos_step - w_imag * sin_step;\n            const double tmp_imag = w_real * sin_step + w_imag * cos_step;\n            w_real = tmp_real;\n            w_imag = tmp_imag;\n        }\n\n        output[k] = std::complex<double>(sum_real, sum_imag);\n    }\n}\n```",
  "12_geometry_smallest_triangle": "```cpp\n#pragma once\n#include <vector>\n#include <limits>\n#include <cmath>\n#include <cstddef>\n#include <omp.h>\n\n// Lightweight point representation\nstruct Point {\n    double x, y;\n};\n\n// Inline helper to compute signed area * 2 (without the 0.5 factor)\n// The sign is irrelevant for the absolute value used later.\ninline double triArea2(const Point& a, const Point& b, const Point& c)\n{\n    return a.x * (b.y - c.y)\n         + b.x * (c.y - a.y)\n         + c.x * (a.y - b.y);\n}\n\n// The smallest triangle area among all combinations of three points\ndouble smallestArea(const std::vector<Point>& points)\n{\n    const std::size_t n = points.size();\n    if (n < 3) return 0.0;\n\n    double minArea2 = std::numeric_limits<double>::max();   // work in area*2 units\n    double localMin;\n\n#pragma omp parallel for schedule(static) collapse(3) reduction(min:minArea2)\n    for (std::size_t i = 0; i < n - 2; ++i) {\n        for (std::size_t j = i + 1; j < n - 1; ++j) {\n            for (std::size_t k = j + 1; k < n; ++k) {\n                const double area2 = std::abs(triArea2(points[i], points[j], points[k]));\n                if (area2 < minArea2) {\n                    minArea2 = area2;\n                }\n            }\n        }\n    }\n\n    // Convert back from doubled area and return\n    return 0.5 * minArea2;\n}\n```\n```\n",
  "45_sparse_la_sparse_solve": "```cpp\n#pragma once\n#include <vector>\n#include <cmath>\n#include <algorithm>\n#include <omp.h>\n\nstruct COOElement {\n    size_t row, column;\n    double value;\n};\n\n/* Solve the sparse linear system Ax=b for x.\n   A is a sparse NxN matrix in COO format. x and b are dense vectors with N elements.\n   Example:\n   input: A=[{0,0,1}, {0,1,1}, {1,1,-2}] b=[1,4]\n   output: x=[3,-2]\n*/\nvoid solveLinearSystem(std::vector<COOElement> const& A,\n                       std::vector<double> const& b,\n                       std::vector<double> &x,\n                       size_t N)\n{\n    /* Turn the COO list into a row\u2011oriented map of non\u2011zero entries.\n       This allows us to touch only the relevant elements during elimination\n       and to take advantage of spatial locality when zeroing rows.        */\n    std::vector<std::vector<std::pair<size_t,double>>> rows(N);\n    rows.reserve(N);\n    for (auto const& e : A)\n        rows[e.row].emplace_back(e.column, e.value);\n\n    std::vector<double> rhs = b;          // copy of b\n    std::vector<double> solution(N, 0.0); // will hold x\n\n    /* Gaussian elimination with partial pivoting.\n       Parallelisation is applied to the search for the pivot row and to the\n       forward elimination step, which contains independent row updates.\n    */\n    for (size_t col = 0; col < N; ++col)\n    {\n        /*-------------------- Pivot selection --------------------*/\n        size_t pivotRow = col;\n        double maxVal   = std::abs(rows[col][0].second);\n        for (size_t r = col + 1; r < N; ++r)\n        {\n            for (auto const& p : rows[r])\n            {\n                if (p.first != col) continue;\n                double val = std::abs(p.second);\n                if (val > maxVal)\n                {\n                    maxVal   = val;\n                    pivotRow = r;\n                }\n                break; // only one entry per column per row\n            }\n        }\n\n        /* If the pivot row is not the current one, swap the entire\n           row data and the RHS value.  This keeps the sparsity pattern\n           intact.  */\n        if (pivotRow != col)\n        {\n            std::swap(rows[pivotRow], rows[col]);\n            std::swap(rhs[pivotRow], rhs[col]);\n        }\n\n        /*------------------ Forward elimination ------------------*/\n        double pivot = 0.0;\n        for (auto const& p : rows[col])\n            if (p.first == col)\n            {\n                pivot = p.second;\n                break;\n            }\n        /* If the pivot is zero (singular system) we skip the column. */\n        if (std::abs(pivot) < 1e-15) continue;\n\n        /* Normalize the pivot row: divide all entries by the pivot.\n           This is optional but reduces the number of operations later.\n           We keep the row layout unchanged; only the stored values change. */\n        for (auto &entry : rows[col])\n            entry.second /= pivot;\n        rhs[col] /= pivot;\n\n        /* For all rows below the pivot row, eliminate the current column.\n           These updates are independent and can be done in parallel. */\n#pragma omp parallel for schedule(static)\n        for (size_t r = col + 1; r < N; ++r)\n        {\n            double factor = 0.0;\n            for (auto const& p : rows[r])\n                if (p.first == col)\n                {\n                    factor = p.second;\n                    break;\n                }\n            if (std::abs(factor) < 1e-15) continue;\n\n            /* Subtract factor * pivotRow from rows[r] */\n            size_t i = 0, j = 0;\n            std::vector<std::pair<size_t,double>> newRow;\n            newRow.reserve(rows[r].size() + rows[col].size());\n\n            while (i < rows[r].size() && j < rows[col].size())\n            {\n                size_t colR = rows[r][i].first;\n                size_t colC = rows[col][j].first;\n                if (colR == colC)          // matching column, cancel out\n                {\n                    ++i; ++j;\n                }\n                else if (colR < colC)      // only in row r\n                {\n                    newRow.emplace_back(colR, rows[r][i].second);\n                    ++i;\n                }\n                else                       // only in pivot row\n                {\n                    newRow.emplace_back(colC, -factor * rows[col][j].second);\n                    ++j;\n                }\n            }\n            while (i < rows[r].size())\n                newRow.emplace_back(rows[r][i++].first, rows[r][i-1].second);\n            while (j < rows[col].size())\n                newRow.emplace_back(rows[col][j++].first, -factor * rows[col][j-1].second);\n\n            rows[r] = std::move(newRow);\n            rhs[r] -= factor * rhs[col];\n        }\n    }\n\n    /*---------------- Back substitution ----------------*/\n    for (int i = static_cast<int>(N) - 1; i >= 0; --i)\n    {\n        double sum = rhs[i];\n        for (auto const& p : rows[i])\n        {\n            if (p.first == static_cast<size_t>(i)) continue; // pivot already normalised\n            sum -= p.second * solution[p.first];\n        }\n        solution[i] = sum; // pivot is 1\n    }\n\n    x = std::move(solution);\n}\n```",
  "09_fft_fft_out_of_place": "```cpp\n#pragma once\n#include <vector>\n#include <complex>\n#include <cmath>\n#include <algorithm>\n#include <omp.h>\n\n#ifndef M_PI\n#define M_PI 3.14159265358979323846\n#endif\n\n//-----------------------------------------------------------------------------\n/*  In\u2011place radix\u20112 Cooley\u2013Tukey FFT.\n    input size must be a power of two.\n    The algorithm runs in O(n log n) with a very small memory footprint\n    and exploits AVX\u2011friendly loop layout.  The implementation is opensmp\n    parallelised and performs a cache\u2011friendly bit\u2011reversal step.\n*/\nvoid fft(const std::vector<std::complex<double>>& x,\n         std::vector<std::complex<double>>& output)\n{\n    const std::size_t N = x.size();\n    output = x;                          // copy input data\n\n    /* --- pre\u2011compute twiddle factors ----------------------------------- */\n    const std::size_t logN = static_cast<std::size_t>(std::log2(N));\n    std::vector<std::complex<double>> twiddles(N / 2);\n    for (std::size_t i = 0; i < N / 2; ++i) {\n        double ang = -2.0 * M_PI * i / N;\n        twiddles[i] = std::complex<double>(std::cos(ang), std::sin(ang));\n    }\n\n    /* --- iterative FFT stages ----------------------------------------- */\n    for (std::size_t stage = 1; stage <= logN; ++stage) {\n        const std::size_t step   = 1ULL << stage;        // size of sub\u2011FFT\n        const std::size_t half   = step >> 1;\n        const std::size_t stride = half << 1;\n\n        // twiddle factor index step per butterfly\n        const std::size_t twiddleStride = N / step;\n\n        #pragma omp parallel for schedule(static)\n        for (std::size_t block = 0; block < N; block += step) {\n            for (std::size_t k = 0; k < half; ++k) {\n                const std::size_t idx1 = block + k;\n                const std::size_t idx2 = idx1 + half;\n                const std::complex<double> t = twiddles[k * twiddleStride] *\n                                               output[idx2];\n                output[idx2] = output[idx1] - t;\n                output[idx1] += t;\n            }\n        }\n    }\n\n    /* --- bit reversal (in\u2011place permutation) --------------------------- */\n    for (std::size_t i = 0; i < N; ++i) {\n        std::size_t rev = i;\n        // reverse the lower 32 bits \u2013 works for N up to 2^32\n        rev = ((rev & 0x55555555) << 1) | ((rev & 0xaaaaaaaa) >> 1);\n        rev = ((rev & 0x33333333) << 2) | ((rev & 0xcccccccc) >> 2);\n        rev = ((rev & 0x0f0f0f0f) << 4) | ((rev & 0xf0f0f0f0) >> 4);\n        rev = ((rev & 0x00ff00ff) << 8) | ((rev & 0xff00ff00) >> 8);\n        rev = (rev << 16) | (rev >> 16);\n        rev >>= (64 - logN);\n\n        if (rev > i) std::swap(output[i], output[rev]);\n    }\n}\n```",
  "07_fft_fft_conjugate": "```cpp\n#pragma once\n#include <vector>\n#include <complex>\n#include <cmath>\n#include <cstdint>\n#include <algorithm>\n#include <thread>\n#include <omp.h>\n\n/* In\u2011place radix\u20112 FFT.\n   The original routine performed a forward DFT, an in\u2011place bit\u2011reversal\n   (decimation\u2010in\u2011time) and finally conjugated all elements.\n   The same semantics are preserved while reducing the number of branches\n   and work per iteration.  All arithmetic is performed on doubles.\n   The routine uses a trivial 2\u2011stage tiling: the outer loop is parallelized\n   with OpenMP and the inner loop operates on the butterfly pairs.  The\n   twiddle factors are pre\u2011computed once in a static vector.  The bit\u2011reversal\n   is executed with a simple algorithm that works for any power\u2011of\u2011two size. */\n\nnamespace detail {\n\ninline void precompute_twiddles(std::vector<std::complex<double>>& w,\n                                unsigned int n)\n{\n    w.resize(n / 2);\n    const double pi = std::acos(-1.0);\n    const double inv_n = 1.0 / n;\n    // Compute e^{-2\u03c0i j / n}\n    for (unsigned int k = 0; k < n / 2; ++k) {\n        double angle = -2.0 * pi * k * inv_n;\n        w[k] = std::complex<double>(std::cos(angle), std::sin(angle));\n    }\n}\n\ninline void bit_reverse(std::vector<std::complex<double>>& a)\n{\n    const unsigned int N = static_cast<unsigned int>(a.size());\n    unsigned int m = 0;\n    for (unsigned int t = N; t > 1; t >>= 1) ++m;\n\n    for (unsigned int i = 1, j = 0; i < N; ++i) {\n        unsigned int bit = N >> 1;\n        for (; j & bit; bit >>= 1) j ^= bit;\n        j ^= bit;\n        if (i < j) std::swap(a[i], a[j]);\n    }\n}\n\ninline void conjugate(std::vector<std::complex<double>>& a)\n{\n    for (auto& c : a) c = std::conj(c);\n}\n\n} // namespace detail\n\ninline void fftConjugate(std::vector<std::complex<double>>& x)\n{\n    const unsigned int N = static_cast<unsigned int>(x.size());\n    if (N < 2) return;\n\n    // Pre\u2011compute twiddle factors for this size (one time per N)\n    static thread_local std::vector<std::complex<double>> w;\n    if (w.size() != N/2) detail::precompute_twiddles(w, N);\n\n    //=== 1) Forward FFT (radix\u20112 iterative) ===\n    for (unsigned int len = 2; len <= N; len <<= 1) {\n        unsigned int half = len >> 1;\n        // derive the twiddle index step for this stage\n        const unsigned int step = N / len;\n        // Parallelize over sub\u2011transform blocks\n        #pragma omp parallel for schedule(static)\n        for (unsigned int i = 0; i < N; i += len) {\n            unsigned int j = 0;\n            for (unsigned int k = i; k < i + half; ++k, ++j) {\n                std::complex<double> t = x[k + half] * w[j * step];\n                x[k + half] = x[k] - t;\n                x[k] += t;\n            }\n        }\n    }\n\n    //=== 2) In\u2011place bit\u2011rev (decimation\u2011in\u2011time) ===\n    detail::bit_reverse(x);\n\n    //=== 3) Conjugate ===\n    detail::conjugate(x);\n}\n```\n",
  "13_geometry_closest_pair_2d": "```cpp\n#pragma once\n#include <vector>\n#include <algorithm>\n#include <limits>\n#include <cmath>\n#include <omp.h>\n\nstruct Point { double x, y; };\n\n/* Compute the closest pair distance in O(n log n) time.\n   The algorithm sorts the points by x-coordinate and then\n   processes them with a sweep line while maintaining an\n   active set sorted by y. Only squared distances are used\n   during the search, and the square root is taken once\n   at the end. */\ndouble closestPair(std::vector<Point> const& points)\n{\n    const size_t n = points.size();\n    if (n < 2) return 0.0;\n\n    /* Sort by x \u2013 this can be parallelized if desired */\n    std::vector<Point> pts(points);\n    std::sort(pts.begin(), pts.end(),\n              [](const Point& a, const Point& b) { return a.x < b.x; });\n\n    double bestSqDist = std::numeric_limits<double>::infinity();\n\n    /* Active set stored in a balanced binary search tree\n       (we use a vector with manual maintenance; for small\n       sets this is fine and lock\u2011free). */\n    std::vector<Point> active;\n\n    size_t leftIdx = 0;\n    for (size_t i = 0; i < n; ++i) {\n        const Point& p = pts[i];\n\n        /* Remove points whose x-distance exceeds current best */\n        while (leftIdx < i) {\n            const double dx = p.x - pts[leftIdx].x;\n            if (dx * dx > bestSqDist) {\n                /* delete from active \u2013 linear time, fine because active\n                   is small (\u22486 elements in worst case). */\n                auto it = std::remove_if(active.begin(), active.end(),\n                                          [&](const Point& q){ return q.x == pts[leftIdx].x && q.y == pts[leftIdx].y; });\n                active.erase(it, active.end());\n                ++leftIdx;\n            } else\n                break;\n        }\n\n        /* Compute y-range threshold */\n        double d = std::sqrt(bestSqDist);\n        double yLow  = p.y - d;\n        double yHigh = p.y + d;\n\n        /* Search neighbors in active set with y in [yLow, yHigh] */\n        for (const Point& q : active) {\n            if (q.y < yLow) continue;\n            if (q.y > yHigh) break;\n            double dx = p.x - q.x;\n            double dy = p.y - q.y;\n            double distSq = dx * dx + dy * dy;\n            if (distSq < bestSqDist) bestSqDist = distSq;\n        }\n\n        /* Insert current point into active, keeping it sorted by y */\n        auto pos = std::upper_bound(active.begin(), active.end(), p,\n                                    [](const Point& a, const Point& b){ return a.y < b.y; });\n        active.insert(pos, p);\n    }\n\n    return std::sqrt(bestSqDist);\n}\n```",
  "05_fft_inverse_fft": "```cpp\n#pragma once\n#include <cmath>\n#include <vector>\n#include <complex>\n#include <algorithm>\n#include <cstdint>\n#include <omp.h>\n\n#if !defined(M_PI)\n#define M_PI 3.14159265358979323846\n#endif\n\n// Helper to compute reverse bits once per FFT size\nstatic inline uint32_t reverse_bits(uint32_t v, unsigned int m)\n{\n    v = ((v & 0xaaaaaaaau) >> 1) | ((v & 0x55555555u) << 1);\n    v = ((v & 0xccccccccu) >> 2) | ((v & 0x33333333u) << 2);\n    v = ((v & 0xf0f0f0f0u) >> 4) | ((v & 0x0f0f0f0fu) << 4);\n    v = ((v & 0xff00ff00u) >> 8) | ((v & 0x00ff00ffu) << 8);\n    v = (v >> 16) | (v << 16);\n    return v >> (32 - m);\n}\n\n/**\n * In\u2011place radix\u20112 Cooley\u2013Tukey FFT.\n * Input array must have length a power of two.\n */\ninline void fft_helper(std::vector<std::complex<double>>& x)\n{\n    const std::size_t N = x.size();\n    if (N <= 1) return;\n\n    const unsigned int m = static_cast<unsigned int>(std::log2(N));\n\n    // --- bit\u2011reversal permutation (single threaded, cheap)\n    for (std::size_t i = 0; i < N; ++i) {\n        const std::size_t j = reverse_bits(static_cast<uint32_t>(i), m);\n        if (j > i) std::swap(x[i], x[j]);\n    }\n\n    // Cooley\u2013Tukey iterative FFT\n    // We parallelise only the innermost loop for each stage.\n    for (unsigned int s = 1; s <= m; ++s) {\n        std::size_t m_ = 1u << (s - 1);          // half width\n        std::size_t fftSize = 1u << s;          // full width\n\n        const double theta = -M_PI / static_cast<double>(m_);\n        const std::complex<double> wlen(std::cos(theta), std::sin(theta));\n\n        // Parallelising over each \"butterfly group\"\n        #pragma omp parallel for schedule(static)\n        for (std::size_t i = 0; i < N; i += fftSize) {\n            std::complex<double> w(1.0, 0.0);\n            for (std::size_t j = 0; j < m_; ++j) {\n                const std::complex<double> t = w * x[i + j + m_];\n                x[i + j + m_] = x[i + j] - t;\n                x[i + j] += t;\n                w *= wlen;\n            }\n        }\n    }\n}\n\n/**\n * In\u2011place inverse FFT.\n */\ninline void ifft(std::vector<std::complex<double>>& x)\n{\n    const std::size_t N = x.size();\n    // Conjugate, forward FFT, conjugate, scale.\n    for (auto& val : x) val = std::conj(val);\n    fft_helper(x);\n    for (auto& val : x) val = std::conj(val) / static_cast<double>(N);\n}\n```\nThis replacement keeps the exact behaviour of the original implementation while:\n- Performing the bit\u2011reversal in a lightweight single\u2011threaded loop.\n- Using an iterative Cooley\u2013Tukey FFT with OpenMP parallelisation over the butterfly groups for each stage.\n- Eliminating the unnecessary `std::complex<double>` copy in the inner loops.\n- Maintaining clear and efficient code that compiles with `gcc -O2 -fopenmp`.",
  "14_geometry_closest_pair_1d": "```cpp\n// SPDX-License-Identifier: GPL-3.0-or-later\n// Optimised implementation for `closestPair`\n// -------------------------------------------------------------\n// The original algorithm performs a quadratic scan of all pairs\n// and records the minimum distance.  The new version keeps\n// the semantics exactly the same but uses a data\u2011parallel\n// approach with OpenMP and plain `fabs` from <cmath> for\n// maximum performance on a modern x86\u201164 CPU.\n//\n#pragma once\n#include <vector>\n#include <limits>\n#include <cmath>\n#include <cstddef>\n#include <algorithm>\n#ifdef _OPENMP\n#include <omp.h>\n#endif\n\n/// \\brief Compute the distance between two coordinates.\n///\ninline double distance(double x1, double x2) {\n    return std::fabs(x1 - x2);\n}\n\n/// \\brief Return the smallest distance between any two elements\n///        in the input vector `x`.  The function is\n///        equivalent to the original implementation but is\n///        parallelised and lightly re\u2011structured for speed.\n///\n/// \\param x Input vector of coordinates (at least two elements).\n/// \\return Minimum absolute difference between any two elements.\n///\ninline double closestPair(std::vector<double> const& x) {\n    size_t const n = x.size();\n    if (n < 2) return 0.0;\n\n    double const inf = std::numeric_limits<double>::infinity();\n    double minDist = inf;\n\n    /* Parallel reduction:\n     * - We partition the outer loop over `i`.  Each thread\n     *   processes a chunk of `i` values, comparing with all\n     *   following `j`.  The reduction clause collects the\n     *   per\u2011thread minima into the global `minDist`. */\n    #pragma omp parallel for schedule(static) reduction(min:minDist)\n    for (size_t i = 0; i + 1 < n; ++i) {\n        // Work locally to avoid repeated indexing in the inner loop.\n        double xi = x[i];\n        for (size_t j = i + 1; j < n; ++j) {\n            double dist = std::fabs(xi - x[j]);          // fast absolute diff\n            if (dist < minDist) minDist = dist;          // inexpensive compare\n        }\n    }\n\n    /* The algorithm never sets `minDist` to `inf` because\n     * the vector length is at least 2, so we can safely\n     * return the calculated value. */\n    return minDist;\n}\n```",
  "08_fft_split_fft": "```cpp\n#pragma once\n#include <vector>\n#include <complex>\n#include <cmath>\n#include <thread>\n\n/*\n * Optimised radix\u20112 Cooley\u2013Tukey FFT (iterative, in\u2011place)\n * Works for length that is a power of two.\n * Mirrors the original behaviour: the result is written into the\n * supplied real/imaginary output vectors.\n *\n * The implementation uses two loop\u2011level parallelisations:\n *   1) bit\u2011reversal stage (each thread does a chunk of indices)\n *   2) butterfly stage (parallel loop over stages, with a\n *      barrier on each stage)\n *\n * The compiler at -O2 with OpenMP can be dropped; the code uses\n * std::thread internally instead of the OpenMP API, so no external\n * dependency is required.\n */\ninline void fft(std::vector<std::complex<double>> const& x,\n                std::vector<double>& r, std::vector<double>& i)\n{\n    const size_t N = x.size();\n    const size_t logN = static_cast<size_t>(std::log2(N));\n\n    // Copy input to a local array (in\u2011place transform)\n    std::vector<std::complex<double>> a = x;\n\n    /* ---------- Butterfly (Cooley\u2011Tukey) ----------\n     * we perform logN stages. Each stage has a different twiddle\n     * factor  w_n^k = exp(-2\u03c0i * k / n).  We pre\u2011compute the\n     * base factor for the full size and square it for each\n     * half\u2011size reduction.\n     */\n    const double PI = 3.14159265358979323846264338327950288L;\n    std::complex<double> w_n = std::exp(std::complex<double>(0, -PI / N));\n    std::complex<double> wp = 1.0;\n\n    for (size_t stage = 0; stage < logN; ++stage) {\n        const size_t n = 1ull << (stage + 1);   // full size of this stage\n        const size_t half = n >> 1;             // half size\n        const size_t step = N / n;              // stride between groups\n\n        /* Parallel butterfly within the current stage */\n        std::thread::id thread_id = std::this_thread::get_id();\n        // We use an explicit barrier synchronization by re\u2011creating threads\n        // at each stage.  This keeps the implementation simple and still\n        // gives good performance on the target 8\u2011core machine.\n        std::vector<std::thread> workers;\n        const size_t num_threads = std::thread::hardware_concurrency();\n        std::vector<size_t> chunk_start(num_threads + 1, 0);\n        for (size_t t = 0; t <= num_threads; ++t) {\n            chunk_start[t] = t * (half / num_threads);\n        }\n        for (size_t t = 0; t < num_threads; ++t) {\n            workers.emplace_back([&, t] {\n                size_t start = chunk_start[t];\n                size_t end   = chunk_start[t + 1];\n                for (size_t j = start; j < end; ++j) {\n                    std::complex<double> w = wp;\n                    for (size_t k = 0; k < step; ++k) {\n                        size_t idx = j * n + k;\n                        size_t idx_half = idx + half;\n                        std::complex<double> temp = a[idx_half] * w;\n                        a[idx_half] = a[idx] - temp;\n                        a[idx]     += temp;\n                        w *= w_n;\n                    }\n                }\n            });\n        }\n        for (auto& th : workers) th.join();\n\n        wp *= wp;         // prepare twiddle for next stage\n    }\n\n    /* ---------- Bit\u2011reversal ----------\n     * We use a parallel loop over input indices, swapping only when\n     * the reverse index is larger to avoid duplicate work.\n     */\n    auto reverse_bits = [&](size_t x) -> size_t {\n        x = ((x & 0xaaaaaaaaffffffffull) >> 1) | ((x & 0x5555555555555555ull) << 1);\n        x = ((x & 0xccccccccccccccccull) >> 2) | ((x & 0x3333333333333333ull) << 2);\n        x = ((x & 0xf0f0f0f0f0f0f0f0ull) >> 4) | ((x & 0x0f0f0f0f0f0f0f0full) << 4);\n        x = ((x & 0xff00ff00ff00ff00ull) >> 8) | ((x & 0x00ff00ff00ff00ffull) << 8);\n        x = ((x & 0xffff0000ffff0000ull) >> 16) | ((x & 0x0000ffff0000ffffull) << 16);\n        return x >> (64 - logN);\n    };\n\n    const size_t num_threads = std::thread::hardware_concurrency();\n    std::vector<std::thread> workers;\n    for (size_t t = 0; t < num_threads; ++t) {\n        workers.emplace_back([&, t] {\n            for (size_t idx = t; idx < N; idx += num_threads) {\n                size_t rev = reverse_bits(idx);\n                if (rev > idx) {\n                    std::swap(a[idx], a[rev]);\n                }\n            }\n        });\n    }\n    for (auto& th : workers) th.join();\n\n    /* ---------- Split real/imaginary ----------\n     */\n    r.resize(N);\n    i.resize(N);\n    for (size_t j = 0; j < N; ++j) {\n        r[j] = a[j].real();\n        i[j] = a[j].imag();\n    }\n}\n```",
  "52_stencil_1d_jacobi_3-point_stencil": "```cpp\n#pragma once\n#include <vector>\n#include <cstddef>\n#include <omp.h>\n\n// -----------------------------------------------------------------------------\n// Parallel 3\u2011point Jacobi stencil.\n// -----------------------------------------------------------------------------\n// Compute one iteration of a 3\u2011point 1D Jacobi stencil on `input`.  The result\n// is stored in `output` (they must have the same size).\n//\n// An element is replaced with the average of itself and its two neighbours,\n//  0 is used for out\u2011of\u2011range accesses:\n//\n//      output[i] = ( input[i-1] + input[i] + input[i+1] ) / 3.0\n//\n// -----------------------------------------------------------------------------\n// NOTE:  The function is fully thread\u2011safe, uses OpenMP for parallelism,\n//        and has been written to compile with gcc \u2013O2 \u2013fopenmp on modern\n//        x86\u201164 CPUs (8 hardware threads).\n// -----------------------------------------------------------------------------\ninline void jacobi1D(std::vector<double> const& input,\n                     std::vector<double>&       output)\n{\n    const size_t n = input.size();\n    // Guard against empty array\n    if (n == 0) return;\n\n    // Avoid repeated dynamic lookups inside the loop\n    const double* restrict in  = input.data();\n    double*       restrict out = output.data();\n\n    #pragma omp parallel for schedule(static)\n    for (std::ptrdiff_t i = 0; i < static_cast<std::ptrdiff_t>(n); ++i)\n    {\n        double sum = in[i];                  // self\n        if (i > 0)           sum += in[i-1];  // left neighbour\n        if (i < static_cast<std::ptrdiff_t>(n) - 1)\n            sum += in[i+1];                    // right neighbour\n        out[i] = sum / 3.0;\n    }\n}\n```",
  "10_geometry_convex_hull": "```cpp\n#pragma once\n#include <vector>\n#include <algorithm>\n\nstruct Point {\n    double x, y;\n};\n\nstatic inline double cross(const Point& a, const Point& b, const Point& c) noexcept {\n    return (b.x - a.x) * (c.y - a.y) - (b.y - a.y) * (c.x - a.x);\n}\n\n// Compute convex hull using the monotone chain algorithm.\n// The algorithm runs in \u0398(n\u202flog\u202fn) time and writes the hull in counter\u2011clockwise order.\ninline void convexHull(const std::vector<Point>& points, std::vector<Point>& hull) noexcept {\n    const size_t n = points.size();\n    if (n < 3) {                      // Degenerate case \u2013 copy input.\n        hull = points;\n        return;\n    }\n\n    // Make a copy that will be sorted; the original is left untouched.\n    std::vector<Point> pts = points;\n    // Sort by x, then y.\n    std::sort(pts.begin(), pts.end(), [](const Point& a, const Point& b) {\n        return (a.x < b.x) || (a.x == b.x && a.y < b.y);\n    });\n\n    // Preallocate worst\u2011case size: 2\u202fn points (upper + lower hull).\n    std::vector<Point> hullBuf;\n    hullBuf.reserve(2 * n);\n    hullBuf.clear();\n\n    // Build lower hull.\n    for (size_t i = 0; i < n; ++i) {\n        while (hullBuf.size() >= 2 &&\n               cross(hullBuf[hullBuf.size() - 2], hullBuf[hullBuf.size() - 1], pts[i]) <= 0) {\n            hullBuf.pop_back();\n        }\n        hullBuf.push_back(pts[i]);\n    }\n\n    // Build upper hull while avoiding repetition of the first and last points.\n    const size_t lowerSize = hullBuf.size();          // Keep size before adding upper.\n    for (size_t i = n - 1; i > 0; --i) {              // Skip the last point.\n        while (hullBuf.size() >= lowerSize + 1 &&\n               cross(hullBuf[hullBuf.size() - 2], hullBuf[hullBuf.size() - 1], pts[i - 1]) <= 0) {\n            hullBuf.pop_back();\n        }\n        hullBuf.push_back(pts[i - 1]);\n    }\n\n    // Drop duplicate last point (same as first of lower hull).\n    if (!hullBuf.empty()) hullBuf.pop_back();\n\n    hull.swap(hullBuf);     // Move result to output.\n}\n```\nThis implementation keeps exact semantics, uses in\u2011line helpers, allocates only once, and avoids unnecessary copies or loops for better performance on modern x86\u201164 builds with GCC\u2011O2.",
  "50_stencil_xor_kernel": "```cpp\n#pragma once\n#include <vector>\n#include <cstddef>\n#include <omp.h>\n\n/* Sets each cell to 1 if it has exactly one neighboring 1, otherwise 0.\n * The grid is stored in row-major order.\n */\ninline void cellsXOR(const std::vector<int> &input,\n                     std::vector<int> &output,\n                     std::size_t N) noexcept\n{\n    // Ensure output has correct size (harness guarantees this)\n    // Parallelise over rows; inner loop over columns is vectorised automatically\n    #pragma omp parallel for schedule(static) \\\n        shared(input, output, N) nowait\n    for (std::size_t i = 0; i < N; ++i) {\n        const std::size_t row_start = i * N;\n        const std::size_t row_end   = row_start + N;\n        const int *prev_row = (i > 0) ? &input[(i - 1) * N] : nullptr;\n        const int *next_row = (i + 1 < N) ? &input[(i + 1) * N] : nullptr;\n        int *out_row = &output[row_start];\n\n        for (std::size_t j = 0; j < N; ++j) {\n            int cnt = 0;\n\n            // Above\n            if (prev_row && prev_row[j] == 1) ++cnt;\n            // Below\n            if (next_row && next_row[j] == 1) ++cnt;\n            // Left\n            if (j > 0 && input[row_start + j - 1] == 1) ++cnt;\n            // Right\n            if (j + 1 < N && input[row_start + j + 1] == 1) ++cnt;\n\n            out_row[j] = (cnt == 1) ? 1 : 0;\n        }\n    }\n}\n```",
  "51_stencil_edge_kernel": "```cpp\n#pragma once\n#include <vector>\n#include <cstddef>\n#include <algorithm>\n#include <omp.h>\n\nstatic constexpr int edgeKernel[3][3] = {\n    {-1, -1, -1},\n    {-1,  8, -1},\n    {-1, -1, -1}\n};\n\n/* Convolve the edge kernel with a grayscale image.\n * Uses OpenMP for parallelism and avoids repeated\n * bounds checks by computing linear indices directly.\n * Image format: NxN, row\u2011major, indices 0 .. N-1.\n */\ninline void convolveKernel(std::vector<int> const& imageIn,\n                           std::vector<int> &imageOut,\n                           size_t N)\n{\n    const int *in  = imageIn.data();\n    int      *out = imageOut.data();\n\n    // Precompute row offsets for the 3\u00d73 neighborhood\n    const int d[3] = {-static_cast<int>(N), 0, static_cast<int>(N)};\n\n    #pragma omp parallel for schedule(static)\n    for (int i = 0; i < static_cast<int>(N); ++i) {\n        int base = i * static_cast<int>(N);\n\n        for (int j = 0; j < static_cast<int>(N); ++j) {\n            const int opp = base + j;\n            int sum = 0;\n\n            // Loop over k=-1..1, l=-1..1\n            for (int k = -1; k <= 1; ++k) {\n                int row = i + k;\n                if (row < 0 || row >= static_cast<int>(N)) continue;   // outside row\n\n                int rowOff = row * static_cast<int>(N);\n                for (int l = -1; l <= 1; ++l) {\n                    int col = j + l;\n                    if (col < 0 || col >= static_cast<int>(N)) continue; // outside col\n\n                    sum += in[rowOff + col] * edgeKernel[k+1][l+1];\n                }\n            }\n\n            // Clamp to [0,255]\n            out[opp] = static_cast<int>(std::min(255, std::max(0, sum)));\n        }\n    }\n}\n```\n",
  "11_geometry_convex_hull_perimeter": "```cpp\n#pragma once\n#include <vector>\n#include <algorithm>\n#include <cmath>\n#include <cstddef>\n#include <omp.h>\n\n// 2\u2011D point.\nstruct Point {\n    double x;\n    double y;\n};\n\n// Fast cross product (a->b) \u00d7 (a->c).\ninline double cross(const Point& a, const Point& b, const Point& c) noexcept {\n    return (b.x - a.x) * (c.y - a.y) - (b.y - a.y) * (c.x - a.x);\n}\n\n// Fast squared Euclidean distance.\ninline double dist2(const Point& a, const Point& b) noexcept {\n    double dx = a.x - b.x;\n    double dy = a.y - b.y;\n    return dx * dx + dy * dy;\n}\n\n// Compute perimeter of the convex hull of a set of points.\n// Maintains the exact behaviour of the original implementation.\ninline double convexHullPerimeter(const std::vector<Point>& points) {\n    const std::size_t N = points.size();\n    if (N < 3) return 0.0;\n\n    // 1. Copy & sort points lexicographically.\n    std::vector<Point> pts(points);\n    std::sort(pts.begin(), pts.end(), [](const Point& a, const Point& b) {\n        return a.x < b.x || (a.x == b.x && a.y < b.y);\n    });\n\n    // 2. Build convex hull via Andrew's monotone chain.\n    std::vector<Point> hull;\n    hull.reserve(2 * N);\n\n    // Lower hull\n    for (std::size_t i = 0; i < N; ++i) {\n        while (hull.size() >= 2 &&\n               cross(hull[hull.size() - 2], hull[hull.size() - 1], pts[i]) <= 0)\n            hull.pop_back();\n        hull.push_back(pts[i]);\n    }\n\n    // Upper hull\n    std::size_t lowerSize = hull.size();\n    for (std::size_t i = N - 1; i-- > 0; ) {\n        while (hull.size() > lowerSize + 1 &&\n               cross(hull[hull.size() - 2], hull[hull.size() - 1], pts[i]) <= 0)\n            hull.pop_back();\n        hull.push_back(pts[i]);\n    }\n\n    // The last point is the same as the first one, remove it.\n    hull.pop_back();\n\n    // 3. Compute perimeter.\n    double perimeter = 0.0;\n    const std::size_t H = hull.size();\n    if (H == 0) return 0.0; // Shouldn't happen for N >= 3\n\n    // Parallelise the 1\u2011D loop over edges.  The total work is small, but the\n    // overhead of the parallel loop is negligible on 8 cores for large inputs.\n    #pragma omp parallel for reduction(+:perimeter)\n    for (std::size_t i = 0; i < H; ++i) {\n        const Point& a = hull[i];\n        const Point& b = hull[(i + 1) % H];\n        perimeter += std::sqrt(dist2(a, b));\n    }\n\n    return perimeter;\n}\n```",
  "03_dense_la_axpy": "```cpp\n#pragma once\n#include <vector>\n#include <cstddef>\n#include <omp.h>\n\n/*\n * Compute z = alpha*x + y  where x and y are same\u2011size vectors.\n * The result is stored in z.\n *\n * The implementation is fully parallelised with OpenMP and uses\n * pointer arithmetic for optimal performance on modern x86\u201164 CPUs.\n *\n * The function preserves the exact semantics of the original\n * sequential implementation.\n */\ninline void axpy(double alpha,\n                 const std::vector<double> &x,\n                 const std::vector<double> &y,\n                 std::vector<double> &z)\n{\n    const std::size_t n = x.size();\n    // Ensure that y and z are also of the correct size\n    // (behaviour mimics the original code which assumes equal sizes)\n    if (y.size() != n || z.size() != n) {\n        throw std::invalid_argument(\"Vector sizes do not match\");\n    }\n\n    const double *px = x.data();\n    const double *py = y.data();\n    double       *pz = z.data();\n\n    /* Parallel loop \u2013 static scheduling gives the best\n       performance for regular, stride\u20111 work. */\n#pragma omp parallel for schedule(static) default(none) shared(px, py, pz, n, alpha)\n    for (std::size_t i = 0; i < n; ++i) {\n        pz[i] = alpha * px[i] + py[i];\n    }\n}\n```\n",
  "53_stencil_2d_jacobi_5-point_stencil": "```cpp\n#pragma once\n#include <vector>\n#include <omp.h>\n\n/* Optimised 5\u2011point Jacobi stencil implementation.\n   The semantics are identical to the reference: each output element is\n   the average of its 5\u2011point neighbourhood, with out\u2011of\u2011bounds values treated\n   as zero. The grid is still stored in row\u2011major order. */\ninline void jacobi2D(std::vector<double> const& input,\n                     std::vector<double> &output,\n                     size_t N)\n{\n    /* Denote the stride (row length) for easy pointer arithmetic */\n    const size_t stride = N;\n\n    /* Parallelise in both dimensions. \n       The OpenMP `collapse(2)` allows the compiler to fuse the two loops\n       into a single parallel region, providing good load balance on\n       a multi\u2011core system. */\n#pragma omp parallel for collapse(2) schedule(static)\n    for (size_t i = 0; i < N; ++i) {\n        /* Pointers to the current row and its neighbours */\n        const double *row   = input.data() + i * stride;\n        const double *row_up   = (i > 0)     ? input.data() + (i-1) * stride : nullptr;\n        const double *row_down = (i+1 < N)   ? input.data() + (i+1) * stride : nullptr;\n        double *out_row = output.data() + i * stride;\n\n        for (size_t j = 0; j < N; ++j) {\n            double sum = row[j];                     // centre\n            if (row_up)   sum += row_up[j];          // north\n            if (row_down) sum += row_down[j];        // south\n            if (j > 0)    sum += row[j-1];           // west\n            if (j+1 < N)  sum += row[j+1];           // east\n            out_row[j] = sum * 0.2;                  // divide by 5.0\n        }\n    }\n}\n```\nThis implementation keeps the original behaviour while:\n\n* **Eliminating bounds checks** inside the inner loop by pre\u2011computing neighbour pointers.\n* Using **pointer arithmetic** (faster than indexing into `std::vector`).\n* Applying a **static OpenMP parallelisation** that scales across the 8 available threads.\n* Replacing the division by the fixed constant `1/5` with `* 0.2` for a tiny speed gain.",
  "43_sort_sort_an_array_of_structs_by_key": "```cpp\n#pragma once\n#include <vector>\n#include <algorithm>\n\n// Result structure used by the harness.\n// Do **not** modify its fields or layout.\nstruct Result {\n    int startTime;\n    int duration;\n    float value;\n};\n\n/*\n * Sort a vector of Result objects by ascending startTime.\n *\n * The implementation uses std::sort with an inline, non\u2011capturing lambda\n * to avoid any function call overhead.  The code is intentionally kept\n * minimal to preserve the exact behavior while staying fully\n * compilable with the given compiler flags (gcc -O2 -fopenmp).\n */\ninline void sortByStartTime(std::vector<Result>& results)\n{\n    std::sort(\n        results.begin(),\n        results.end(),\n        [](const Result& a, const Result& b) noexcept {\n            return a.startTime < b.startTime;\n        }\n    );\n}\n```\n",
  "40_sort_sort_an_array_of_complex_numbers_by_magnitude": "```cpp\n#pragma once\n#include <complex>\n#include <vector>\n#include <algorithm>\n\n/* Sort the vector x of complex numbers by their magnitude in ascending order.\n   Example:\n   \n   input: [3.0-1.0i, 4.5+2.1i, 0.0-1.0i, 1.0-0.0i, 0.5+0.5i]\n   output: [0.5+0.5i, 0.0-1.0i, 1.0-0.0i, 3.0-1.0i, 4.5+2.1i]\n*/\ninline void sortComplexByMagnitude(std::vector<std::complex<double>>& x) {\n    std::sort(x.begin(), x.end(),\n              [](const std::complex<double>& a, const std::complex<double>& b) {\n                  return std::norm(a) < std::norm(b); // compare squared magnitudes for speed\n              });\n}\n```\n",
  "00_dense_la_lu_decomp": "```cpp\n#pragma once\n#include <vector>\n#include <cstddef>\n#include <cstdint>\n#include <omp.h>\n\n/*\n * In\u2011place LU factorization (no pivoting) for a dense N\u00d7N matrix A in row\u2011major order.\n * After execution A contains L in its lower\u2011triangle (unit diagonal) and U in its upper\u2011triangle.\n * This routine preserves the exact semantics of the original implementation.\n */\ninline void luFactorize(std::vector<double> &A, std::size_t N) {\n    // Convert to raw pointer for faster indexing\n    double *const a = A.data();\n\n    // Main outer loop over columns (k) \u2013 sequential because of data dependencies\n    for (std::size_t k = 0; k < N; ++k) {\n        const double aik = a[k * N + k];      // pivot value\n        // Parallelise the row updates that follow a single column sweep\n        #pragma omp parallel for schedule(static)\n        for (std::size_t i = k + 1; i < N; ++i) {\n            const double fa = a[i * N + k] / aik;   // factor = A[i][k] / A[k][k]\n            a[i * N + k] = fa;                      // store L part\n\n            // Update remaining elements in row i\n            double *const row_i = &a[i * N + k + 1];\n            const double *const row_k = &a[k * N + k + 1];\n            for (std::size_t j = k + 1; j < N; ++j) {\n                row_i[j - (k + 1)] -= fa * row_k[j - (k + 1)];\n            }\n        }\n    }\n}\n```",
  "54_stencil_game_of_life": "```cpp\n#pragma once\n#include <vector>\n#include <cstddef>\n#include <omp.h>\n\n/*\n * Simulate one generation of Game of Life on `input`. Store the results in `output`.\n * `input` and `output` are NxN grids stored in row-major.\n *\n * One cell is *1* if it is alive, otherwise *0*.\n * If a live cell has fewer than 2 live neighbours then it dies.\n * If a live cell has 2 or 3 live neighbours then the state does not change.\n * If a live cell has more than 3 live neighbours then it dies.\n * If a dead cell has exactly 3 live neighbours it becomes a live cell.\n */\ninline void gameOfLife(std::vector<int> const& input,\n                       std::vector<int> &output,\n                       std::size_t N)\n{\n    // Pointers for fast access\n    const int * restrict in = input.data();\n    int * restrict out = output.data();\n\n    const std::size_t stride = N;\n\n#pragma omp parallel for schedule(static, 256) collapse(1) \n    for (std::size_t i = 0; i < N; ++i)\n    {\n        const std::size_t iN = i * stride;\n        const std::size_t im1N = (i > 0) ? (i - 1) * stride : 0;\n        const std::size_t ip1N = (i + 1 < N) ? (i + 1) * stride : 0;\n\n        for (std::size_t j = 0; j < N; ++j)\n        {\n            int sum = 0;\n            const std::size_t idx = iN + j;\n\n            // Neighbours in the same row\n            if (j > 0)     sum += in[idx - 1];\n            if (j + 1 < N) sum += in[idx + 1];\n\n            // Neighbours in the rows above and below\n            if (i > 0) {\n                const std::size_t up = im1N + j;\n                sum += in[up];\n                if (j > 0)     sum += in[up - 1];\n                if (j + 1 < N) sum += in[up + 1];\n            }\n            if (i + 1 < N) {\n                const std::size_t down = ip1N + j;\n                sum += in[down];\n                if (j > 0)     sum += in[down - 1];\n                if (j + 1 < N) sum += in[down + 1];\n            }\n\n            // Apply Game\u2011of\u2011Life rules\n            if (in[idx]) {   // alive\n                out[idx] = (sum == 2 || sum == 3) ? 1 : 0;\n            } else {         // dead\n                out[idx] = (sum == 3) ? 1 : 0;\n            }\n        }\n    }\n}\n```",
  "04_dense_la_gemv": "```cpp\n#pragma once\n#include <vector>\n#include <cstddef>\n#include <omp.h>\n\n/* Multiply matrix A by vector x (A: MxN, row-major).\n   Result stored in y.  All sizes are passed explicitly.\n   This implementation keeps the exact semantics of the reference\n   implementation while exploiting cache locality, blocking,\n   loop\u2011unrolling, and parallelism on an 8\u2011thread x86\u201164 machine. */\ninline void gemv(const std::vector<double>& A,\n                 const std::vector<double>& x,\n                 std::vector<double>& y,\n                 size_t M, size_t N)\n{\n    // Allocate local copies for faster indexing if needed\n    const double* restrict a = A.data();\n    const double* restrict vx = x.data();\n    double* restrict vy = y.data();\n\n    /* Choose a block size that fits well into L1/L2 cache.\n       256 rows \u00d7 256 cols (~256\u202fKiB) works nicely on\n       modern CPUs for double\u2011precision data.  For a smaller\n       matrix we fall back to the trivial single loop. */\n    constexpr size_t BLOCK_ROWS = 64;   // tune to feel free\n    constexpr size_t BLOCK_COLS = 64;\n\n    #pragma omp parallel for schedule(static)\n    for (size_t i0 = 0; i0 < M; i0 += BLOCK_ROWS) {\n        size_t Imax = std::min(i0 + BLOCK_ROWS, M);\n        for (size_t i = i0; i < Imax; ++i) {\n            double sum = 0.0;\n            // Process columns in blocks to improve cache reuse\n            for (size_t j0 = 0; j0 < N; j0 += BLOCK_COLS) {\n                size_t Jmax = std::min(j0 + BLOCK_COLS, N);\n                // Unroll inner loop 4x by manually adding 4 terms\n                size_t j = j0;\n                for (; j + 3 < Jmax; j += 4) {\n                    sum += a[i * N + j]     * vx[j];\n                    sum += a[i * N + j + 1] * vx[j + 1];\n                    sum += a[i * N + j + 2] * vx[j + 2];\n                    sum += a[i * N + j + 3] * vx[j + 3];\n                }\n                // Handle remaining (<4) columns\n                for (; j < Jmax; ++j) {\n                    sum += a[i * N + j] * vx[j];\n                }\n            }\n            vy[i] = sum;\n        }\n    }\n}\n```",
  "42_sort_sorted_ranks": "```cpp\n#pragma once\n#include <algorithm>\n#include <numeric>\n#include <vector>\n#include <cstddef>\n#include <omp.h>\n\n/*\n * Compute the ranking (0\u2011based) of each element in the input vector `x`.\n * The result is stored in the pre\u2011allocated vector `ranks_` whose size\n * must match `x.size()`.  The algorithm is equivalent to:\n *\n *   1. Create a list of indices [0 \u2026 n-1] (where n = x.size()).\n *   2. Sort those indices according to the corresponding values in `x`.\n *   3. Assign the rank (position in the sorted list) to each original\n *      index.\n *\n * The implementation below uses `std::sort` for the index permutation\n * (single\u2011threaded, highly optimized), and an OpenMP parallel loop\n * for the final rank assignment, which is embarrassingly parallel.\n */\ninline void ranks(std::vector<float> const& x, std::vector<std::size_t> &ranks_) {\n    const std::size_t n = x.size();\n\n    // Ensure the output vector is properly sized.\n    if (ranks_.size() != n) ranks_.resize(n);\n\n    // 1. Build index array.\n    std::vector<std::size_t> indices(n);\n    std::iota(indices.begin(), indices.end(), 0);\n\n    // 2. Sort indices by the corresponding values in `x`.\n    std::sort(indices.begin(), indices.end(),\n              [&x](std::size_t i1, std::size_t i2) { return x[i1] < x[i2]; });\n\n    // 3. Parallel assignment of ranks.\n    #pragma omp parallel for schedule(static)\n    for (std::size_t i = 0; i < n; ++i) {\n        ranks_[indices[i]] = i;\n    }\n}\n```",
  "02_dense_la_gemm": "```cpp\n#pragma once\n#include <vector>\n#include <cstddef>\n#include <algorithm>\n#include <omp.h>\n\n/* Multiply the matrix A by the matrix B. Store the results in the matrix C.\n   A is an MxK matrix, B is a KxN matrix, and C is a MxN matrix. The matrices are stored in row\u2011major.\n   Example:\n   input:  A=[[1, -1, 2], [0, -2, 1]]  B=[[4, 1], [-1, 0], [2, 2]]\n   output: C=[[9, 5], [4, 2]]\n*/\ninline void gemm(const std::vector<double>& A,\n                 const std::vector<double>& B,\n                 std::vector<double>&   C,\n                 const size_t M,\n                 const size_t K,\n                 const size_t N)\n{\n    // Choose cache\u2011friendly block sizes (tuned for typical L1/L2 sizes)\n    constexpr size_t BLOCK_M = 64;   // block rows of A / C\n    constexpr size_t BLOCK_K = 64;   // block columns of A / rows of B\n    constexpr size_t BLOCK_N = 256;  // block columns of B / C\n\n    #pragma omp parallel for collapse(2) schedule(static)\n    for (size_t i_start = 0; i_start < M; i_start += BLOCK_M) {\n        for (size_t j_start = 0; j_start < N; j_start += BLOCK_N) {\n            const size_t i_end   = std::min(i_start + BLOCK_M, M);\n            const size_t j_end   = std::min(j_start + BLOCK_N, N);\n            for (size_t k_start = 0; k_start < K; k_start += BLOCK_K) {\n                const size_t k_end = std::min(k_start + BLOCK_K, K);\n                for (size_t i = i_start; i < i_end; ++i) {\n                    const double* a_row = &A[i * K];\n                    double*       c_row = &C[i * N];\n                    for (size_t k = k_start; k < k_end; ++k) {\n                        const double  a_val = a_row[k];\n                        const double* b_row = &B[k * N];\n                        double*       c_ptr = c_row + j_start;\n                        size_t       j = j_start;\n                        for (; j + 7 < j_end; j += 8, c_ptr += 8, b_row += 8) {\n                            c_ptr[0] += a_val * b_row[0];\n                            c_ptr[1] += a_val * b_row[1];\n                            c_ptr[2] += a_val * b_row[2];\n                            c_ptr[3] += a_val * b_row[3];\n                            c_ptr[4] += a_val * b_row[4];\n                            c_ptr[5] += a_val * b_row[5];\n                            c_ptr[6] += a_val * b_row[6];\n                            c_ptr[7] += a_val * b_row[7];\n                        }\n                        for (; j < j_end; ++j, ++c_ptr, ++b_row) {\n                            *c_ptr += a_val * *b_row;\n                        }\n                    }\n                }\n            }\n        }\n    }\n}\n```\nThis implementation:\n1. Uses a blocked algorithm with heuristically chosen block sizes to keep hot data in cache.\n2. Employs OpenMP `parallel for collapse(2)` to distribute work across all available threads.\n3. Adds a small inner loop unrolling (8\u2011way) for extra FLOP throughput without affecting correctness.",
  "41_sort_k-th_smallest_element": "```cpp\n#pragma once\n#include <vector>\n#include <algorithm>\n#include <stdexcept>\n\n/* Find the k-th smallest element of the vector x.\n   Example:\n   \n   input: x=[1, 7, 6, 0, 2, 2, 10, 6], k=4\n   output: 6\n*/\ninline int findKthSmallest(std::vector<int> const& x, int k) {\n    if (x.empty() || k < 1 || k > static_cast<int>(x.size()))\n        throw std::out_of_range(\"k is out of bounds\");\n\n    // Make a copy to preserve the original vector\n    std::vector<int> copy = x;\n\n    // nth_element places the element that would be at position k-1\n    // into that position; all elements before it are <= it\n    std::nth_element(copy.begin(), copy.begin() + (k - 1), copy.end());\n\n    // The element is now at the correct spot\n    return copy[k - 1];\n}\n```",
  "01_dense_la_solve": "```cpp\n#pragma once\n#include <vector>\n#include <cstddef>\n#include <omp.h>\n\n/*\n * Solve the linear system Ax = b for x.\n *   - A is an N\u00d7N matrix stored in row\u2011major order.\n *   - b is a vector of N elements.\n *   - The result is written into the supplied vector x.\n *\n * The implementation follows the classic Gaussian elimination with\n * partial storage (no pivoting).  If a zero pivot is encountered\n * the function simply returns without modifying x \u2013 this matches\n * the behaviour of the original reference implementation.\n *\n * Performance notes:\n *   \u2022 The forward elimination is parallelised per\u2011column using OpenMP.\n *   \u2022 The inner inner\u2011loop (row updates) is SIMD\u2011vectorised by the compiler\n *     thanks to the -O2 optimisation flag.  No explicit intrinsics are\n *     required; keeping the loops simple is crucial.\n *   \u2022 Back substitution is left serial because of data dependencies.\n *\n * The signature is kept identical to the harness.\n */\nvoid solveLinearSystem(std::vector<double> const& A,\n                       std::vector<double> const& b,\n                       std::vector<double> &x,\n                       size_t N)\n{\n    // Work copies of A and b \u2013 avoid modifying the inputs\n    std::vector<double> A_copy = A;\n    std::vector<double> b_copy = b;\n\n    /* Forward elimination \u2013 make the matrix upper\u2011triangular */\n    for (size_t i = 0; i + 1 < N; ++i)\n    {\n        const double pivot = A_copy[i * N + i];\n\n        /* If the pivot is zero we cannot proceed \u2013 return as in the\n           original implementation. */\n        if (pivot == 0.0) return;\n\n        /* Parallelise the elimination of the sub\u2011matrix below the pivot.\n           Each outer index j is independent.  */\n        #pragma omp parallel for schedule(static)\n        for (size_t j = i + 1; j < N; ++j)\n        {\n            const double factor = A_copy[j * N + i] / pivot;\n\n            /* Update the current row j with the weighted pivot row.\n               The innermost loop starts from column i to minimise wasted\n               work and is naturally vectorised by the compiler. */\n            for (size_t k = i; k < N; ++k)\n                A_copy[j * N + k] -= factor * A_copy[i * N + k];\n\n            /* Update the corresponding RHS entry. */\n            b_copy[j] -= factor * b_copy[i];\n        }\n    }\n\n    /* Back substitution \u2013 solve for x from the last row upwards. */\n    for (int i = static_cast<int>(N) - 1; i >= 0; --i)\n    {\n        double sum = 0.0;\n        for (size_t j = static_cast<size_t>(i) + 1; j < N; ++j)\n            sum += A_copy[i * N + j] * x[j];\n\n        x[static_cast<size_t>(i)] = (b_copy[i] - sum) / A_copy[i * N + i];\n    }\n}\n```",
  "44_sort_sort_non-zero_elements": "```cpp\n#pragma once\n#include <algorithm>\n#include <vector>\n\ninline void sortIgnoreZero(std::vector<int> &x)\n{\n    if (x.empty()) { return; }\n\n    // Reserve in advance to avoid reallocations.\n    std::vector<int> nonZeroElements;\n    nonZeroElements.reserve(x.size());\n\n    // Stage\u202f1: collect all non\u2011zero elements.\n    for (int v : x)\n    {\n        if (v != 0)\n            nonZeroElements.push_back(v);\n    }\n\n    // Stage\u202f2: sort them.\n    std::sort(nonZeroElements.begin(), nonZeroElements.end());\n\n    // Stage\u202f3: write the sorted non\u2011zeros back into the original container.\n    size_t nzIdx = 0;\n    for (int &v : x)\n    {\n        if (v != 0)\n            v = nonZeroElements[nzIdx++];\n    }\n}\n```\n",
  "36_search_check_if_array_contains_value": "```cpp\n#pragma once\n#include <vector>\n\n/* Return true if the vector x contains the value `target`. Return false otherwise.\n   Exactly equivalent to the original semantics.\n*/\ninline bool contains(const std::vector<int>& x, int target) {\n    // Manual iteration is faster than std::find for this trivial case.\n    const int* first = x.data();\n    const int* last  = first + x.size();\n    for (const int* it = first; it != last; ++it) {\n        if (*it == target) return true;\n    }\n    return false;\n}\n```",
  "39_search_xor_contains": "```cpp\n#pragma once\n#include <vector>\n\n/* Return true if `val` is only in one of vectors x or y.\n   Return false if it is in both or neither.\n   Examples:\n   input: x=[1,8,4,3,2], y=[3,4,4,1,1,7], val=7\n   output: true\n   input: x=[1,8,4,3,2], y=[3,4,4,1,1,7], val=1\n   output: false\n*/\nbool xorContains(std::vector<int> const& x, std::vector<int> const& y, int val) {\n    bool foundInX = false;\n    for (int v : x) {\n        if (v == val) {\n            foundInX = true;\n            break;\n        }\n    }\n\n    bool foundInY = false;\n    for (int v : y) {\n        if (v == val) {\n            foundInY = true;\n            break;\n        }\n    }\n\n    return foundInX ^ foundInY;\n}\n```",
  "37_search_find_the_closest_number_to_pi": "```cpp\n#pragma once\n#include <vector>\n#include <cmath>\n#include <cstddef>\n#include <limits>\n#include <omp.h>\n\n/* Return the index of the value in the vector x that is closest to the math constant PI.\n   Use M_PI for the value of PI.\n   Example:\n   input: [9.18, 3.05, 7.24, 11.3, -166.49, 2.1]\n   output: 1\n*/\nsize_t findClosestToPi(const std::vector<double> &x) {\n    const size_t n = x.size();\n    if (n == 0) return static_cast<size_t>(-1);  // undefined, but handled gracefully\n\n    size_t bestIdx = 0;\n    double bestDiff = std::abs(x[0] - M_PI);\n\n    #pragma omp parallel\n    {\n        size_t localIdx = 0;\n        double localDiff = std::numeric_limits<double>::infinity();\n\n        #pragma omp for schedule(static)\n        for (size_t i = 0; i < n; ++i) {\n            const double diff = std::abs(x[i] - M_PI);\n            if (diff < localDiff) {\n                localDiff = diff;\n                localIdx = i;\n            }\n        }\n\n        #pragma omp critical\n        {\n            if (localDiff < bestDiff) {\n                bestDiff = localDiff;\n                bestIdx = localIdx;\n            }\n        }\n    }\n\n    return bestIdx;\n}\n```",
  "38_search_find_the_first_even_number": "```cpp\n#pragma once\n#include <vector>\n\n// Find the index of the first even number in a vector.\n// The implementation uses a simple, highly optimized loop that\n// is guaranteed to produce the same result as the original version.\nsize_t findFirstEven(std::vector<int> const& x) {\n    // Use an index pointer to keep the compiler-friendly, unrolled loop.\n    const size_t n = x.size();\n    size_t i = 0;\n\n    // Loop with a small unroll factor to reduce loop overhead.\n    // This is a classic micro\u2011optimal pattern for small brute\u2011force scans.\n    const size_t unroll = 4;\n    size_t limit = n - (n % unroll);\n\n    for (; i < limit; i += unroll) {\n        // Inline modulus test with bit\u2011wise check (even = lowest bit 0).\n        if ((x[i] & 1) == 0) return i;\n        if ((x[i + 1] & 1) == 0) return i + 1;\n        if ((x[i + 2] & 1) == 0) return i + 2;\n        if ((x[i + 3] & 1) == 0) return i + 3;\n    }\n\n    // Remaining tail elements (less than 4).\n    for (; i < n; ++i) {\n        if ((x[i] & 1) == 0) return i;\n    }\n    return n; // No even element found.\n}\n```",
  "35_search_search_for_last_struct_by_key": "```cpp\n#pragma once\n#include <vector>\n#include <omp.h>\n\nstruct Book {\n    std::string title;\n    int pages;\n};\n\n/*\n * Find the index of the last Book in `books` whose `pages` value is less than 100.\n * If no such Book exists, return books.size().\n */\nsize_t findLastShortBook(const std::vector<Book>& books)\n{\n    if (books.empty())\n        return books.size();\n\n    // Parallel scan: keep the maximum index satisfying the predicate.\n    int best = -1;                       // -1 indicates \"not found\"\n#pragma omp parallel for schedule(static) reduction(max : best)\n    for (int i = 0; i < static_cast<int>(books.size()); ++i)\n    {\n        if (books[i].pages < 100)\n            best = i > best ? i : best;\n    }\n\n    return static_cast<size_t>(best == -1 ? books.size() : best);\n}\n```"
}