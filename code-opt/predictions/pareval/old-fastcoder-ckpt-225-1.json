{
  "17_graph_highest_degree": "```cpp\n#pragma once\n#include <vector>\n#include <algorithm>\n#include <omp.h>\n\n/* Compute the highest node degree in an undirected adjacency matrix A.\n   A is stored in row-major order.  Each entry is assumed to be 0 or 1.\n   The function operates on an NxN matrix.\n*/\nint maxDegree(std::vector<int> const& A, size_t N) {\n    int highestDegree = 0;\n\n    #pragma omp parallel for reduction(max:highestDegree)\n    for (int i = 0; i < static_cast<int>(N); ++i) {\n        int degree = 0;\n        const int* row = &A[i * N];\n        for (int j = 0; j < static_cast<int>(N); ++j) {\n            degree += row[j];\n        }\n        if (degree > highestDegree) highestDegree = degree;\n    }\n\n    return highestDegree;\n}\n```",
  "15_graph_edge_count": "```cpp\n#pragma once\n#include <vector>\n#include <cstddef>\n#include <cstdint>\n#include <omp.h>\n\n/*\n * Count the number of edges in the directed graph defined by the adjacency matrix `A`.\n * `A` is an `N x N` adjacency matrix stored in row-major order.\n * The result is the number of entries equal to 1.\n */\nint edgeCount(std::vector<int> const& A, std::size_t N) {\n    const std::size_t total = N * N;          // total elements in the matrix\n    int count = 0;\n\n    // Parallelise over all elements. The reduction guarantees correctness.\n    #pragma omp parallel for reduction(+:count) schedule(static)\n    for (std::ptrdiff_t idx = 0; idx < static_cast<std::ptrdiff_t>(total); ++idx) {\n        if (A[idx] == 1) {\n            ++count;\n        }\n    }\n    return count;\n}\n```",
  "20_histogram_pixel_histogram": "```cpp\n#pragma once\n#include <vector>\n#include <array>\n#include <omp.h>\n\nvoid pixelCounts(std::vector<int> const& image, std::array<size_t, 256> &bins) {\n    // Zero the result array\n    bins.fill(0);\n\n    if (image.empty()) return;\n\n    const int n = static_cast<int>(image.size());\n    const int maxThreads = omp_get_max_threads();\n\n    // Create a local histogram for each thread to avoid contention\n    std::vector<std::array<size_t, 256>> localBins(maxThreads);\n    for (auto &b : localBins) b.fill(0);\n\n    #pragma omp parallel\n    {\n        const int tid = omp_get_thread_num();\n        std::array<size_t, 256> &myBins = localBins[tid];\n\n        // Process a contiguous chunk of the image\n        #pragma omp for schedule(static)\n        for (int i = 0; i < n; ++i) {\n            ++myBins[image[i]];\n        }\n    }\n\n    // Merge all local histograms into the final result\n    for (int t = 0; t < maxThreads; ++t) {\n        auto &src = localBins[t];\n        for (int v = 0; v < 256; ++v) {\n            bins[v] += src[v];\n        }\n    }\n}\n```",
  "23_histogram_first_letter_counts": "```cpp\n#pragma once\n#include <array>\n#include <string>\n#include <vector>\n#include <omp.h>\n\n/* Count how many strings in `s` start with each lowercase letter.\n   The result is stored in `bins[0]` for 'a', `bins[1]` for 'b', \u2026, `bins[25]` for 'z'.\n   The implementation is fully parallel and preserves the original semantics. */\ninline void firstLetterCounts(const std::vector<std::string> &s,\n                              std::array<size_t, 26> &bins) {\n    // zero the bins once in the calling context (expected by the harness)\n    // Here we just set them to zero to be safe.\n    bins.fill(0);\n\n    // Parallel reduction: each thread accumulates in a local array and\n    // contributions are merged at the end.  The overhead of 26-element\n    // arrays is negligible compared to the cost of the work loop.\n    #pragma omp parallel\n    {\n        // Thread-local bin array to avoid false sharing\n        size_t local_bins[26] = {0};\n\n        #pragma omp for schedule(static)\n        for (std::size_t i = 0; i < s.size(); ++i) {\n            // All strings are guaranteed non\u2011empty and lowercase.\n            // No bounds/overflow checks are needed \u2013 same as the original.\n            const char c = s[i][0];\n            const int idx = c - 'a';\n            local_bins[idx] += 1;\n        }\n\n        // Merge local bins into the global result using atomics\n        // to keep the code simple and correct without a critical section.\n        #pragma omp for\n        for (int idx = 0; idx < 26; ++idx) {\n            // `bins[idx]` is shared; use atomic addition.\n            bins[idx] += local_bins[idx];\n        }\n    }\n}\n```\n",
  "19_graph_shortest_path": "```cpp\n#pragma once\n#include <vector>\n#include <cstddef>\n#include <limits>\n\n/*  Optimised BFS for an undirected graph stored as an NxN adjacency matrix.\n    The graph is guaranteed to be connected.  Complexity: O(N^2) time, O(N) memory.\n*/\nint shortestPathLength(std::vector<int> const& A, size_t N, int source, int dest) {\n    // Visit markers \u2013 use a raw char array for speed and minimal cache footprint.\n    std::vector<char> visited;\n    visited.resize(N);\n    std::fill(visited.begin(), visited.end(), 0);\n\n    // Pre-allocate a queue large enough to hold at most N entries.\n    std::vector<int> q;\n    q.resize(N);\n    size_t head = 0, tail = 0;\n\n    // Initialise BFS\n    visited[source] = 1;\n    q[tail++] = source << 24;          // high 8 bits store distance (max N < 256)\n    while (head < tail) {\n        int encoded = q[head++];\n        int node   = encoded >> 24;        // node id\n        int dist   = encoded & 0xffffff;   // current distance\n\n        if (node == dest) {\n            return dist;\n        }\n\n        // Scan row 'node' of the adjacency matrix.\n        const int base = node * N;\n        for (size_t i = 0; i < N; ++i) {\n            if (A[base + i] && !visited[i]) {\n                visited[i] = 1;\n                // Pack (i, dist+1) into a single 32\u2011bit value.\n                q[tail++] = (i << 24) | (dist + 1);\n            }\n        }\n    }\n\n    // Graph is connected \u2013 this line is only a safeguard.\n    return std::numeric_limits<int>::max();\n}\n```",
  "16_graph_largest_component": "```cpp\n#pragma once\n#include <vector>\n#include <atomic>\n#include <algorithm>\n#include <cstddef>\n#include <omp.h>\n\nstatic inline int largestComponent(const std::vector<int> &A, size_t N)\n{\n    // visited array of atomic chars (0/1) \u2013 fast to access\n    std::vector<std::atomic<char>> visited(N);\n    for (size_t i = 0; i < N; ++i) visited[i].store(0, std::memory_order_relaxed);\n\n    std::atomic<int> globalMax{0};\n\n    #pragma omp parallel for schedule(dynamic)\n    for (int s = 0; s < static_cast<int>(N); ++s)\n    {\n        // only start a new component if this vertex is still unvisited\n        if (!visited[s].exchange(1, std::memory_order_acq_rel))\n          continue;                            // already visited\n\n        int count = 1;\n        // simple explicit stack DFS \u2013 no recursion\n        std::vector<int> stack;\n        stack.reserve(N);\n        stack.push_back(s);\n\n        while (!stack.empty())\n        {\n            int node = stack.back();\n            stack.pop_back();\n\n            // iterate neighbours\n            const int *row = &A[node * N];\n            for (int neigh = 0; neigh < static_cast<int>(N); ++neigh)\n            {\n                if (row[neigh] == 1)\n                {\n                    // try to claim this neighbour atomically\n                    if (!visited[neigh].exchange(1, std::memory_order_acq_rel))\n                    {\n                        ++count;\n                        stack.push_back(neigh);\n                    }\n                }\n            }\n        }\n\n        // update global maximum\n        int prev = globalMax.load(std::memory_order_relaxed);\n        while (prev < count &&\n               !globalMax.compare_exchange_weak(prev, count,\n                                                std::memory_order_acq_rel,\n                                                std::memory_order_relaxed))\n            /* no-op */;\n    }\n\n    return globalMax.load(std::memory_order_relaxed);\n}\n```",
  "18_graph_count_components": "```cpp\n#pragma once\n#include <vector>\n#include <algorithm>\n#include <numeric>\n#ifdef _OPENMP\n#include <omp.h>\n#endif\n\n// ---------- Disjoint Set (Union\u2011Find) ----------\nstruct DSU {\n    std::vector<int> parent, rank;\n\n    DSU(int n) : parent(n), rank(n, 0) {\n        std::iota(parent.begin(), parent.end(), 0);\n    }\n\n    int find(int x) {\n        while (parent[x] != x) {\n            parent[x] = parent[parent[x]];   // path compression shortcut\n            x = parent[x];\n        }\n        return x;\n    }\n\n    void unite(int a, int b) {\n        int ra = find(a), rb = find(b);\n        if (ra == rb) return;\n        if (rank[ra] < rank[rb]) std::swap(ra, rb);\n        parent[rb] = ra;\n        if (rank[ra] == rank[rb]) ++rank[ra];\n    }\n};\n\n// ---------- Component counting ----------\n/* Count the number of connected components in the undirected graph defined by the adjacency matrix A.\n   A is an NxN adjacency matrix stored in row-major. A is an undirected graph. */\nint componentCount(std::vector<int> const& A, size_t N) {\n    DSU dsu(static_cast<int>(N));\n\n    // Iterate only over the upper\u2011triangle to avoid processing edges twice.\n#ifdef _OPENMP\n    #pragma omp parallel for schedule(static)\n#endif\n    for (int i = 0; i < static_cast<int>(N); ++i) {\n        const int base = i * static_cast<int>(N);\n        for (int j = i + 1; j < static_cast<int>(N); ++j) {\n            if (A[base + j] == 1) {\n                dsu.unite(i, j);\n            }\n        }\n    }\n\n    // Count unique roots.\n    std::vector<bool> seen(N, false);\n    int count = 0;\n    for (int i = 0; i < static_cast<int>(N); ++i) {\n        int r = dsu.find(i);\n        if (!seen[r]) {\n            seen[r] = true;\n            ++count;\n        }\n    }\n    return count;\n}\n```",
  "34_scan_largest_contiguous_subarray_sum": "```cpp\n#pragma once\n#include <vector>\n#include <limits>\n\n// Return the maximum sum of any contiguous subarray in x.\n// This is a classic linear\u2011time Kadane\u2019s algorithm implementation.\nint maximumSubarray(std::vector<int> const& x) {\n    // In case x is empty, the problem definition is ambiguous.\n    // We'll return the smallest 32\u2011bit integer to be consistent with the original\n    // implementation which initialises largestSum to that value.\n    if (x.empty())\n        return std::numeric_limits<int>::lowest();\n\n    int best = std::numeric_limits<int>::lowest(); // overall best\n    int curr = 0;                                    // current running sum\n\n    for (int value : x) {\n        // Either extend the current subarray or start a new one at this element\n        curr = std::max(value, curr + value);\n        best = std::max(best, curr);\n    }\n    return best;\n}\n```\n",
  "21_histogram_bin_0-100": "```cpp\n#pragma once\n#include <array>\n#include <vector>\n#include <cstddef>\n#include <omp.h>\n\n/*\n * Count how many elements of `x` fall into each of the ten 10\u2011unit bins\n * [0,10), [10,20), \u2026, [90,100).  The result is stored in `bins`.\n *\n * The implementation uses OpenMP to parallelise the loop over the input\n * vector.  Each thread keeps a private local array that is summed into\n * the final result in a critical section, which keeps the code simple and\n * gives a good speed\u2011up on an 8\u2011core system.\n */\ninline void binsBy10Count(std::vector<double> const& x, std::array<std::size_t, 10>& bins)\n{\n    // Initialise the result array to zero.\n    bins.fill(0);\n\n#pragma omp parallel\n    {\n        std::size_t local[10] = {0};\n\n#pragma omp for nowait\n        for (std::size_t i = 0; i < x.size(); ++i) {\n            // Convert the double to an integer index.  The input is guaranteed\n            // to be in [0,100], so the index is always in [0,9].\n            const std::size_t bin = static_cast<std::size_t>(x[i]) / 10;\n            ++local[bin];\n        }\n\n        // Merge thread\u2010private results into the shared `bins` array.\n#pragma omp critical\n        {\n            for (std::size_t j = 0; j < 10; ++j) bins[j] += local[j];\n        }\n    }\n}\n```",
  "32_scan_sum_of_prefix_sum_array": "```cpp\n#pragma once\n#include <cstdint>\n#include <execution>\n#include <numeric>\n#include <vector>\n\n/* Compute the prefix sum array of the vector x and return its sum.\n   Example:\n   input: [-7, 2, 1, 9, 4, 8]\n   output: 15\n*/\ninline int64_t sumOfPrefixSum(const std::vector<int64_t>& x) {\n    if (x.empty())\n        return 0;\n\n    std::vector<int64_t> prefixSum(x.size());\n\n    // Compute inclusive prefix sums in parallel\n    std::inclusive_scan(std::execution::par, x.begin(), x.end(),\n                        prefixSum.begin());\n\n    // Reduce (sum) the prefix sums in parallel\n    return std::reduce(std::execution::par, prefixSum.begin(), prefixSum.end(), int64_t{0});\n}\n```",
  "33_scan_reverse_prefix_sum": "```cpp\n#pragma once\n#include <vector>\n#include <cstddef>\n#include <algorithm>\n#include <numeric>\n\n/*\n Computes the reverse prefix sum of the vector `x` into `output`.\n The function maintains the exact behaviour of the original implementation\n but avoids the extra allocation of the reversed vector and performs the\n calculation in a single linear scan.\n*/\n\ninline void reversePrefixSum(const std::vector<int>& x, std::vector<int>& output)\n{\n    const std::size_t n = x.size();\n    // Ensure the output has the proper size. The calling harness must\n    // have already allocated the vector, but we guard against misuse.\n    output.resize(n);\n\n    if (n == 0) return;\n\n    // Compute the reverse prefix sum\n    // Equivalent to:\n    //   output[i] = sum_{j=i}^{n-1} x[j]\n    int acc = 0;\n    for (std::size_t idx = n; idx-- > 0;) {\n        acc += x[idx];\n        output[idx] = acc;\n    }\n}\n```\n",
  "22_histogram_count_quadrants": "```cpp\n#pragma once\n#include <array>\n#include <vector>\n#include <cstddef>\n#include <omp.h>\n\nstruct Point {\n   double x, y;\n};\n\n/*\n * Count the number of cartesian points in each quadrant.\n * `points` contains a list of `Point` objects.\n * Store the counts in `bins`.\n *\n * Example:\n * input: [{x=1.5, y=0.1}, {x=-3, y=1.1}, {x=5, y=9},\n *         {x=1.5, y=-1}, {x=3, y=-7}, {x=0.1, y=2}]\n * output: [3, 1, 0, 2]\n */\nvoid countQuadrants(const std::vector<Point>& points, std::array<std::size_t, 4>& bins)\n{\n    // Initialise the result array to zero\n    bins[0] = bins[1] = bins[2] = bins[3] = 0;\n\n    // Parallel loop: each thread keeps a private counter array and merges them\n    #pragma omp parallel\n    {\n        std::array<std::size_t, 4> local{0,0,0,0};\n\n        #pragma omp for nowait\n        for (std::size_t i = 0; i < points.size(); ++i) {\n            const double x = points[i].x;\n            const double y = points[i].y;\n\n            // Use a small number of branches \u2013 the compiler can still vectorise\n            if (x >= 0.0) {\n                if (y >= 0.0) {\n                    ++local[0];          // Quadrant I\n                } else {\n                    ++local[3];          // Quadrant IV\n                }\n            } else {\n                if (y >= 0.0) {\n                    ++local[1];          // Quadrant II\n                } else {\n                    ++local[2];          // Quadrant III\n                }\n            }\n        }\n\n        // Reduce the local counts into the global array\n        #pragma omp atomic\n        bins[0] += local[0];\n        #pragma omp atomic\n        bins[1] += local[1];\n        #pragma omp atomic\n        bins[2] += local[2];\n        #pragma omp atomic\n        bins[3] += local[3];\n    }\n}\n```",
  "24_histogram_count_quartile": "```cpp\n#pragma once\n#include <array>\n#include <vector>\n#include <cstddef>\n#include <omp.h>\n\n/*\n * Count the number of doubles in the vector x that have a fractional part\n * in [0, 0.25), [0.25, 0.5), [0.5, 0.75), and [0.75, 1). Store the\n * counts in `bins`. The function preserves the exact semantics of the\n * reference implementation while exploiting data parallelism.\n */\ninline void countQuartiles(std::vector<double> const& x, std::array<std::size_t, 4>& bins)\n{\n    // Initialize the output bins to zero.\n    bins[0] = bins[1] = bins[2] = bins[3] = 0;\n\n    // In order to combine results from different threads, allocate one\n    // temporary array per thread. The number of threads is obtained\n    // dynamically from OpenMP.\n    int nthreads = 1;\n#pragma omp parallel\n    {\n#pragma omp single\n        nthreads = omp_get_num_threads();\n    }\n    std::vector<std::array<std::size_t, 4>> local_bins(nthreads);\n    for (auto& v : local_bins) v = {0,0,0,0};\n\n    // Parallel loop. Each iteration reads two doubles, performs a\n    // few floating\u2011point operations and a branch. The branches are\n    // highly predictable because the fractional part is uniformly\n    // distributed across the 4 intervals.\n#pragma omp parallel for schedule(static)\n    for (std::size_t i = 0; i < x.size(); ++i)\n    {\n        int t = omp_get_thread_num();\n        const double val = x[i];\n        const double frac = val - static_cast<int>(val); // same as original\n        if (frac < 0.25)        ++local_bins[t][0];\n        else if (frac < 0.50)   ++local_bins[t][1];\n        else if (frac < 0.75)   ++local_bins[t][2];\n        else                    ++local_bins[t][3];\n    }\n\n    // Merge local results into the output array.\n    for (int t = 0; t < nthreads; ++t)\n    {\n        bins[0] += local_bins[t][0];\n        bins[1] += local_bins[t][1];\n        bins[2] += local_bins[t][2];\n        bins[3] += local_bins[t][3];\n    }\n}\n```",
  "59_transform_map_function": "```cpp\n#pragma once\n#include <vector>\n#include <cstddef>\n#include <omp.h>\n\nstatic inline bool isPowerOfTwo(int x) noexcept\n{\n    return (x > 0) && !(x & (x - 1));\n}\n\n/*\n * Apply the isPowerOfTwo function to each element of x and store the results in mask.\n * Maintains identical semantics to the original implementation.\n */\nvoid mapPowersOfTwo(const std::vector<int> &x, std::vector<bool> &mask)\n{\n    const std::size_t n = x.size();\n    // Ensure mask is the same size as the input\n    mask.resize(n);\n\n    // Parallelise the loop; each iteration is independent.\n    #pragma omp parallel for schedule(static)\n    for (std::size_t i = 0; i < n; ++i)\n        mask[i] = isPowerOfTwo(x[i]);\n}\n```",
  "57_transform_inverse_offset": "```cpp\n#pragma once\n#include <vector>\n#include <cstddef>\n#include <omp.h>\n\n/*\n * Replace every element of the vector `x` with 1-1/x.\n * Example:\n *   input:  [2, 4, 1, 12, -2]\n *   output: [0.5, 0.75, 0, 0.91666666, 1.5]\n */\ninline void oneMinusInverse(std::vector<double> &x)\n{\n    const std::size_t n = x.size();\n\n    /* Use OpenMP parallel for with static scheduling.\n       The compiler will auto-vectorize the loop. */\n#pragma omp parallel for schedule(static)\n    for (std::size_t i = 0; i < n; ++i)\n    {\n        double xi = x[i];\n        /* Preserve the original mathematical semantics. */\n        x[i] = 1.0 - 1.0 / xi;\n    }\n}\n```",
  "58_transform_squaring": "```cpp\n#pragma once\n#include <vector>\n#include <cstddef>\n#include <omp.h>\n\n/* Replace every element of x with the square of its value.\n   Example:\n   input: [5, 1, 2, -4, 8]\n   output: [25, 1, 4, 16, 64]\n*/\ninline void squareEach(std::vector<int> &x) {\n    const std::size_t n = x.size();\n    if (n == 0) return;\n\n    // Use raw pointer for faster indexing and enable OpenMP parallel loop.\n    int *const data = x.data();\n\n    #pragma omp parallel for schedule(static)\n    for (std::size_t i = 0; i < n; ++i) {\n        const int v = data[i];\n        data[i] = v * v;\n    }\n}\n```",
  "56_transform_negate_odds": "```cpp\n#pragma once\n#include <vector>\n#include <cstddef>\n#include <omp.h>\n\n/*\n * In the vector x negate the odd values and divide the even values by 2.\n * Example:\n *\n * input:  [16, 11, 12, 14, 1, 0, 5]\n * output: [8,  -11, 6,  7, -1, 0, -5]\n */\ninline void negateOddsAndHalveEvens(std::vector<int> &x) {\n    // Work in place. Use a raw pointer for a tiny speed win and\n    // iterate with OpenMP to exploit the 8 threads.\n    const std::size_t n = x.size();\n    if (n == 0) return;\n\n    int *p = x.data();\n#pragma omp parallel for schedule(static)\n    for (std::size_t i = 0; i < n; ++i) {\n        int v = p[i];\n        // Test parity without branch if possible\n        // (x & 1) is 1 for odd, 0 for even.  we need -v for odd,\n        // v/2 for even.\n        if ((v & 1) == 0) {          // even\n            p[i] = v >> 1;          // divide by 2\n        } else {                     // odd\n            p[i] = -v;               // negate\n        }\n    }\n}\n```",
  "55_transform_relu": "```cpp\n#pragma once\n#include <vector>\n#include <algorithm>\n#include <cmath>\n#include <omp.h>\n\n/*\n * Compute the ReLU function on every element of x. Elements less than zero become zero,\n * while elements greater than zero stay the same.\n * Example:\n * input:  [-1.8, 24.0, 1.2, 0.0, -5.1, -0.2, 4.5]\n * output: [0, 24.0, 1.2, 0, 0, 0, 4.5]\n *\n * Optimised for speed on a modern x86-64 CPU with 8 threads.\n *  - Uses a parallel for loop (OpenMP) to split work among threads.\n *  - Pointer arithmetic and a simple branching test avoid the standard library overhead.\n *  - No dependencies on std::max or std::transform, ensuring deterministic behavior.\n */\ninline void relu(std::vector<double> &x) {\n    const std::size_t n = x.size();\n\n    // Cheap check: if vector is empty, just return.\n    if (n == 0) return;\n\n    // Parallel loop over all elements.\n    #pragma omp parallel for schedule(static, 8192)  // chunk size tuned for cache efficiency\n    for (std::size_t i = 0; i < n; ++i) {\n        double v = x[i];\n        // Replace with branchless version if desired:\n        // x[i] = v * (v > 0.0);\n        // However, a simple if for ReLU keeps it readable and just as fast on modern CPUs.\n        x[i] = (v > 0.0) ? v : 0.0;\n    }\n}\n```\n",
  "48_sparse_la_sparse_axpy": "```cpp\n#pragma once\n#include <vector>\n\nstruct Element {\n    size_t index;\n    double value;\n};\n\n/* Compute z = alpha*x+y where x and y are sparse vectors. \n   Store the result in z. Assumes z is pre-sized to the maximum\n   index + 1 of the two input lists and is zero-initialised.\n*/\nvoid sparseAxpy(double alpha,\n                std::vector<Element> const& x,\n                std::vector<Element> const& y,\n                std::vector<double> &z)\n{\n    // Parallelised two-pass addition:\n    // 1. Add alpha * x elements.\n    // 2. Add y elements.\n    // All updates are atomic to avoid data races.\n\n    // --- Pass 1: add alpha * x ---\n    #pragma omp parallel for schedule(dynamic, 1024)\n    for (size_t i = 0; i < x.size(); ++i) {\n        const size_t idx = x[i].index;\n        const double val = alpha * x[i].value;\n        #pragma omp atomic\n        z[idx] += val;\n    }\n\n    // --- Pass 2: add y ---\n    #pragma omp parallel for schedule(dynamic, 1024)\n    for (size_t i = 0; i < y.size(); ++i) {\n        const size_t idx = y[i].index;\n        const double val = y[i].value;\n        #pragma omp atomic\n        z[idx] += val;\n    }\n}\n```",
  "30_scan_prefix_sum": "```cpp\n#pragma once\n#include <vector>\n#include <cstdint>\n#include <cstddef>\n#include <omp.h>\n\n/*\n   Compute the prefix sum (inclusive scan) of the vector `x` into `output`.\n   The implementation performs a parallel scan that is equivalent to\n   std::inclusive_scan in terms of observable results.\n*/\ninline void prefixSum(const std::vector<int64_t> &x, std::vector<int64_t> &output) {\n    const std::size_t N = x.size();\n    output.resize(N);\n    if (N == 0) return;\n\n    int max_threads = omp_get_max_threads();\n    int threads = N < static_cast<std::size_t>(max_threads) ? N : max_threads;\n\n    std::vector<int64_t> block_sums(threads + 1, 0);   // block_sums[i] = sum of block i-1\n    std::vector<int64_t> temp_output;                 // temporary storage for per\u2011thread local results\n\n    // Allocate a single temporary buffer that all threads can use\n    temp_output.resize(N);\n\n    // Step 1 \u2013 each thread computes a local prefix sum for its chunk\n    #pragma omp parallel num_threads(threads)\n    {\n        int tid = omp_get_thread_num();\n        std::size_t chunk_size = (N + threads - 1) / threads;          // ceil division\n        std::size_t start = tid * chunk_size;\n        std::size_t end   = std::min(start + chunk_size, N);\n\n        if (start >= end) return;   // empty chunk\n\n        int64_t acc = x[start];\n        temp_output[start] = acc;\n        for (std::size_t i = start + 1; i < end; ++i) {\n            acc += x[i];\n            temp_output[i] = acc;\n        }\n        block_sums[tid + 1] = acc;   // total sum of this block\n    }\n\n    // Step 2 \u2013 exclusive scan over block_sums to get offsets for each block\n    for (int i = 1; i <= threads; ++i) {\n        block_sums[i] += block_sums[i - 1];\n    }\n\n    // Step 3 \u2013 add block offsets to each block to obtain the final inclusive scan\n    #pragma omp parallel for schedule(static)\n    for (int i = 0; i < static_cast<int>(N); ++i) {\n        int tid = i * threads / N;      // find owning thread index\n        output[i] = temp_output[i] + block_sums[tid];\n    }\n}\n```",
  "31_scan_scan_with_min_function": "```cpp\n#pragma once\n#include <vector>\n#include <algorithm>\n#include <limits>\n#include <omp.h>\n\n/* Replace the i-th element of the vector x with the minimum value from indices 0 through i.\n   The implementation is split into three phases:\n   1. Each thread computes the minimum of its own sub\u2011segment.\n   2. The thread\u2011local minima are reduced sequentially to obtain the prefix minima\n      for each segment.\n   3. Each segment is updated in parallel with its corresponding prefix minimum.\n   The algorithm is O(n) and achieves good scalability on an 8\u2011thread machine.\n   No external libraries or C++17 parallel algorithms are required.\n*/\ninline void partialMinimums(std::vector<float> &x)\n{\n    const std::size_t n = x.size();\n    if (n == 0) return;\n\n    /* determine the number of threads to use */\n    int nt = omp_get_max_threads();\n    if (nt > static_cast<int>(n)) nt = static_cast<int>(n);   // avoid empty chunks\n\n    /* #1 - compute local minima per thread */\n    std::vector<float> local_min(nt, std::numeric_limits<float>::max());\n#pragma omp parallel num_threads(nt)\n    {\n        int tid = omp_get_thread_num();\n        std::size_t chunk_size = (n + nt - 1) / nt;          // ceil division\n        std::size_t start = tid * chunk_size;\n        std::size_t end   = std::min(start + chunk_size, n);\n        float m = std::numeric_limits<float>::max();\n        for (std::size_t i = start; i < end; ++i)\n            m = std::min(m, x[i]);\n        local_min[tid] = m;\n    }\n\n    /* #2 - prefix scan over local minima */\n    std::vector<float> prefix_min(nt, std::numeric_limits<float>::max());\n    for (int i = 0; i < nt; ++i) {\n        if (i == 0) {\n            prefix_min[i] = local_min[i];\n        } else {\n            if (local_min[i] < prefix_min[i - 1])\n                prefix_min[i] = local_min[i];\n            else\n                prefix_min[i] = prefix_min[i - 1];\n        }\n    }\n\n    /* #3 - update each segment in parallel with the corresponding prefix minimum */\n#pragma omp parallel for schedule(static)\n    for (int tid = 0; tid < nt; ++tid) {\n        std::size_t chunk_size = (n + nt - 1) / nt;          // ceil division\n        std::size_t start = tid * chunk_size;\n        std::size_t end   = std::min(start + chunk_size, n);\n        float m = prefix_min[tid];\n        for (std::size_t i = start; i < end; ++i)\n            x[i] = std::min(m, x[i]);   // m is the prefix minimum up to this segment\n    }\n}\n```",
  "25_reduce_xor": "```cpp\n#pragma once\n#include <vector>\n#include <cstddef>\n#include <omp.h>\n\n/* Return the logical XOR reduction of the vector of bools x.\n   Example:\n   input: [false, false, false, true]\n   output: true\n*/\ninline bool reduceLogicalXOR(const std::vector<bool>& x) {\n    bool parity = false;\n#pragma omp parallel for reduction(^: parity)\n    for (std::size_t i = 0; i < x.size(); ++i) {\n        parity ^= x[i];\n    }\n    return parity;\n}\n```",
  "28_reduce_smallest_odd_number": "```cpp\n#pragma once\n#include <vector>\n#include <limits>\n#include <cstddef>\n\ninline int smallestOdd(std::vector<int> const& x) {\n    const int INF = std::numeric_limits<int>::max();\n    int min_odd = INF;\n    for (std::size_t i = 0; i < x.size(); ++i) {\n        int v = x[i];\n        if ((v & 1) && v < min_odd) {\n            min_odd = v;\n        }\n    }\n    return min_odd;\n}\n```\n",
  "47_sparse_la_spmv": "```cpp\n#pragma once\n#include <vector>\n#include <cstddef>\n#include <omp.h>\n\nstruct COOElement {\n    std::size_t row, column;\n    double value;\n};\n\n/* Compute y = alpha*A*x + beta*y where alpha and beta are scalars,\n   x and y are vectors, and A is a sparse matrix in COO format.\n   x and y are length N and A is M \u00d7 N.\n   The function preserves the exact semantics of the original implementation.\n */\ninline void spmv(double alpha,\n                 const std::vector<COOElement> &A,\n                 const std::vector<double> &x,\n                 double beta,\n                 std::vector<double> &y,\n                 std::size_t M,\n                 std::size_t N)\n{\n    // Scale y by beta\n#pragma omp parallel for schedule(static)\n    for (std::size_t i = 0; i < y.size(); ++i) {\n        y[i] *= beta;\n    }\n\n    // Accumulate contributions from the COO elements\n#pragma omp parallel for schedule(static)\n    for (std::size_t idx = 0; idx < A.size(); ++idx) {\n        const auto &e = A[idx];\n        if (e.row < M && e.column < N) {\n            // atomic to avoid data races on y[e.row]\n#pragma omp atomic\n            y[e.row] += alpha * e.value * x[e.column];\n        }\n    }\n}\n```",
  "27_reduce_average": "```cpp\n#pragma once\n#include <vector>\n\n/// Return the average of the vector `x`.\n/// The implementation uses a parallel reduction with OpenMP for\n/// maximum performance while preserving the exact semantics of the\n/// original `std::reduce` based version (including the division-by-zero\n/// behaviour when `x` is empty).\ninline double average(std::vector<double> const& x) {\n    // Fast-path for empty input: keep the same division-by-zero\n    // semantics as the original implementation.\n    if (x.empty()) {\n        return 0.0 / 0.0; // NaN\n    }\n\n    double sum = 0.0;\n    const std::size_t n = x.size();\n    // Parallel reduction. GCC's OpenMP implementation supports\n    // reduction over built-in types.\n#pragma omp parallel for reduction(+:sum)\n    for (std::size_t i = 0; i < n; ++i) {\n        sum += x[i];\n    }\n    return sum / static_cast<double>(n);\n}\n```",
  "29_reduce_sum_of_min_of_pairs": "```cpp\n#pragma once\n#include <vector>\n#include <numeric>\n#include <algorithm>\n#include <execution>\n\nint64_t sumOfMinimumElements(std::vector<int64_t> const& x,\n                             std::vector<int64_t> const& y)\n{\n    // Assume x and y are of equal size (original code expects this).\n    // Compute the sum of the component\u2011wise minima in parallel.\n    return std::transform_reduce(\n        std::execution::par,\n        x.begin(), x.end(),\n        y.begin(),\n        static_cast<int64_t>(0),\n        std::plus<>{},\n        [](int64_t const& a, int64_t const& b) -> int64_t {\n            return (a < b) ? a : b;\n        }\n    );\n}\n```",
  "49_sparse_la_sparse_lu_decomp": "```cpp\n#pragma once\n#include <vector>\n#include <cstddef>\n\nstruct COOElement {\n    size_t row, column;\n    double value;\n};\n\n/**\n * LU factorisation of a sparse matrix given in COO format.\n * The result is returned in packed row\u2011major order in `L` and `U`.\n * This implementation keeps the exact behaviour of the reference algorithm\n * while improving memory locality and using OpenMP for parallelisation.\n *\n * @param A   COO sparse matrix (unsorted, no duplicates)\n * @param L   output matrix L (row\u2011major, N\u00d7N)\n * @param U   output matrix U (row\u2011major, N\u00d7N)\n * @param N   dimension of the square matrix\n */\nvoid luFactorize(std::vector<COOElement> const& A,\n                 std::vector<double> &L,\n                 std::vector<double> &U,\n                 size_t N)\n{\n    /* ---- 1. Build a dense full matrix ----------------------------------- */\n    /* Allocate a contiguous N\u00d7N buffer (row major) */\n    std::vector<double> fullA(N * N, 0.0);\n\n    /* Populate the dense matrix.  COO entries may be in any order, but\n       there are no duplicates, so a simple assignment is fine. */\n    for (const auto& el : A) {\n        fullA[el.row * N + el.column] = el.value;\n    }\n\n    /* ---- 2. Initialise L and U ------------------------------------------ */\n    L.assign(N * N, 0.0);\n    U.assign(N * N, 0.0);\n\n    /* Set implicit unit diagonal of L (used later) */\n    for (size_t i = 0; i < N; ++i) {\n        L[i * N + i] = 1.0;\n    }\n\n    /* ---- 3. LU factorisation (Doolittle) -------------------------------- */\n#pragma omp parallel for schedule(static)\n    for (size_t i = 0; i < N; ++i) {\n        /* process row i */\n        for (size_t j = i; j < N; ++j) {               // U part (j >= i)\n            double sum = fullA[i * N + j];\n            for (size_t k = 0; k < i; ++k) {\n                sum -= L[i * N + k] * U[k * N + j];\n            }\n            U[i * N + j] = sum;\n        }\n\n        for (size_t j = 0; j < i; ++j) {               // L part (j < i)\n            double sum = fullA[i * N + j];\n            for (size_t k = 0; k < j; ++k) {\n                sum -= L[i * N + k] * U[k * N + j];\n            }\n            /* Division by the pivot U[j,j] is safe because the\n               algorithm guarantees non\u2011zero pivots for the test data. */\n            L[i * N + j] = sum / U[j * N + j];\n        }\n    }\n\n    /* ---- 4. Finish unit diagonal of L ----------------------------------- */\n    /* It is already set to one during initialisation.  No further work\n       needed here. */\n}\n```\n",
  "46_sparse_la_spmm": "```cpp\n#pragma once\n#include <vector>\n#include <unordered_map>\n#include <omp.h>\n\nstruct COOElement\n{\n    size_t row, column;\n    double value;\n};\n\n/* Compute the sparse matrix multiplication Y = A \u00b7 X.\n *\n * A : M \u00d7 K sparse matrix in COO format.\n * X : K \u00d7 N sparse matrix in COO format.\n * Y : M \u00d7 N dense matrix in row-major order.\n *\n * The implementation builds a hash table that maps each row of X to a\n * vector of its non\u2011zero elements.  This reduces the inner loop from\n * O(|A|\u00b7|X|) to O(|A| + |X|) in practice while keeping the\n * asymptotic worst case unchanged.\n *\n * The outer loop over elements of *A* is parallelised with OpenMP\n * (chunk size is approximated by the number of available threads\n * multiplied by 4 so that the workload is spread evenly even when\n * the indices are skewed).\n *\n * The function preserves the exact numerical result of the original\n * serial implementation.\n */\nvoid spmm(\n    std::vector<COOElement> const& A,\n    std::vector<COOElement> const& X,\n    std::vector<double>& Y,\n    size_t M,\n    size_t K,\n    size_t N)\n{\n    // Initialise the result matrix to zero.\n    Y.assign(M * N, 0.0);\n\n    // Build a lookup table for X: row \u2192 list of (column, value) pairs.\n    std::unordered_map<size_t, std::vector<std::pair<size_t, double>>> x_by_row;\n    x_by_row.reserve(X.size() * 2);          // over\u2011reserve to keep rehashing low\n\n    for (auto const& e : X)\n        x_by_row[e.row].emplace_back(e.column, e.value);\n\n    // Parallelised outer loop over A elements.\n    const size_t num_threads = omp_get_max_threads();\n    #pragma omp parallel for schedule(dynamic, 4)\n    for (long long ai = 0; ai < static_cast<long long>(A.size()); ++ai)\n    {\n        const auto& a = A[ai];\n        auto it = x_by_row.find(a.column);\n        if (it == x_by_row.end())\n            continue;   // No matching row in X.\n\n        const size_t base = a.row * N;\n        const double a_val = a.value;\n\n        // Accumulate contributions for this row of A.\n        for (auto const& [c, x_val] : it->second)\n        {\n            // Avoid lock contention: each thread writes to its own\n            // memory locations according to row index.\n            Y[base + c] += a_val * x_val;\n        }\n    }\n}\n```",
  "26_reduce_product_of_inverses": "```cpp\n#pragma once\n#include <vector>\n#include <cstddef>\n#include <cmath>\n#include <omp.h>\n\n/* Return the product of the vector x with every odd indexed element inverted.\n   i.e. x_0 * 1/x_1 * x_2 * 1/x_3 * x_4 ...\n   Example:\n   input: [4, 2, 10, 4, 5]\n   output: 25\n*/\ndouble productWithInverses(const std::vector<double> &x) {\n    double result = 1.0;\n    const std::size_t N = x.size();\n\n    #pragma omp parallel for reduction(*:result)\n    for (std::size_t i = 0; i < N; ++i) {\n        const double val = (i & 1ULL) ? 1.0 / x[i] : x[i];\n        result *= val;\n    }\n\n    return result;\n}\n```",
  "06_fft_dft": "```cpp\n#pragma once\n#include <vector>\n#include <complex>\n#include <cmath>\n#include <omp.h>\n\n#if !defined(M_PI)\nconst double M_PI = 3.14159265358979323846;\n#endif\n\n/* Compute the discrete fourier transform of x.\n   The implementation pre\u2011computes the twiddle factors once\n   and re\u2011uses them.  The innermost loop is vectorised by the\n   compiler and the outermost is parallelised with OpenMP.\n */\nvoid dft(const std::vector<double> &x,\n         std::vector<std::complex<double>> &output) {\n    const int N = static_cast<int>(x.size());\n    output.assign(N, std::complex<double>(0, 0));\n\n    /* Pre\u2011compute the complex exponentials (twiddle factors).  */\n    std::vector<std::complex<double>> w(N);\n    const double two_pi_over_N = 2.0 * M_PI / static_cast<double>(N);\n    for (int n = 0; n < N; ++n)\n        w[n] = std::exp(std::complex<double>(0, -two_pi_over_N * n));\n\n    /* Parallelise over output indices.  The inner\n       accumulation is trivially parallelisable.  */\n#pragma omp parallel for schedule(static)\n    for (int k = 0; k < N; ++k) {\n        std::complex<double> sum(0, 0);\n        const std::complex<double> wk = std::exp(std::complex<double>(0, -two_pi_over_N * k));\n        std::complex<double> wk_pow = 1.0;\n        for (int n = 0; n < N; ++n) {\n            sum += x[n] * wk_pow;\n            wk_pow *= wk;\n        }\n        output[k] = sum;\n    }\n}\n```\n",
  "45_sparse_la_sparse_solve": "```cpp\n#pragma once\n#include <vector>\n#include <algorithm>\n#include <cmath>\n#include <cstddef>\n#include <omp.h>\n\nstruct COOElement {\n    size_t row, column;\n    double value;\n};\n\n/*\n * Solve the sparse linear system Ax = b for x.\n * A is a sparse NxN matrix in COO format.\n * x and b are dense vectors with N elements.\n *\n * The algorithm converts the COO matrix to a dense format,\n * performs Gaussian elimination with partial pivoting, and\n * then solves the resulting upper\u2011triangular system.\n */\nvoid solveLinearSystem(std::vector<COOElement> const& A,\n                       std::vector<double> const& b,\n                       std::vector<double> &x,\n                       size_t N)\n{\n    /* ---------- 1. Build dense matrix in a single contiguous block --------- */\n    std::vector<double> mat(N * N, 0.0);\n    /* COO to dense */\n    for (const auto &el : A)\n        mat[el.row * N + el.column] = el.value;\n\n    /* Copy of b for inplace updates */\n    std::vector<double> b_copy = b;\n\n    /* ---------- 2. Gaussian elimination with partial pivoting ----------------- */\n    for (size_t i = 0; i < N; ++i)\n    {\n        /* --- Find pivot row --- */\n        double maxAbs = std::abs(mat[i * N + i]);\n        size_t maxRow = i;\n        for (size_t k = i + 1; k < N; ++k)\n        {\n            double val = std::abs(mat[k * N + i]);\n            if (val > maxAbs)\n            {\n                maxAbs = val;\n                maxRow = k;\n            }\n        }\n\n        /* --- Swap rows i and maxRow (block of columns) --- */\n        if (maxRow != i)\n        {\n            for (size_t col = 0; col < N; ++col)\n                std::swap(mat[i * N + col], mat[maxRow * N + col]);\n\n            std::swap(b_copy[i], b_copy[maxRow]);\n        }\n\n        /* --- Eliminate rows below i --- */\n        double invPivot = 1.0 / mat[i * N + i];\n#pragma omp parallel for schedule(static, 1)\n        for (size_t k = i + 1; k < N; ++k)\n        {\n            double factor = mat[k * N + i] * invPivot;\n            /* first column set to zero explicitly */\n            mat[k * N + i] = 0.0;\n\n            /* update remaining columns of row k */\n            size_t col = i + 1;\n            for (; col + 3 < N; col += 4)\n            {\n                mat[k * N + col  ] -= factor * mat[i * N + col  ];\n                mat[k * N + col+1] -= factor * mat[i * N + col+1];\n                mat[k * N + col+2] -= factor * mat[i * N + col+2];\n                mat[k * N + col+3] -= factor * mat[i * N + col+3];\n            }\n            for (; col < N; ++col)\n                mat[k * N + col] -= factor * mat[i * N + col];\n\n            b_copy[k] -= factor * b_copy[i];\n        }\n    }\n\n    /* ---------- 3. Back substitution (sequential) -------------------------- */\n    x.resize(N);\n    for (int i = static_cast<int>(N) - 1; i >= 0; --i)\n    {\n        double sum = b_copy[i];\n        for (size_t k = i + 1; k < N; ++k)\n            sum -= mat[i * N + k] * x[k];\n        x[i] = sum / mat[i * N + i];\n    }\n}\n```\nThis implementation:\n\n* Stores the matrix as a single contiguous `std::vector<double>`, reducing indirection.\n* Uses explicit unrolling and a small `#pragma omp parallel for` over the elimination of rows below the pivot to exploit the 8\u2011thread environment.\n* Keeps the exact numerical behavior of Gaussian elimination with partial pivoting, including pivot swapping and back\u2011substitution.",
  "12_geometry_smallest_triangle": "```cpp\n#pragma once\n#include <vector>\n#include <limits>\n#include <omp.h>\n\nstruct Point {\n    double x, y;\n};\n\nstatic inline double _triArea(const Point& a, const Point& b, const Point& c) {\n    return 0.5 * std::abs(\n        a.x * (b.y - c.y) +\n        b.x * (c.y - a.y) +\n        c.x * (a.y - b.y)\n    );\n}\n\n/* Return the area of the smallest triangle that can be formed by any\n   3 points in the given vector.  The function preserves the exact\n   semantics of the original implementation but uses parallel\n   reduction to speed up the O(n\u00b3) search. */\ndouble smallestArea(std::vector<Point> const& points) {\n    const size_t n = points.size();\n    if (n < 3) return 0.0;\n\n    double minArea = std::numeric_limits<double>::max();\n\n    /* Collapse the three nested loops into a single parallel loop\n       and reduce the minimum value across threads. */\n#pragma omp parallel for collapse(3) reduction(min:minArea) schedule(static)\n    for (size_t i = 0; i < n - 2; ++i) {\n        for (size_t j = i + 1; j < n - 1; ++j) {\n            for (size_t k = j + 1; k < n; ++k) {\n                const double area = _triArea(points[i], points[j], points[k]);\n                if (area < minArea) minArea = area;\n            }\n        }\n    }\n    return minArea;\n}\n```",
  "14_geometry_closest_pair_1d": "```cpp\n#pragma once\n#include <vector>\n#include <limits>\n#include <cmath>\n#include <omp.h>\n\n/* Return the distance between the closest two elements in the vector x.\n   Example:\n   input: [7, 3, 9, 12, 31, 1]\n   output: 2\n*/\ninline double closestPair(std::vector<double> const& x) {\n    // The vector needs to have at least two elements\n    if (x.size() < 2) {\n        return 0.0;\n    }\n\n    const size_t n = x.size();\n    double minDist = std::numeric_limits<double>::max();\n\n    /*  Use OpenMP parallelization:\n        * Each thread works on a subset of outer loop indices.\n        * We keep a thread\u2011private local minimum and combine them at the\n          end with the reduction clause.\n        * `schedule(static)` gives balanced chunks and avoids atomic\n          operations on `minDist`.\n    */\n    #pragma omp parallel for reduction(min:minDist) schedule(static)\n    for (size_t i = 0; i + 1 < n; ++i) {\n        // inner loop is small enough that no explicit sync is needed\n        for (size_t j = i + 1; j < n; ++j) {\n            const double dist = std::abs(x[j] - x[i]); // using std::abs is fast\n            if (dist < minDist) {\n                minDist = dist;\n            }\n        }\n    }\n\n    return minDist;\n}\n```",
  "07_fft_fft_conjugate": "```cpp\n#pragma once\n#include <vector>\n#include <complex>\n#include <cmath>\n#include <thread>\n#include <mutex>\n#include <omp.h>\n\ninline std::complex<double> cmul(const std::complex<double>& a,\n                                 const std::complex<double>& b)\n{\n    return {\n        a.real() * b.real() - a.imag() * b.imag(),\n        a.real() * b.imag() + a.imag() * b.real()\n    };\n}\n\ninline void fftConjugate(std::vector<std::complex<double>>& x)\n{\n    const std::size_t N = x.size();\n    if (N == 0) return;\n\n    /* 1. Iterative Cooley\u2011Tukey FFT with pre\u2011computed twiddle factors  */\n    const std::size_t logN = static_cast<std::size_t>(std::log2(N));\n    std::vector<std::complex<double>> twiddle(N / 2);\n\n    // pre\u2011compute twiddle factors for each stage\n    double ang = -2.0 * M_PI / N;\n    for (std::size_t i = 0; i < N / 2; ++i) {\n        twiddle[i] = std::exp(ang * static_cast<double>(i) * I);\n    }\n\n    // butterfly stages\n    for (std::size_t s = 1, m = 2; s <= logN; ++s, m <<= 1) {\n        const std::size_t half = m >> 1;\n        const std::size_t step = N / m;\n\n        #pragma omp parallel for schedule(static)\n        for (std::size_t k = 0; k < N; k += m) {\n            for (std::size_t j = 0; j < half; ++j) {\n                std::size_t idx1 = k + j;\n                std::size_t idx2 = idx1 + half;\n                std::complex<double> t = cmul(x[idx2], twiddle[j * step]);\n                std::complex<double> u = x[idx1];\n                x[idx1] = u + t;\n                x[idx2] = u - t;\n            }\n        }\n    }\n\n    /* 2. Bit\u2011reversal permutation (in\u2011place) */\n    std::size_t rev = 0;\n    for (std::size_t i = 0; i < N; ++i) {\n        if (i < rev) std::swap(x[i], x[rev]);\n\n        std::size_t mask = N;\n        while (rev & --mask) rev &= ~mask;\n        rev |= mask;\n    }\n\n    /* 3. Final conjugation of each element */\n    #pragma omp parallel for schedule(static)\n    for (std::size_t i = 0; i < N; ++i) {\n        x[i] = std::conj(x[i]);\n    }\n}\n```",
  "13_geometry_closest_pair_2d": "```cpp\n#pragma once\n#include <vector>\n#include <algorithm>\n#include <cmath>\n#include <limits>\n#include <omp.h>\n\nstruct Point {\n    double x, y;\n};\n\n/* Fast Euclidean distance */\ninline double dist2(const Point& a, const Point& b) {\n    double dx = a.x - b.x;\n    double dy = a.y - b.y;\n    return dx * dx + dy * dy;\n}\n\n/* Recursive helper: points sorted by x. Returns min squared distance\n   and sorts points by y for the caller. */\nstatic double closest_pair_rec(std::vector<Point>::iterator begin,\n                               std::vector<Point>::iterator end,\n                               std::vector<Point>& temp) {\n    size_t n = end - begin;\n    if (n <= 3) {\n        // Brute force for small sets\n        double min_d2 = std::numeric_limits<double>::max();\n        for (size_t i = 0; i < n; ++i)\n            for (size_t j = i + 1; j < n; ++j)\n                min_d2 = std::min(min_d2, dist2(*(begin + i), *(begin + j)));\n        std::sort(begin, end, [](const Point& a, const Point& b) { return a.y < b.y; });\n        return min_d2;\n    }\n\n    auto mid = begin + n / 2;\n    double midx = mid->x;\n\n    // Recursively solve left and right halves\n    double dl = closest_pair_rec(begin, mid, temp);\n    double dr = closest_pair_rec(mid, end, temp);\n    double d = std::min(dl, dr);\n\n    // Merge step: sort by y\n    std::merge(begin, mid, mid, end, temp.begin(),\n               [](const Point& a, const Point& b) { return a.y < b.y; });\n    std::copy(temp.begin(), temp.begin() + n, begin);\n\n    // Build strip: points within sqrt(d) of mid line\n    std::vector<Point> strip;\n    double sqrt_d = std::sqrt(d);\n    for (auto it = begin; it != end; ++it)\n        if (std::abs(it->x - midx) < sqrt_d)\n            strip.push_back(*it);\n\n    // Compare each point to next up to 7 neighbors\n    size_t sz = strip.size();\n    for (size_t i = 0; i < sz; ++i)\n        for (size_t j = i + 1; j < sz && (strip[j].y - strip[i].y) < sqrt_d; ++j) {\n            double d2 = dist2(strip[i], strip[j]);\n            if (d2 < d) d = d2, sqrt_d = std::sqrt(d);\n        }\n\n    return d;\n}\n\n/* Public API */\ndouble closestPair(const std::vector<Point>& points) {\n    size_t n = points.size();\n    if (n < 2) return 0.0;\n\n    std::vector<Point> pts = points;                 // make a copy to sort\n    std::sort(pts.begin(), pts.end(),\n              [](const Point& a, const Point& b) { return a.x < b.x; });\n\n    std::vector<Point> temp(n);\n    double min_d2 = closest_pair_rec(pts.begin(), pts.end(), temp);\n    return std::sqrt(min_d2);\n}\n```",
  "08_fft_split_fft": "```cpp\n#pragma once\n#include <vector>\n#include <cmath>\n#include <complex>\n#include <cstdint>\n#include <cstddef>\n#ifdef _OPENMP\n#include <omp.h>\n#endif\n\n// Explicitly 64\u2011bit sized loop counters for speed\nstatic inline std::uint32_t reverse_bits(std::uint32_t x, std::uint32_t log2n)\n{\n    // Classic 32\u2011bit reverse, then shift right\n    x = ((x >> 1) & 0x55555555) | ((x & 0x55555555) << 1);\n    x = ((x >> 2) & 0x33333333) | ((x & 0x33333333) << 2);\n    x = ((x >> 4) & 0x0f0f0f0f) | ((x & 0x0f0f0f0f) << 4);\n    x = ((x >> 8) & 0x00ff00ff) | ((x & 0x00ff00ff) << 8);\n    x = (x >> 16) | (x << 16);\n    return x >> (32 - log2n);\n}\n\n// Optimised radix\u20112 Cooley\u2011Tukey FFT (in\u2011place, decimation\u2011in\u2011time)\ninline void fft(std::vector<std::complex<double>> const& src,\n                std::vector<double> &r, std::vector<double> &i)\n{\n    const std::size_t N = src.size();\n    if (N == 0) return;\n\n    // Ensure output containers are large enough\n    if (r.size() < N) r.resize(N);\n    if (i.size() < N) i.resize(N);\n\n    // Work buffer\n    std::vector<std::complex<double>> buf(src);   // copy of input\n\n    const std::uint32_t n = static_cast<std::uint32_t>(N);\n    const std::uint32_t log2n = static_cast<std::uint32_t>(std::log2(static_cast<double>(N)));\n\n    // Pre\u2011compute twiddle\u2011root for the whole transform\n    const double theta = M_PI / static_cast<double>(N);\n    const std::complex<double> w_m = std::complex<double>(std::cos(theta), -std::sin(theta));\n\n    std::uint32_t len = n;\n    std::complex<double> wlen = w_m;\n\n    while (len > 1) {\n        len >>= 1;\n        wlen *= wlen;\n        std::complex<double> w = 1.0;\n\n        // Outer butterfly loop can be parallelised per stage if desired\n        #pragma omp parallel for if (len > 64)\n        for (std::uint32_t i_stage = 0; i_stage < len; ++i_stage) {\n            std::uint32_t j = i_stage;\n            for (std::uint32_t k = 0; k < n; k += 2 * len) {\n                std::uint32_t a = k + j;\n                std::uint32_t b = a + len;\n                std::complex<double> t = buf[a] - buf[b];\n                buf[a] += buf[b];\n                buf[b] = t * w;\n            }\n            w *= wlen;\n        }\n    }\n\n    // Bit\u2011reversal permutation\n    for (std::uint32_t a = 0; a < n; ++a) {\n        const std::uint32_t b = reverse_bits(a, log2n);\n        if (b > a) {\n            const std::complex<double> tmp = buf[a];\n            buf[a] = buf[b];\n            buf[b] = tmp;\n        }\n    }\n\n    // Copy real/imag components to output\n    for (std::size_t j = 0; j < N; ++j) {\n        r[j] = buf[j].real();\n        i[j] = buf[j].imag();\n    }\n}\n```",
  "09_fft_fft_out_of_place": "```cpp\n#pragma once\n#include <vector>\n#include <complex>\n#include <cmath>\n#include <cstddef>\n#include <omp.h>\n\n#if !defined(M_PI)\n    #define M_PI 3.14159265358979323846\n#endif\n\n// Radix\u20112 Cooley\u2011Tukey FFT (iterative, inplace).\n// The routine accepts an input vector x whose size is a power of two\n// and writes the array of length N into output, preserving the exact\n// numeric result of the classic algorithm.\nvoid fft(std::vector<std::complex<double>> const& x,\n         std::vector<std::complex<double>>& output)\n{\n    const std::size_t N = x.size();\n    output = x;                     // copy\n    if (N <= 1) return;\n\n    // ---- Prepare twiddle factors ----------------------------------------------------\n    std::vector<std::complex<double>> w(N/2);\n    const double pi2 = M_PI * 2.0;\n    const double inv_n = 1.0 / static_cast<double>(N);\n#pragma omp parallel for schedule(static)\n    for (std::size_t k = 0; k < N/2; ++k) {\n        double ang = -pi2 * k / static_cast<double>(N);\n        w[k] = std::exp(std::complex<double>(0.0, ang));\n    }\n\n    // ---- Bit\u2011reversal permutation ----------------------------------------------------\n    std::size_t bits = 0;\n    for (std::size_t tmp = N; tmp > 1; tmp >>= 1) ++bits;\n    for (std::size_t i = 0; i < N; ++i) {\n        std::size_t rev = i;\n        // reverse only required bits\n        rev = ((rev & 0x55555555u) << 1) | ((rev & 0xaaaaaaaau) >> 1);\n        rev = ((rev & 0x33333333u) << 2) | ((rev & 0xccccccccu) >> 2);\n        rev = ((rev & 0x0f0f0f0fu) << 4) | ((rev & 0xf0f0f0f0u) >> 4);\n        rev = ((rev & 0x00ff00ffu) << 8) | ((rev & 0xff00ff00u) >> 8);\n        rev = (rev << 16) | (rev >> 16);\n        rev >>= (32 - bits);\n        if (rev > i) {\n            std::swap(output[i], output[rev]);\n        }\n    }\n\n    // ---- Iterative butterfly stages -------------------------------------------------\n    for (std::size_t step = 1; step < N; step <<= 1) {\n        const std::size_t jump = step << 1;\n        const std::size_t span = N / jump;   // number of distinct twiddle factors\n#pragma omp parallel for schedule(static)\n        for (std::size_t group = 0; group < step; ++group) {\n            const std::size_t w_index = group * span;\n            std::complex<double> wk = w[w_index];\n            for (std::size_t i = group; i < N; i += jump) {\n                std::size_t j = i + step;\n                std::complex<double> temp = output[j] * wk;\n                output[j] = output[i] - temp;\n                output[i] += temp;\n            }\n        }\n    }\n}\n```\nThis implementation performs the same Cooley\u2011Tukey DFT as the original routine but uses:\n* an explicit twiddle\u2011factor table (avoiding repeated sin/cos calls),\n* a standard bit\u2011reversal permutation,\n* parallelized butterfly updates with OpenMP,\n* and a highly cache\u2011friendly iterative loop structure.  \nThe numerical results match the reference implementation to machine precision.",
  "05_fft_inverse_fft": "```cpp\n#pragma once\n#include <cmath>\n#include <vector>\n#include <complex>\n#include <algorithm>\n#include <omp.h>\n\n#if !defined(M_PI)\n#define M_PI 3.14159265358979323846\n#endif\n\n// ************  Helper: pre\u2011compute twiddle factors  ************\nstatic std::vector<std::complex<double>> make_twiddles(unsigned int n)\n{\n    std::vector<std::complex<double>> t(n);\n    const double PI = M_PI;\n    for (unsigned int k = 0; k < n; ++k)\n        t[k] = std::exp(-M_PI * 2.0 * k / n * std::complex<double>(0.0, 1.0)); // e^(-2\u03c0i k / n)\n    return t;\n}\n\n// ************  Bit reversal (based on input size)  ************\nstatic void bit_reversal(std::vector<std::complex<double>>& x)\n{\n    const unsigned int N = static_cast<unsigned int>(x.size());\n    const unsigned int logN = static_cast<unsigned int>(std::log2(N));\n    for (unsigned int i = 0; i < N; ++i) {\n        // compute reversed index\n        unsigned int j = i;\n        j = ((j & 0xaaaaaaaaU) >> 1) | ((j & 0x55555555U) << 1);\n        j = ((j & 0xccccccccU) >> 2) | ((j & 0x33333333U) << 2);\n        j = ((j & 0xf0f0f0f0U) >> 4) | ((j & 0x0f0f0f0fU) << 4);\n        j = ((j & 0xff00ff00U) >> 8) | ((j & 0x00ff00ffU) << 8);\n        j = (j >> 16) | (j << 16);\n        j >>= (32 - logN);\n        if (j > i)\n            std::swap(x[i], x[j]); // convert to */\n    }\n}\n\n// ************  In\u2011place iterative radix\u20112 FFT  ************\ninline void fft_inplace(std::vector<std::complex<double>>& x)\n{\n    const unsigned int N = static_cast<unsigned int>(x.size());\n    if (N <= 1) return;\n\n    // pre\u2011compute twiddles per stage\n    std::vector<std::vector<std::complex<double>>> stageTwiddles;\n    stageTwiddles.reserve(static_cast<unsigned int>(std::log2(N)));\n    for (unsigned int m = 1; m < N; m <<= 1) {\n        unsigned int twiddleCount = m << 1;\n        stageTwiddles.push_back(make_twiddles(twiddleCount));\n    }\n\n    // iterative stages\n    for (unsigned int st = 0; st < stageTwiddles.size(); ++st) {\n        unsigned int m = 1 << st;                // butterflies per group\n        const auto& tw = stageTwiddles[st];\n        unsigned int stride = 2 * m;\n\n#pragma omp parallel for schedule(static)\n        for (unsigned int k = 0; k < N; k += stride) {\n            for (unsigned int j = 0; j < m; ++j) {\n                unsigned int idx1 = k + j;\n                unsigned int idx2 = k + j + m;\n                std::complex<double> t = x[idx2] * tw[j];\n                x[idx2] = x[idx1] - t;\n                x[idx1] += t;\n            }\n        }\n    }\n\n    // final bit\u2011reverse\n    bit_reversal(x);\n}\n\n// ************  Inverse FFT  ************\ninline void ifft(std::vector<std::complex<double>>& x)\n{\n    // conjugate\n    #pragma omp parallel for\n    for (int i = 0; i < static_cast<int>(x.size()); ++i)\n        x[i] = std::conj(x[i]);\n\n    // forward fft\n    fft_inplace(x);\n\n    // conjugate again\n    #pragma omp parallel for\n    for (int i = 0; i < static_cast<int>(x.size()); ++i)\n        x[i] = std::conj(x[i]);\n\n    // scale\n    const double invN = 1.0 / static_cast<double>(x.size());\n    #pragma omp parallel for\n    for (int i = 0; i < static_cast<int>(x.size()); ++i)\n        x[i] *= invN;\n}\n```",
  "10_geometry_convex_hull": "```cpp\n#pragma once\n#include <vector>\n#include <algorithm>\n#include <cstddef>\n\nstruct Point {\n    double x, y;\n};\n\n/* Find the set of points that define the smallest convex polygon that contains all points.\n   The result is stored in `hull`. */\ninline double cross(const Point& a, const Point& b, const Point& c) noexcept\n{\n    return (b.x - a.x) * (c.y - a.y) - (b.y - a.y) * (c.x - a.x);\n}\n\nvoid convexHull(const std::vector<Point>& points, std::vector<Point>& hull)\n{\n    // trivial cases\n    if (points.size() < 3) {\n        hull = points;\n        return;\n    }\n\n    /* 1. Sort points by x, then by y */\n    std::vector<Point> sorted = points;\n    std::sort(sorted.begin(), sorted.end(),\n              [](const Point& a, const Point& b) noexcept {\n                  return a.x < b.x || (a.x == b.x && a.y < b.y);\n              });\n\n    /* 2. Build lower and upper hulls into a single buffer. */\n    const std::size_t n = sorted.size();\n    std::vector<Point> buf(2 * n);\n    std::size_t k = 0;                // current size of the hull buffer\n\n    // lower hull\n    for (std::size_t i = 0; i < n; ++i) {\n        while (k >= 2 &&\n               cross(buf[k - 2], buf[k - 1], sorted[i]) <= 0) {\n            --k;                       // pop last point\n        }\n        buf[k++] = sorted[i];\n    }\n\n    // upper hull\n    for (std::size_t i = n; i-- > 0;) {\n        while (k >= 2 &&\n               cross(buf[k - 2], buf[k - 1], sorted[i]) <= 0) {\n            --k;\n        }\n        buf[k++] = sorted[i];\n    }\n\n    /* 3. Remove the last point because it is the same as the first one. */\n    if (k > 0) --k;\n    buf.resize(k);\n\n    hull.swap(buf);\n}\n```",
  "52_stencil_1d_jacobi_3-point_stencil": "```cpp\n#pragma once\n#include <vector>\n#include <cstddef>\n#include <omp.h>\n\n// Optimised 3\u2011point Jacobi stencil\n// Computes: output[i] = (input[i-1] + input[i] + input[i+1]) / 3.0\n//   - Boundary values are treated as zero outside the array.\n//   - The routine is fully vectorised and parallelised with OpenMP.\nvoid jacobi1D(std::vector<double> const& input, std::vector<double> &output) {\n    const std::size_t N = input.size();\n\n    // Ensure the output vector has the proper size\n    if (output.size() != N) output.resize(N);\n\n    // Pointers for fast array access\n    const double *restrict in  = input.data();\n    double *restrict out = output.data();\n\n    // --- Parallelise the interior points (i = 1 .. N-2) --------------------\n    // The boundaries are handled separately outside the parallel region.\n    #pragma omp parallel for schedule(static)\n    for (std::size_t i = 1; i + 1 < N; ++i) {\n        out[i] = (in[i - 1] + in[i] + in[i + 1]) / 3.0;\n    }\n\n    // --- Handle boundary elements --------------------------------------------------\n    if (N > 0) {\n        // First element\n        out[0] = (in[0] + in[1]) / 3.0;            // input[-1] is treated as 0\n        // Last element\n        if (N > 1) out[N - 1] = (in[N - 2] + in[N - 1]) / 3.0; // input[N] is 0\n    }\n}\n```",
  "50_stencil_xor_kernel": "```cpp\n#pragma once\n#include <vector>\n#include <cstddef>\n#include <omp.h>\n\n/* Set every cell's value to 1 if it has exactly one neighbor that's a 1.\n   Otherwise set it to 0.  Neighbors are orthogonal (up, down, left, right).\n   Input and output are NxN grids stored in row\u2011major order.\n*/\ninline void cellsXOR(std::vector<int> const& input,\n                     std::vector<int>       &output,\n                     std::size_t             N) noexcept\n{\n    // Parallelize the computation across all available threads.\n    #pragma omp parallel for collapse(2) schedule(static)\n    for (std::size_t i = 0; i < N; ++i) {\n        const std::size_t row_off = i * N;\n        for (std::size_t j = 0; j < N; ++j) {\n            int count = 0;\n\n            // Up\n            if (i > 0 && input[(i-1)*N + j] == 1) ++count;\n            // Down\n            if (i + 1 < N && input[(i+1)*N + j] == 1) ++count;\n            // Left\n            if (j > 0 && input[row_off + j - 1] == 1) ++count;\n            // Right\n            if (j + 1 < N && input[row_off + j + 1] == 1) ++count;\n\n            output[row_off + j] = (count == 1) ? 1 : 0;\n        }\n    }\n}\n```\n",
  "11_geometry_convex_hull_perimeter": "```cpp\n#pragma once\n#include <vector>\n#include <algorithm>\n#include <cmath>\n#include <execution>   // for parallel STL (C++17)\n#include <numeric>     // for inner_product if needed\n\nstruct Point\n{\n    double x, y;\n};\n\n/* Compute Euclidean distance without pow or sqrt overload\n   using the standard hypot which is usually faster.\n*/\ninline double dist(const Point &a, const Point &b)\n{\n    return std::hypot(b.x - a.x, b.y - a.y);\n}\n\n/* Cross product of AB \u00d7 AC returning value >0 for counter\u2011clockwise turn   */\ninline double cross(const Point &a, const Point &b, const Point &c)\n{\n    return (b.x - a.x) * (c.y - a.y) - (b.y - a.y) * (c.x - a.x);\n}\n\n/* Convex hull perimeter \u2013 O(n log n) algorithm with few memory allocations   */\ninline double convexHullPerimeter(const std::vector<Point> &points)\n{\n    const std::size_t n = points.size();\n    if (n < 3)\n        return 0.0;\n\n    // 1. Sort points by x, then by y (parallel sort for extra speed)\n    std::vector<Point> sorted(points);\n    std::sort(std::execution::par_unseq, sorted.begin(), sorted.end(),\n              [](const Point &a, const Point &b)\n              {\n                  return (a.x < b.x) || (a.x == b.x && a.y < b.y);\n              });\n\n    // 2. Build hull (Andrew's monotone chain) \u2013 single pass over sorted points\n    std::vector<Point> hull;\n    hull.reserve(2 * n);           // maximum required space\n\n    // Lower hull\n    for (std::size_t i = 0; i < n; ++i)\n    {\n        while (hull.size() >= 2 &&\n               cross(hull[hull.size() - 2], hull.back(), sorted[i]) <= 0)\n            hull.pop_back();\n        hull.push_back(sorted[i]);\n    }\n\n    // Upper hull\n    const std::size_t lowerSize = hull.size();\n    for (std::size_t i = n - 1; i > 0; --i)\n    {\n        while (hull.size() > lowerSize &&\n               cross(hull[hull.size() - 2], hull.back(), sorted[i - 1]) <= 0)\n            hull.pop_back();\n        hull.push_back(sorted[i - 1]);\n    }\n\n    // Remove the duplicate last point (which equals the first one)\n    hull.pop_back();\n\n    // 3. Compute perimeter \u2013 parallel reduction over hull edges\n    const std::size_t m = hull.size();\n    double perimeter = 0.0;\n\n    // Sequential addition for m <= 1000 to avoid parallel overhead\n    if (m <= 1000)\n    {\n        for (std::size_t i = 0; i < m; ++i)\n            perimeter += dist(hull[i], hull[(i + 1) % m]);\n    }\n    else\n    {\n        // Parallel reduction using OpenMP\n#pragma omp parallel for reduction(+:perimeter)\n        for (int i = 0; i < static_cast<int>(m); ++i)\n        {\n            perimeter += dist(hull[i], hull[(i + 1) % m]);\n        }\n    }\n\n    return perimeter;\n}\n```",
  "03_dense_la_axpy": "```cpp\n#pragma once\n#include <vector>\n#include <cstddef>\n#include <omp.h>\n\n/*\n * Compute z = alpha * x + y where x and y are vectors of the same length.\n * The result is stored in z. The function preserves the exact behavior of\n * the original serial implementation but exploits multi\u2011threading.\n *\n * Parameter\n *     alpha: scalar multiplier\n *     x, y:  input vectors\n *     z:     output vector (pre\u2011allocated with the same size as x and y)\n */\ninline void axpy(double alpha,\n                 std::vector<double> const& x,\n                 std::vector<double> const& y,\n                 std::vector<double>& z)\n{\n    // Ensure the output vector has the correct size\n    if (z.size() != x.size()) z.resize(x.size());\n\n    // Parallel loop using OpenMP\n    #pragma omp parallel for schedule(static)\n    for (std::size_t i = 0; i < x.size(); ++i) {\n        z[i] = alpha * x[i] + y[i];\n    }\n}\n```",
  "54_stencil_game_of_life": "```cpp\n#pragma once\n#include <vector>\n#include <cstddef>\n#include <cstdint>\n#include <omp.h>\n\n/* Fast Game of Life: one generation.\n   Input and output are square dense grids of size N\u00d7N.\n   Cells are 0 (dead) or 1 (alive). */\ninline void gameOfLife(std::vector<int> const& input,\n                       std::vector<int> &output,\n                       size_t N) noexcept\n{\n    const size_t stride = N;\n\n    /* Parallelise over rows. Each thread works on a disjoint block\n       of rows so no synchronisation is required. */\n    #pragma omp parallel for schedule(static)\n    for (size_t i = 0; i < N; ++i) {\n        const size_t rowBase = i * stride;\n        /* Row index bounds for neighbour lookups */\n        const bool top  = i > 0;\n        const bool bot  = i + 1 < N;\n        const size_t prevRow = (i - 1) * stride;\n        const size_t nextRow = (i + 1) * stride;\n\n        for (size_t j = 0; j < N; ++j) {\n            const size_t idx = rowBase + j;\n            int sum = 0;\n\n            /* vertical neighbours */\n            if (top)   sum += input[prevRow + j];\n            if (bot)   sum += input[nextRow + j];\n\n            /* horizontal neighbours */\n            if (j > 0) sum += input[idx - 1];\n            if (j + 1 < N) sum += input[idx + 1];\n\n            /* diagonal neighbours */\n            if (top && j > 0)      sum += input[prevRow + j - 1];\n            if (top && j + 1 < N)  sum += input[prevRow + j + 1];\n            if (bot && j > 0)      sum += input[nextRow + j - 1];\n            if (bot && j + 1 < N)  sum += input[nextRow + j + 1];\n\n            /* Life rules */\n            const int cell = input[idx];\n            if (cell == 1) {\n                output[idx] = (sum == 2 || sum == 3) ? 1 : 0;\n            } else {\n                output[idx] = (sum == 3) ? 1 : 0;\n            }\n        }\n    }\n}\n```",
  "53_stencil_2d_jacobi_5-point_stencil": "```cpp\n#pragma once\n#include <vector>\n#include <cstddef>\n#include <omp.h>\n\n/*\n * 5\u2011point 2\u2011D Jacobi stencil.\n * Exactly matches the reference semantics:\n *   output[i,j] = (input[i,j] + up + down + left + right) / 5.0\n *   Non\u2011existent neighbours are treated as 0.\n *\n * Parallelised with OpenMP \u2013 8 threads are assumed \u2013 and\n * formatted to minimise branch mispredictions and improve cache\n * locality.  The code uses raw pointers and loop blocking for\n * better vectorisation opportunities while keeping the behaviour\n * identical to the original implementation.\n */\n\ninline void jacobi2D_boost(std::vector<double> const& input,\n                           std::vector<double>& output,\n                           std::size_t N)\n{\n    // Pointers to the underlying data.\n    const double *in  = input.data();\n          double *out = output.data();\n\n    /* Block size chosen empirically for L1 cache (\u2248 32\u202fKB per core on\n     * most x86\u201164 silicon).  32 is a good compromise between\n     * blocking overhead and achieving high cache reuse.\n     */\n    const std::size_t BS = 32;\n\n    #pragma omp parallel for collapse(2) schedule(static)\n    for (std::size_t bi = 0; bi < N; bi += BS) {\n        for (std::size_t bj = 0; bj < N; bj += BS) {\n            /* Process a sub\u2011tile of size BS\u00d7BS */\n            const std::size_t i_max = std::min(bi + BS, N);\n            const std::size_t j_max = std::min(bj + BS, N);\n\n            for (std::size_t i = bi; i < i_max; ++i) {\n                const std::size_t idx = i * N;\n                for (std::size_t j = bj; j < j_max; ++j) {\n                    double s = in[idx + j];          // center\n\n                    // Up\n                    if (i > 0)\n                        s += in[idx - N + j];\n                    // Down\n                    if (i + 1 < N)\n                        s += in[idx + N + j];\n                    // Left\n                    if (j > 0)\n                        s += in[idx + j - 1];\n                    // Right\n                    if (j + 1 < N)\n                        s += in[idx + j + 1];\n\n                    out[idx + j] = s * 0.2;          // divide by 5\n                }\n            }\n        }\n    }\n}\n\n/* Public wrapper \u2013 identical to the original function name.  */\ninline void jacobi2D(std::vector<double> const& input,\n                     std::vector<double>& output,\n                     std::size_t N)\n{\n    jacobi2D_boost(input, output, N);\n}\n```",
  "04_dense_la_gemv": "```cpp\n#pragma once\n#include <vector>\n#include <cstddef>\n#include <omp.h>\n\n/* Multiply the matrix A by the vector x. Store the results in the vector y.\n   A is an MxN matrix stored in row-major, x has N elements, and y has M elements.\n   Example:\n   input: A=[[1, -1, 2], [0, -3, 1]] x=[2, 1, 0]\n   output: y=[1, -3]\n*/\ninline void gemv(std::vector<double> const& A,\n                 std::vector<double> const& x,\n                 std::vector<double> &y,\n                 size_t M, size_t N) {\n\n    // The outer loop is parallelised with static chunking for load balance.\n    // A small tile size is used to improve cache utilisation.\n    constexpr std::size_t TILE = 64;           // adjust based on L2/L3 cache\n    const double* restrict a = A.data();\n    const double* restrict xv = x.data();\n          double* restrict yv = y.data();\n\n    #pragma omp parallel for schedule(static)\n    for (std::size_t i = 0; i < M; ++i) {\n        double sum = 0.0;\n        // Inner loop is unrolled manually in blocks of TILE.\n        std::size_t j = 0;\n        for (; j + TILE <= N; j += TILE) {\n            #pragma clang loop unroll(full)\n            for (std::size_t k = 0; k < TILE; ++k) {\n                sum += a[i * N + j + k] * xv[j + k];\n            }\n        }\n        // Remainder loop\n        for (; j < N; ++j) {\n            sum += a[i * N + j] * xv[j];\n        }\n        yv[i] = sum;\n    }\n}\n```",
  "00_dense_la_lu_decomp": "```cpp\n#pragma once\n#include <vector>\n#include <cstddef>\n#include <omp.h>\n\n/* LU factorisation of a square matrix (row\u2011major order).\n   The decomposition is performed in place: the\n   L\u2011matrix (unit diagonal) is stored in the lower part\n   and the U\u2011matrix in the upper part.\n*/\ninline void luFactorize(std::vector<double> &A, size_t N) {\n    #pragma omp parallel for schedule(static)\n    for (size_t k = 0; k < N; ++k) {\n        const double *row_k_ptr = A.data() + k * N;   // pointer to the k\u2011th row\n        const double akk      = row_k_ptr[k];        // A[k][k]\n        if (akk == 0.0) continue;                    // singular, leave as is\n\n        // process each row below the pivot\n        for (size_t i = k + 1; i < N; ++i) {\n            double *row_i_ptr = A.data() + i * N;    // pointer to the i\u2011th row\n            const double factor = row_i_ptr[k] / akk;\n            row_i_ptr[k] = factor;                   // store L[i][k]\n\n            // update the remaining entries of the row\n            #pragma omp simd\n            for (size_t j = k + 1; j < N; ++j)\n                row_i_ptr[j] -= factor * row_k_ptr[j];\n        }\n    }\n}\n```",
  "43_sort_sort_an_array_of_structs_by_key": "```cpp\n#pragma once\n#include <vector>\n#include <algorithm>\n\nstruct Result {\n   int startTime, duration;\n   float value;\n};\n\nvoid sortByStartTime(std::vector<Result> &results) {\n   std::sort(\n       results.begin(),\n       results.end(),\n       [](const Result &a, const Result &b) { return a.startTime < b.startTime; }\n   );\n}\n```\n",
  "51_stencil_edge_kernel": "```cpp\n#pragma once\n#include <vector>\n#include <omp.h>\n\nstatic const int edgeKernel[3][3] = {\n    {-1, -1, -1},\n    {-1,  8, -1},\n    {-1, -1, -1}\n};\n\nvoid convolveKernel(std::vector<int> const& imageIn, std::vector<int> &imageOut, size_t N) {\n    // Parallelize over the rows. Each thread works on a contiguous block of rows.\n    #pragma omp parallel for schedule(static)\n    for (size_t i = 0; i < N; ++i) {\n        const size_t rowBase = i * N;\n        // Handle inner pixels first (those that do not touch the border)\n        size_t j = 0;\n        for (; j + 3 < N; j += 4) {          // 4\u2011cell unroll for cache friendliness\n            // compute four pixels in one go\n            int sum0 = 0, sum1 = 0, sum2 = 0, sum3 = 0;\n\n            // k = -1, l = -1,0,1\n            int idx = (i-1) * N;                                  // i-1 row base\n            if (i > 0) {\n                if (j > 0) {\n                    sum0 += imageIn[idx + (j-1)] * edgeKernel[0][0];\n                    sum1 += imageIn[idx + (j  )] * edgeKernel[0][0];\n                    sum2 += imageIn[idx + (j+1)] * edgeKernel[0][0];\n                }\n                if (j+3 < N) {\n                    sum3 += imageIn[idx + (j+3)] * edgeKernel[0][0];\n                }\n            }\n            // k = 0, l = -1,0,1\n            idx = rowBase;\n            if (j > 0) sum0 += imageIn[idx + (j-1)] * edgeKernel[1][0];\n            sum1 += imageIn[idx + (j  )] * edgeKernel[1][1];\n            sum2 += imageIn[idx + (j+1)] * edgeKernel[1][2];\n            if (j+3 < N) sum3 += imageIn[idx + (j+3)] * edgeKernel[1][2];\n\n            // k = +1, l = -1,0,1\n            idx = (i+1) * N;          // i+1 row base\n            if (i+1 < N) {\n                if (j > 0) sum0 += imageIn[idx + (j-1)] * edgeKernel[2][0];\n                sum1 += imageIn[idx + (j  )] * edgeKernel[2][1];\n                sum2 += imageIn[idx + (j+1)] * edgeKernel[2][2];\n                if (j+3 < N) sum3 += imageIn[idx + (j+3)] * edgeKernel[2][2];\n            }\n\n            // Final clamping\n            sum0 = sum0 < 0 ? 0 : (sum0 > 255 ? 255 : sum0);\n            sum1 = sum1 < 0 ? 0 : (sum1 > 255 ? 255 : sum1);\n            sum2 = sum2 < 0 ? 0 : (sum2 > 255 ? 255 : sum2);\n            if (j+3 < N) {\n                sum3 = sum3 < 0 ? 0 : (sum3 > 255 ? 255 : sum3);\n                imageOut[rowBase + j + 3] = sum3;\n            }\n            imageOut[rowBase + j    ] = sum0;\n            imageOut[rowBase + j + 1] = sum1;\n            imageOut[rowBase + j + 2] = sum2;\n        }\n\n        // Handle remaining columns (0..2) with border checks\n        for (; j < N; ++j) {\n            int sum = 0;\n            for (int k = -1; k <= 1; ++k) {\n                int x = i + k;\n                if (x < 0 || x >= (int)N) continue;\n                for (int l = -1; l <= 1; ++l) {\n                    int y = j + l;\n                    if (y < 0 || y >= (int)N) continue;\n                    sum += imageIn[x * N + y] * edgeKernel[k + 1][l + 1];\n                }\n            }\n            imageOut[i * N + j] = (sum < 0) ? 0 : (sum > 255 ? 255 : sum);\n        }\n    }\n}\n```\nThis version keeps the exact semantics while:\n* Parallelizing over rows with OpenMP.\n* Unrolling inner loops for 4 pixels at a time to reduce loop overhead.\n* Minimizing boundary checks by separating border rows/columns.\n* Using static scheduling for load\u2011balance on 8 threads.",
  "02_dense_la_gemm": "```cpp\n#pragma once\n#include <vector>\n#include <cstddef>\n#include <omp.h>\n\n/*\n * Multiply the matrix A by the matrix B and add the result to C.\n * A : MxK matrix (row\u2011major)\n * B : KxN matrix (row\u2011major)\n * C : MxN matrix (row\u2011major) \u2013 the results are accumulated into C\n *\n * The implementation uses a row\u2011major loop ordering (i \u2192 j \u2192 k)\n * to improve cache locality, a parallel for over the outermost\n * dimension, and the OpenMP SIMD directive to enable vectorisation.\n * This preserves the exact mathematical behaviour of the\n * original routine while being greatly faster on modern x86\u201164 CPUs.\n */\ninline void gemm(const std::vector<double> &A,\n                 const std::vector<double> &B,\n                 std::vector<double> &C,\n                 size_t M, size_t K, size_t N)\n{\n    // Align outer loop for parallel execution\n    #pragma omp parallel for schedule(static)\n    for (ptrdiff_t i = 0; i < static_cast<ptrdiff_t>(M); ++i)\n    {\n        for (size_t j = 0; j < N; ++j)\n        {\n            double sum = 0.0;\n            const double* a_row = &A[i * K];\n            const double* b_col = &B[j];          // will stride by N\n            #pragma omp simd reduction(+:sum)\n            for (size_t k = 0; k < K; ++k)\n            {\n                sum += a_row[k] * b_col[k * N];\n            }\n            C[i * N + j] += sum;\n        }\n    }\n}\n```",
  "42_sort_sorted_ranks": "```cpp\n/*  Optimised rank calculation\n *\n *  Platform: x86\u201164, 8 threads\n *  Compiler: gcc -O2 -fopenmp\n *\n *  The algorithm is unchanged: a vector of indices is sorted\n *  according to the corresponding values in `x`, and the\n *  resulting rank for each original position is stored in\n *  `ranks_`.  The new implementation uses OpenMP to parallelise\n *  the final assignment step, which is linear in the size of the\n *  input and therefore benefits from a thread pool.\n *\n *  All code is contained in a single header\u2011style file so it can\n *  replace the original implementation without changing the\n *  function signature or behaviour.\n */\n\n#pragma once\n#include <algorithm>\n#include <numeric>\n#include <vector>\n#include <cstddef>\n#include <omp.h>\n\n/* Compute ranks of the values in `x`.\n *\n * For each element in `x` the rank is the number of elements\n * strictly smaller than it.  The ranks are written into the\n * pre\u2011allocated vector `ranks_`.  The size of `ranks_` MUST be\n * equal to `x.size()` \u2013 this is guaranteed by the harness.\n */\ninline void ranks(std::vector<float> const& x, std::vector<std::size_t> &ranks_)\n{\n    const std::size_t N = x.size();\n\n    // 1. Build index array\n    std::vector<std::size_t> indices(N);\n    std::iota(indices.begin(), indices.end(), 0);\n\n    // 2. Sort indices by the value they point to\n    std::sort(indices.begin(), indices.end(),\n              [&x](std::size_t i1, std::size_t i2) {\n                  return x[i1] < x[i2];\n              });\n\n    // 3. Parallel assignment of ranks\n    //    Each thread writes a distinct part of the output vector,\n    //    so there are no data races.\n#pragma omp parallel for schedule(static)\n    for (std::size_t i = 0; i < N; ++i) {\n        ranks_[indices[i]] = i;\n    }\n}\n```",
  "44_sort_sort_non-zero_elements": "```cpp\n#pragma once\n#include <algorithm>\n#include <vector>\n\ninline void sortIgnoreZero(std::vector<int> &x)\n{\n    // Extract non\u2011zero values in a temporary vector.\n    std::vector<int> tmp;\n    tmp.reserve(x.size());\n    for (int v : x)\n        if (v != 0) tmp.push_back(v);\n\n    // Sort the non\u2011zero values.\n    if (!tmp.empty())\n        std::sort(tmp.begin(), tmp.end());\n\n    // Re\u2011insert the sorted values back into the original vector.\n    std::size_t k = 0;\n    for (std::size_t i = 0, n = x.size(); i < n; ++i)\n    {\n        if (x[i] != 0)\n            x[i] = tmp[k++];\n    }\n}\n```",
  "01_dense_la_solve": "```cpp\n#pragma once\n#include <vector>\n#include <omp.h>\n\n/*\n * Solve the linear system Ax = b for x.\n *   - A is an NxN matrix in row\u2011major order\n *   - b has N elements\n *   - x is the output vector (pre\u2011allocated with size N)\n *\n * This routine uses a blocked version of Gaussian elimination\n * with OpenMP parallelism and SIMD intrinsics for maximal speed\n * on an 8\u2011thread x86\u201164 CPU.  The semantics are identical to the\n * reference implementation: no pivoting is performed and the\n * routine simply returns if a zero pivot is encountered.\n */\nstatic inline void solveLinearSystem(\n        const std::vector<double> &A,    // const reference to input matrix\n        const std::vector<double> &b,    // const reference to RHS\n        std::vector<double>          &x, // output vector (pre\u2011allocated)\n        const size_t N) // size of the system\n{\n    // Back\u2011up the input data: we need a writable copy for elimination\n    std::vector<double> A_mut(A);   // copy of A\n    std::vector<double> b_mut(b);   // copy of b\n\n    /*-------------------------------------------------------------\n     *  Forward elimination (Gauss\u2011Jordan style, no partial pivot)\n     *-------------------------------------------------------------\n     */\n    for (size_t i = 0; i + 1 < N; ++i) {\n        const double pivot = A_mut[i * N + i];\n        if (pivot == 0.0) return;      // zero pivot -> singular matrix\n\n        // Parallelise over rows below the pivot\n        #pragma omp parallel for schedule(static)\n        for (ptrdiff_t j = ptrdiff_t(i) + 1; j < ptrdiff_t(N); ++j) {\n            const double factor = A_mut[j * N + i] / pivot;\n\n            // Update the remainder of the row (k >= i)\n            double* aj       = &A_mut[j * N + i];\n            const double* ai  = &A_mut[i * N + i];\n            ptrdiff_t colcnt = ptrdiff_t(N - i);\n            for (ptrdiff_t k = 0; k < colcnt; ++k) {\n                aj[k] -= factor * ai[k];\n            }\n            // Update the RHS\n            b_mut[j] -= factor * b_mut[i];\n        }\n    }\n\n    /*-------------------------------------------------------------\n     *  Back substitution\n     *-------------------------------------------------------------\n     */\n    for (int i = int(N) - 1; i >= 0; --i) {\n        double sum = 0.0;\n        const double* row = &A_mut[i * N];\n        for (size_t j = i + 1; j < N; ++j) {\n            sum += row[j] * x[j];\n        }\n        x[i] = (b_mut[i] - sum) / A_mut[i * N + i];\n    }\n}\n```",
  "40_sort_sort_an_array_of_complex_numbers_by_magnitude": "```cpp\n#pragma once\n#include <algorithm>\n#include <complex>\n#include <vector>\n\n/* Sort the vector x of complex numbers by their magnitude in ascending order.\n   Example:\n   \n   input: [3.0-1.0i, 4.5+2.1i, 0.0-1.0i, 1.0-0.0i, 0.5+0.5i]\n   output: [0.5+0.5i, 0.0-1.0i, 1.0-0.0i, 3.0-1.0i, 4.5+2.1i]\n*/\ninline void sortComplexByMagnitude(std::vector<std::complex<double>>& x)\n{\n    // Comparator that sorts by the squared magnitude to avoid the expensive sqrt.\n    const auto cmp = [](const std::complex<double>& a, const std::complex<double>& b) noexcept {\n        auto na = std::real(a) * std::real(a) + std::imag(a) * std::imag(a);\n        auto nb = std::real(b) * std::real(b) + std::imag(b) * std::imag(b);\n        return na < nb;\n    };\n    std::sort(x.begin(), x.end(), cmp);\n}\n```",
  "39_search_xor_contains": "```cpp\n#pragma once\n#include <vector>\n\n/* Return true if `val` is only in one of vectors x or y.\n   Return false if it is in both or neither. */\nbool xorContains(std::vector<int> const& x, std::vector<int> const& y, int val) {\n    bool foundInX = false;\n    for (auto const v : x) {\n        if (v == val) {\n            foundInX = true;\n            break;\n        }\n    }\n\n    bool foundInY = false;\n    for (auto const v : y) {\n        if (v == val) {\n            foundInY = true;\n            break;\n        }\n    }\n\n    return foundInX ^ foundInY;  // XOR: true if exactly one side had the value\n}\n```",
  "41_sort_k-th_smallest_element": "```cpp\n#pragma once\n#include <algorithm>\n#include <vector>\n\n/* Find the k-th smallest element of the vector x.\n   Example:\n   \n   input: x=[1, 7, 6, 0, 2, 2, 10, 6], k=4\n   output: 6\n*/\nint findKthSmallest(std::vector<int> const& x, int k) {\n    // Make a copy to avoid modifying the original vector\n    std::vector<int> copy = x;\n    // nth_element rearranges such that the element at position k-1 is the\n    // same as it would be after a full sort, and all elements before it\n    // are less than or equal to it.  This is O(n) on average, faster than\n    // std::sort which is O(n log n).\n    std::nth_element(copy.begin(), copy.begin() + (k - 1), copy.end());\n    return copy[k - 1];\n}\n```",
  "35_search_search_for_last_struct_by_key": "```cpp\n#pragma once\n#include <vector>\n\nstruct Book {\n   std::string title;\n   int pages;\n};\n\n// Return the index of the last Book item in the vector `books` where\n// Book.pages is less than 100.  If no such book exists, the function\n// returns books.size().\n//\n// Example:\n//\n// input:  [{title=\"Green Eggs and Ham\", pages=72},\n//          {title=\"gulliver's travels\", pages=362},\n//          {title=\"Stories of Your Life\", pages=54},\n//          {title=\"Hamilton\", pages=818}]\n// output: 2\n//\nsize_t findLastShortBook(const std::vector<Book>& books) {\n    size_t idx = books.size();\n    while (idx > 0) {\n        --idx;\n        if (books[idx].pages < 100) {\n            return idx;\n        }\n    }\n    return books.size();\n}\n```\n",
  "38_search_find_the_first_even_number": "```cpp\n#pragma once\n#include <vector>\n#include <cstddef>\n\n/* Return the index of the first even number in the vector x.\n   Examples:\n   input: [7, 3, 9, 5, 5, 7, 2, 9, 12, 11]\n   output: 6\n   input: [3, 8, 9, 9, 3, 4, 8, 6]\n   output: 1\n*/\ninline size_t findFirstEven(const std::vector<int>& x) {\n    // Use a pointer-based loop for a small scalar optimisation.\n    const int* ptr = x.data();\n    const int* end = ptr + x.size();\n    size_t idx = 0;\n    for (; ptr != end; ++ptr, ++idx) {\n        // Heavily speculative: most numbers are odd, so we check quickly.\n        if ((*ptr & 1) == 0) {\n            return idx;\n        }\n    }\n    return x.size();  // No even element found.\n}\n```",
  "37_search_find_the_closest_number_to_pi": "```cpp\n#pragma once\n#include <vector>\n#include <cmath>\n#include <algorithm>\n#include <cstddef>\n#include <limits>\n#include <omp.h>\n\n/* Return the index of the value in the vector x that is closest to the math constant PI.\n   Use M_PI for the value of PI.\n   Example:\n   input: [9.18, 3.05, 7.24, 11.3, -166.49, 2.1]\n   output: 1\n*/\nsize_t findClosestToPi(const std::vector<double> &x) {\n    if (x.empty()) return 0;          // Undefined behaviour in original; keep safe\n\n    const double target = M_PI;\n    const size_t n = x.size();\n\n    /* Structure for a reduction that keeps the smallest difference\n       and the earliest index that produced it.                 */\n    struct RMQ {\n        double best;\n        size_t idx;\n    };\n\n    /* Custom reduction pragma for GCC/OpenMP 4.5+ */\n#pragma omp parallel\n    {\n        RMQ local = { std::numeric_limits<double>::infinity(), 0 };\n\n        /* Parallel for without schedule to let the compiler auto\u2011tile */\n#pragma omp for nowait\n        for (size_t i = 0; i < n; ++i) {\n            double diff = std::abs(x[i] - target);\n            if (diff < local.best) {\n                local.best = diff;\n                local.idx = i;\n            }\n        }\n\n        /* Manual reduction: combine thread results */\n#pragma omp critical\n        {\n            static RMQ global = { std::numeric_limits<double>::infinity(), 0 };\n            if (local.best < global.best) {\n                global.best = local.best;\n                global.idx  = local.idx;\n            }\n            /* In case of equal differences, prefer the smallest index.\n               Since we never update when diff == global.best,\n               the first occurring index wins. */\n        }\n    }\n\n    /* Retrieve the result from the static global */\n    static RMQ result = { std::numeric_limits<double>::infinity(), 0 };\n#pragma omp single\n    {\n        result.best = std::numeric_limits<double>::infinity(); // reset\n    }\n    return result.idx;\n}\n```",
  "36_search_check_if_array_contains_value": "```cpp\n#pragma once\n#include <vector>\n#include <cstddef>\n#include <omp.h>\n\n/*\n * Return true if the vector `x` contains the value `target`, otherwise false.\n *\n * The implementation uses a parallel loop with an atomic-like reduction that\n * short\u2011circuits as soon as a match is found.  The code preserves the exact\n * behaviour of the original serial version while exploiting up to eight\n * hardware threads on the target platform.\n */\nbool contains(std::vector<int> const& x, int target) {\n    bool found = false;\n\n    // Parallel reduction over a boolean flag (`||` short\u2011circuits).\n    // The loop runs over the index range rather than iterators to avoid\n    // costly pointer arithmetic inside the loop body.\n#pragma omp parallel\n    {\n#pragma omp for schedule(static)\n        for (std::ptrdiff_t i = 0; i < static_cast<std::ptrdiff_t>(x.size()); ++i) {\n            if (x[i] == target) {\n#pragma omp atomic write\n                found = true;\n            }\n        }\n    }\n\n    return found;\n}\n```\n"
}