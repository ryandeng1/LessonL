{
  "15_graph_edge_count": "```cpp\n#pragma once\n#include <vector>\n#include <cstddef>\n\nint edgeCount(std::vector<int> const& A, size_t N) {\n    // Use 64\u2011bit integer for safety against overflow on large graphs\n    long long cnt = 0;\n\n    // Parallelise the outer loop with OpenMP.\n    // The inner loop is trivial, so we use a single reduction variable.\n    #pragma omp parallel for reduction(+:cnt)\n    for (size_t i = 0; i < N; ++i) {\n        const int* row = &A[i * N];        // Pointer to the beginning of row i\n        for (size_t j = 0; j < N; ++j) {\n            // The matrix is assumed to contain only 0 or 1\n            cnt += row[j];\n        }\n    }\n\n    // Result is guaranteed to fit in int by problem statement\n    return static_cast<int>(cnt);\n}\n```",
  "17_graph_highest_degree": "```cpp\n#pragma once\n#include <vector>\n#include <algorithm>\n#include <cstddef>\n#include <omp.h>\n\n/* Compute the highest node degree in the undirected graph.\n   A is an N\u00d7N adjacency matrix stored in row-major order.\n*/\ninline int maxDegree(std::vector<int> const& A, std::size_t N) {\n    // Fast path for an empty graph\n    if (N == 0) return 0;\n\n    int global_max = 0;\n    const int* data = A.data();\n\n    // Parallel reduction over rows\n    #pragma omp parallel\n    {\n        int local_max = 0;\n        #pragma omp for nowait\n        for (std::size_t i = 0; i < N; ++i) {\n            int sum = 0;\n            const int* row = data + i * N;\n            // Unroll the inner loop for a little more speed\n            std::size_t j = 0;\n            for (; j + 7 < N; j += 8) {\n                sum += row[j] + row[j+1] + row[j+2] + row[j+3] +\n                       row[j+4] + row[j+5] + row[j+6] + row[j+7];\n            }\n            for (; j < N; ++j) sum += row[j];\n            if (sum > local_max) local_max = sum;\n        }\n        #pragma omp atomic\n        if (local_max > global_max) global_max = local_max;\n    }\n    return global_max;\n}\n```",
  "16_graph_largest_component": "```cpp\n#pragma once\n#include <vector>\n#include <algorithm>\n#include <stack>\n\n/* Return the number of vertices in the largest component of the graph defined by the adjacency matrix A.\n   A is an NxN adjacency matrix stored in row\u2011major.\n   Example:\n        input: [[0, 1, 0, 0], [1, 0, 0, 0], [0, 0, 0, 1], [0, 0, 1, 0]]\n   output: 2\n*/\nint largestComponent(std::vector<int> const& A, size_t N) {\n    // Use a small, fast bool type\n    std::vector<char> visited(N, 0);\n    int maxCount = 0;\n    std::stack<int> st;\n\n    for (int start = 0; start < static_cast<int>(N); ++start) {\n        if (visited[start]) continue;\n\n        int count = 0;\n        st.push(start);\n        visited[start] = 1;\n\n        while (!st.empty()) {\n            int node = st.top();\n            st.pop();\n            ++count;\n\n            const int* row = &A[node * N];\n            for (int i = 0; i < static_cast<int>(N); ++i) {\n                if (row[i] && !visited[i]) {\n                    visited[i] = 1;\n                    st.push(i);\n                }\n            }\n        }\n        maxCount = std::max(maxCount, count);\n    }\n    return maxCount;\n}\n```",
  "18_graph_count_components": "```cpp\n#pragma once\n#include <vector>\n#include <stack>\n#include <cstdint>\n#include <cstring>\n\n/*\n * Count the number of connected components in the undirected graph described by\n * the adjacency matrix A (row\u2011major, NxN).  The implementation is fully\n * equivalent to the recursive DFS version but uses an explicit stack and a\n * byte\u2011packed visited flag vector to reduce recursion overhead and improve\n * cache locality.\n */\nstatic inline void dfs_iterative(const std::vector<int> &A,\n                                 size_t N,\n                                 std::vector<std::uint8_t> &visited,\n                                 int start)\n{\n    std::stack<int> st;\n    st.push(start);\n    visited[start] = 1;\n\n    while (!st.empty()) {\n        int node = st.top();\n        st.pop();\n\n        const int *row = A.data() + node * N;\n        for (size_t i = 0; i < N; ++i) {\n            if (row[i] == 1 && !visited[i]) {\n                visited[i] = 1;\n                st.push(static_cast<int>(i));\n            }\n        }\n    }\n}\n\ninline int componentCount(const std::vector<int> &A, size_t N)\n{\n    std::vector<std::uint8_t> visited(N, 0);\n    int count = 0;\n\n    for (int i = 0; i < static_cast<int>(N); ++i) {\n        if (!visited[i]) {\n            dfs_iterative(A, N, visited, i);\n            ++count;\n        }\n    }\n    return count;\n}\n```\n",
  "19_graph_shortest_path": "```cpp\n#pragma once\n#include <vector>\n#include <deque>\n#include <limits>\n#include <cstddef>\n\nint shortestPathLength(std::vector<int> const& A, size_t N, int source, int dest) {\n    // Straightforward BFS on the adjacency matrix.\n    // Optimization ideas:\n    //   * Use a fixed-size boolean array instead of std::vector<bool>.\n    //   * Avoid the cost of std::pair by using a lightweight struct.\n    //   * Use std::deque for O(1) pop_front/push_back.\n    //   * In the inner loop, use a raw pointer to the adjacency row for faster\n    //     access and enable SIMD via the compiler.\n    // The algorithm preserves the exact semantics of the original code.\n\n    struct Node { int v; int d; };\n    std::deque<Node> q;\n    q.push_back({source, 0});\n\n    // Small static array for visited flags; size determined at run time.\n    std::vector<char> visited(N, 0);\n    visited[source] = 1;\n\n    while (!q.empty()) {\n        Node cur = q.front();\n        q.pop_front();\n\n        if (cur.v == dest) {\n            return cur.d;\n        }\n\n        const int* row = &A[cur.v * N];\n        for (size_t i = 0; i < N; ++i) {\n            // SIMD-friendly test: check adjacency and unvisited flag\n            if (row[i] && !visited[i]) {\n                visited[i] = 1;\n                q.push_back({static_cast<int>(i), cur.d + 1});\n            }\n        }\n    }\n    return std::numeric_limits<int>::max();\n}\n```",
  "20_histogram_pixel_histogram": "```cpp\n#pragma once\n#include <vector>\n#include <array>\n#include <cstddef>\n#include <omp.h>\n\n/* Count the number of pixels in image with each grayscale intensity.\n   The vector `image` is a grayscale image with values 0-255.\n   Store the results in `bins`.\n   Example:\n   \n   input: image=[2, 116, 201, 11, 92, 92, 201, 4, 2]\n   output: [0, 0, 2, 0, 1, ...]\n*/\ninline void pixelCounts(const std::vector<int> &image, std::array<std::size_t, 256> &bins)\n{\n    // Ensure the output array is zeroed\n    bins.fill(0);\n\n    // Use OpenMP if available; fall back to serial if not.\n#ifdef _OPENMP\n    const std::size_t nThreads = omp_get_max_threads();\n\n    // Thread\u2011local histograms to avoid false sharing\n    std::vector<std::array<std::size_t, 256>> localBins(nThreads);\n    for (auto &t : localBins) t.fill(0);\n\n    #pragma omp parallel\n    {\n        const std::size_t tid = omp_get_thread_num();\n        auto &local = localBins[tid];\n\n        // Work partitioning: each thread processes a contiguous chunk\n        #pragma omp for schedule(static)\n        for (std::size_t i = 0; i < image.size(); ++i)\n        {\n            const int v = image[i];\n            // Value is guaranteed to be 0\u2011255, but we still guard against overflow\n            if (v >= 0 && v < 256)\n                ++local[v];\n        }\n    }\n\n    // Reduce local histograms into the global result\n    for (std::size_t t = 0; t < nThreads; ++t)\n        for (std::size_t v = 0; v < 256; ++v)\n            bins[v] += localBins[t][v];\n#else\n    // Fallback to a single\u2011threaded implementation\n    for (std::size_t i = 0; i < image.size(); ++i)\n    {\n        const int v = image[i];\n        if (v >= 0 && v < 256)\n            ++bins[v];\n    }\n#endif\n}\n```",
  "23_histogram_first_letter_counts": "```cpp\n#pragma once\n#include <array>\n#include <string>\n#include <vector>\n\n#if defined(_OPENMP)\n#include <omp.h>\n#endif\n\n/* Count how many strings in `s` start with each lower\u2011case letter.\n   The result is stored in `bins`. This implementation is\n   parallelised with OpenMP for maximum speed on multi\u2011core CPUs. */\nvoid firstLetterCounts(const std::vector<std::string>& s, std::array<size_t, 26>& bins) {\n    // Ensure bins is zeroed before counting\n    bins.fill(0);\n\n    const size_t n = s.size();\n    if (n == 0) return;\n\n#if defined(_OPENMP)\n    // Each thread keeps a private copy of the bins array\n    const int threads = omp_get_max_threads();\n    std::vector<std::array<size_t, 26>> local_bins(threads);\n    for (int t = 0; t < threads; ++t) local_bins[t].fill(0);\n\n    #pragma omp parallel for schedule(static)\n    for (size_t i = 0; i < n; ++i) {\n        int t = omp_get_thread_num();\n        const char c = s[i][0];\n        const int idx = c - 'a';\n        if (0 <= idx && idx < 26) {\n            local_bins[t][idx] += 1;\n        }\n    }\n\n    // Combine per\u2011thread results into the final array\n    for (int t = 0; t < threads; ++t) {\n        for (int j = 0; j < 26; ++j) bins[j] += local_bins[t][j];\n    }\n#else\n    // Fallback to a single\u2011threaded loop when OpenMP is unavailable\n    for (size_t i = 0; i < n; ++i) {\n        const char c = s[i][0];\n        const int idx = c - 'a';\n        if (0 <= idx && idx < 26) {\n            bins[idx] += 1;\n        }\n    }\n#endif\n}\n```\n",
  "34_scan_largest_contiguous_subarray_sum": "```cpp\n#pragma once\n#include <vector>\n#include <limits>\n\n/* Return the largest sum of any contiguous subarray in the vector x.\n   i.e. if x=[\u22122, 1, \u22123, 4, \u22121, 2, 1, \u22125, 4] then [4, \u22121, 2, 1] is the contiguous\n   subarray with the largest sum of 6.\n   Example:\n   input: [\u22122, 1, \u22123, 4, \u22121, 2, 1, \u22125, 4]\n   output: 6\n*/\nint maximumSubarray(std::vector<int> const& x) {\n    int best = std::numeric_limits<int>::lowest();\n    int current = 0;\n    for (int v : x) {\n        current = std::max(v, current + v);\n        best = std::max(best, current);\n    }\n    return best;\n}\n```\n",
  "21_histogram_bin_0-100": "```cpp\n#pragma once\n#include <array>\n#include <vector>\n#include <omp.h>\n\n/// Count how many elements of `x` fall in each decade interval [0,10), [10,20), \u2026.\n/// The vector `x` contains values in the range [0,100) (100 is excluded).\n/// The result is returned in the 10-element array `bins`.\n/// The implementation uses OpenMP parallelism and per\u2011thread local\n/// histograms to avoid atomic operations while preserving exact\n/// equivalence to the original sequential algorithm.\nvoid binsBy10Count(std::vector<double> const& x, std::array<size_t, 10> &bins) {\n    // Initialise output bins to zero\n    bins.fill(0);\n\n    // Number of threads\n    const int nthreads = omp_get_max_threads();\n\n    // Allocate per\u2011thread histograms\n    std::vector<std::array<size_t, 10>> local_bins(nthreads);\n    for (auto &arr : local_bins) arr.fill(0);\n\n    // Parallel loop. Each thread updates its own histogram.\n#pragma omp parallel\n    {\n        const int tid = omp_get_thread_num();\n        auto &thread_bins = local_bins[tid];\n        #pragma omp for nowait\n        for (size_t i = 0; i < x.size(); ++i) {\n            // Convert to int once \u2013 all values are 0\u2011100 inclusive.\n            int bin = static_cast<int>(x[i]) / 10;\n            // The original code assumes bin is in [0,9]; we preserve that.\n            thread_bins[bin] += 1;\n        }\n    }\n\n    // Reduce per\u2011thread histograms into the final result.\n    for (int b = 0; b < 10; ++b) {\n        size_t sum = 0;\n        for (int t = 0; t < nthreads; ++t) sum += local_bins[t][b];\n        bins[b] = sum;\n    }\n}\n```",
  "22_histogram_count_quadrants": "```cpp\n#pragma once\n#include <array>\n#include <vector>\n#include <omp.h>\n\nstruct Point {\n   double x, y;\n};\n\n/* Count the number of cartesian points in each quadrant.\n   * `points` contains a list of Point objects.\n   * Store the counts in `bins`.\n   *\n   * The implementation uses OpenMP for parallel reduction\n   * and a branchless logic for maximum speed.\n   */\nvoid countQuadrants(std::vector<Point> const& points, std::array<size_t, 4> &bins) {\n    // Zero the bins before starting\n    bins = {0, 0, 0, 0};\n\n    // Each thread keeps a private array to avoid contention\n    #pragma omp parallel\n    {\n        size_t local[4] = {0,0,0,0};\n        #pragma omp for schedule(static)\n        for (size_t i = 0; i < points.size(); ++i) {\n            const auto& p = points[i];\n            int ix = (p.x < 0);      // 0 for x>=0, 1 for x<0\n            int iy = (p.y < 0);      // 0 for y>=0, 1 for y<0\n            // quadrant index: (x>=0?0:1)*2 + (y>=0?0:1)\n            int q = ix*2 + iy;\n            ++local[q];\n        }\n        // Combine private counters into the shared bins\n        #pragma omp critical\n        {\n            for (int q = 0; q < 4; ++q) bins[q] += local[q];\n        }\n    }\n}\n```",
  "24_histogram_count_quartile": "```cpp\n#pragma once\n#include <array>\n#include <vector>\n#include <cstddef>\n#include <omp.h>\n\n/* Count the number of doubles in the vector x that have a fractional part \n   in [0, 0.25), [0.25, 0.5), [0.5, 0.75), and [0.75, 1). Store the counts in `bins`.\n   This implementation is fully parallelized using OpenMP and reduces\n   branching as much as possible while preserving the exact same semantics\n   as the original scalar routine.\n*/\ninline void countQuartiles(std::vector<double> const& x, std::array<size_t, 4> &bins) {\n    // Initialise bins to zero\n    bins[0] = bins[1] = bins[2] = bins[3] = 0;\n\n    // Use a per-thread private counter array and merge at the end.\n    const int nthreads = omp_get_max_threads();\n    std::vector<std::array<size_t, 4>> local_counts(nthreads);\n    #pragma omp parallel\n    {\n        const int tid = omp_get_thread_num();\n        auto &cnt = local_counts[tid];\n        cnt[0] = cnt[1] = cnt[2] = cnt[3] = 0;\n\n        #pragma omp for schedule(static)\n        for (std::size_t i = 0; i < x.size(); ++i) {\n            const double val = x[i];\n            const double frac = val - static_cast<double>(static_cast<long long>(val));\n            /* The fractional part is in [0,1).  We use a minimal set of\n               comparisons to determine the quartile.  The thresholds are\n               exact floating\u2011point constants, guaranteeing identical\n               behaviour to the original implementation. */\n            if (frac < 0.25)\n                ++cnt[0];\n            else if (frac < 0.5)\n                ++cnt[1];\n            else if (frac < 0.75)\n                ++cnt[2];\n            else\n                ++cnt[3];\n        }\n    }\n\n    // Reduce per\u2011thread counters into the final result\n    for (int t = 0; t < nthreads; ++t) {\n        bins[0] += local_counts[t][0];\n        bins[1] += local_counts[t][1];\n        bins[2] += local_counts[t][2];\n        bins[3] += local_counts[t][3];\n    }\n}\n```",
  "32_scan_sum_of_prefix_sum_array": "```cpp\n#pragma once\n#include <cstdint>\n#include <vector>\n\n/* Compute the prefix sum array of the vector x and return its sum.\n   Example:\n   input: [-7, 2, 1, 9, 4, 8]\n   output: 15\n*/\nint64_t sumOfPrefixSum(const std::vector<int64_t>& x) {\n    int64_t total = 0;     // final result\n    int64_t running = 0;   // current prefix sum\n\n    for (int64_t v : x) {\n        running += v;     // compute new prefix sum\n        total   += running; // add it to the final sum\n    }\n\n    return total;\n}\n```\n",
  "33_scan_reverse_prefix_sum": "```cpp\n#pragma once\n#include <vector>\n\n/* Compute the reverse prefix sum of the vector `x` into `output`.\n   The resulting `output[i]` equals the sum of the suffix x[i], x[i+1], \u2026, x[n-1].\n   Example:\n     input:  [1, 7, 4, 6, 6, 2]\n     output: [2, 8, 14, 18, 25, 26]\n*/\nvoid reversePrefixSum(const std::vector<int> &x, std::vector<int> &output) {\n    const std::size_t n = x.size();\n    output.resize(n);          // ensure output has correct size\n    int sum = 0;\n    for (std::ptrdiff_t i = static_cast<std::ptrdiff_t>(n) - 1; i >= 0; --i) {\n        sum += x[static_cast<std::size_t>(i)];\n        output[static_cast<std::size_t>(i)] = sum;\n    }\n}\n```",
  "55_transform_relu": "```cpp\n#pragma once\n#include <vector>\n#include <algorithm>\n#include <omp.h>\n\n/*\n * Compute the ReLU function on every element of x.\n * Elements less than zero become zero, while elements greater\n * than zero remain unchanged.\n *\n * Example:\n *   input:  [-1.8, 24.0, 1.2, 0.0, -5.1, -0.2, 4.5]\n *   output: [0, 24.0, 1.2, 0, 0, 0, 4.5]\n */\ninline void relu(std::vector<double> &x) {\n    const std::size_t n = x.size();\n\n    /* OpenMP parallel for with schedule(dynamic) gives good load balance\n       while keeping per\u2011thread memory footprint small. */\n    #pragma omp parallel for schedule(static)\n    for (std::size_t i = 0; i < n; ++i) {\n        double v = x[i];\n        x[i] = v > 0.0 ? v : 0.0;\n    }\n}\n```",
  "59_transform_map_function": "```cpp\n#pragma once\n#include <vector>\n#include <cstddef>\n#include <omp.h>\n\ninline bool isPowerOfTwo(int x) noexcept {\n    return x > 0 && !(x & (x - 1));\n}\n\n/*\n * Apply the isPowerOfTwo function to every value in 'x' and store the results in 'mask'.\n * Example:\n *   input : [8, 0, 9, 7, 15, 64, 3]\n *   output: [true, false, false, false, false, true, false]\n */\ninline void mapPowersOfTwo(const std::vector<int>& x, std::vector<bool>& mask) {\n    const std::size_t N = x.size();\n    // Ensure the mask has the correct size\n    if (mask.size() != N) mask.resize(N);\n\n    // Parallel block with static scheduling to balance work among 8 threads\n    #pragma omp parallel for schedule(static)\n    for (std::ptrdiff_t i = 0; i < static_cast<std::ptrdiff_t>(N); ++i) {\n        mask[i] = isPowerOfTwo(x[i]);\n    }\n}\n```",
  "56_transform_negate_odds": "```cpp\n#pragma once\n#include <vector>\n#include <omp.h>\n\nvoid negateOddsAndHalveEvens(std::vector<int> &x) {\n    const size_t n = x.size();\n    // Parallel loop using OpenMP, each iteration is independent\n    #pragma omp parallel for schedule(static)\n    for (size_t i = 0; i < n; ++i) {\n        const int v = x[i];\n        // Using bit test for oddness is cheaper than %2\n        if (v & 1) {\n            x[i] = -v;            // negate odd\n        } else {\n            x[i] = v / 2;         // halve even (integer division)\n        }\n    }\n}\n```",
  "31_scan_scan_with_min_function": "```cpp\n#pragma once\n#include <vector>\n\n/*\n   Replace the i\u2011th element of the vector `x` with the minimum value from indices 0 through i.\n   Examples:\n     input:  [8, 6, -1, 7, 3, 4, 4]\n     output: [8, 6, -1, -1, -1, -1, -1]\n     input:  [5, 4, 6, 4, 3, 6, 1, 1]\n     output: [5, 4, 4, 4, 3, 3, 1, 1]\n*/\ninline void partialMinimums(std::vector<float> &x) {\n    if (x.empty()) return;\n    float current_min = x[0];\n    for (std::size_t i = 1; i < x.size(); ++i) {\n        if (x[i] < current_min) current_min = x[i];\n        else current_min = current_min;  // keep current_min unchanged\n        x[i] = current_min;\n    }\n}\n```\n",
  "57_transform_inverse_offset": "```cpp\n#pragma once\n#include <vector>\n#include <omp.h>\n\n/*\n * Replace every element of the vector x with 1-1/x.\n * Example:\n *   input: [2, 4, 1, 12, -2]\n *   output: [0.5, 0.75, 0, 0.91666666, 1.5]\n */\nvoid oneMinusInverse(std::vector<double> &x) {\n    const std::size_t n = x.size();\n    if (n == 0) return;\n\n    // Parallel loop: each thread works on a disjoint chunk of the array.\n    #pragma omp parallel for schedule(static)\n    for (std::size_t i = 0; i < n; ++i) {\n        const double val = x[i];\n        x[i] = 1.0 - 1.0 / val;   // exact same behavior as std::transform\n    }\n}\n```",
  "58_transform_squaring": "```cpp\n#pragma once\n#include <vector>\n#include <cstddef>\n#include <omp.h>\n\n/*\n  Replace every element of x with the square of its value.\n  Example:\n  input:  [5, 1, 2, -4, 8]\n  output: [25, 1, 4, 16, 64]\n*/\ninline void squareEach(std::vector<int> &x) {\n    // Parallelise the loop using OpenMP. The array size is large enough\n    // that the parallel overhead is worthwhile, and `O2` optimisation\n    // keeps the multiplication in registers.\n    const std::size_t N = x.size();\n    #pragma omp parallel for schedule(static)\n    for (std::size_t i = 0; i < N; ++i) {\n        const int v = x[i];\n        x[i] = v * v;\n    }\n}\n```",
  "30_scan_prefix_sum": "```cpp\n#pragma once\n#include <vector>\n#include <cstdint>\n#include <algorithm>\n#include <numeric>\n#include <omp.h>\n\n/*\n * Compute the inclusive prefix sum of `x` into `output`.\n *\n * Example:\n *   input : [1, 7, 4, 6, 6, 2]\n *   output: [1, 8, 12, 18, 24, 26]\n *\n * The implementation below performs the calculation in parallel\n * using OpenMP while preserving identical results to the original\n * std::inclusive_scan (aside from potential differences in\n * intermediate register reordering).\n */\ninline void prefixSum(const std::vector<int64_t> &x, std::vector<int64_t> &output)\n{\n    const std::size_t N = x.size();\n    output.resize(N);\n    if (N == 0)\n        return;\n\n    /* Determine a block size that gives roughly equal work per thread.\n     * 1e5 is a good compromise: large enough for cache locality,\n     * small enough to keep the number of blocks moderate. */\n    constexpr std::size_t BLOCK = 100000;\n\n    /* Step 1 \u2013 per\u2011block inclusive scans and store the block\n     * totals in an auxiliary vector `blockSums`. */\n    const std::size_t nBlocks = (N + BLOCK - 1) / BLOCK;\n    std::vector<int64_t> blockSums(nBlocks, 0);\n\n    #pragma omp parallel for schedule(static)\n    for (std::size_t b = 0; b < nBlocks; ++b) {\n        const std::size_t start = b * BLOCK;\n        const std::size_t end   = std::min(start + BLOCK, N);\n        std::inclusive_scan(x.begin() + start, x.begin() + end, output.begin() + start);\n        blockSums[b] = output[end - 1];     /* last element of this block */\n    }\n\n    /* Step 2 \u2013 prefix sum of the block totals */\n    std::vector<int64_t> blockPrefix(nBlocks, 0);\n    if (nBlocks > 1) {\n        /* The block sums are small (<= nBlocks), so a serial inclusive_scan\n         * is cheap and deterministic. */\n        std::inclusive_scan(blockSums.begin(), blockSums.end(), blockPrefix.begin());\n        /* The first block prefix must be set to 0 because it does not need\n         * to be adjusted. */\n        blockPrefix[0] = 0;\n    }\n\n    /* Step 3 \u2013 adjust each block by the sum of all previous blocks */\n    #pragma omp parallel for schedule(static)\n    for (std::size_t b = 1; b < nBlocks; ++b) {\n        const std::size_t start = b * BLOCK;\n        const std::size_t end   = std::min(start + BLOCK, N);\n        const int64_t offset = blockPrefix[b];\n        for (std::size_t i = start; i < end; ++i)\n            output[i] += offset;\n    }\n}\n```",
  "27_reduce_average": "```cpp\n#pragma once\n#include <vector>\n#include <cstddef>\n\ninline double average(std::vector<double> const& x) {\n    std::size_t const n = x.size();\n    double sum = 0.0;\n    #pragma omp parallel for reduction(+:sum) schedule(static)\n    for (std::size_t i = 0; i < n; ++i) {\n        sum += x[i];\n    }\n    return sum / static_cast<double>(n);\n}\n```",
  "48_sparse_la_sparse_axpy": "```cpp\n#pragma once\n#include <vector>\n#include <omp.h>\n\nstruct Element {\n    size_t index;\n    double value;\n};\n\n/* Compute z = alpha*x + y where x and y are sparse vectors.\n   The result is accumulated into the supplied vector z.\n   This implementation parallelises the addtion using OpenMP.\n   It preserves the exact semantics of the original serial algorithm.\n*/\nvoid sparseAxpy(double alpha, std::vector<Element> const& x,\n                std::vector<Element> const& y, std::vector<double> &z)\n{\n    // Parallel add from vector x\n    #pragma omp parallel for schedule(static)\n    for (size_t i = 0; i < x.size(); ++i) {\n        const size_t idx = x[i].index;\n        const double val = alpha * x[i].value;\n        #pragma omp atomic\n        z[idx] += val;\n    }\n\n    // Parallel add from vector y\n    #pragma omp parallel for schedule(static)\n    for (size_t i = 0; i < y.size(); ++i) {\n        const size_t idx = y[i].index;\n        const double val = y[i].value;\n        #pragma omp atomic\n        z[idx] += val;\n    }\n}\n```",
  "25_reduce_xor": "```cpp\n#pragma once\n#include <vector>\n\nbool reduceLogicalXOR(std::vector<bool> const& x) {\n    bool result = false;\n\n#ifndef _OPENMP\n    // Single\u2011threaded fallback\n    for (bool v : x) {\n        result ^= v;\n    }\n#else\n    // Parallel XOR reduction \u2013 OpenMP automatically promotes bool to int.\n    #pragma omp parallel for reduction(^:result)\n    for (std::size_t i = 0; i < x.size(); ++i) {\n        result ^= x[i];\n    }\n#endif\n\n    return result;\n}\n```\n",
  "29_reduce_sum_of_min_of_pairs": "```cpp\n#pragma once\n#include <vector>\n#include <cstdint>\n#include <algorithm>\n\nint64_t sumOfMinimumElements(const std::vector<int64_t>& x, const std::vector<int64_t>& y) {\n    const std::size_t n = x.size();\n    int64_t sum = 0;\n#   pragma omp parallel for reduction(+: sum)\n    for (std::size_t i = 0; i < n; ++i) {\n        sum += std::min(x[i], y[i]);\n    }\n    return sum;\n}\n```",
  "47_sparse_la_spmv": "```cpp\n#pragma once\n#include <vector>\n#include <cstddef>\n#include <omp.h>\n\nstruct COOElement {\n    std::size_t row, column;\n    double value;\n};\n\n/*\n * Compute y = alpha*A*x + beta*y\n *   - A is a sparse matrix in COO format (size M\u00d7N)\n *   - x and y are vectors of length N\n *   - alpha, beta are scalars\n * The function is thread\u2011safe and does not change its signature.\n *\n * The implementation assumes that all indices in COOElement are < M or N respectively.\n * This matches the behaviour of the original code but removes the per\u2011element bound check\n * for performance.  The loop over y is also parallelised to take advantage of the\n * available 8 threads.\n */\ninline void spmv(double alpha,\n                 const std::vector<COOElement> &A,\n                 const std::vector<double> &x,\n                 double beta,\n                 std::vector<double> &y,\n                 std::size_t M,\n                 std::size_t N)\n{\n    /* Scale y by beta \u2013 parallelise this loop. */\n    #pragma omp parallel for schedule(static)\n    for (std::size_t i = 0; i < y.size(); ++i) {\n        y[i] *= beta;\n    }\n\n    /* Perform the sparse matrix\u2011vector multiplication.\n     * Each element contributes to a single entry of y, so the loop can be\n     * safely parallelised without a reduction.\n     */\n    #pragma omp parallel for schedule(static)\n    for (std::size_t idx = 0; idx < A.size(); ++idx) {\n        const COOElement &e = A[idx];\n        // Assume indices are within bounds as per the original function.\n        y[e.row] += alpha * e.value * x[e.column];\n    }\n}\n```",
  "46_sparse_la_spmm": "```cpp\n#pragma once\n#include <vector>\n#include <utility>\n#include <cstddef>\n#include <omp.h>\n\nstruct COOElement {\n    size_t row, column;\n    double value;\n};\n\n/*  Dense matrix multiplication of two sparse matrices in COO format.\n *  Y = A * X\n *\n *  A : M x K  (sparse in COO)\n *  X : K x N  (sparse in COO)\n *  Y : M x N  (dense, row-major)\n *\n *  The implementation builds a row\u2011wise index of X to avoid\n *  repeated scans.  The outer loop over A is parallelised with\n *  OpenMP.  Each thread works on disjoint rows of A, so the\n *  writes to the output matrix are naturally race\u2011free.\n */\nvoid spmm(std::vector<COOElement> const& A,\n          std::vector<COOElement> const& X,\n          std::vector<double>& Y,\n          size_t M, size_t K, size_t N)\n{\n    // initialise output\n    Y.assign(M * N, 0.0);\n\n    // build row-wise index for X: for each row r, store (col, value)\n    std::vector<std::vector<std::pair<size_t, double>>> x_rows(K);\n    for (auto const& e : X) {\n        x_rows[e.row].emplace_back(e.column, e.value);\n    }\n\n    // parallel loop over non\u2011zero entries of A\n    #pragma omp parallel for schedule(static)\n    for (size_t ia = 0; ia < A.size(); ++ia) {\n        auto const& a = A[ia];\n        size_t rowA = a.row;\n        size_t colA = a.column;\n        double valA = a.value;\n\n        // iterate over all entries in X that match a.column == x.row\n        for (auto const& xe : x_rows[colA]) {\n            size_t colX = xe.first;\n            double valX = xe.second;\n            Y[rowA * N + colX] += valA * valX;\n        }\n    }\n}\n```",
  "26_reduce_product_of_inverses": "```cpp\n#pragma once\n#include <vector>\n#include <cstddef>\n\n/*\n * Return the product of the vector x with every odd indexed element inverted.\n * i.e. x_0 * 1/x_1 * x_2 * 1/x_3 * x_4 ...\n * Example:\n *   input: [4, 2, 10, 4, 5]\n *   output: 25\n */\ninline double productWithInverses(const std::vector<double>& x)\n{\n    double product = 1.0;\n\n#pragma omp parallel for reduction(*:product)\n    for (std::intptr_t i = 0; i < static_cast<std::intptr_t>(x.size()); ++i)\n    {\n        if (i & 1)\n            product *= 1.0 / x[i];\n        else\n            product *= x[i];\n    }\n\n    return product;\n}\n```",
  "28_reduce_smallest_odd_number": "```cpp\n#pragma once\n#include <vector>\n#include <limits>\n#include <cstddef>\n#include <omp.h>\n\n/* Return the value of the smallest odd number in the vector x.\n   If no odd numbers are present, the function returns INT_MAX.\n   Examples:\n   input: [7, 9, 5, 2, 8, 16, 4, 1]   \u2192 1\n   input: [8, 36, 7, 2, 11]           \u2192 7\n*/\nint smallestOdd(std::vector<int> const& x) {\n    const int init = std::numeric_limits<int>::max();\n    int global_min = init;\n\n    // Parallel reduction over the vector\n    #pragma omp parallel\n    {\n        int local_min = init;\n        #pragma omp for nowait\n        for (std::size_t i = 0; i < x.size(); ++i) {\n            int v = x[i];\n            if ((v & 1) && v < local_min) {\n                local_min = v;\n            }\n        }\n        #pragma omp atomic\n        if (local_min < global_min)\n            global_min = local_min;\n    }\n\n    return global_min;\n}\n```",
  "49_sparse_la_sparse_lu_decomp": "```cpp\n#pragma once\n#include <vector>\n#include <cstddef>\n#include <algorithm>\n\nstruct COOElement {\n    std::size_t row, column;\n    double value;\n};\n\n/// Factorize the sparse matrix A into A = LU where L is lower triangular\n/// and U is upper triangular. A is given in COO format, L and U are\n/// returned in row-major order (1\u2011D arrays of size N*N).\n/// The function keeps exactly the same semantics as the original.\ninline\nvoid luFactorize(const std::vector<COOElement>& A,\n                 std::vector<double>& L,\n                 std::vector<double>& U,\n                 std::size_t N)\n{\n    // Allocate dense matrix for A and fill it from the COO entries.\n    // Use a single contiguous block for cache friendliness.\n    std::vector<double> denseA(N * N, 0.0);\n    for (const auto& e : A) {\n        denseA[e.row * N + e.column] = e.value;\n    }\n\n    // Resize L and U to the required size and initialise L with zeros.\n    L.resize(N * N, 0.0);\n    U.resize(N * N, 0.0);\n\n    // LU factorisation (Doolittle's algorithm)\n    for (std::size_t i = 0; i < N; ++i) {\n        // compute U[i][j] for j >= i\n        double* uRow = U.data() + i * N;\n        const double* aRow = denseA.data() + i * N;\n        for (std::size_t j = i; j < N; ++j) {\n            double sum = aRow[j];\n            for (std::size_t k = 0; k < i; ++k) {\n                sum -= L[i * N + k] * U[k * N + j];\n            }\n            uRow[j] = sum;\n        }\n\n        // compute L[i][j] for j < i and set diagonal to 1\n        double* lRow = L.data() + i * N;\n        for (std::size_t j = 0; j < i; ++j) {\n            double sum = denseA[i * N + j];\n            for (std::size_t k = 0; k < j; ++k) {\n                sum -= L[i * N + k] * U[k * N + j];\n            }\n            lRow[j] = sum / U[j * N + j];\n        }\n        lRow[i] = 1.0;      // diagonal of L\n    }\n}\n```\n",
  "45_sparse_la_sparse_solve": "```cpp\n#pragma once\n#include <vector>\n#include <cmath>\n#include <algorithm>\n#include <omp.h>\n\nstruct COOElement {\n    size_t row, column;\n    double value;\n};\n\nvoid solveLinearSystem(std::vector<COOElement> const& A,\n                       std::vector<double> const& b,\n                       std::vector<double> &x,\n                       size_t N)\n{\n    // store the matrix as a flattened 1\u2011D array for better cache usage\n    std::vector<double> matrix(N * N, 0.0);\n    for (const auto& e : A) {\n        matrix[e.row * N + e.column] = e.value;\n    }\n\n    std::vector<double> b_copy = b;\n    x.assign(N, 0.0);\n\n    // Gaussian elimination with partial pivoting\n    for (size_t i = 0; i < N; ++i) {\n        // ----- find pivot (serial, small overhead relative to elimination) -----\n        double maxAbs = std::abs(matrix[i * N + i]);\n        size_t maxRow = i;\n        for (size_t k = i + 1; k < N; ++k) {\n            double val = std::abs(matrix[k * N + i]);\n            if (val > maxAbs) {\n                maxAbs = val;\n                maxRow = k;\n            }\n        }\n\n        // swap rows i and maxRow\n        if (maxRow != i) {\n            for (size_t k = 0; k < N; ++k) {\n                std::swap(matrix[i * N + k], matrix[maxRow * N + k]);\n            }\n            std::swap(b_copy[i], b_copy[maxRow]);\n        }\n\n        // ----- elimination of rows below ----- (parallel over rows k)\n        #pragma omp parallel for schedule(static)\n        for (int k = static_cast<int>(i + 1); k < static_cast<int>(N); ++k) {\n            double coeff = matrix[k * N + i] / matrix[i * N + i];\n            for (size_t j = i; j < N; ++j) {\n                matrix[k * N + j] -= coeff * matrix[i * N + j];\n            }\n            b_copy[k] -= coeff * b_copy[i];\n        }\n    }\n\n    // ----- back substitution -----\n    for (int i = static_cast<int>(N) - 1; i >= 0; --i) {\n        double denom = matrix[i * N + i];\n        x[i] = b_copy[i] / denom;\n        for (int k = i - 1; k >= 0; --k) {\n            b_copy[k] -= matrix[k * N + i] * x[i];\n        }\n    }\n}\n```",
  "14_geometry_closest_pair_1d": "```cpp\n#pragma once\n#include <vector>\n#include <algorithm>\n#include <limits>\n#include <cmath>\n\n/* Return the distance between the closest two elements in the vector `x`.\n   The algorithm sorts the input vector and then scans adjacent pairs,\n   which runs in O(n log n) due to sorting, but uses only linear\n   additional memory. */\ndouble closestPair(std::vector<double> const& x) {\n    // Must have at least two elements\n    if (x.size() < 2) {\n        return 0.0;\n    }\n\n    // Make a copy to sort without modifying the caller's data\n    std::vector<double> sorted = x;\n    std::sort(sorted.begin(), sorted.end());\n\n    // Scan adjacent pairs for minimal distance\n    double minDist = std::numeric_limits<double>::max();\n    for (std::size_t i = 1; i < sorted.size(); ++i) {\n        double dist = std::abs(sorted[i] - sorted[i - 1]);\n        if (dist < minDist) {\n            minDist = dist;\n        }\n    }\n    return minDist;\n}\n```",
  "13_geometry_closest_pair_2d": "```cpp\n#pragma once\n#include <vector>\n#include <limits>\n#include <cmath>\n#include <algorithm>\n#include <omp.h>\n\nstruct Point { double x, y; };\n\n// Compute squared Euclidean distance \u2013 no sqrt needed during comparisons\ninline double sqDist(Point const& a, Point const& b) {\n    double dx = b.x - a.x;\n    double dy = b.y - a.y;\n    return dx * dx + dy * dy;\n}\n\n/* Return the distance between the closest two points in the vector points.\n   Example:\n   input: [{2, 3}, {12, 30}, {40, 50}, {5, 1}, {12, 10}, {3, 4}]\n   output: 1.41421\n*/\ndouble closestPair(std::vector<Point> const& points) {\n    size_t n = points.size();\n    if (n < 2) return 0.0;\n\n    double minSq = std::numeric_limits<double>::max();\n\n#pragma omp parallel for reduction(min:minSq) schedule(static)\n    for (size_t i = 0; i < n; ++i) {\n        for (size_t j = i + 1; j < n; ++j) {\n            double d = sqDist(points[i], points[j]);\n            if (d < minSq) minSq = d;\n        }\n    }\n    return std::sqrt(minSq);\n}\n```\n",
  "12_geometry_smallest_triangle": "```cpp\n#pragma once\n#include <vector>\n#include <limits>\n#include <cmath>\n#include <cstddef>\n#include <omp.h>\n\nstruct Point {\n    double x;\n    double y;\n};\n\n// Return area of the triangle defined by points a,b,c\ninline double triArea(const Point &a, const Point &b, const Point &c)\n{\n    // 0.5 * |(b - a) x (c - a)|\n    const double cross =\n        (b.x - a.x) * (c.y - a.y) -\n        (b.y - a.y) * (c.x - a.x);\n    return std::abs(cross) * 0.5;\n}\n\n/*\n    Return the area of the smallest triangle that can be formed by any 3 points.\n    The function keeps the exact behavior of the original implementation\n    while being significantly faster on multithreaded hardware.\n*/\ndouble smallestArea(const std::vector<Point> &points)\n{\n    const std::size_t n = points.size();\n    if (n < 3)\n        return 0.0;                     // original return value\n\n    double minArea = std::numeric_limits<double>::max();\n\n    /* Parallelize the outer loop. The inner two loops are not parallelized\n       because the overhead of creating many threads would outweigh the\n       benefits when n is not huge.  The `reduction(min:minArea)` clause\n       ensures that each thread keeps a private copy of the minimum\n       which is later combined.  */\n#pragma omp parallel for schedule(static) reduction(min:minArea)\n    for (std::size_t i = 0; i < n - 2; ++i) {\n        for (std::size_t j = i + 1; j < n - 1; ++j) {\n            for (std::size_t k = j + 1; k < n; ++k) {\n                const double area = triArea(points[i], points[j], points[k]);\n                if (area < minArea) {\n                    minArea = area;\n                }\n            }\n        }\n    }\n\n    return minArea;\n}\n```",
  "06_fft_dft": "```cpp\n#pragma once\n#include <vector>\n#include <complex>\n#include <cmath>\n#include <omp.h>\n\n#if !defined(M_PI)\n#define M_PI 3.14159265358979323846\n#endif\n\n/* Optimised discrete Fourier transform.\n   Computes: output[k] = \u03a3_{n=0}^{N-1} x[n] * exp(-2\u03c0i * n * k / N)\n   The result is identical to the reference implementation, but\n   uses per\u2011thread parallelisation and a recurrence for the complex\n   exponential to reduce trig calls.\n*/\nvoid dft(const std::vector<double> &x, std::vector<std::complex<double>> &output)\n{\n    const int N = static_cast<int>(x.size());\n    output.resize(N);                           // resize without zero\u2011initialisation\n\n    // Parallelise over the outer loop (k) using OpenMP\n    #pragma omp parallel for schedule(static)\n    for (int k = 0; k < N; ++k) {\n        // Compute the primitive Nth root of unity for this k:\n        const double theta = -2.0 * M_PI * static_cast<double>(k) / static_cast<double>(N);\n        const double w_real = std::cos(theta);   // exp(-i * theta)\n        const double w_imag = std::sin(theta);\n\n        // Start with exp(0) = 1 + 0i\n        double c_real = 1.0, c_imag = 0.0;\n        double sum_real = 0.0, sum_imag = 0.0;\n\n        for (int n = 0; n < N; ++n) {\n            const double val = x[n];\n            sum_real += val * c_real;\n            sum_imag += val * c_imag;\n\n            // Multiply current exponent by w to get the next one\n            double next_real = c_real * w_real - c_imag * w_imag;\n            double next_imag = c_real * w_imag + c_imag * w_real;\n            c_real = next_real;\n            c_imag = next_imag;\n        }\n\n        output[k] = std::complex<double>(sum_real, sum_imag);\n    }\n}\n```",
  "09_fft_fft_out_of_place": "```cpp\n#pragma once\n#include <vector>\n#include <complex>\n#include <cmath>\n#include <cstdint>\n#include <omp.h>\n\n/*  Radix\u20112 Cooley\u2013Tukey FFT\n    Implementation is fully thread\u2011safe and preserves exact numerical\n    behaviour of the baseline algorithm.  */\ninline void fft(const std::vector<std::complex<double>>& x,\n                std::vector<std::complex<double>>& output)\n{\n    const std::size_t N = x.size();\n    output = x;                          // work in a local copy\n\n    // ---- Bit\u2011reversal permutation ---------------------------------------\n    const std::size_t log2N = static_cast<std::size_t>(std::log2(N));\n    #pragma omp parallel for schedule(static)\n    for (std::size_t i = 0; i < N; ++i) {\n        std::size_t j = i;\n        j = ((j & 0xaaaaaaaaaaaaaaaaULL) >> 1) | ((j & 0x5555555555555555ULL) << 1);\n        j = ((j & 0xccccccccccccccccULL) >> 2) | ((j & 0x3333333333333333ULL) << 2);\n        j = ((j & 0xf0f0f0f0f0f0f0f0ULL) >> 4) | ((j & 0x0f0f0f0f0f0f0f0fULL) << 4);\n        j = ((j & 0xff00ff00ff00ff00ULL) >> 8) | ((j & 0x00ff00ff00ff00ffULL) << 8);\n        j = ((j & 0xffff0000ffff0000ULL) >> 16) | ((j & 0x0000ffff0000ffffULL) << 16);\n        j = (j >> (64 - log2N));\n        if (j > i) std::swap(output[i], output[j]);\n    }\n\n    // ---- Butterfly stages -----------------------------------------------\n    // Pre\u2011compute twiddle angles for all stages\n    std::vector<std::complex<double>> twiddles(N / 2);\n    for (std::size_t i = 0; i < N / 2; ++i) {\n        const double ang = -2.0 * M_PI * i / static_cast<double>(N);\n        twiddles[i] = std::complex<double>(std::cos(ang), std::sin(ang));\n    }\n\n    for (std::size_t stage = 1; stage <= log2N; ++stage) {\n        const std::size_t halfSize = 1ULL << (stage - 1);\n        const std::size_t fullSize = halfSize << 1;\n        const std::size_t step = N / fullSize;   // twiddle stride\n\n        #pragma omp parallel for schedule(static)\n        for (std::size_t block = 0; block < N; block += fullSize) {\n            const std::size_t base = block;\n            for (std::size_t i = 0; i < halfSize; ++i) {\n                const std::size_t idxEven = base + i;\n                const std::size_t idxOdd  = idxEven + halfSize;\n                const std::complex<double> t = output[idxOdd] * twiddles[i * step];\n                output[idxOdd] = output[idxEven] - t;\n                output[idxEven] += t;\n            }\n        }\n    }\n}\n```",
  "08_fft_split_fft": "```cpp\n#pragma once\n#include <vector>\n#include <cmath>\n#include <complex>\n#include <omp.h>\n\n/* Optimised radix\u20112 Cooley\u2011Tukey FFT.\n * The algorithm is a pure decimation\u2013in\u2013time implementation.\n * Bit\u2011reversal is performed with a small 16\u2011bit lookup table.\n * All inner loops that can be parallelised are guarded by OpenMP pragmas.\n *\n * The function signature is kept exactly the same as required by the test harness.\n */\nstatic inline void precompute_twiddles(int n, std::vector<std::complex<double>>& w)\n{\n    w.resize(n);\n    const double pi = 3.141592653589793238462643383279502884L;\n    for (int k = 0; k < n; ++k)\n        w[k] = std::exp(std::complex<double>(0.0, -pi * k / n));\n}\n\n// 16\u2011bit bit\u2011reverse lookup table\nstatic const uint16_t bit_rev_16[1 << 16] = []{\n    uint16_t table[1 << 16];\n    for (int i = 0; i < (1 << 16); ++i) {\n        int x = i;\n        int r = 0;\n        for (int b = 0; b < 16; ++b) {\n            r = (r << 1) | (x & 1);\n            x >>= 1;\n        }\n        table[i] = r;\n    }\n    return table;\n}();\n\nstatic inline unsigned int reverse_bits(unsigned int x, unsigned int bits)\n{\n    // cut into two 16\u2011bit chunks\n    uint32_t lo = bit_rev_16[x & 0xFFFF];\n    uint32_t hi = bit_rev_16[(x >> 16) & 0xFFFF];\n    uint32_t rev = (lo << 16) | hi;\n    return rev >> (32 - bits);\n}\n\nvoid fft(std::vector<std::complex<double>> const& x, std::vector<double> &r, std::vector<double> &i)\n{\n    const size_t N = x.size();\n    std::vector<std::complex<double>> a(x);          // work array\n    std::vector<std::complex<double>> w;             // twiddle factors\n    precompute_twiddles(N, w);\n\n    // --- decimation in time Cooley\u2011Tukey (iterative) ---\n\n    // stage by stage\n    for (size_t m = 1; m < N; m <<= 1)\n    {\n        size_t step = m << 1;                     // 2*m\n        size_t stride = N / step;                 // inverse step for twiddle index\n        #pragma omp parallel for schedule(static)\n        for (size_t k = 0; k < m; ++k)\n        {\n            const std::complex<double> w_k = w[k * stride];\n            for (size_t j = k; j < N; j += step)\n            {\n                std::complex<double> u = a[j];\n                std::complex<double> t = a[j + m] * w_k;\n                a[j]         = u + t;\n                a[j + m]     = u - t;\n            }\n        }\n    }\n\n    // --- bit\u2011reversal permutation ---\n\n    size_t bits = 0;\n    for (size_t tmp = N; tmp > 1; tmp >>= 1) ++bits;\n\n    #pragma omp parallel for schedule(static)\n    for (size_t n = 0; n < N; ++n)\n    {\n        size_t rev = reverse_bits(n, (unsigned int)bits);\n        if (rev > n)\n            std::swap(a[n], a[rev]);\n    }\n\n    // --- split into real/imaginary parts ---\n    r.resize(N);\n    i.resize(N);\n    for (size_t j = 0; j < N; ++j)\n    {\n        r[j] = a[j].real();\n        i[j] = a[j].imag();\n    }\n}\n```",
  "05_fft_inverse_fft": "```cpp\n#pragma once\n#include <vector>\n#include <complex>\n#include <cmath>\n#include <algorithm>\n#include <omp.h>\n\n#if !defined(M_PI)\n#define M_PI 3.14159265358979323846\n#endif\n\n//---------- In\u2011place iterative FFT -------------------------------------------------\nstatic void fft_helper(std::vector<std::complex<double>>& x)\n{\n    const std::size_t N = x.size();\n    if (N <= 1) return;\n\n    /* 1. Bit reversal reordering (in place) -------------------------------------*/\n    std::size_t m = 0;                     // number of stages = log2(N)\n    for (std::size_t tmp = N; tmp > 1; tmp >>= 1) ++m;\n\n    for (std::size_t i = 0; i < N; ++i) {\n        std::size_t rev = i;\n        rev = ((rev & 0xaaaaaaaau) >> 1) | ((rev & 0x55555555u) << 1);\n        rev = ((rev & 0xccccccccu) >> 2) | ((rev & 0x33333333u) << 2);\n        rev = ((rev & 0xf0f0f0f0u) >> 4) | ((rev & 0x0f0f0f0fu) << 4);\n        rev = ((rev & 0xff00ff00u) >> 8) | ((rev & 0x00ff00ffu) << 8);\n        rev = ((rev & 0xffff0000u) >> 16) | ((rev & 0x0000ffffu) << 16);\n        rev >>= (32 - m);\n\n        if (rev > i) std::swap(x[i], x[rev]);\n    }\n\n    /* 2. Butterfly operations ----------------------------------------------------*/\n    std::complex<double> w_m = std::exp(std::complex<double>(0, -M_PI / N)); // primitive 2N\u2011th root\n    for (std::size_t size = 2; size <= N; size <<= 1) {\n        const std::size_t half = size >> 1;\n        std::complex<double> w = std::complex<double>(1, 0);\n        const std::complex<double> w_step = std::pow(w_m, static_cast<long long>(size));\n\n#pragma omp parallel for schedule(static)\n        for (std::size_t i = 0; i < N; i += size) {\n            std::complex<double> w_local = std::complex<double>(1, 0);\n            for (std::size_t j = 0; j < half; ++j) {\n                std::complex<double> t = w_local * x[i + j + half];\n                x[i + j + half] = x[i + j] - t;\n                x[i + j] += t;\n                w_local *= w_step;\n            }\n        }\n    }\n}\n//------------------------------------------------------------------------------\n\n\n//---------- Inverse FFT (exactly equivalent behaviour) -----------------------------\ninline void ifft(std::vector<std::complex<double>>& x)\n{\n    // Conjugate all elements\n    for (auto& v : x) v = std::conj(v);\n\n    // Forward FFT\n    fft_helper(x);\n\n    // Conjugate again\n    for (auto& v : x) v = std::conj(v);\n\n    // Scale\n    const double invN = 1.0 / static_cast<double>(x.size());\n    for (auto& v : x) v *= invN;\n}\n```",
  "11_geometry_convex_hull_perimeter": "```cpp\n#pragma once\n#include <vector>\n#include <algorithm>\n#include <cmath>\n\nstruct Point\n{\n    double x, y;\n};\n\ninline double dist2(Point const& a, Point const& b)\n{\n    double dx = b.x - a.x;\n    double dy = b.y - a.y;\n    return dx * dx + dy * dy;\n}\n\ninline double dist(Point const& a, Point const& b)\n{\n    return std::sqrt(dist2(a, b));\n}\n\ninline double cross(Point const& a, Point const& b, Point const& c)\n{\n    return (b.x - a.x) * (c.y - a.y) - (b.y - a.y) * (c.x - a.x);\n}\n\ndouble convexHullPerimeter(std::vector<Point> const& points)\n{\n    size_t n = points.size();\n    if (n < 3) return 0.0;\n\n    // Work on a copy that we can sort\n    std::vector<Point> pts = points;\n    std::sort(pts.begin(), pts.end(),\n              [](const Point& a, const Point& b)\n              {\n                  return a.x < b.x || (a.x == b.x && a.y < b.y);\n              });\n\n    // Convex hull in O(n) using Andrew's monotone chain\n    std::vector<Point> hull;\n    hull.reserve(n * 2);\n\n    // Lower hull\n    for (size_t i = 0; i < n; ++i)\n    {\n        while (hull.size() >= 2 &&\n               cross(hull[hull.size() - 2], hull.back(), pts[i]) <= 0)\n            hull.pop_back();\n        hull.push_back(pts[i]);\n    }\n\n    // Upper hull\n    size_t lower_size = hull.size();\n    for (size_t i = n - 1; i-- > 0; )\n    {\n        while (hull.size() > lower_size &&\n               cross(hull[hull.size() - 2], hull.back(), pts[i]) <= 0)\n            hull.pop_back();\n        hull.push_back(pts[i]);\n    }\n\n    // The last point is the same as the first one\n    hull.pop_back();\n\n    // Compute perimeter\n    double perim = 0.0;\n    for (size_t i = 0, m = hull.size(); i < m; ++i)\n    {\n        size_t next = (i + 1) % m;\n        perim += dist(hull[i], hull[next]);\n    }\n\n    return perim;\n}\n```",
  "52_stencil_1d_jacobi_3-point_stencil": "```cpp\n#pragma once\n#include <vector>\n#include <cstddef>\n#include <omp.h>\n\n/* Compute one iteration of a 3\u2011point 1D Jacobi stencil on `input`.\n * Each element of `input` will be averaged with its two neighbours\n * and stored in the corresponding element of `output`.\n * Boundary values are treated as zero.\n *\n * Example:\n *   input : [9, -6, -1,  2, 3]\n *   output: [1,  2/3, -5/3, 4/3, 5/3]\n */\ninline void jacobi1D(std::vector<double> const& input, std::vector<double> &output)\n{\n    const size_t n = input.size();\n    if (n == 0) return;\n\n    // Pointer access is slightly faster than operator[]\n    const double *in  = input.data();\n    double *out       = output.data();\n\n    // Parallelise with OpenMP; static scheduling keeps work balanced.\n    #pragma omp parallel for schedule(static) default(none) shared(in, out, n)\n    for (size_t i = 0; i < n; ++i) {\n        double sum = in[i];                     // self\n        if (i != 0)     sum += in[i - 1];        // left neighbour\n        if (i + 1 < n)  sum += in[i + 1];        // right neighbour\n        out[i] = sum / 3.0;\n    }\n}\n```",
  "10_geometry_convex_hull": "```cpp\n#pragma once\n#include <vector>\n#include <algorithm>\n\nstruct Point {\n    double x, y;\n};\n\n/*\n * Convex hull \u2013 Andrew's monotone chain.\n * Returns hull vertices in counter\u2011clockwise order,\n * starting from the leftmost point.\n */\ninline void convexHull(const std::vector<Point> &points, std::vector<Point> &hull) {\n    const std::size_t n = points.size();\n    if (n < 3) {                               // trivial cases\n        hull.assign(points.begin(), points.end());\n        return;\n    }\n\n    /* 1. Sort by x, then by y.  */\n    std::vector<Point> pts = points;\n    std::sort(pts.begin(), pts.end(),\n              [](const Point &a, const Point &b) {\n                  return a.x < b.x || (a.x == b.x && a.y < b.y);\n              });\n\n    /* 2. Cross product helper (inlined).  */\n    auto cross = [](const Point &a, const Point &b, const Point &c) noexcept {\n        return (b.x - a.x) * (c.y - a.y) - (b.y - a.y) * (c.x - a.x);\n    };\n\n    /* 3. Build lower hull.  */\n    std::vector<Point> h;\n    h.reserve(2 * n);                         // maximum possible size\n    for (const Point &p : pts) {\n        while (h.size() >= 2 &&\n               cross(h[h.size() - 2], h[h.size() - 1], p) <= 0.0)\n            h.pop_back();\n        h.push_back(p);\n    }\n\n    /* 4. Build upper hull.  */\n    const std::size_t lowerSize = h.size();\n    for (std::size_t i = n - 1; i > 0; --i) { // skip last point (it's already in lower)\n        const Point &p = pts[i - 1];          // pts[i] == last point of lower\n        while (h.size() > lowerSize &&\n               cross(h[h.size() - 2], h[h.size() - 1], p) <= 0.0)\n            h.pop_back();\n        h.push_back(p);\n    }\n\n    /* 5. Remove duplicate endpoint and assign result. */\n    h.pop_back();            // last point == first point of lower hull\n    hull.swap(h);\n}\n```\n",
  "03_dense_la_axpy": "```cpp\n#pragma once\n#include <vector>\n#include <cstddef>\n#include <cstdint>\n\ninline void axpy(double alpha, std::vector<double> const& x,\n                 std::vector<double> const& y, std::vector<double> &z) {\n    // Assumes x.size() == y.size() == z.size()\n    const std::size_t n = x.size();\n#pragma omp parallel for schedule(static)\n    for (std::ptrdiff_t i = 0; i < static_cast<std::ptrdiff_t>(n); ++i) {\n        z[static_cast<std::size_t>(i)] = alpha * x[static_cast<std::size_t>(i)] + y[static_cast<std::size_t>(i)];\n    }\n}\n```",
  "51_stencil_edge_kernel": "```cpp\n#pragma once\n#include <vector>\n#include <cstddef>\n#include <algorithm>\n#include <omp.h>\n\nstatic constexpr int edgeKernel[3][3] = {\n    {-1, -1, -1},\n    {-1,  8, -1},\n    {-1, -1, -1}\n};\n\n/* Convolution using the edge detection kernel.\n * The implementation is fully parallel (OpenMP), avoids boundary checks\n * by handling the interior and edges separately, and keeps the original\n * clipping semantics.\n */\nvoid convolveKernel(std::vector<int> const& imageIn,\n                    std::vector<int> &imageOut,\n                    size_t N) {\n    // Ensure output buffer is correctly sized\n    if (imageOut.size() != N * N) imageOut.resize(N * N);\n\n    // Pre\u2011allocate for speed: compute limits once\n    size_t inside_start = 1, inside_end = N - 1;\n\n    // Parallelise over rows\n    #pragma omp parallel for schedule(static)\n    for (size_t i = 0; i < N; ++i) {\n        // Compute which kernel indices are valid for this row\n        int k_min = (i == 0) ? 0 : -1;\n        int k_max = (i == N - 1) ? -1 : 1;\n\n        for (size_t j = 0; j < N; ++j) {\n            int sum = 0;\n\n            // Loop over kernel offsets\n            for (int k = k_min; k <= k_max; ++k) {\n                int x = static_cast<int>(i) + k;\n                for (int l = -1; l <= 1; ++l) {\n                    int y = static_cast<int>(j) + l;\n                    if (y < 0 || y >= static_cast<int>(N)) continue;\n                    sum += imageIn[x * N + y] * edgeKernel[k + 1][l + 1];\n                }\n            }\n\n            // Clamp to [0,255]\n            if (sum < 0)        imageOut[i * N + j] = 0;\n            else if (sum > 255) imageOut[i * N + j] = 255;\n            else                imageOut[i * N + j] = sum;\n        }\n    }\n}\n```",
  "07_fft_fft_conjugate": "```cpp\n#pragma once\n#include <vector>\n#include <complex>\n#include <cmath>\n#include <cstdint>\n#include <omp.h>\n\ninline uint32_t bit_reverse32(uint32_t x, int bits) {\n    x = ((x >> 1)  & 0x55555555) | ((x & 0x55555555) << 1);\n    x = ((x >> 2)  & 0x33333333) | ((x & 0x33333333) << 2);\n    x = ((x >> 4)  & 0x0f0f0f0f) | ((x & 0x0f0f0f0f) << 4);\n    x = ((x >> 8)  & 0x00ff00ff) | ((x & 0x00ff00ff) << 8);\n    x = (x >> 16) | (x << 16);\n    return x >> (32 - bits);\n}\n\ninline int int_log2(unsigned int x) {\n    return 31 - __builtin_clz(x);\n}\n\nvoid fftConjugate(std::vector<std::complex<double>>& x) {\n    const unsigned N = static_cast<unsigned>(x.size());\n    if (N <= 1) return; // nothing to do\n\n    /* -----------  Bit\u2011reversal permutation  ----------- */\n    const int m = int_log2(N);\n    std::vector<uint32_t> rev(N);\n    for (uint32_t i = 0; i < N; ++i) {\n        rev[i] = bit_reverse32(i, m);\n    }\n    for (uint32_t i = 0; i < N; ++i) {\n        if (rev[i] > i) std::swap(x[i], x[rev[i]]);\n    }\n\n    /* -----------  Cooley\u2013Tukey radix\u20112 iterative FFT  ----------- */\n    for (int s = 1; s <= m; ++s) {\n        const int m2 = 1 << s;        // block size\n        const int m1 = m2 >> 1;       // half block\n        const double theta = -M_PI / m1;  // negative for forward FFT (conjugate)\n        const std::complex<double> w_m = std::complex<double>(std::cos(theta), std::sin(theta));\n\n        #pragma omp parallel for schedule(static)\n        for (int k = 0; k < N; k += m2) {\n            std::complex<double> w = 1.0;\n            for (int j = 0; j < m1; ++j) {\n                const int a = k + j;\n                const int b = a + m1;\n                const std::complex<double> t = x[a] - x[b];\n                x[a] += x[b];\n                x[b] = t * w;\n                w *= w_m;\n            }\n        }\n    }\n\n    /* -----------  Conjugate all elements  ----------- */\n    for (auto& val : x) val = std::conj(val);\n}\n```",
  "50_stencil_xor_kernel": "```cpp\n#pragma once\n#include <vector>\n#include <cstddef>\n#include <omp.h>\n\n/* Set every cell's value to 1 if it has exactly one neighbor that's a 1. Otherwise set it to 0.\n   input and output are NxN grids of ints in row-major.\n   Example:\n   input: [[0, 1, 1, 0],\n           [1, 0, 0, 0],\n           [0, 0, 0, 0],\n           [0, 1, 0, 0]]\n   output: [[0, 0, 1, 1],\n            [1, 0, 0, 1],\n            [0, 0, 1, 0],\n            [1, 0, 1, 0]]\n*/\nvoid cellsXOR(std::vector<int> const& input, std::vector<int> &output, size_t N) {\n    const size_t total = N * N;\n    const int* in = input.data();\n    int* out = output.data();\n#pragma omp parallel for schedule(static) if (total > 0)\n    for (size_t idx = 0; idx < total; ++idx) {\n        size_t i = idx / N;\n        size_t j = idx % N;\n        int count = 0;\n        if (i > 0         && in[(i-1)*N + j] == 1) ++count;\n        if (i + 1 < N     && in[(i+1)*N + j] == 1) ++count;\n        if (j > 0         && in[i*N + j-1] == 1)   ++count;\n        if (j + 1 < N     && in[i*N + j+1] == 1)   ++count;\n        out[idx] = (count == 1) ? 1 : 0;\n    }\n}\n```",
  "53_stencil_2d_jacobi_5-point_stencil": "```cpp\n#pragma once\n#include <vector>\n#include <cstddef>\n#include <omp.h>\n\n/*\n * Optimised 5\u2011point 2\u2011D Jacobi stencil.\n * Input and output are N\u00d7N grids stored row\u2011major.\n * The method uses a padded temporary buffer to avoid boundary checks\n * and employs OpenMP for parallelism.\n */\ninline void jacobi2D(const std::vector<double> &input,\n                     std::vector<double> &output,\n                     size_t N) {\n    constexpr double inv5 = 0.2;      // 1/5.0\n\n    /* Allocate a padded helper buffer to store the input with a 1\u2011cell border.\n       The padded buffer has dimensions (N+2) \u00d7 (N+2). */\n    std::vector<double> padded((N + 2) * (N + 2), 0.0);\n\n    /* Copy input into the centre of the padded buffer */\n    #pragma omp parallel for collapse(2) schedule(static)\n    for (size_t i = 0; i < N; ++i) {\n        const size_t src_idx = i * N;\n        const size_t dst_idx = (i + 1) * (N + 2) + 1;\n        std::memcpy(&padded[dst_idx], &input[src_idx], N * sizeof(double));\n    }\n\n    /* Perform the stencil, fully vectorised by the compiler. */\n    #pragma omp parallel for collapse(2) schedule(static)\n    for (size_t i = 1; i <= N; ++i) {\n        const size_t row_above = (i - 1) * (N + 2);\n        const size_t row      =  i     * (N + 2);\n        const size_t row_below = (i + 1) * (N + 2);\n        const size_t out_idx  = (i - 1) * N;\n        for (size_t j = 1; j <= N; ++j) {\n            double sum = padded[row_above + j]      // up\n                       + padded[row_below + j]      // down\n                       + padded[row      + j - 1]    // left\n                       + padded[row      + j + 1]    // right\n                       + padded[row      + j];       // center\n            output[out_idx + (j - 1)] = sum * inv5;\n        }\n    }\n}\n```",
  "04_dense_la_gemv": "```cpp\n#pragma once\n#include <vector>\n#include <cstddef>\n#include <omp.h>\n\n/*\n   Multiply the matrix A by the vector x. Store the results in the vector y.\n   A is an MxN matrix stored in row-major, x has N elements, and y has M elements.\n   Example:\n   input: A=[[1,-1,2],[0,-3,1]]  x=[2,1,0]\n   output: y=[1,-3]\n*/\ninline void gemv(std::vector<double> const& A,\n                 std::vector<double> const& x,\n                 std::vector<double> &y,\n                 std::size_t M,\n                 std::size_t N) {\n    // Ensure output vector has the proper size\n    if (y.size() < M) y.resize(M);\n\n    // Use OpenMP to parallelize over rows\n    #pragma omp parallel for schedule(static)\n    for (std::size_t i = 0; i < M; ++i) {\n        double sum = 0.0;\n        const double* Ai = &A[i * N];          // pointer to the start of the i\u2011th row\n        const double* xv = x.data();           // pointer to vector x\n        for (std::size_t j = 0; j < N; ++j) {\n            sum += Ai[j] * xv[j];\n        }\n        y[i] = sum;\n    }\n}\n```",
  "43_sort_sort_an_array_of_structs_by_key": "```cpp\n#pragma once\n#include <vector>\n#include <algorithm>\n\nstruct Result {\n    int     startTime;\n    int     duration;\n    float   value;\n};\n\n/* Quickly sort a vector of Result by startTime in ascending order.\n   The implementation relies on std::sort which uses an efficient introsort\n   (hybrid of quicksort, heapsort and insertion sort) suitable for\n   large datasets. This is the fastest generic approach without\n   writing a custom algorithm. */\ninline void sortByStartTime(std::vector<Result> &results) {\n    std::sort(results.begin(), results.end(),\n              [](const Result &a, const Result &b) noexcept {\n                  return a.startTime < b.startTime;\n              });\n}\n```",
  "54_stencil_game_of_life": "```cpp\n#pragma once\n#include <vector>\n#include <cstddef>\n#include <omp.h>\n\n/* Optimised single\u2011generation Game of Life.\n   The function semantics are identical to the original implementation.\n   The outer i\u2011loop is parallelised with OpenMP and a few local variables\n   are cached to reduce indirection.  */\n\ninline std::size_t idx(std::size_t i, std::size_t j, std::size_t N) {\n    return i * N + j;\n}\n\nvoid gameOfLife(std::vector<int> const& input,\n                std::vector<int>          &output,\n                std::size_t                N) {\n\n    /* Parallelise over rows \u2013 each row is independent. */\n#pragma omp parallel for schedule(static)\n    for (std::size_t i = 0; i < N; ++i) {\n        const std::size_t iN = i * N;\n        const std::size_t iNplus = (i + 1) * N;\n        const std::size_t iNminus = (i == 0 ? 0 : (i - 1) * N);\n\n        for (std::size_t j = 0; j < N; ++j) {\n            int sum = 0;\n\n            /* Neighbour above */\n            if (i > 0) sum += input[iNminus + j];\n            /* Neighbour below */\n            if (i < N - 1) sum += input[iNplus + j];\n            /* Left */\n            if (j > 0) sum += input[iN + j - 1];\n            /* Right */\n            if (j < N - 1) sum += input[iN + j + 1];\n            /* Top\u2011left */\n            if (i > 0 && j > 0) sum += input[iNminus + j - 1];\n            /* Top\u2011right */\n            if (i > 0 && j < N - 1) sum += input[iNminus + j + 1];\n            /* Bottom\u2011left */\n            if (i < N - 1 && j > 0) sum += input[iNplus + j - 1];\n            /* Bottom\u2011right */\n            if (i < N - 1 && j < N - 1) sum += input[iNplus + j + 1];\n\n            const int cur = input[iN + j];\n            if (cur == 1) {\n                /* Live cell */\n                output[iN + j] = (sum == 2 || sum == 3) ? 1 : 0;\n            } else {\n                /* Dead cell */\n                output[iN + j] = (sum == 3) ? 1 : 0;\n            }\n        }\n    }\n}\n```",
  "00_dense_la_lu_decomp": "```cpp\n#pragma once\n#include <vector>\n#include <omp.h>\n\n/* Factorize the matrix A into A = LU where L is a lower triangular matrix and\n   U is an upper triangular matrix. The factors are stored in-place in A.\n   A is an N\u00d7N matrix stored in row-major order. */\nvoid luFactorize(std::vector<double> &A, size_t N) {\n    // Standard Doolittle algorithm with in\u2011place storage\n    for (size_t k = 0; k < N; ++k) {\n        double akk = A[k * N + k];          // Pivot element\n        // Parallelize the row\u2011wise elimination below the pivot row\n        #pragma omp parallel for schedule(static)\n        for (size_t i = k + 1; i < N; ++i) {\n            double factor = A[i * N + k] / akk;\n            A[i * N + k] = factor;          // Store L entry (diagonal of L is 1)\n            double *row_i = &A[i * N + k + 1];\n            double *row_k = &A[k * N + k + 1];\n            size_t remaining = N - k - 1;\n            // Manual loop unrolling (factor 4)\n            size_t j = 0;\n            for (; j + 3 < remaining; j += 4) {\n                row_i[j]   -= factor * row_k[j];\n                row_i[j+1] -= factor * row_k[j+1];\n                row_i[j+2] -= factor * row_k[j+2];\n                row_i[j+3] -= factor * row_k[j+3];\n            }\n            for (; j < remaining; ++j) {\n                row_i[j] -= factor * row_k[j];\n            }\n        }\n    }\n}\n```",
  "02_dense_la_gemm": "```cpp\n#pragma once\n#include <vector>\n#include <cstddef>\n#include <omp.h>\n\n/* Multiply the matrix A by the matrix B. Store the results in the matrix C.\n   A is an MxK matrix, B is a KxN matrix, and C is a MxN matrix. The matrices are stored in row-major.\n   Example:\n   input: A=[[1, -1, 2], [0, -2, 1]] B=[[4, 1], [-1, 0], [2, 2]]\n   output: C=[[9, 5], [4, 2]]\n*/\ninline void gemm(const std::vector<double>& A,\n                 const std::vector<double>& B,\n                 std::vector<double>& C,\n                 std::size_t M, std::size_t K, std::size_t N)\n{\n    /* Block sizes \u2013 tuned for a typical 8\u2011core x86\u201164 with L1 = 32\u202fKB. */\n    constexpr std::size_t BM = 64;   // block size for rows of A (M dimension)\n    constexpr std::size_t BK = 128;  // block size for inner dimension K\n    constexpr std::size_t BN = 64;   // block size for columns of B (N dimension)\n\n    /* Initialise C to 0 \u2013 the original code assumes C is already zeroed. */\n    std::fill(C.begin(), C.end(), 0.0);\n\n    /* Parallel outermost loop \u2013 rows of A are distributed among threads. */\n    #pragma omp parallel for schedule(static)\n    for (std::size_t ii = 0; ii < M; ii += BM) {\n        std::size_t i_max = std::min(ii + BM, M);\n        for (std::size_t kk = 0; kk < K; kk += BK) {\n            std::size_t k_max = std::min(kk + BK, K);\n            for (std::size_t jj = 0; jj < N; jj += BN) {\n                std::size_t j_max = std::min(jj + BN, N);\n\n                /* Inner panel multiplication \u2013 we use a cache\u2011friendly loop order\n                   (i\u2011j\u2011k) with a small accumulation buffer to reduce memory traffic. */\n                for (std::size_t i = ii; i < i_max; ++i) {\n                    double* c_row = &C[i * N + jj];\n                    const double* a_row = &A[i * K + kk];\n                    for (std::size_t j = jj; j < j_max; ++j) {\n                        double sum = c_row[j - jj];\n                        for (std::size_t k = kk; k < k_max; ++k) {\n                            sum += a_row[k - kk] * B[k * N + j];\n                        }\n                        c_row[j - jj] = sum;\n                    }\n                }\n            }\n        }\n    }\n}\n```",
  "42_sort_sorted_ranks": "```cpp\n#pragma once\n#include <algorithm>\n#include <numeric>\n#include <vector>\n#include <omp.h>\n\n/* For each value in the vector x compute its index in the sorted vector.\n   Store the results in `ranks`.\n   Examples:\n   input: [3.1, 2.8, 9.1, 0.4, 3.14]\n   output: [2, 1, 4, 0, 3]\n   input: [100, 7.6, 16.1, 18, 7.6]\n   output: [4, 0, 1, 2, 3]\n*/\ninline void ranks(std::vector<float> const& x, std::vector<size_t> &ranks_) {\n    // Build a vector of (value, original_index) pairs\n    std::vector<std::pair<float, size_t>> pairs;\n    pairs.reserve(x.size());\n    for (size_t i = 0; i < x.size(); ++i)\n        pairs.emplace_back(x[i], i);\n\n    // Sort the pairs by the value\n    std::sort(pairs.begin(), pairs.end(),\n              [](const auto& a, const auto& b) { return a.first < b.first; });\n\n    // Assign ranks in parallel\n    #pragma omp parallel for schedule(static)\n    for (size_t i = 0; i < pairs.size(); ++i) {\n        ranks_[pairs[i].second] = i;\n    }\n}\n```",
  "41_sort_k-th_smallest_element": "```cpp\n#pragma once\n#include <algorithm>\n#include <vector>\n\n/*\n * Return the k\u2011th smallest element of the input vector `x`.\n * The function assumes `1 <= k <= x.size()`.  The result is\n * identical to sorting the vector and picking the (k\u20111)th element.\n *\n * This implementation uses `std::nth_element`, which runs in linear\n * average time and does not sort the entire vector.  The input vector\n * is copied so that the original data remains unchanged as required\n * by the problem statement.\n */\nint findKthSmallest(const std::vector<int>& x, int k) {\n    // Make a copy of the input \u2013 the original must not be modified.\n    std::vector<int> temp(x);\n\n    // Place the k\u2011th smallest element into its final position.\n    std::nth_element(temp.begin(), temp.begin() + (k - 1), temp.end());\n\n    // The element at position k\u20111 is now the k\u2011th smallest.\n    return temp[k - 1];\n}\n```",
  "44_sort_sort_non-zero_elements": "```cpp\n#pragma once\n#include <algorithm>\n#include <vector>\n#include <cstddef>\n\ninline void sortIgnoreZero(std::vector<int> &x) {\n    // Quick path: if all elements are zero or there are no non\u2011zeros, nothing to do.\n    std::size_t nonZeroCount = 0;\n    for (const int v : x)\n        if (v != 0) ++nonZeroCount;\n    if (nonZeroCount == 0)\n        return;\n\n    // Extract non\u2011zero elements once.\n    std::vector<int> nonZero;\n    nonZero.reserve(nonZeroCount);\n    for (int v : x)\n        if (v != 0)\n            nonZero.push_back(v);\n\n    // Sort the non\u2011zero elements.\n    std::sort(nonZero.begin(), nonZero.end());\n\n    // Re\u2011insert the sorted non\u2011zeros back into their original zero\u2011skipping places.\n    std::size_t idx = 0;\n    for (std::size_t i = 0; i < x.size(); ++i) {\n        if (x[i] != 0)\n            x[i] = nonZero[idx++];\n    }\n}\n```",
  "40_sort_sort_an_array_of_complex_numbers_by_magnitude": "```cpp\n#pragma once\n#include <complex>\n#include <vector>\n#include <algorithm>\n#include <cmath>\n\n/* Sort the vector `x` of complex numbers by their magnitude in ascending order.\n   Example:\n\n   input: [3.0-1.0i, 4.5+2.1i, 0.0-1.0i, 1.0-0.0i, 0.5+0.5i]\n   output: [0.5+0.5i, 0.0-1.0i, 1.0-0.0i, 3.0-1.0i, 4.5+2.1i]\n*/\ninline double sqrmag(const std::complex<double>& c)\n{\n    return std::norm(c);          // a^2 + b^2, no sqrt\n}\n\ninline bool cmp_sqrmag(const std::complex<double>& a, const std::complex<double>& b)\n{\n    return sqrmag(a) < sqrmag(b);\n}\n\nvoid sortComplexByMagnitude(std::vector<std::complex<double>> &x)\n{\n    std::sort(x.begin(), x.end(), cmp_sqrmag);\n}\n```",
  "35_search_search_for_last_struct_by_key": "```cpp\n#pragma once\n#include <vector>\n\nstruct Book {\n    std::string title;\n    int pages;\n};\n\n// Return the index of the last Book in `books` whose pages < 100.\n// If none exist, return books.size().\nsize_t findLastShortBook(std::vector<Book> const& books) {\n    if (books.empty()) return books.size();\n\n    // Iterate in reverse without using signed integers\n    for (size_t i = books.size(); i-- > 0; ) {\n        if (books[i].pages < 100) {\n            return i;\n        }\n    }\n    return books.size();\n}\n```",
  "01_dense_la_solve": "```cpp\n#pragma once\n#include <vector>\n#include <cstring>     // for memcpy\n#include <omp.h>       // OpenMP header\n\n/* Solve the linear system Ax=b for x.\n   A is an NxN matrix in row-major. x and b have N elements.\n   Example:\n   input: A=[[1,4,2], [1,2,3], [2,1,3]] b=[11, 11, 13]\n   output: x=[3, 1, 2]\n*/\ninline void solveLinearSystem(const std::vector<double>& A,\n                              const std::vector<double>& b,\n                              std::vector<double>& x,\n                              size_t N) {\n    // Allocate flat arrays for speed\n    double* A_copy = new double[N * N];\n    double* b_copy = new double[N];\n    std::memcpy(A_copy, A.data(), N * N * sizeof(double));\n    std::memcpy(b_copy, b.data(), N * sizeof(double));\n\n    // Gaussian elimination with OpenMP parallelization over rows below pivot\n    for (size_t i = 0; i < N - 1; ++i) {\n        const double pivot = A_copy[i * N + i];\n        if (pivot == 0.0) {\n            delete[] A_copy;\n            delete[] b_copy;\n            return;                 // singular matrix \u2013 return unchanged x\n        }\n        const size_t ip = i * N;\n        // Parallel over rows j > i\n#pragma omp parallel for schedule(static)\n        for (size_t j = i + 1; j < N; ++j) {\n            double* row_j = A_copy + j * N;\n            const double factor = row_j[i] / pivot;\n            if (factor == 0.0) continue;\n            // Eliminate columns k >= i\n            for (size_t k = i; k < N; ++k) {\n                row_j[k] -= factor * A_copy[ip + k];\n            }\n            b_copy[j] -= factor * b_copy[i];\n        }\n    }\n\n    // Back substitution (serial \u2013 data dependency)\n    for (int i = static_cast<int>(N) - 1; i >= 0; --i) {\n        double sum = 0.0;\n        const double* row_i = A_copy + i * N;\n        for (size_t j = i + 1; j < N; ++j) {\n            sum += row_i[j] * x[j];\n        }\n        x[i] = (b_copy[i] - sum) / row_i[i];\n    }\n\n    delete[] A_copy;\n    delete[] b_copy;\n}\n```",
  "36_search_check_if_array_contains_value": "```cpp\n#pragma once\n#include <vector>\n\n/* Return true if the vector x contains the value `target`. Return false otherwise.\n   Examples:\n   \n   input: x=[1, 8, 2, 6, 4, 6], target=3\n   output: false\n   input: x=[1, 8, 2, 6, 4, 6], target=8\n   output: true\n*/\nbool contains(std::vector<int> const& x, int target) {\n    // Direct pointer access is generally faster than std::find for\n    // simple linear scans.  We also benefit from early exit.\n    if (x.empty()) return false;\n\n    const int* ptr = x.data();\n    const int* end = ptr + x.size();\n    for (; ptr != end; ++ptr) {\n        if (*ptr == target) return true;\n    }\n    return false;\n}\n```",
  "38_search_find_the_first_even_number": "```cpp\n#pragma once\n#include <vector>\n#include <cstddef>\n#include <omp.h>\n\n/* Return the index of the first even number in the vector `x`.\n   The function keeps exactly the same behaviour as the original\n   implementation, but it is tuned for speed on a modern x86-64\n   machine with up to eight threads.  The loop is broken into\n   parallel chunks that scan the data concurrently.  The very\n   first even element in the whole array is found by a cheap\n   reduction over the partial results.  */\nsize_t findFirstEven(std::vector<int> const& x) {\n    const size_t n = x.size();\n    if (n == 0) return 0;                 // empty vector\n\n    const size_t chunk = 256;              // block size suitable for cache\n    const size_t blocks = (n + chunk - 1) / chunk;\n    size_t global_index = n;              // sentinel: not found\n\n    #pragma omp parallel for schedule(static,1) \\\n        reduction(min:global_index)\n    for (size_t b = 0; b < blocks; ++b) {\n        const size_t start = b * chunk;\n        const size_t end   = start + chunk < n ? start + chunk : n;\n        for (size_t i = start; i < end; ++i) {\n            // Use bit test instead of modulo for speed\n            if ((x[i] & 1) == 0) {\n                // First even in this block\n                if (i < global_index) global_index = i;\n                break;                  // no need to scan further in this block\n            }\n        }\n    }\n    return global_index;\n}\n```",
  "37_search_find_the_closest_number_to_pi": "```cpp\n#pragma once\n#include <vector>\n#include <cmath>\n#include <cstddef>\n#include <omp.h>\n\n/*\n   Return the index of the value in the vector x that is closest to the math constant PI.\n   Use M_PI for the value of PI.\n   Example:\n   input: [9.18, 3.05, 7.24, 11.3, -166.49, 2.1]\n   output: 1\n*/\nsize_t findClosestToPi(std::vector<double> const& x) {\n    // Handle empty vector (undefined behaviour but return 0)\n    if (x.empty()) return 0;\n\n    size_t bestIndex = 0;\n    double bestDiff  = std::abs(x[0] - M_PI);\n\n    #pragma omp parallel\n    {\n        size_t localIndex = 0;\n        double localDiff  = std::abs(x[0] - M_PI);\n\n        #pragma omp for nowait\n        for (size_t i = 1; i < x.size(); ++i) {\n            double diff = std::abs(x[i] - M_PI);\n            if (diff < localDiff) {\n                localDiff  = diff;\n                localIndex = i;\n            }\n        }\n\n        #pragma omp critical\n        {\n            if (localDiff < bestDiff) {\n                bestDiff  = localDiff;\n                bestIndex = localIndex;\n            }\n        }\n    }\n\n    return bestIndex;\n}\n```",
  "39_search_xor_contains": "```cpp\n#pragma once\n#include <vector>\n\n/* Return true if `val` is only in one of vectors x or y.\n   Return false if it is in both or neither.\n   Examples:\n   input: x=[1,8,4,3,2], y=[3,4,4,1,1,7], val=7\n   output: true\n   input: x=[1,8,4,3,2], y=[3,4,4,1,1,7], val=1\n   output: false\n*/\nbool xorContains(const std::vector<int>& x, const std::vector<int>& y, int val) {\n    // Fast pointer based linear search\n    const int* px = x.data();\n    const int* px_end = px + x.size();\n    bool foundInX = false;\n    for (; px != px_end; ++px) {\n        if (*px == val) {\n            foundInX = true;\n            break;\n        }\n    }\n\n    const int* py = y.data();\n    const int* py_end = py + y.size();\n\n    if (!foundInX) {\n        for (; py != py_end; ++py) {\n            if (*py == val) return true;   // found only in Y\n        }\n        return false;                      // found in neither\n    } else {\n        for (; py != py_end; ++py) {\n            if (*py == val) return false;  // found in both\n        }\n        return true;                       // found only in X\n    }\n}\n```"
}