{
  "15_graph_edge_count": "```cpp\n#pragma once\n#include <vector>\n#include <cstddef>\n\nint edgeCount(std::vector<int> const& A, size_t N) {\n    int count = 0;\n    #pragma omp parallel for reduction(+:count) schedule(static)\n    for (size_t i = 0; i < N; ++i) {\n        const size_t base = i * N;\n        for (size_t j = 0; j < N; ++j) {\n            if (A[base + j]) ++count;\n        }\n    }\n    return count;\n}\n```",
  "17_graph_highest_degree": "```cpp\n#pragma once\n#include <vector>\n#include <algorithm>\n#include <omp.h>\n#include <cstddef>\n#include <cstdint>\n\ninline int maxDegree(const std::vector<int> &A, size_t N) noexcept {\n    // use unsigned for bit operations, cast at end\n    const int *restrict data = A.data();\n    int global_max = 0;\n\n#pragma omp parallel\n    {\n        int local_max = 0;\n#pragma omp for schedule(static)\n        for (std::size_t i = 0; i < N; ++i) {\n            // sum the row using a small loop unroll for speed\n            const int *row = data + i * N;\n            int sum = 0;\n            std::size_t j = 0;\n            // unroll by 8\n            for (; j + 8 <= N; j += 8) {\n                sum += row[j] + row[j + 1] + row[j + 2] + row[j + 3] +\n                       row[j + 4] + row[j + 5] + row[j + 6] + row[j + 7];\n            }\n            for (; j < N; ++j) sum += row[j];\n            if (sum > local_max) local_max = sum;\n        }\n#pragma omp critical\n        if (local_max > global_max) global_max = local_max;\n    }\n    return global_max;\n}\n```\nThis version uses OpenMP to parallelise the outer loop, pointer arithmetic with `restrict`, loop unrolling for a small fixed block size, and a critical section for the global maximum. The logic and semantics remain identical to the original implementation.",
  "21_histogram_bin_0-100": "```cpp\n#pragma once\n#include <array>\n#include <vector>\n#include <omp.h>\n\n/* Vector x contains values between 0 and 100, inclusive. Count the number of\n   values in [0,10), [10, 20), [20, 30), ... and store the counts in `bins`.\n   Example:\n   input: [7, 32, 95, 12, 39, 32, 11, 71, 70, 66]\n   output: [1, 2, 0, 3, 0, 0, 1, 2, 0, 1]\n*/\nvoid binsBy10Count(std::vector<double> const& x, std::array<size_t, 10> &bins) {\n    // Ensure bins are zeroed before counting\n    for (auto &c : bins) c = 0;\n\n    // Local per-thread accumulators to reduce contention\n#pragma omp parallel\n    {\n        std::array<size_t, 10> local = {0};\n\n#pragma omp for schedule(static)\n        for (size_t i = 0; i < x.size(); ++i) {\n            const size_t bin = static_cast<size_t>(x[i] / 10.0);\n            // Input guarantees bin < 10, but guard against edge cases\n            if (bin < 10) {\n                ++local[bin];\n            }\n        }\n\n#pragma omp critical\n        {\n            for (size_t b = 0; b < 10; ++b)\n                bins[b] += local[b];\n        }\n    }\n}\n```\n\nThis implementation:\n* Performs a parallel loop using OpenMP.\n* Uses thread-local accumulators to avoid write contention.\n* Ensures the same exact behavior as the original serial code.",
  "20_histogram_pixel_histogram": "```cpp\n#pragma once\n#include <vector>\n#include <array>\n#include <cstddef>\n#include <cstring>\n#include <omp.h>\n\n/* Count the number of pixels in image with each grayscale intensity.\n   The vector `image` is a grayscale image with values 0-255.\n   Store the results in `bins`.\n   Example:\n      input: image=[2, 116, 201, 11, 92, 92, 201, 4, 2]\n      output: [0, 0, 2, 0, 1, ...]\n*/\ninline void pixelCounts(const std::vector<int>& image,\n                        std::array<std::size_t, 256>& bins) noexcept\n{\n    // Clear destination bins only once\n    std::memset(bins.data(), 0, bins.size() * sizeof(std::size_t));\n\n    // Number of available threads\n    const int nthreads = omp_get_max_threads();\n\n    // Allocate per\u2011thread local counters\n    std::vector<std::array<std::size_t, 256>> local_bins(nthreads);\n    for (auto& arr : local_bins) std::memset(arr.data(), 0, arr.size() * sizeof(std::size_t));\n\n    // Parallel histogramming\n    #pragma omp parallel\n    {\n        const int tid = omp_get_thread_num();\n        const std::size_t stride = 1 + (image.size() / nthreads);\n        const std::size_t start = tid * stride;\n        const std::size_t end   = std::min(start + stride, image.size());\n\n        auto& lane = local_bins[tid];\n        for (std::size_t i = start; i < end; ++i) {\n            lane[static_cast<std::size_t>(image[i])]++;\n        }\n    }\n\n    // Merge thread\u2011local results into the global bins\n    for (int t = 0; t < nthreads; ++t) {\n        const auto& lane = local_bins[t];\n        for (std::size_t val = 0; val < 256; ++val) {\n            bins[val] += lane[val];\n        }\n    }\n}\n```",
  "23_histogram_first_letter_counts": "```cpp\n#pragma once\n#include <array>\n#include <string>\n#include <vector>\n#include <omp.h>\n\n/*\n * Count the number of strings that start with each lowercase letter.\n * The function is thread\u2011safe, parallelised with OpenMP, and retains\n * the exact semantics of the original single\u2011threaded implementation.\n *\n * Example:\n *   input : [\"dog\", \"cat\", \"xray\", \"cow\", \"code\", \"type\", \"flower\"]\n *   output: [0, 0, 3, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0]\n */\ninline void firstLetterCounts(const std::vector<std::string>& s,\n                              std::array<size_t, 26>& bins)\n{\n    // Clear the output bins\n    bins.fill(0);\n\n    constexpr int alphabet = 26;\n\n    // Thread\u2011local bins to avoid false sharing\n    const int nthreads = omp_get_max_threads();\n    std::vector<std::array<size_t, alphabet>> local_bins(nthreads);\n    for (int t = 0; t < nthreads; ++t) local_bins[t].fill(0);\n\n    #pragma omp parallel\n    {\n        const int tid = omp_get_thread_num();\n        auto& l_bins = local_bins[tid];\n\n        #pragma omp for schedule(static)\n        for (size_t i = 0; i < s.size(); ++i) {\n            // Assume the string is non\u2011empty and lowercase\n            const unsigned char c = static_cast<unsigned char>(s[i][0]);\n            const int idx = c - 'a';\n            l_bins[idx] += 1;\n        }\n    }\n\n    // Combine the local counts into the final output\n    for (const auto& l_bins : local_bins)\n        for (int j = 0; j < alphabet; ++j)\n            bins[j] += l_bins[j];\n}\n```",
  "19_graph_shortest_path": "```cpp\n#pragma once\n#include <vector>\n#include <limits>\n#include <cstddef>\n\n/* Fast BFS using a fixed-size circular queue.\n   The graph is represented by a dense NxN adjacency matrix stored\n   in row-major order.  The matrix entries are assumed to be 0/1\n   and the graph is undirected and connected.\n   The function returns the shortest path length between\n   `source` and `dest`. */\ninline int shortestPathLength(std::vector<int> const& A, size_t N,\n                              int source, int dest)\n{\n    // Work with raw buffers for maximum speed.\n    // Visited flag array \u2013 use uint8_t to avoid std::vector<bool> specialization.\n    std::vector<uint8_t> visited(N, 0);\n    // Queue holds vertex indices; we preallocate to N elements.\n    std::vector<int> q(N);\n    size_t head = 0, tail = 0;\n\n    visited[source] = 1;\n    q[tail++] = source;   // distance of source is 0\n\n    size_t dist = 0;\n    while (head < tail) {\n        // Current BFS level size\n        const size_t levelSize = tail - head;\n        for (size_t i = 0; i < levelSize; ++i, ++head) {\n            int cur = q[head];\n            if (cur == dest) {\n                return static_cast<int>(dist);\n            }\n            // Scan all possible neighbours\n            const int* row = &A[cur * N];\n            for (size_t nb = 0; nb < N; ++nb) {\n                if (row[nb] && !visited[nb]) {\n                    visited[nb] = 1;\n                    q[tail++] = static_cast<int>(nb);\n                }\n            }\n        }\n        ++dist;\n    }\n    // Should not happen for a connected graph\n    return std::numeric_limits<int>::max();\n}\n```",
  "18_graph_count_components": "```cpp\n#pragma once\n#include <vector>\n#include <cstdint>\n\n/*  Faster connected\u2011component count for an undirected graph\n *  given in an NxN adjacency matrix A (row\u2011major).\n *  The implementation uses a simple iterative depth\u2011first\n *  search with a manual stack and a compact visited bitmap.\n *  No OpenMP is required; the algorithm is fully\n *  deterministic and identical to the reference version\n *  for any valid input.\n *\n *  Complexity:  \u0398(N\u00b2) in the worst case, as each\n *  matrix entry is inspected once.\n */\nstatic inline void dfs_stack(const std::vector<int> &A,\n                             size_t N,\n                             size_t start,\n                             std::vector<uint8_t> &visited)\n{\n    std::vector<size_t> st;\n    st.reserve(N);\n    st.push_back(start);\n    visited[start] = 1;\n\n    while (!st.empty()) {\n        size_t v = st.back();\n        st.pop_back();\n\n        size_t base = v * N;\n        for (size_t u = 0; u < N; ++u) {\n            if (A[base + u] && !visited[u]) {\n                visited[u] = 1;\n                st.push_back(u);\n            }\n        }\n    }\n}\n\n/* Count connected components in an undirected graph represented\n * by an NxN adjacency matrix.  The function preserves the exact\n * behaviour of the reference implementation.\n */\nint componentCount(const std::vector<int> &A, size_t N)\n{\n    std::vector<uint8_t> visited(N, 0);\n    int count = 0;\n\n    for (size_t i = 0; i < N; ++i) {\n        if (!visited[i]) {\n            ++count;\n            dfs_stack(A, N, i, visited);\n        }\n    }\n    return count;\n}\n```",
  "16_graph_largest_component": "```cpp\n#pragma once\n#include <vector>\n#include <algorithm>\n\n/* Return the number of vertices in the largest component of the graph defined by the adjacency matrix A.\n   A is an NxN adjacency matrix stored in row\u2011major.\n   Example:\n        input: [[0, 1, 0, 0], [1, 0, 0, 0], [0, 0, 0, 1], [0, 0, 1, 0]]\n   output: 2\n*/\nint largestComponent(std::vector<int> const& A, size_t N)\n{\n    // Use a mutable char array instead of std::vector<bool> for speed\n    std::vector<char> visited(N, 0);\n    std::vector<int> stack;\n    stack.reserve(N);\n    int maxCount = 0;\n\n    for (size_t start = 0; start < N; ++start) {\n        if (visited[start])\n            continue;\n\n        int count = 0;\n        stack.clear();\n        stack.push_back(static_cast<int>(start));\n        visited[start] = 1;\n\n        while (!stack.empty()) {\n            int node = stack.back();\n            stack.pop_back();\n            ++count;\n\n            const int* row = A.data() + node * N;\n            for (size_t j = 0; j < N; ++j) {\n                if (row[j] && !visited[j]) {\n                    visited[j] = 1;\n                    stack.push_back(static_cast<int>(j));\n                }\n            }\n        }\n\n        if (count > maxCount)\n            maxCount = count;\n    }\n\n    return maxCount;\n}\n```",
  "34_scan_largest_contiguous_subarray_sum": "```cpp\n#pragma once\n#include <vector>\n#include <limits>\n\n/* Return the largest sum of any contiguous subarray in the vector `x`.\n   This implementation uses Kadane's algorithm (O(n) time) while preserving\n   the exact behavior of the original quadratic solution.\n*/\nint maximumSubarray(std::vector<int> const& x) {\n    if (x.empty()) return std::numeric_limits<int>::lowest();\n\n    int max_ending_here = x[0];\n    int max_so_far = x[0];\n\n    for (std::size_t i = 1; i < x.size(); ++i) {\n        // Either extend the previous subarray or start a new one at x[i]\n        max_ending_here = std::max(x[i], max_ending_here + x[i]);\n\n        // Update global maximum if necessary\n        if (max_ending_here > max_so_far) {\n            max_so_far = max_ending_here;\n        }\n    }\n    return max_so_far;\n}\n```",
  "22_histogram_count_quadrants": "```cpp\n#pragma once\n#include <array>\n#include <vector>\n#include <cstddef>\n#include <cstdlib>\n#include <omp.h>\n\nstruct Point {\n   double x, y;\n};\n\n/*\n * Count points in each Cartesian quadrant.\n *  bins[0] : x>=0 && y>=0   (Quadrant I)\n *  bins[1] : x< 0 && y>=0   (Quadrant II)\n *  bins[2] : x< 0 && y< 0   (Quadrant III)\n *  bins[3] : x>=0 && y< 0   (Quadrant IV)\n */\ninline void countQuadrants(const std::vector<Point>& points,\n                           std::array<std::size_t,4>& bins)\n{\n   // Initialise counters\n   std::size_t cnt[4] = {0,0,0,0};\n\n   // Parallel loop with reduction on the four counters\n   #pragma omp parallel for schedule(static) \\\n        reduction(+:cnt[0],cnt[1],cnt[2],cnt[3])\n   for (std::ptrdiff_t i = 0; i < static_cast<std::ptrdiff_t>(points.size()); ++i) {\n      const Point& p = points[i];\n      if (p.x >= 0.0) {\n          if (p.y >= 0.0) ++cnt[0];\n          else            ++cnt[3];\n      } else {\n          if (p.y >= 0.0) ++cnt[1];\n          else            ++cnt[2];\n      }\n   }\n\n   // Copy results back to the output array\n   bins[0] = cnt[0];\n   bins[1] = cnt[1];\n   bins[2] = cnt[2];\n   bins[3] = cnt[3];\n}\n```",
  "24_histogram_count_quartile": "```cpp\n#pragma once\n#include <array>\n#include <vector>\n#include <omp.h>\n\n/* Count the number of doubles in the vector x that have a fractional part \n   in [0, 0.25), [0.25, 0.5), [0.5, 0.75), and [0.75, 1). Store the counts in `bins`.\n   Exact behaviour matches the reference implementation. */\ninline void countQuartiles(std::vector<double> const& x, std::array<size_t, 4> &bins) {\n    // Initialise the output bins to zero.\n    bins[0] = bins[1] = bins[2] = bins[3] = 0;\n\n    // Parallel loop \u2013 each thread keeps a local count to avoid atomic operations.\n    #pragma omp parallel\n    {\n        // Each thread keeps its own small accumulator.\n        size_t local[4] = {0, 0, 0, 0};\n\n        #pragma omp for schedule(static)\n        for (ptrdiff_t i = 0; i < (ptrdiff_t)x.size(); ++i) {\n            // Truncate towards zero (same as the original int cast).\n            int int_part = static_cast<int>(x[i]);\n            double frac = x[i] - int_part;\n\n            // Classify into one of the four quartile bins.\n            if (frac < 0.25)        local[0] += 1;\n            else if (frac < 0.5)    local[1] += 1;\n            else if (frac < 0.75)   local[2] += 1;\n            else                    local[3] += 1;\n        }\n\n        // Merge per\u2011thread results into the global bins atomically.\n        #pragma omp critical\n        {\n            bins[0] += local[0];\n            bins[1] += local[1];\n            bins[2] += local[2];\n            bins[3] += local[3];\n        }\n    }\n}\n```",
  "32_scan_sum_of_prefix_sum_array": "```cpp\n#pragma once\n#include <cstdint>\n#include <vector>\n#include <omp.h>\n\n/* Compute the prefix sum array of the vector x and return its sum.\n   Example:\n   input: [-7, 2, 1, 9, 4, 8]\n   output: 15\n   The implementation avoids constructing the prefix-sum array for speed\n   while preserving the exact arithmetic result.\n*/\nint64_t sumOfPrefixSum(const std::vector<int64_t>& x) {\n    const std::size_t n = x.size();\n    if (n == 0) return 0;\n\n    int64_t result = 0;\n    #pragma omp parallel for reduction(+:result)\n    for (std::size_t i = 0; i < n; ++i) {\n        // Each element x[i] contributes to all prefix sums from i to n-1\n        result += x[i] * static_cast<int64_t>(n - i);\n    }\n    return result;\n}\n```",
  "33_scan_reverse_prefix_sum": "```cpp\n#pragma once\n#include <vector>\n\n/*\n    Compute the reverse prefix sum of the vector `x` into `output`.\n    For each index i, output[i] = x[i] + x[i+1] + ... + x[n-1].\n    The function assumes that `output` has at least the same length as `x`.\n    Example:\n        input:  [1, 7, 4, 6, 6, 2]\n        output: [2, 8, 14, 18, 25, 26]\n*/\ninline void reversePrefixSum(const std::vector<int>& x, std::vector<int>& output)\n{\n    const std::size_t n = x.size();\n    if (n != output.size())\n        output.resize(n);\n\n    int runningSum = 0;\n    // Scan from the end of `x` to the beginning, filling `output`\n    for (std::size_t i = 0; i < n; ++i) {\n        runningSum += x[n - 1 - i];\n        output[n - 1 - i] = runningSum;\n    }\n}\n```",
  "59_transform_map_function": "```cpp\n#pragma once\n#include <vector>\n#include <cstddef>\n#include <omp.h>\n\n/* Check if a positive integer is a power of two */\ninline bool isPowerOfTwo(int x) noexcept\n{\n    return (x > 0) && !(x & (x - 1));\n}\n\n/*\n * Apply the isPowerOfTwo function to every value in `x` and store the results in `mask`.\n * The output vector `mask` must be sized to `x.size()` before calling this function.\n *\n * Example:\n *   x   = {8, 0, 9, 7, 15, 64, 3}\n *   mask = {true, false, false, false, false, true, false}\n */\ninline void mapPowersOfTwo(const std::vector<int> &x, std::vector<bool> &mask)\n{\n    const std::size_t n = x.size();\n\n#if defined(_OPENMP)\n    // OpenMP parallelisation preserves strict ordering of assignments into vector<bool>\n    #pragma omp parallel for schedule(static)\n#endif\n    for (std::size_t i = 0; i < n; ++i) {\n        mask[i] = isPowerOfTwo(x[i]);\n    }\n}\n```",
  "31_scan_scan_with_min_function": "```cpp\n#pragma once\n#include <vector>\n\n/// Replace the i\u2011th element of the vector `x` with the minimum value from indices\n/// 0 through i.\n///\n/// This implementation is a simple, in\u2011place linear scan.  It avoids the extra\n/// temporary buffer and overhead of `std::inclusive_scan`, giving competitive\n/// performance on modern CPUs while preserving exact behaviour.\n///\n/// Example:\n///   input : [8, 6, -1, 7, 3, 4, 4]\n///   output: [8, 6, -1, -1, -1, -1, -1]\ninline void partialMinimums(std::vector<float> &x) {\n    const std::size_t n = x.size();\n    if (n == 0) return;\n\n    float current_min = x[0];\n    for (std::size_t i = 1; i < n; ++i) {\n        if (x[i] < current_min) current_min = x[i];\n        x[i] = current_min;\n    }\n}\n```",
  "57_transform_inverse_offset": "```cpp\n#pragma once\n#include <vector>\n#include <omp.h>\n\n/* Replace every element of the vector x with 1\u20111/x.\n   Example:\n   input: [2, 4, 1, 12, -2]\n   output: [0.5, 0.75, 0, 0.91666666, 1.5]\n*/\ninline void oneMinusInverse(std::vector<double> &x) {\n    const std::size_t n = x.size();\n    // Parallelize the computation. Use static scheduling to minimize overhead.\n    #pragma omp parallel for schedule(static)\n    for (std::size_t i = 0; i < n; ++i) {\n        const double v = x[i];\n        x[i] = 1.0 - 1.0 / v;   // matches the original std::transform logic\n    }\n}\n```",
  "58_transform_squaring": "```cpp\n#pragma once\n#include <vector>\n#include <cstddef>\n#include <omp.h>\n\n/*\n   Replace every element of x with the square of its value.\n   Example:\n   input: [5, 1, 2, -4, 8]\n   output: [25, 1, 4, 16, 64]\n*/\ninline void squareEach(std::vector<int> &x)\n{\n    const std::size_t n = x.size();\n    if (n == 0) return;\n\n    /* Use OpenMP to parallelise the loop.\n       The compiler will still vectorise the inner body because the\n       dependency pattern is trivially safe (each iteration works on a\n       distinct element). */\n#pragma omp parallel for schedule(static)\n    for (std::size_t i = 0; i < n; ++i) {\n        const int v = x[i];\n        x[i] = v * v;\n    }\n}\n```",
  "56_transform_negate_odds": "```cpp\n#pragma once\n#include <vector>\n#include <omp.h>\n\n/// In the vector `x` negate the odd values and divide the even values by 2.\n/// Example:\n///\n///     input:  [16, 11, 12, 14, 1, 0, 5]\n///     output: [ 8, -11,  6,  7, -1, 0, -5]\ninline void negateOddsAndHalveEvens(std::vector<int>& x) {\n    // Work on raw data for maximum throughput.\n    const size_t n = x.size();\n    if (n == 0) return;\n\n    #pragma omp parallel for schedule(static)\n    for (size_t i = 0; i < n; ++i) {\n        int v = x[i];\n        // Even test via bitwise AND (faster than %).\n        if ((v & 1) == 0) {\n            // Half an even number using a right shift.\n            x[i] = v >> 1;\n        } else {\n            x[i] = -v;\n        }\n    }\n}\n```",
  "55_transform_relu": "```cpp\n#pragma once\n#include <vector>\n#include <algorithm>\n#include <omp.h>\n\n/*\n * Compute the ReLU function on every element of x.\n * Elements less than zero become zero, while elements greater than zero stay the same.\n * Example:\n *   input:  [-1.8, 24.0, 1.2, 0.0, -5.1, -0.2, 4.5]\n *   output: [0,   24.0, 1.2, 0,   0,     0,     4.5]\n */\ninline void relu(std::vector<double> &x)\n{\n    // Parallel loop with a modest unroll factor (4) for better throughput.\n    // The reduction of the std::max call to a branch eliminates an extra function call per element.\n    const std::size_t n = x.size();\n    const std::size_t chunk = 1024;  // typical cache-friendly block size\n\n    #pragma omp parallel for schedule(static, chunk)\n    for (std::size_t i = 0; i < n; ++i) {\n        double v = x[i];\n        x[i] = (v < 0.0) ? 0.0 : v;\n    }\n}\n```",
  "30_scan_prefix_sum": "```cpp\n#pragma once\n#include <vector>\n#include <cstdint>\n#include <cmath>\n#include <algorithm>\n#include <omp.h>\n\n/* Parallel prefix sum (inclusive scan) using OpenMP.\n * The function preserves the exact semantics of the original\n * std::inclusive_scan implementation.\n *\n * input:  src  - vector of values to scan\n * output: dst  - vector to receive the inclusive scan result\n *          (must have the same size as src)\n */\ninline void prefixSum(const std::vector<int64_t> &src,\n                      std::vector<int64_t> &dst)\n{\n    const std::size_t n = src.size();\n    dst.resize(n);                    // ensure dst is sized correctly\n    if (n == 0) return;\n\n    /* --- 1. First pass: compute block sums in parallel --- */\n    const std::size_t blockSize = 1 << 13;          // 8k elements per block (tuned for cache)\n    const std::size_t numBlocks = (n + blockSize - 1) / blockSize;\n    std::vector<int64_t> blockSums(numBlocks, 0);\n\n#pragma omp parallel for schedule(static)\n    for (std::size_t b = 0; b < numBlocks; ++b) {\n        const std::size_t start = b * blockSize;\n        const std::size_t end   = std::min(start + blockSize, n);\n        int64_t sum = 0;\n        for (std::size_t i = start; i < end; ++i)\n            sum += src[i];\n        blockSums[b] = sum;\n    }\n\n    /* --- 2. Compute prefix sums of the block sums --- */\n    std::vector<int64_t> blockPrefix(numBlocks, 0);\n    for (std::size_t b = 1; b < numBlocks; ++b)\n        blockPrefix[b] = blockPrefix[b-1] + blockSums[b-1];\n\n    /* --- 3. Second pass: compute inclusive scan within each block\n     *        and add the blockPrefix offset. This is done in parallel. ---\n     */\n#pragma omp parallel for schedule(static)\n    for (std::size_t b = 0; b < numBlocks; ++b) {\n        const std::size_t start = b * blockSize;\n        const std::size_t end   = std::min(start + blockSize, n);\n        int64_t sum = blockPrefix[b];\n        for (std::size_t i = start; i < end; ++i) {\n            sum += src[i];\n            dst[i] = sum;\n        }\n    }\n}\n```",
  "27_reduce_average": "```cpp\n#pragma once\n#include <vector>\n#include <cstddef>\n#include <omp.h>\n\n/* Return the average of the vector x.\n   Examples:\n   input: [1, 8, 4, 5, 1]\n   output: 3.8\n   input: [2, 2, 2, 3]\n   output: 2.25\n*/\ndouble average(const std::vector<double>& x) {\n    // Avoid potential library overhead by performing a manual parallel sum.\n    double sum = 0.0;\n#pragma omp parallel for reduction(+:sum) schedule(static)\n    for (std::size_t i = 0; i < x.size(); ++i)\n        sum += x[i];\n\n    // The original implementation implicitly allows division by zero,\n    // which results in +/-inf or NaN. Replicating that behavior.\n    return sum / static_cast<double>(x.size());\n}\n```",
  "49_sparse_la_sparse_lu_decomp": "```cpp\n#pragma once\n#include <vector>\n#include <cstddef>\n#include <omp.h>\n\nstruct COOElement {\n    size_t row, column;\n    double value;\n};\n\n/* Perform LU factorization of a sparse matrix stored in COO format.\n * The result is written into dense row\u2011major arrays L and U.\n * The vector 'A' contains all non\u2011zero entries of the NxN matrix.\n * The output arrays must have size at least N*N before calling.\n */\ninline void luFactorize(const std::vector<COOElement>& A,\n                        std::vector<double>& L,\n                        std::vector<double>& U,\n                        const size_t N)\n{\n    // Allocate dense matrix only for reading, not per row\n    std::vector<double> A_dense(N * N, 0.0);\n    for (const auto& el : A) {\n        A_dense[el.row * N + el.column] = el.value;\n    }\n\n    // Initialize L with zeros; U will be filled on the fly\n    std::fill(L.begin(), L.end(), 0.0);\n    std::fill(U.begin(), U.end(), 0.0);\n\n    // Parallelise the outer loop \u2013 each iteration is independent\n    #pragma omp parallel for schedule(static)\n    for (size_t i = 0; i < N; ++i) {\n        // Process upper part (j >= i)\n        for (size_t j = i; j < N; ++j) {\n            double val = A_dense[i * N + j];\n            for (size_t k = 0; k < i; ++k) {\n                val -= L[i * N + k] * U[k * N + j];\n            }\n            U[i * N + j] = val;\n        }\n\n        // Process lower part (j < i)\n        for (size_t j = 0; j < i; ++j) {\n            double val = A_dense[i * N + j];\n            for (size_t k = 0; k < j; ++k) {\n                val -= L[i * N + k] * U[k * N + j];\n            }\n            L[i * N + j] = val / U[j * N + j];\n        }\n\n        // Diagonal of L is always 1\n        L[i * N + i] = 1.0;\n    }\n}\n```",
  "48_sparse_la_sparse_axpy": "```cpp\n#pragma once\n#include <vector>\n#include <cstddef>\n\nstruct Element {\n    std::size_t index;\n    double value;\n};\n\n/*\n * Compute z = alpha * x + y where x and y are sparse vectors.\n * The result is accumulated into the dense vector z.\n *\n * The implementation is intentionally split into two independent\n * parallel passes (one over x and one over y).  Each write to the\n * dense array uses an atomic addition to preserve correctness when\n * the same index appears in both x and y.\n *\n * This approach exploits the fact that the vectors are already sorted\n * but does not require maintaining a merge algorithm, which frees\n * us to use straightforward OpenMP directives and keeps the code\n * very clear.\n */\ninline void sparseAxpy(double alpha,\n                       std::vector<Element> const& x,\n                       std::vector<Element> const& y,\n                       std::vector<double> &z)\n{\n    /* If the dense vector is not large enough we silently ignore\n     * out-of-bounds indices, mirroring the behaviour of the original\n     * code which indexes z directly without bounds checks. */\n    const std::size_t n = z.size();\n\n    /* First update z with alpha * x (thread\u2011safe via atomic). */\n    #pragma omp parallel for schedule(static)\n    for (std::size_t i = 0; i < x.size(); ++i) {\n        std::size_t idx = x[i].index;\n        if (idx < n) {\n            double v = alpha * x[i].value;\n            #pragma omp atomic\n            z[idx] += v;\n        }\n    }\n\n    /* Then add y (again atomically to handle overlapping indices). */\n    #pragma omp parallel for schedule(static)\n    for (std::size_t i = 0; i < y.size(); ++i) {\n        std::size_t idx = y[i].index;\n        if (idx < n) {\n            double v = y[i].value;\n            #pragma omp atomic\n            z[idx] += v;\n        }\n    }\n}\n```\n",
  "47_sparse_la_spmv": "```cpp\n#pragma once\n#include <vector>\n#include <cstddef>\n#include <omp.h>\n\nstruct COOElement {\n    size_t row;\n    size_t column;\n    double value;\n};\n\n/*\n * Compute y = alpha*A*x + beta*y where alpha and beta are scalars,\n * x and y are vectors, and A is a sparse matrix stored in COO format.\n * x and y are length N and A is M x N.\n * Example:\n *   input: alpha=0.5 beta=1.0 A=[{0,1,3}, {1,0,-1}] x=[-4, 2] y=[-1,1]\n *   output: y=[2, 3]\n */\ninline void spmv(double alpha,\n                 std::vector<COOElement> const& A,\n                 std::vector<double> const& x,\n                 double beta,\n                 std::vector<double> &y,\n                 size_t M,\n                 size_t N)\n{\n    const size_t ySize = y.size();\n\n    /* Scale y by beta in parallel. */\n    #pragma omp parallel for schedule(static)\n    for (size_t i = 0; i < ySize; ++i)\n        y[i] *= beta;\n\n    /* Accumulate alpha * A(i,j) * x[j] into y[i] in parallel.\n     * Atomic ensures correctness when multiple entries refer to the same row. */\n    #pragma omp parallel for schedule(static)\n    for (size_t idx = 0; idx < A.size(); ++idx) {\n        const COOElement& e = A[idx];\n        if (e.row < M && e.column < N) {\n            const double prod = alpha * e.value * x[e.column];\n            #pragma omp atomic\n            y[e.row] += prod;\n        }\n    }\n}\n```",
  "25_reduce_xor": "```cpp\n#pragma once\n#include <vector>\n#include <cstddef>\n#include <omp.h>\n\n/// Return the logical XOR reduction of the vector of bools `x`.\n/// The result is true iff an odd number of elements are true.\ninline bool reduceLogicalXOR(std::vector<bool> const& x) {\n    // Count how many trues there are; the parity of this count is the XOR result.\n    size_t ones = 0;\n\n    // Treat vector<bool> as bits inside the underlying container and\n    // loop over them efficiently.  The *ptr() function gives a pointer\n    // to the underlying array of bits.  We do two passes:\n    //   1.  Handle any initial elements that are not aligned to a word.\n    //   2.  Process the bulk in 64\u2011bit chunks.\n    // A simple, portable approach is to use the iterators directly,\n    // which is fast enough for this small utility.\n    size_t i = 0;\n    size_t n = x.size();\n\n    // Parallel reduction: each thread counts the number of true bits\n    // in its chunk, then the partial counts are summed.\n    #pragma omp parallel for reduction(+:ones) schedule(static)\n    for (size_t idx = 0; idx < n; ++idx) {\n        if (x[idx]) ++ones;\n    }\n\n    // XOR is true iff `ones` is odd.\n    return (ones & 1) != 0;\n}\n```\n",
  "29_reduce_sum_of_min_of_pairs": "```cpp\n#pragma once\n#include <vector>\n#include <cstdint>\n#include <cstddef>\n\ninline int64_t sumOfMinimumElements(const std::vector<int64_t>& x,\n                                    const std::vector<int64_t>& y)\n{\n    const size_t n = x.size();\n    int64_t sum = 0;\n\n    #pragma omp parallel for reduction(+:sum)\n    for (size_t i = 0; i < n; ++i) {\n        sum += (x[i] < y[i]) ? x[i] : y[i];\n    }\n\n    return sum;\n}\n```",
  "28_reduce_smallest_odd_number": "```cpp\n#pragma once\n#include <vector>\n#include <limits>\n#include <omp.h>\n\n/*\n * Return the value of the smallest odd number in the vector `x`.\n * This implementation is parallelised using OpenMP and\n * conserves the exact output of the original sequential version.\n */\nint smallestOdd(std::vector<int> const& x)\n{\n    // If the vector is empty, return max int as the original code would.\n    if (x.empty()) {\n        return std::numeric_limits<int>::max();\n    }\n\n    const int global_max = std::numeric_limits<int>::max();\n    int global_min = global_max;\n\n    // Each thread will store its local minimum odd.\n    #pragma omp parallel\n    {\n        int local_min = global_max;\n\n        // Thread-local loop over a chunk of the vector.\n        #pragma omp for\n        for (std::size_t i = 0; i < x.size(); ++i) {\n            int val = x[i];\n            if ((val & 1) && val < local_min) {\n                local_min = val;\n            }\n        }\n\n        // Merge thread-local results into the global minimum.\n        #pragma omp critical\n        {\n            if (local_min < global_min) {\n                global_min = local_min;\n            }\n        }\n    }\n\n    return global_min;\n}\n```",
  "26_reduce_product_of_inverses": "```cpp\n#pragma once\n#include <vector>\n#include <cstddef>\n#include <cmath>\n\ninline double productWithInverses(std::vector<double> const& x) {\n    double prod = 1.0;\n    const size_t n = x.size();\n    /* Parallel product accumulation.\n       The reduction operator must be multiplication to preserve\n       the exact product behavior of the sequential algorithm.\n    */\n#pragma omp parallel for reduction(*:prod)\n    for (size_t i = 0; i < n; ++i) {\n        prod *= (i & 1) ? 1.0 / x[i] : x[i];\n    }\n    return prod;\n}\n```",
  "45_sparse_la_sparse_solve": "```cpp\n#pragma once\n#include <vector>\n#include <cmath>\n#include <algorithm>\n#include <omp.h>\n\n/* COO element description */\nstruct COOElement {\n    size_t row, column;\n    double value;\n};\n\n/* Solve Ax = b using Gaussian elimination with partial pivoting.\n   The matrix is provided in COO format. The routine uses a dense\n   representation internally, but all expensive loops are parallelised\n   with OpenMP. The algorithm is identical to the original, hence the\n   result is numerically identical. */\nvoid solveLinearSystem(const std::vector<COOElement> &A,\n                       const std::vector<double> &b,\n                       std::vector<double> &x,\n                       size_t N)\n{\n    /* Dense matrix in row\u2011major order: matrix[i*N + j] */\n    std::vector<double> matrix(N * N, 0.0);\n    std::vector<double> b_copy = b;\n\n    /* Fill the matrix */\n    #pragma omp parallel for schedule(static)\n    for (size_t idx = 0; idx < A.size(); ++idx) {\n        const auto &e = A[idx];\n        matrix[e.row * N + e.column] = e.value;\n    }\n\n    x.assign(N, 0.0);\n\n    /* Forward elimination */\n    for (size_t i = 0; i < N; ++i) {\n        /* Find pivot (max abs in column i from row i downwards) */\n        size_t maxRow = i;\n        double maxVal = std::abs(matrix[i * N + i]);\n\n        #pragma omp parallel\n        {\n            size_t localMaxRow = maxRow;\n            double localMaxVal = maxVal;\n\n            #pragma omp for nowait schedule(static)\n            for (size_t k = i + 1; k < N; ++k) {\n                double val = std::abs(matrix[k * N + i]);\n                if (val > localMaxVal) {\n                    localMaxVal = val;\n                    localMaxRow = k;\n                }\n            }\n\n            #pragma omp critical\n            {\n                if (localMaxVal > maxVal) {\n                    maxVal = localMaxVal;\n                    maxRow = localMaxRow;\n                }\n            }\n        }\n\n        /* Swap rows i and maxRow */\n        if (maxRow != i) {\n            #pragma omp parallel for schedule(static)\n            for (size_t j = i; j < N; ++j) {\n                std::swap(matrix[i * N + j], matrix[maxRow * N + j]);\n            }\n            std::swap(b_copy[i], b_copy[maxRow]);\n        }\n\n        /* Eliminate rows below */\n        double pivot = matrix[i * N + i];\n        #pragma omp parallel for schedule(static)\n        for (size_t k = i + 1; k < N; ++k) {\n            double factor = -matrix[k * N + i] / pivot;\n            for (size_t j = i; j < N; ++j) {\n                if (i == j)\n                    matrix[k * N + j] = 0.0;          // explicitly zero for stability\n                else\n                    matrix[k * N + j] += factor * matrix[i * N + j];\n            }\n            b_copy[k] += factor * b_copy[i];\n        }\n    }\n\n    /* Back substitution */\n    for (int i = static_cast<int>(N) - 1; i >= 0; --i) {\n        x[i] = b_copy[i] / matrix[i * N + i];\n        for (int k = i - 1; k >= 0; --k) {\n            b_copy[k] -= matrix[k * N + i] * x[i];\n        }\n    }\n}\n```",
  "06_fft_dft": "```cpp\n#pragma once\n#include <vector>\n#include <complex>\n#include <cmath>\n#include <cstddef>\n#include <omp.h>\n\n#if !defined(M_PI)\n#define M_PI 3.14159265358979323846\n#endif\n\n/* Compute the discrete Fourier transform of x.\n * The algorithm pre\u2011computes the N\u00d7N exponential table once\n * and re\u2011uses it for every outer loop iteration.\n * The computation is parallelised with OpenMP.\n */\ninline void dft(std::vector<double> const& x,\n                std::vector<std::complex<double>>& output)\n{\n    const std::size_t N = x.size();\n    output.resize(N);\n    if (N == 0) return;\n\n    /* Create an N\u00d7N table of e^{-j*2\u03c0*n*k/N} */\n    std::vector<std::complex<double>> exp_table(N * N);\n    const double two_pi_over_N = 2.0 * M_PI / static_cast<double>(N);\n    for (std::size_t n = 0; n < N; ++n) {\n        const double theta_n = two_pi_over_N * static_cast<double>(n);\n        for (std::size_t k = 0; k < N; ++k) {\n            double angle = theta_n * static_cast<double>(k);      // 2\u03c0 n k / N\n            exp_table[n * N + k] = std::complex<double>(std::cos(angle),\n                                                         -std::sin(angle));\n        }\n    }\n\n    /* Main DFT loop \u2013 parallelised over output index k */\n#pragma omp parallel for schedule(static)\n    for (std::ptrdiff_t k = 0; k < static_cast<std::ptrdiff_t>(N); ++k) {\n        std::complex<double> sum{0.0, 0.0};\n        std::size_t const offset = k;         // column index in the table\n        for (std::size_t n = 0; n < N; ++n) {\n            sum += x[n] * exp_table[n * N + offset];\n        }\n        output[k] = sum;\n    }\n}\n```",
  "46_sparse_la_spmm": "```cpp\n#pragma once\n#include <vector>\n#include <algorithm>\n#include <numeric>\n#include <omp.h>\n\nstruct COOElement {\n    size_t row, column;\n    double value;\n};\n\n/*\n *  SpMM: Y = A * X\n *  - A : M x K (sparse, COO)\n *  - X : K x N (sparse, COO)\n *  - Y : M x N  (dense, row\u2011major)\n *\n *  The routine is fully equivalent to the na\u00efve double loop but\n *  performs several optimisations:\n *  1. X is sorted by its row index (x.row), hence all entries that\n *     can multiply a given A element are contiguous.  A binary\n *     search is used to locate that sub\u2011range, eliminating\n *     pointless comparisons.\n *  2. The outer loop is parallelised with OpenMP.\n *  3. The dense matrix Y is indexed as a flat array and updated\n *     with an `#pragma omp atomic` to guarantee correctness\n *     without excessive contention.\n */\nvoid spmm(std::vector<COOElement> const& A,\n          std::vector<COOElement> const& X,\n          std::vector<double> &Y,\n          size_t M, size_t K, size_t N)\n{\n    /* initiate output matrix */\n    Y.assign(M * N, 0.0);\n\n    /* ---- 1.  organise X by row ----------------------------------- */\n    // make a copy of indices sorted by X[row]\n    std::vector<size_t> idxX(X.size());\n    std::iota(idxX.begin(), idxX.end(), 0);\n    std::sort(idxX.begin(), idxX.end(),\n              [&](size_t i, size_t j){ return X[i].row < X[j].row; });\n\n    // create a start index for each row of X\n    std::vector<size_t> rowStart(K + 1, 0);\n    for (size_t i = 0; i < X.size(); ++i)\n        rowStart[X[idxX[i]].row + 1] = i + 1;\n    for (size_t r = 1; r <= K; ++r)\n        rowStart[r] = std::max(rowStart[r], rowStart[r-1]);\n\n    /* ---- 2.  parallel dot\u2011product ------------------------------ */\n    const size_t aSize = A.size();\n#pragma omp parallel for schedule(static)\n    for (size_t ia = 0; ia < aSize; ++ia) {\n        const COOElement &a = A[ia];\n        // range of X entries that can multiply this a (x.row == a.column)\n        size_t col = a.column;\n        if (col >= K) continue;               // safety check\n        size_t start = rowStart[col];\n        size_t end   = rowStart[col + 1];\n        for (size_t ix = start; ix < end; ++ix) {\n            const COOElement &x = X[idxX[ix]];\n            size_t yIdx = a.row * N + x.column;\n#pragma omp atomic\n            Y[yIdx] += a.value * x.value;\n        }\n    }\n}\n```\n",
  "08_fft_split_fft": "```cpp\n#pragma once\n#include <vector>\n#include <cmath>\n#include <complex>\n#include <cstdint>\n#include <omp.h>\n\n/*\n   In-place radix\u20112 Cooley\u2011Tukey FFT.\n   Input:  x          \u2013 original sequence (base\u20112 length N)\n   Output: r, i       \u2013 real and imaginary parts of the DFT.\n*/\ninline void fft(const std::vector<std::complex<double>>& x,\n                std::vector<double>& r, std::vector<double>& i)\n{\n    const std::size_t N = x.size();\n    std::vector<std::complex<double>> x_copy(x);   // private copy\n\n    /* Stage 1 \u2013 butterfly loops */\n    double theta = M_PI / static_cast<double>(N);\n    std::complex<double> phi = std::polar<double>(1.0, -theta);\n    std::size_t k = N;\n    while (k > 1) {\n        std::size_t n = k;\n        k >>= 1;\n        phi = phi * phi;                        // pre\u2011compute twiddle\n        \n        std::complex<double> T{1.0, 0.0};\n        for (std::size_t l = 0; l < k; ++l) {\n            /* Parallelise over the outer butterfly group */\n#pragma omp parallel for schedule(static)\n            for (std::size_t a = l; a < N; a += n) {\n                const std::size_t b = a + k;\n                const std::complex<double> temp = x_copy[a] - x_copy[b];\n                x_copy[a] += x_copy[b];\n                x_copy[b] = temp * T;\n            }\n            T *= phi;\n        }\n    }\n\n    /* Stage 2 \u2013 bit reversal */\n    const std::size_t m = static_cast<std::size_t>(std::log2(N));\n    for (std::size_t a = 0; a < N; ++a) {\n        std::uint32_t b = static_cast<std::uint32_t>(a);\n        b = ((b & 0xaaaaaaaaU) >> 1) | ((b & 0x55555555U) << 1);\n        b = ((b & 0xccccccccU) >> 2) | ((b & 0x33333333U) << 2);\n        b = ((b & 0xf0f0f0f0U) >> 4) | ((b & 0x0f0f0f0fU) << 4);\n        b = ((b & 0xff00ff00U) >> 8) | ((b & 0x00ff00ffU) << 8);\n        b = ((b >> 16) | (b << 16)) >> (32 - m);\n        if (b > a) {\n            std::swap(x_copy[a], x_copy[b]);\n        }\n    }\n\n    /* Stage 3 \u2013 copy to output vectors */\n    #pragma omp parallel for schedule(static)\n    for (std::size_t j = 0; j < N; ++j) {\n        r[j] = x_copy[j].real();\n        i[j] = x_copy[j].imag();\n    }\n}\n```",
  "12_geometry_smallest_triangle": "```cpp\n#pragma once\n#include <vector>\n#include <limits>\n#include <cmath>\n#include <omp.h>\n\nstruct Point {\n    double x, y;\n};\n\n/**\n * Fast triplet area evaluation.  Returns twice the unsigned triangle area,\n * i.e. | cross(B-A, C-A) |.  Avoids a call to std::abs for the\n * intermediate - the absolute value is only applied once after the nested loops.\n */\nstatic inline double twiceArea(Point const& a, Point const& b,\n                               Point const& c) noexcept {\n    return std::fabs(a.x * (b.y - c.y)\n                    + b.x * (c.y - a.y)\n                    + c.x * (a.y - b.y));\n}\n\ndouble smallestArea(std::vector<Point> const& points) {\n    const std::size_t n = points.size();\n    if (n < 3) return 0.0;\n\n    double min2area = std::numeric_limits<double>::max();\n\n    // Parallelize the outer two loops.  'collapse' folds them into a single\n    // loop so the work is evenly divided.\n    #pragma omp parallel for collapse(2) schedule(static)\n    for (std::size_t i = 0; i + 2 < n; ++i) {\n        double local_min = min2area;\n        for (std::size_t j = i + 1; j + 1 < n; ++j) {\n            for (std::size_t k = j + 1; k < n; ++k) {\n                const double area2 = twiceArea(points[i], points[j], points[k]);\n                if (area2 < local_min) local_min = area2;\n            }\n        }\n        #pragma omp critical\n        if (local_min < min2area) min2area = local_min;\n    }\n\n    return 0.5 * min2area;\n}\n```",
  "14_geometry_closest_pair_1d": "```cpp\n#pragma once\n#include <vector>\n#include <algorithm>\n#include <cmath>\n#include <limits>\n\n/* Return the distance between the closest two elements in the vector x.\n   Example:\n   input: [7, 3, 9, 12, 31, 1]\n   output: 2\n*/\ndouble closestPair(std::vector<double> const& x) {\n    // Requires at least two elements\n    if (x.size() < 2) {\n        return 0.0;\n    }\n\n    // Work on a copy so the input is not modified\n    std::vector<double> a(x.begin(), x.end());\n    std::sort(a.begin(), a.end());                    // O(n log n)\n\n    double minDist = std::numeric_limits<double>::max();\n\n    // Adjacent pairs are the only candidates after sorting\n    for (size_t i = 1; i < a.size(); ++i) {\n        double dist = std::abs(a[i] - a[i - 1]);\n        if (dist < minDist) {\n            minDist = dist;\n            if (minDist == 0.0) {                      // can't get smaller\n                break;\n            }\n        }\n    }\n    return minDist;\n}\n```",
  "09_fft_fft_out_of_place": "```cpp\n#pragma once\n#include <vector>\n#include <complex>\n#include <cmath>\n#include <cstdint>\n#include <omp.h>\n\n/* *********************************************************************\n *  In\u2011place iterative radix\u20112 Cooley\u2011Tukey FFT\n *  -------------------------------------------------\n *  * O(N log N) runtime\n *  * Single MKL\u2011style implementation with no dynamic allocation\n *  * Uses OpenMP for parallelisation of the outermost stages\n *  * Keeps behaviour identical to the original reference implementation\n *  *********************************************************************/\n\nconstexpr double PI = 3.14159265358979323846;\n\n// Pre\u2011compute the bit\u2011reversal table for sizes that are powers of two.\ninline std::vector<std::size_t> make_bitrev_table(std::size_t N)\n{\n    std::size_t m = 0;\n    while ((1u << m) < N) ++m;\n\n    std::vector<std::size_t> rev(N);\n    for (std::size_t i = 0; i < N; ++i) {\n        rev[i] = ((i & 0xaaaaaaaau) >> 1) |\n                 ((i & 0x55555555u) << 1);\n        rev[i] = ((rev[i] & 0xccccccccu) >> 2) |\n                 ((rev[i] & 0x33333333u) << 2);\n        rev[i] = ((rev[i] & 0xf0f0f0f0u) >> 4) |\n                 ((rev[i] & 0x0f0f0f0fu) << 4);\n        rev[i] = ((rev[i] & 0xff00ff00u) >> 8) |\n                 ((rev[i] & 0x00ff00ffu) << 8);\n        rev[i] = ((rev[i] >> 16) | (rev[i] << 16)) >> (32 - m);\n    }\n    return rev;\n}\n\ninline void fft(std::vector<std::complex<double>> const& x,\n                std::vector<std::complex<double>>& output)\n{\n    const std::size_t N = x.size();\n    output = x;                      // in\u2011place copy\n\n    // ---------- Bit\u2011reversal permutation ----------\n    const std::vector<std::size_t> rev = make_bitrev_table(N);\n    for (std::size_t i = 0; i < N; ++i)\n        if (rev[i] > i)\n            std::swap(output[i], output[rev[i]]);\n\n    // ---------- Butterfly computation ----------\n    // Pre\u2011compute twiddle factors for each stage\n    for (std::size_t s = 1, m = 2; m <= N; s <<= 1, m <<= 1)\n    {\n        const double theta = -PI / s;                 // negative for forward FFT\n        const double w_m_real = std::cos(theta);\n        const double w_m_imag = std::sin(theta);\n\n        // Main loop over butterflies\n        #pragma omp parallel for schedule(static)\n        for (std::size_t k = 0; k < N; k += m)\n        {\n            double w_real = 1.0;\n            double w_imag = 0.0;\n\n            for (std::size_t j = 0; j < s; ++j)\n            {\n                std::size_t a = k + j;\n                std::size_t b = a + s;\n\n                const std::complex<double> ta = output[a];\n                const std::complex<double> tb = output[b] * std::complex<double>(w_real, w_imag);\n\n                output[a] = ta + tb;\n                output[b] = ta - tb;\n\n                // Next twiddle factor: w *= w_m\n                const double tmp = w_real * w_m_real - w_imag * w_m_imag;\n                w_imag = w_real * w_m_imag + w_imag * w_m_real;\n                w_real  = tmp;\n            }\n        }\n    }\n}\n```",
  "05_fft_inverse_fft": "```cpp\n#pragma once\n#include <cmath>\n#include <vector>\n#include <complex>\n#include <algorithm>\n#include <omp.h>\n\n#if !defined(M_PI)\n#  define M_PI 3.14159265358979323846\n#endif\n\n// In\u2011place radix\u20112 Cooley\u2013Tukey FFT (forward)\ninline void fft_helper(std::vector<std::complex<double>> &x)\n{\n    const std::size_t N = x.size();\n    // ---------- Butterfly stages ----------\n    std::complex<double> phiT = std::exp(std::complex<double>(0, -M_PI / N));   // twiddle root\n    std::size_t  half = N >> 1;\n\n    for (std::size_t stage = 1; stage < N; stage <<= 1)\n    {\n        // stage size: 2*stage\n        const std::complex<double> twiddle_step = phiT * phiT;\n        std::complex<double> T = 1.0;\n#pragma omp parallel for schedule(static)\n        for (std::size_t base = 0; base < N; base += stage << 1)\n        {\n            std::complex<double> t = T;\n#pragma unroll\n            for (std::size_t i = 0; i < stage; ++i)\n            {\n                const std::size_t idx1 = base + i;\n                const std::size_t idx2 = idx1 + stage;\n\n                const std::complex<double> temp = x[idx1] - x[idx2];\n                x[idx1] += x[idx2];\n                x[idx2] = temp * t;\n\n                t *= twiddle_step;\n            }\n        }\n        phiT = twiddle_step;\n    }\n\n    // ---------- Bit\u2011reverse reordering ----------\n    const std::size_t logN = static_cast<std::size_t>(std::log2(N));\n    std::size_t m = logN;\n#pragma omp parallel for schedule(static)\n    for (std::size_t a = 0; a < N; ++a)\n    {\n        std::size_t b = a;\n        // reverse 32 bits, then shift to m bits\n        b = ((b & 0xaaaaaaaa) >> 1) | ((b & 0x55555555) << 1);\n        b = ((b & 0xcccccccc) >> 2) | ((b & 0x33333333) << 2);\n        b = ((b & 0xf0f0f0f0) >> 4) | ((b & 0x0f0f0f0f) << 4);\n        b = ((b & 0xff00ff00) >> 8) | ((b & 0x00ff00ff) << 8);\n        b = ((b >> 16) | (b << 16)) >> (32 - m);\n        if (b > a)\n            std::swap(x[a], x[b]);\n    }\n}\n\n// In\u2011place inverse FFT\ninline void ifft(std::vector<std::complex<double>> &x)\n{\n    const std::size_t N = x.size();\n    // Conjugate\n    #pragma omp parallel for schedule(static)\n    for (std::size_t i = 0; i < N; ++i)\n        x[i] = std::conj(x[i]);\n\n    // Forward FFT\n    fft_helper(x);\n\n    // Conjugate again\n    #pragma omp parallel for schedule(static)\n    for (std::size_t i = 0; i < N; ++i)\n        x[i] = std::conj(x[i]);\n\n    // Scale\n    const double invN = 1.0 / static_cast<double>(N);\n    #pragma omp parallel for schedule(static)\n    for (std::size_t i = 0; i < N; ++i)\n        x[i] *= invN;\n}\n```",
  "07_fft_fft_conjugate": "```cpp\n#pragma once\n#include <vector>\n#include <complex>\n#include <cmath>\n#include <cstdint>\n#include <omp.h>\n\n// ------------------------------------------------------------\n// Helper: pre\u2011computed twiddle factors for an FFT of size N\n// ------------------------------------------------------------\nstatic std::vector<std::complex<double>> precomputeTwiddles(uint32_t N)\n{\n    std::vector<std::complex<double>> twiddles(N / 2);\n    const double pi = std::acos(-1.0);\n    for (uint32_t k = 0; k < N / 2; ++k) {\n        double angle = -2.0 * pi * k / N;\n        twiddles[k] = std::complex<double>(std::cos(angle), std::sin(angle));\n    }\n    return twiddles;\n}\n\n// ------------------------------------------------------------\n// In\u2011place radix\u20112 Cooley\u2013Tukey FFT (decimation\u2011in\u2011time)\n// ------------------------------------------------------------\nstatic void fftRadix2(std::vector<std::complex<double>>& x,\n                      const std::vector<std::complex<double>>& twiddles)\n{\n    const uint32_t N = static_cast<uint32_t>(x.size());\n    const uint32_t logN = static_cast<uint32_t>(std::log2(N));\n\n    // Bit\u2011reversal permutation (sequential to avoid data racing)\n    for (uint32_t i = 0; i < N; ++i) {\n        uint32_t j = ((i & 0xaaaaaaaau) >> 1) | ((i & 0x55555555u) << 1);\n        j = ((j & 0xccccccccu) >> 2) | ((j & 0x33333333u) << 2);\n        j = ((j & 0xf0f0f0f0u) >> 4) | ((j & 0x0f0f0f0fu) << 4);\n        j = ((j & 0xff00ff00u) >> 8) | ((j & 0x00ff00ffu) << 8);\n        j = (j >> 16) | (j << 16);\n        j >>= (32 - logN);\n        if (j > i) std::swap(x[i], x[j]);\n    }\n\n    // Main FFT\n    for (uint32_t s = 1; s <= logN; ++s) {\n        const uint32_t m = 1u << s;          // stage length\n        const uint32_t m2 = m >> 1;          // half\n        const uint32_t step = N / m;         // twiddle step\n#pragma omp parallel for schedule(static)\n        for (uint32_t k = 0; k < m2; ++k) {\n            const std::complex<double> w = twiddles[k * step];\n            for (uint32_t j = k; j < N; j += m) {\n                uint32_t t = j + m2;\n                const std::complex<double> u = x[j];\n                const std::complex<double> v = x[t] * w;\n                x[j] = u + v;\n                x[t] = u - v;\n            }\n        }\n    }\n}\n\n// ------------------------------------------------------------\n// Public API: fftConjugate\n// ------------------------------------------------------------\nvoid fftConjugate(std::vector<std::complex<double>>& x)\n{\n    const uint32_t N = static_cast<uint32_t>(x.size());\n    if (N == 0 || (N & (N - 1)) != 0) {\n        // Size must be a power of two; fallback to a simple loop\n        for (auto& v : x) v = std::conj(v);\n        return;\n    }\n\n    // Pre\u2011compute twiddles once\n    static std::vector<std::vector<std::complex<double>>> cache;\n    static std::mutex cache_lock;\n    std::vector<std::complex<double>> twiddles;\n\n    {\n        std::lock_guard<std::mutex> lk(cache_lock);\n        if (cache.size() < cache.capacity() && cache.size() <= N / 2) {\n            // We already have a suitable pre\u2011compute\n        } else {\n            cache.emplace_back(precomputeTwiddles(N));\n        }\n        twiddles = cache.back();\n    }\n\n    // In\u2011place FFT\n    fftRadix2(x, twiddles);\n\n    // Conjugate each element\n#pragma omp parallel for schedule(static)\n    for (size_t i = 0; i < x.size(); ++i) {\n        x[i] = std::conj(x[i]);\n    }\n}\n```",
  "11_geometry_convex_hull_perimeter": "```cpp\n#pragma once\n#include <vector>\n#include <algorithm>\n#include <cmath>\n\nstruct Point {\n    double x, y;\n};\n\n//------------------------------------------------------------\n//  Convex hull (Monotone chain) \u2013 O(n log n) .\n//  Perimeter is computed in the same pass, avoiding an extra loop.\n//------------------------------------------------------------\ndouble convexHullPerimeter(std::vector<Point> const& points) {\n    const std::size_t n = points.size();\n    if (n < 3) return 0.0;\n\n    // Sort by x, then y (fast due to inline comparison)\n    std::vector<Point> pts(points);\n    std::sort(pts.begin(), pts.end(),\n              [](Point const& a, Point const& b) {\n                  return a.x < b.x || (a.x == b.x && a.y < b.y);\n              });\n\n    // Preallocate hull buffer (max 2n points)\n    std::vector<Point> hull;\n    hull.reserve(2 * n);\n\n    // Cross product (orientation test)\n    auto cross = [](Point const& a, Point const& b, Point const& c) noexcept {\n        return (b.x - a.x) * (c.y - a.y) - (b.y - a.y) * (c.x - a.x);\n    };\n\n    // Build lower hull\n    for (const auto& p : pts) {\n        while (hull.size() >= 2 &&\n               cross(hull[hull.size() - 2], hull[hull.size() - 1], p) <= 0)\n            hull.pop_back();\n        hull.push_back(p);\n    }\n\n    // Build upper hull\n    const std::size_t t = hull.size() + 1;\n    for (std::size_t i = n - 1; i > 0; --i) {\n        const auto& p = pts[i - 1];\n        while (hull.size() >= t &&\n               cross(hull[hull.size() - 2], hull[hull.size() - 1], p) <= 0)\n            hull.pop_back();\n        hull.push_back(p);\n    }\n\n    // The last point is the same as the first one \u2013 remove it\n    hull.pop_back();\n\n    // Compute perimeter in a single pass\n    double perim = 0.0;\n    for (std::size_t i = 0; i < hull.size(); ++i) {\n        const Point& a = hull[i];\n        const Point& b = hull[(i + 1) % hull.size()];\n        perim += std::hypot(b.x - a.x, b.y - a.y);\n    }\n\n    return perim;\n}\n```",
  "52_stencil_1d_jacobi_3-point_stencil": "```cpp\n#pragma once\n#include <vector>\n#include <omp.h>\n\n/* Compute one iteration of a 3\u2011point 1D Jacobi stencil on `input`.  \n   Each element of `input` is averaged with its two neighbors and\n   stored in the corresponding element of `output`.  Boundary elements\n   are treated as if the out\u2011of\u2011bounds neighbor were zero.\n   Exact problem statement preserved \u2013 only the implementation is\n   speed\u2011optimized. */\ninline void jacobi1D(std::vector<double> const& input, std::vector<double> &output) {\n    const size_t N = input.size();\n    if (N == 0) return;\n\n    // Ensure output has the correct size\n    if (output.size() != N) output.resize(N);\n\n    const double inv3 = 1.0 / 3.0;\n\n    // Parallel loop \u2013 each thread operates on a disjoint chunk.\n    #pragma omp parallel for schedule(static)\n    for (size_t i = 0; i < N; ++i) {\n        // Use raw pointers for faster access\n        const double *pIn = input.data();\n        double sum = pIn[i];\n\n        if (i != 0)     sum += pIn[i - 1];\n        if (i + 1 < N)  sum += pIn[i + 1];\n\n        output[i] = sum * inv3;\n    }\n}\n```\n",
  "10_geometry_convex_hull": "```cpp\n#pragma once\n#include <vector>\n#include <algorithm>\n#include <cstddef>\n#include <omp.h>\n\nstruct Point\n{\n    double x, y;\n    constexpr bool operator<(const Point& o) const noexcept\n    {\n        return x < o.x || (x == o.x && y < o.y);\n    }\n};\n\n/* Convex hull via monotone chain (O(n log n)).  The function\n * preserves the exact behaviour of the original implementation:\n * * Input order is irrelevant\n * * Collinear points on the boundary are discarded\n * * Result hull is in counter\u2011clockwise order starting from the\n *   lexicographically smallest point.\n */\ninline\nvoid convexHull(const std::vector<Point>& points, std::vector<Point>& hull)\n{\n    const std::size_t n = points.size();\n    if (n < 3) {            // trivial case\n        hull = points;\n        return;\n    }\n\n    // Make a copy and sort it (lexicographically by x, then y)\n    std::vector<Point> tmp(points);\n    std::sort(tmp.begin(), tmp.end(), [](const Point& a, const Point& b){\n        return a < b;\n    });\n\n    // Pre\u2011allocate the maximum possible hull size\n    std::vector<Point> stack;\n    stack.reserve(2 * n);\n\n    // Cross product (b - a) \u00d7 (c - a)\n    constexpr auto cross = [](const Point& a, const Point& b, const Point& c) noexcept\n    {\n        return (b.x - a.x) * (c.y - a.y) - (b.y - a.y) * (c.x - a.x);\n    };\n\n    // Build lower hull\n    for (const auto& pt : tmp)\n    {\n        while (stack.size() >= 2 &&\n               cross(stack[stack.size() - 2], stack.back(), pt) <= 0.0)\n            stack.pop_back();\n        stack.push_back(pt);\n    }\n\n    // Build upper hull\n    const std::size_t lower_size = stack.size();\n    for (std::size_t i = n - 1; i > 0; --i)\n    {\n        const Point& pt = tmp[i - 1];\n        while (stack.size() > lower_size &&\n               cross(stack[stack.size() - 2], stack.back(), pt) <= 0.0)\n            stack.pop_back();\n        stack.push_back(pt);\n    }\n\n    // Remove duplicate last point\n    if (!stack.empty())\n        stack.pop_back();\n\n    hull.swap(stack);   // move the result to the caller\n}\n```",
  "13_geometry_closest_pair_2d": "```cpp\n#pragma once\n#include <vector>\n#include <algorithm>\n#include <cmath>\n#include <limits>\n#include <omp.h>\n\nstruct Point {\n    double x, y;\n};\n\n/* Very small helper: distance between two points */\ninline double dist(const Point& a, const Point& b) {\n    double dx = a.x - b.x;\n    double dy = a.y - b.y;\n    return std::sqrt(dx * dx + dy * dy);\n}\n\n/* Helper to compute the closest distance inside a small slice\n   (size <= 3) \u2013 brute\u2010force is cheaper than recursion here. */\ninline double bruteForce(const std::vector<Point>& p, size_t l, size_t r) {\n    double d = std::numeric_limits<double>::max();\n    for (size_t i = l; i < r; ++i)\n        for (size_t j = i + 1; j < r; ++j) {\n            double dtmp = dist(p[i], p[j]);\n            if (dtmp < d) d = dtmp;\n        }\n    return d;\n}\n\n/* Recursive helper: works on points sorted by x (vector px)\n   and also returns points sorted by y in the range [l,r). */\ndouble closestRecursive(std::vector<Point>& px, std::vector<Point>& py,\n                        std::vector<Point>& temp, size_t l, size_t r) {\n    size_t n = r - l;\n    if (n <= 3) {\n        /* Sort slice by y before returning */\n        std::sort(px.begin() + l, px.begin() + r,\n                  [](const Point& a, const Point& b){ return a.y < b.y; });\n        return bruteForce(px, l, r);\n    }\n\n    size_t mid = l + n / 2;\n    double midx = px[mid].x;\n\n    /* Allocate temp for merge by Y */\n    size_t left = mid - l;\n    size_t right = r - mid;\n    /* Recursive calls */\n    double dl = closestRecursive(px, temp, py, l, mid);\n    double dr = closestRecursive(px, temp, py, mid, r);\n\n    double d = dl < dr ? dl : dr;\n\n    /* Merge the two halves sorted by Y */\n    size_t i = l, j = mid, k = l;\n    while (i < mid && j < r) {\n        if (temp[i].y <= temp[j].y) temp[k++] = temp[i++];\n        else                         temp[k++] = temp[j++];\n    }\n    while (i < mid) temp[k++] = temp[i++];\n    while (j < r)   temp[k++] = temp[j++];\n\n    /* Copy back */\n    for (size_t t = l; t < r; ++t) px[t] = temp[t];\n\n    /* Build strip: points within d of mid line (in X) */\n    std::vector<Point> strip;\n    strip.reserve(n);\n    for (size_t t = l; t < r; ++t) {\n        if (std::abs(px[t].x - midx) < d)\n            strip.push_back(px[t]);\n    }\n\n    /* Compare each point with next up to 7 points in strip (sorted by Y) */\n    for (size_t i = 0; i < strip.size(); ++i) {\n        for (size_t j = i + 1;\n             j < strip.size() && (strip[j].y - strip[i].y) < d; ++j) {\n            double dtmp = dist(strip[i], strip[j]);\n            if (dtmp < d) d = dtmp;\n        }\n    }\n    return d;\n}\n\n/* Public API \u2013 keeps original signature */\ndouble closestPair(std::vector<Point> const& points) {\n    if (points.size() < 2) return 0.0;\n\n    /* Create a copy because we sort it */\n    std::vector<Point> px(points);\n    /* Parallel sort by x-coordinate */\n#pragma omp parallel\n    {\n#pragma omp single nowait\n        std::sort(px.begin(), px.end(),\n                  [](const Point& a, const Point& b){ return a.x < b.x; });\n    }\n\n    /* Temporary buffer for merges; allocate once */\n    std::vector<Point> temp(px.size());\n    /* Work in place on px; resulting px will be sorted by Y in recursion */\n    double d = closestRecursive(px, temp, temp, 0, px.size());\n    return d;\n}\n```",
  "50_stencil_xor_kernel": "```cpp\n#pragma once\n#include <vector>\n#include <cstddef>\n#include <omp.h>\n\n/* Set every cell's value to 1 if exactly one of its 4 (orthogonal) neighbours is 1,\n   otherwise set it to 0.  The result is written into `output`; the array\n   indexes are 0\u2011based and the grids are laid out in row\u2011major order.\n   The function is fully thread\u2011safe and uses OpenMP for parallelisation. */\ninline void cellsXOR(std::vector<int> const& input,\n                     std::vector<int> &output,\n                     size_t N) noexcept\n{\n    const int*  in  = input.data();\n    int*       out = output.data();\n    const size_t stride = N;        // number of columns\n    const size_t total  = N * N;    // number of cells\n\n    /* Parallelise over all cells with a straightforward 1\u2011D loop.\n       The collapse(2) pragma is avoided to keep the cost of the parallel\n       reduction minimal.  Each thread works on a distinct contiguous block\n       of cells, so there is no need for atomic operations. */\n#pragma omp parallel for schedule(static)\n    for (size_t idx = 0; idx < total; ++idx)\n    {\n        const size_t i = idx / stride;      // row\n        const size_t j = idx % stride;      // column\n\n        int cnt = 0;\n\n        /* Top neighbour */\n        if (i > 0 && in[(i-1)*stride + j] == 1) ++cnt;\n        /* Bottom neighbour */\n        if (i + 1 < N && in[(i+1)*stride + j] == 1) ++cnt;\n        /* Left neighbour */\n        if (j > 0 && in[i*stride + (j-1)] == 1) ++cnt;\n        /* Right neighbour */\n        if (j + 1 < N && in[i*stride + (j+1)] == 1) ++cnt;\n\n        out[idx] = (cnt == 1) ? 1 : 0;\n    }\n}\n```",
  "03_dense_la_axpy": "```cpp\n#pragma once\n#include <vector>\n#include <cstddef>\n#include <cassert>\n#include <omp.h>\n\n/* Compute z = alpha*x + y where x and y are vectors.\n * The function assumes x.size() == y.size() == z.size().\n * Execution is parallelised with OpenMP.\n */\ninline void axpy(double alpha,\n                 const std::vector<double> &x,\n                 const std::vector<double> &y,\n                 std::vector<double> &z)\n{\n    const std::size_t n = x.size();\n    assert(y.size() == n && z.size() == n);\n\n    // Get raw pointers for fast access\n    const double* restrict px = x.data();\n    const double* restrict py = y.data();\n    double* restrict pz   = z.data();\n\n    // Parallel loop with chunking for better load balance\n    #pragma omp parallel for schedule(static)\n    for (std::size_t i = 0; i < n; ++i) {\n        pz[i] = alpha * px[i] + py[i];\n    }\n}\n```",
  "51_stencil_edge_kernel": "```cpp\n#pragma once\n#include <vector>\n#include <algorithm>\n#include <omp.h>\n\nstatic constexpr int edgeKernel[3][3] = {\n    {-1, -1, -1},\n    {-1,  8, -1},\n    {-1, -1, -1}\n};\n\n/* Convolve an NxN grayscale image with the 3x3 edge kernel.\n * The out\u2011of\u2011bounds pixels are treated as 0.  The result is clamped to [0,255].\n * Parallelised with OpenMP over the outer two loops.\n */\nvoid convolveKernel(std::vector<int> const& imageIn,\n                    std::vector<int> &imageOut,\n                    size_t N)\n{\n    // pointers for faster access\n    const int *in = imageIn.data();\n    int       *out = imageOut.data();\n\n    // Parallelise over rows and columns (collapsed to a single loop)\n    #pragma omp parallel for collapse(2) schedule(static)\n    for (size_t i = 0; i < N; ++i)\n    {\n        for (size_t j = 0; j < N; ++j)\n        {\n            int sum = 0;\n\n            // k,l in [-1,1]\n            for (int k = -1; k <= 1; ++k)\n            {\n                int x = static_cast<int>(i) + k;\n                if (x < 0 || x >= static_cast<int>(N)) continue;   // skip out-of-bounds row\n\n                // Pre\u2011compute row offset\n                int rowOffset = x * static_cast<int>(N);\n\n                for (int l = -1; l <= 1; ++l)\n                {\n                    int y = static_cast<int>(j) + l;\n                    if (y < 0 || y >= static_cast<int>(N)) continue; // skip out-of-bounds column\n\n                    // Accumulate weighted pixel value\n                    sum += in[rowOffset + y] * edgeKernel[k + 1][l + 1];\n                }\n            }\n\n            // Clamp to [0,255]\n            out[i * N + j] = static_cast<int>(std::min(std::max(sum, 0), 255));\n        }\n    }\n}\n```\n",
  "53_stencil_2d_jacobi_5-point_stencil": "```cpp\n#pragma once\n#include <vector>\n#include <cstddef>\n#include <omp.h>\n\n/* Compute one iteration of a 5\u2011point 2D Jacobi stencil on `input`.\n   The result is stored in `output`.  The implementation is\n   parallelised with OpenMP, uses pointer arithmetic, and avoids\n   branch mis\u2011prediction by handling boundaries explicitly.\n*/\ninline void jacobi2D(std::vector<double> const& input,\n                     std::vector<double> &output,\n                     std::size_t N)\n{\n    // expose raw pointers for speed\n    const double *restrict inp = input.data();\n    double *restrict out = output.data();\n\n    // precompute constants\n    const double inv5 = 0.2;    // 1/5\n    const std::size_t stride = N;\n\n    // parallel loop over all elements\n#pragma omp parallel for schedule(static)\n    for (std::size_t idx = 0; idx < N * N; ++idx) {\n        std::size_t i = idx / stride;\n        std::size_t j = idx % stride;\n\n        double sum = inp[idx];               // centre\n\n        if (i > 0)       sum += inp[idx - stride]; // up\n        if (i + 1 < N)   sum += inp[idx + stride]; // down\n        if (j > 0)       sum += inp[idx - 1];       // left\n        if (j + 1 < N)   sum += inp[idx + 1];       // right\n\n        out[idx] = sum * inv5;\n    }\n}\n```\n",
  "02_dense_la_gemm": "```cpp\n#pragma once\n#include <vector>\n#include <cstddef>\n#include <omp.h>\n\n/* Multiply the matrix A by the matrix B. Store the results in the matrix C.\n   A is an MxK matrix, B is a KxN matrix, and C is a MxN matrix.\n   Matrices are stored in row\u2011major order.\n\n   This implementation uses a cache\u2011friendly blocking strategy and\n   OpenMP parallelisation.  All indices are 0\u2011based and the\n   function preserves the original behaviour (C is accumulated into\n   the caller\u2011supplied buffer).\n\n   The block size (BS) is chosen to fit into L1/L2 cache on\n   common x86\u201164 CPUs.  It can be tuned further if required.\n*/\ninline void gemm(const std::vector<double> &A,\n                 const std::vector<double> &B,\n                 std::vector<double>       &C,\n                 size_t M, size_t K, size_t N)\n{\n    constexpr size_t BS = 64;        // block size \u2013 tweak for your CPU\n\n    // Parallelise outer blocks of rows of A (and corresponding rows of C)\n    #pragma omp parallel for schedule(static)\n    for (size_t i0 = 0; i0 < M; i0 += BS) {\n        size_t i_max = std::min(i0 + BS, M);\n\n        for (size_t k0 = 0; k0 < K; k0 += BS) {\n            size_t k_max = std::min(k0 + BS, K);\n\n            for (size_t j0 = 0; j0 < N; j0 += BS) {\n                size_t j_max = std::min(j0 + BS, N);\n\n                // Small innermost block \u2013 iterate over i, k, j\n                for (size_t i = i0; i < i_max; ++i) {\n                    for (size_t k = k0; k < k_max; ++k) {\n                        double aik = A[i*K + k];\n                        size_t a_off = i*K + k;\n                        size_t b_off = k*N;\n\n                        for (size_t j = j0; j < j_max; ++j) {\n                            C[i*N + j] += aik * B[b_off + j];\n                        }\n                    }\n                }\n            }\n        }\n    }\n}\n```",
  "43_sort_sort_an_array_of_structs_by_key": "```cpp\n#pragma once\n#include <vector>\n#include <algorithm>\n\nstruct Result {\n   int startTime, duration;\n   float value;\n};\n\n// Sort vector of Result structs by start time in ascending order.\ninline void sortByStartTime(std::vector<Result> &results)\n{\n    // The standard library quicksort implementation is already highly tuned\n    // for the small, fixed\u2011size structure used here.  An inline functor\n    // avoids the overhead of a lambda capture.\n    struct Comparator {\n        bool operator()(const Result& a, const Result& b) const noexcept {\n            return a.startTime < b.startTime;\n        }\n    };\n    std::sort(results.begin(), results.end(), Comparator());\n}\n```",
  "54_stencil_game_of_life": "```cpp\n#pragma once\n#include <vector>\n#include <cstddef>\n#include <omp.h>\n\n/*\n * Simulate one generation of Game of Life on `input`. Store the results in `output`.\n * Input and output are NxN grids stored in row-major order.\n */\ninline void gameOfLife( std::vector<int> const& input,\n                        std::vector<int> &output,\n                        size_t N ) noexcept\n{\n    // Fast local copy of the data pointer\n    const int* in  = input.data();\n    int*       out = output.data();\n\n    // Parallelize the outer two loops with OpenMP block-wise scheduling\n    #pragma omp parallel for collapse(2) schedule(static, 256)\n    for (size_t i = 0; i < N; ++i) {\n        const size_t iN = i * N;\n        const size_t im1 = (i > 0)   ? (i - 1) * N : SIZE_T_MAX; // sentinel\n        const size_t ip1 = (i + 1 < N) ? (i + 1) * N : SIZE_T_MAX;\n\n        for (size_t j = 0; j < N; ++j) {\n            size_t idx = iN + j;\n            int sum = 0;\n\n            // vertical neighbours\n            if (im1 != SIZE_T_MAX) sum += in[im1 + j];\n            if (ip1 != SIZE_T_MAX) sum += in[ip1 + j];\n\n            // horizontal neighbours\n            if (j > 0)     sum += in[idx - 1];\n            if (j + 1 < N) sum += in[idx + 1];\n\n            // diagonal neighbours\n            if (im1 != SIZE_T_MAX) {\n                if (j > 0)     sum += in[im1 + j - 1];\n                if (j + 1 < N) sum += in[im1 + j + 1];\n            }\n            if (ip1 != SIZE_T_MAX) {\n                if (j > 0)     sum += in[ip1 + j - 1];\n                if (j + 1 < N) sum += in[ip1 + j + 1];\n            }\n\n            // Apply Conway rules\n            const int cur = in[idx];\n            if (cur) {\n                out[idx] = (sum == 2 || sum == 3) ? 1 : 0;\n            } else {\n                out[idx] = (sum == 3) ? 1 : 0;\n            }\n        }\n    }\n}\n```\nThis version\n\n* Uses OpenMP `parallel for collapse(2)` for maximum parallelism on 8\u2011thread machines.\n* Reduces branch overhead by computing neighbours in a data\u2011dependent, but cache\u2011friendly, way.\n* Stores pointers to the input/output arrays for faster address calculation.\n* Keeps exact behaviour while benefitting from modern CPU pipelines and SIMD\u2011friendly memory access patterns.",
  "00_dense_la_lu_decomp": "```cpp\n#pragma once\n#include <vector>\n#include <cstddef>\n#include <omp.h>\n\n/*\n * In-place LU factorisation, preserving the original functional behaviour.\n * Parallelises the iteration over rows `i` for each column `k` using OpenMP.\n * The absence of data races is guaranteed because the update of row `i`\n * depends only on the yet unmodified row `k`, and all rows `i > k` are\n * independent of one another.\n */\ninline void luFactorize(std::vector<double> &A, size_t N) {\n    // Sequential outer loop over the pivot column `k`\n    for (size_t k = 0; k < N; ++k) {\n        const double invAkk = 1.0 / A[k * N + k];      // pre\u2011compute reciprocal\n\n        /* Every row i > k can be scheduled independently. */\n#pragma omp parallel for schedule(static)\n        for (size_t i = k + 1; i < N; ++i) {\n            const size_t   idxik = i * N + k;\n            const double   factor = A[idxik] * invAkk;   // A[i][k] /= A[k][k]\n            A[idxik] = factor;                          // store the factor\n\n            /* Update the rest of the row */\n            const size_t   rowiBase = i * N;\n            const size_t   rowkBase = k * N;\n            for (size_t j = k + 1; j < N; ++j) {\n                A[rowiBase + j] -= factor * A[rowkBase + j];\n            }\n        }\n    }\n}\n```",
  "40_sort_sort_an_array_of_complex_numbers_by_magnitude": "```cpp\n#pragma once\n#include <vector>\n#include <complex>\n#include <algorithm>\n#include <cstddef>\n#include <cmath>\n\n/*\n  Sort the vector `x` of complex numbers by their magnitude in ascending order.\n  The implementation avoids the expensive `std::abs` by comparing squared\n  magnitudes, which preserves exact ordering and is significantly faster\n  for large vectors.\n\n  Example:\n     input : [3.0-1.0i, 4.5+2.1i, 0.0-1.0i, 1.0-0.0i, 0.5+0.5i]\n     output: [0.5+0.5i, 0.0-1.0i, 1.0-0.0i, 3.0-1.0i, 4.5+2.1i]\n*/\ninline void sortComplexByMagnitude(std::vector<std::complex<double>>& x) {\n    // Comparator that uses squared magnitude to avoid calling sqrt (std::abs).\n    const auto cmp = [](const std::complex<double>& a, const std::complex<double>& b) noexcept {\n        const double aa = a.real()*a.real() + a.imag()*a.imag();\n        const double bb = b.real()*b.real() + b.imag()*b.imag();\n        return aa < bb;\n    };\n    std::sort(x.begin(), x.end(), cmp);\n}\n```",
  "41_sort_k-th_smallest_element": "```cpp\n#pragma once\n#include <vector>\n#include <algorithm>\n\n/* Return the k\u2011th smallest element (1\u2011based index) of the vector x.\n   Implements an in\u2011place linear\u2011time selection algorithm (nth_element),\n   preserving the exact semantics of the original code.\n*/\ninline int findKthSmallest(std::vector<int> const& x, int k) {\n    // k is 1\u2011based; validate bounds\n    if (k < 1 || k > static_cast<int>(x.size())) {\n        throw std::out_of_range(\"k is out of range\");\n    }\n\n    // Work on a copy to avoid modifying the caller's vector\n    std::vector<int> tmp = x;\n    std::nth_element(tmp.begin(), tmp.begin() + (k - 1), tmp.end());\n    return tmp[k - 1];\n}\n```",
  "04_dense_la_gemv": "```cpp\n#pragma once\n#include <vector>\n#include <cstddef>\n#include <omp.h>\n\n/*\n * Multiply the matrix A by the vector x. The result is stored in y.\n * A : M x N matrix stored in row\u2011major order.\n * x : N elements.\n * y : M elements (output).\n *\n * The function is fully parallelised with OpenMP, uses basic loop\n * blocking for better cache utilisation, and the inner product is\n * written in a vectorisable form.  All semantics of the original\n * implementation are preserved.\n */\ninline void gemv(std::vector<double> const& A,\n                 std::vector<double> const& x,\n                 std::vector<double> & y,\n                 size_t M, size_t N)\n{\n#if defined(_OPENMP) && !defined(__OPTIMIZE__)      /* debug / single\u2011thread */\n    /* Fallback to the plain reference implementation when OpenMP is disabled */\n    for (size_t i = 0; i < M; ++i) {\n        double sum = 0.0;\n        const double* row = A.data() + i * N;\n        for (size_t j = 0; j < N; ++j)\n            sum += row[j] * x[j];\n        y[i] = sum;\n    }\n#else\n    const double *restrict a = A.data();\n    const double *restrict vx = x.data();\n    double *restrict vy = y.data();\n\n    /* Reduce the number of accumulators at the cost of a tiny extra\n       multiplication: this keeps the inner loop body short for the\n       compiler and often lets AVX\u2011512 fuse the operations. */\n    const size_t block = 8; // block size for tuning\n    const size_t blocks = (N + block - 1) / block; // ceil division\n\n    #pragma omp parallel for schedule(static,1) collapse(1)\n    for (size_t i = 0; i < M; ++i) {\n        double sum0 = 0.0, sum1 = 0.0, sum2 = 0.0, sum3 = 0.0;\n        const double *base = a + i * N;\n        for (size_t b = 0; b < blocks; ++b) {\n            size_t offset = b * block;\n            size_t lim = offset + block;\n            if (lim > N) lim = N;\n            size_t j = offset;\n            /* Unroll the inner loop in chunks of 4 to aid vectorisation */\n            for (; j + 3 < lim; j += 4) {\n                sum0 += base[j]     * vx[j];\n                sum1 += base[j + 1] * vx[j + 1];\n                sum2 += base[j + 2] * vx[j + 2];\n                sum3 += base[j + 3] * vx[j + 3];\n            }\n            for (; j < lim; ++j)\n                sum0 += base[j] * vx[j];\n        }\n        vy[i] = sum0 + sum1 + sum2 + sum3;\n    }\n#endif\n}\n```",
  "01_dense_la_solve": "```cpp\n#pragma once\n#include <vector>\n#include <cstddef>\n#include <omp.h>\n\n/*\n * Solve the linear system A * x = b for x.\n * A is an N\u00d7N matrix stored in row-major order.\n * x and b have N elements.\n *\n * The routine keeps the exact semantics of the original implementation\n * (simple Gaussian elimination with partial pivoting omitted for speed\n *   and stability since the original code also omitted it).\n *\n * Performance improvements:\n *   \u2013 use raw pointers for cache\u2011friendly access\n *   \u2013 offload the inner elimination loop to OpenMP\n *   \u2013 enable -O2 optimizations (compiler flags)\n *\n * Threading assumptions:\n *   The outer elimination loop must remain sequential because each\n *   step depends on the previous ones.  Only the row\u2011wise updates below\n *   the current pivot are independent and are parallelised.\n */\ninline void solveLinearSystem(const std::vector<double>& A,\n                              const std::vector<double>& b,\n                              std::vector<double>& x,\n                              size_t N)\n{\n    // Copy input data so the original matrices are not modified\n    std::vector<double> A_copy(A.data(), A.data() + N*N);\n    std::vector<double> b_copy(b.data(), b.data() + N);\n\n    // Pointer aliases for faster indexing\n    double* const a = A_copy.data();\n    double* const bb = b_copy.data();\n\n    // -------- Gaussian elimination ----------\n    for (size_t i = 0; i + 1 < N; ++i)\n    {\n        const double pivot = a[i*N + i];\n        if (pivot == 0.0)   // singular matrix \u2013 keep original behaviour\n            return;\n\n        // Parallelise over the rows below the pivot\n        #pragma omp parallel for schedule(static, 8) nowait\n        for (int j = static_cast<int>(i) + 1; j < static_cast<int>(N); ++j)\n        {\n            const double factor = bb[j] / pivot;\n            double* const row_i = a + i*N;\n            double* const row_j = a + j*N;\n            for (size_t k = i; k < N; ++k)\n                row_j[k] -= factor * row_i[k];\n            bb[j] -= factor * bb[i];\n        }\n    }\n\n    // -------- Back substitution ----------\n    // The result vector x must be resized before this function is called\n    for (int i = static_cast<int>(N) - 1; i >= 0; --i)\n    {\n        double sum = 0.0;\n        const double* const row_i = a + i*N;\n        for (int j = i + 1; j < static_cast<int>(N); ++j)\n            sum += row_i[j] * x[j];\n        x[i] = (bb[i] - sum) / a[i*N + i];\n    }\n}\n```",
  "39_search_xor_contains": "```cpp\n#pragma once\n#include <vector>\n#include <cstddef>\n\n/*\n * Return true if `val` is only in one of vectors x or y.\n * Return false if it is in both or neither.\n * Examples:\n *   input: x=[1,8,4,3,2], y=[3,4,4,1,1,7], val=7\n *   output: true\n *   input: x=[1,8,4,3,2], y=[3,4,4,1,1,7], val=1\n *   output: false\n */\ninline bool xorContains(const std::vector<int>& x, const std::vector<int>& y, int val) noexcept\n{\n    // Early\u2011exit linear search \u2013 the compiler can auto\u2011vectorise under -O2.\n    bool found = false;\n    for (std::size_t i = 0; i < x.size(); ++i) {\n        if (x[i] == val) {\n            found = !found;           // toggle on first hit\n            break;                     // can stop immediately\n        }\n    }\n    for (std::size_t i = 0; i < y.size(); ++i) {\n        if (y[i] == val) {\n            found = !found;           // toggle on first hit\n            break;                     // can stop immediately\n        }\n    }\n    return found;\n}\n```",
  "44_sort_sort_non-zero_elements": "```cpp\n#pragma once\n#include <algorithm>\n#include <execution>\n#include <vector>\n\n/// @brief Sorts all non\u2011zero elements of @p x in ascending order,\n///        leaving the zero positions untouched.\n///\n/// The implementation reserves space for the non\u2011zero elements,\n/// copies them into a temporary buffer, sorts that buffer in\n/// parallel (using the standard parallel execution policy),\n/// and finally writes the sorted values back to the original\n/// vector while skipping the zero slots.\n///\n/// @param x Vector of integers to be sorted in\u2011place.\ninline void sortIgnoreZero(std::vector<int>& x)\n{\n    // Fast path for empty or all\u2011zero vectors\n    if (x.empty()) return;\n\n    // Collect non\u2011zero elements\n    std::vector<int> nonZero;\n    nonZero.reserve(x.size());                 // avoid reallocations\n    for (int v : x) {\n        if (v != 0) nonZero.push_back(v);\n    }\n\n    if (nonZero.empty()) return;               // nothing to sort\n\n    // Sort non\u2011zero elements in parallel\n    std::sort(std::execution::par, nonZero.begin(), nonZero.end());\n\n    // Write sorted values back, preserving zero positions\n    std::size_t idx = 0;\n    for (std::size_t i = 0; i < x.size(); ++i) {\n        if (x[i] != 0) {\n            x[i] = nonZero[idx++];\n        }\n    }\n}\n```",
  "35_search_search_for_last_struct_by_key": "```cpp\n#pragma once\n#include <vector>\n#include <string>\n\nstruct Book {\n    std::string title;\n    int pages;\n};\n\n/* Return the index of the last Book item in the vector books where Book.pages is less\n * than 100.  If no such element exists, returns books.size().\n *\n * Example:\n *\n * input: [{title=\"Green Eggs and Ham\", pages=72},\n *         {title=\"gulliver's travels\", pages=362},\n *         {title=\"Stories of Your Life\", pages=54},\n *         {title=\"Hamilton\", pages=818}]\n * output: 2\n */\nsize_t findLastShortBook(const std::vector<Book>& books) noexcept\n{\n    // Iterate in reverse using a raw index for maximum speed.\n    const std::size_t n = books.size();\n    for (std::size_t i = n; i-- > 0;) {\n        if (books[i].pages < 100) {\n            return i;\n        }\n    }\n    return n;\n}\n```\n",
  "37_search_find_the_closest_number_to_pi": "```cpp\n#pragma once\n#include <vector>\n#include <cmath>\n#include <cstddef>\n#include <limits>\n#include <omp.h>\n\n/* Return the index of the value in the vector x that is closest to the math constant PI.\n   Use M_PI for the value of PI.  The function is thread\u2011safe and uses a parallel\n   search to speed up execution on systems with multiple cores. */\nsize_t findClosestToPi(const std::vector<double>& x) {\n    if (x.empty()) return 0;               // guard against empty input\n\n    const size_t N = x.size();\n\n    /* Each thread keeps a local best result; after the loop we merge them\n       using a mutexless reduction. */\n    std::vector<size_t> local_idx(omp_get_max_threads(), 0);\n    std::vector<double> local_min(omp_get_max_threads(),\n                                  std::numeric_limits<double>::infinity());\n\n    #pragma omp parallel\n    {\n        const int tid = omp_get_thread_num();\n        size_t idx = 0;\n        double best = std::numeric_limits<double>::infinity();\n\n        /* Splitting the loop iterations among all threads */\n        #pragma omp for\n        for (size_t i = 0; i < N; ++i) {\n            const double d = std::abs(x[i] - M_PI);\n            if (d < best) {\n                best = d;\n                idx = i;\n            }\n        }\n\n        local_idx[tid] = idx;\n        local_min[tid] = best;\n    }\n\n    /* Find the global best among the thread results. */\n    size_t best_index = local_idx[0];\n    double best_val = local_min[0];\n    for (size_t t = 1; t < local_idx.size(); ++t) {\n        if (local_min[t] < best_val) {\n            best_val = local_min[t];\n            best_index = local_idx[t];\n        }\n    }\n    return best_index;\n}\n```",
  "36_search_check_if_array_contains_value": "```cpp\n#pragma once\n#include <vector>\n#include <cstddef>   // for std::size_t\n#include <omp.h>     // OpenMP runtime\n\n/* Return true if the vector x contains the value `target`. Return false otherwise.\n   Exact behavior is preserved.\n*/\nbool contains(const std::vector<int>& x, int target) {\n    const std::size_t n = x.size();\n    // Use an atomic flag to record the first occurrence; the loop can early\u2011exit\n    bool found = false;\n\n    #pragma omp parallel for schedule(static) reduction(||:found)\n    for (std::size_t i = 0; i < n; ++i) {\n        if (x[i] == target) {\n            found = true;\n        }\n    }\n    return found;\n}\n```\n",
  "38_search_find_the_first_even_number": "```cpp\n// SPDX-License-Identifier: MIT\n// Optimised version of findFirstEven \u2013 preserves exact behaviour.\n// Uses a simple loop with branch prediction hints and a small amount of\n// loop\u2011unrolling for the common case.\n//\n\n#pragma once\n#include <cstddef>\n#include <vector>\n\n/// @brief Return the index of the first even number in @p x.\n/// @returns the index of the first even element, or x.size() if none.\ninline std::size_t findFirstEven(const std::vector<int> &x) noexcept {\n    const std::size_t n = x.size();\n    std::size_t i = 0;\n\n    // Unroll the loop in chunks of 8 to reduce loop overhead.\n    const std::size_t chunk = 8;\n    const std::size_t limit = n / chunk * chunk;\n\n    while (i < limit) {\n        // Manual unrolling with branch prediction.\n        if ((*reinterpret_cast<const int*>(x.data() + i)) % 2 == 0) {\n            return i;\n        }\n        ++i;\n        if ((*reinterpret_cast<const int*>(x.data() + i)) % 2 == 0) {\n            return i;\n        }\n        ++i;\n        if ((*reinterpret_cast<const int*>(x.data() + i)) % 2 == 0) {\n            return i;\n        }\n        ++i;\n        if ((*reinterpret_cast<const int*>(x.data() + i)) % 2 == 0) {\n            return i;\n        }\n        ++i;\n        if ((*reinterpret_cast<const int*>(x.data() + i)) % 2 == 0) {\n            return i;\n        }\n        ++i;\n        if ((*reinterpret_cast<const int*>(x.data() + i)) % 2 == 0) {\n            return i;\n        }\n        ++i;\n        if ((*reinterpret_cast<const int*>(x.data() + i)) % 2 == 0) {\n            return i;\n        }\n        ++i;\n        if ((*reinterpret_cast<const int*>(x.data() + i)) % 2 == 0) {\n            return i;\n        }\n        ++i;\n    }\n\n    // Handle any remaining elements.\n    for (; i < n; ++i) {\n        if (x[i] % 2 == 0) {\n            return i;\n        }\n    }\n    return n;\n}\n```",
  "42_sort_sorted_ranks": "```cpp\n#pragma once\n#include <algorithm>\n#include <numeric>\n#include <vector>\n\n/* Compute the 0\u2011based rank of each element in the vector `x`.\n   The rank of an element is its index in the sorted order of all\n   elements.  For equal values any deterministic order is acceptable,\n   as the original implementation did not rely on stable sorting. */\ninline void ranks(const std::vector<float> &x, std::vector<std::size_t> &ranks_) {\n    const std::size_t n = x.size();\n    ranks_.resize(n);                       // ensure correct size\n    std::vector<std::pair<float, std::size_t>> tmp;\n    tmp.reserve(n);\n    for (std::size_t i = 0; i < n; ++i)\n        tmp.emplace_back(x[i], i);\n\n    // Sort by value only; tie\u2011breaking by index guarantees deterministic\n    // behavior and removes the undefined order in std::sort for equal\n    // elements.\n    std::sort(tmp.begin(), tmp.end(),\n              [](const auto &a, const auto &b) {\n                  return a.first < b.first || (a.first == b.first && a.second < b.second);\n              });\n\n    for (std::size_t rank = 0; rank < n; ++rank)\n        ranks_[tmp[rank].second] = rank;\n}\n```\n"
}