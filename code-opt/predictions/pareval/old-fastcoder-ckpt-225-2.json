{
  "15_graph_edge_count": "```cpp\n#pragma once\n#include <vector>\n#include <omp.h>\n\n/* Count the number of edges in the directed graph defined by the adjacency matrix A.\n   A is an NxN adjacency matrix stored in row-major.\n   Example:\n     input: [[0, 0, 0, 1], [0, 0, 0, 1], [0, 0, 0, 1], [1, 1, 1, 0]]\n   output: 3\n*/\nint edgeCount(const std::vector<int> &A, const size_t N) {\n    int count = 0;\n    #pragma omp parallel for reduction(+:count) schedule(static)\n    for (int i = 0; i < static_cast<int>(N); ++i) {\n        const int base = i * static_cast<int>(N);\n        for (int j = 0; j < static_cast<int>(N); ++j) {\n            if (A[base + j] == 1) {\n                ++count;\n            }\n        }\n    }\n    return count;\n}\n```",
  "17_graph_highest_degree": "```cpp\n#pragma once\n#include <vector>\n#include <cstddef>\n#include <algorithm>\n#include <omp.h>\n\n/* Compute the highest node degree in the undirected graph.\n   The graph is given by a row\u2011major NxN adjacency matrix A.\n   The function is fully thread\u2011safe and uses OpenMP\n   to parallelise the per\u2011row degree summations.\n*/\nint maxDegree(std::vector<int> const& A, size_t N) {\n    int highestDegree = 0;\n#if defined(_OPENMP)\n    #pragma omp parallel for reduction(max:highestDegree)\n#endif\n    for (size_t i = 0; i < N; ++i) {\n        int degree = 0;\n        const int* row = A.data() + i * N;\n        // Simple cache friendly loop\n        for (size_t j = 0; j < N; ++j) {\n            degree += row[j];\n        }\n        if (degree > highestDegree) highestDegree = degree;\n    }\n    return highestDegree;\n}\n```\n",
  "21_histogram_bin_0-100": "```cpp\n#pragma once\n#include <array>\n#include <vector>\n#include <cstddef>\n#include <omp.h>\n\n/* Vector x contains values between 0 and 100, inclusive. Count the number\n   of values in [0,10), [10, 20), [20, 30), ... and store the counts in `bins`.\n   Example:\n   input: [7, 32, 95, 12, 39, 32, 11, 71, 70, 66]\n   output: [1, 2, 0, 3, 0, 0, 1, 2, 0, 1]\n*/\ninline void binsBy10Count(std::vector<double> const& x, std::array<std::size_t, 10>& bins) {\n    /* Ensure the result array is zeroed before use. */\n    bins.fill(0);\n\n    /* Parallel loop with reduction on the entire array. */\n    #pragma omp parallel for schedule(static) reduction(+: bins[:10])\n    for (std::size_t i = 0; i < x.size(); ++i) {\n        const std::size_t bin = static_cast<std::size_t>(x[i] / 10.0);\n        /* Values at 100 fall into bin 10, which is out of range.*/\n        if (bin < bins.size())\n            ++bins[bin];\n    }\n}\n```\n",
  "20_histogram_pixel_histogram": "```cpp\n#pragma once\n#include <vector>\n#include <array>\n#include <cstddef>\n#include <omp.h>\n\n/* Count the number of pixels in image with each grayscale intensity.\n   The vector `image` is a grayscale image with values 0-255.\n   Store the results in `bins`. */\ninline void pixelCounts(std::vector<int> const& image,\n                        std::array<std::size_t, 256>& bins) noexcept\n{\n    // initialise output array to zero\n    bins.fill(0);\n\n    // thread\u2011private local bins to avoid contention\n    const int n_threads = omp_get_max_threads();\n    std::vector<std::array<std::size_t, 256>> local(n_threads);\n    for (auto& a : local) a.fill(0);\n\n    // parallel loop\n    #pragma omp parallel\n    {\n        const int t = omp_get_thread_num();\n        auto &lb = local[t];\n        const std::size_t stride = omp_get_num_threads();\n        const std::size_t N = image.size();\n\n        // each thread processes a strided chunk\n        for (std::size_t i = t; i < N; i += stride)\n            lb[static_cast<std::size_t>(image[i])]++;\n    }\n\n    // merge local bins into result\n    for (int t = 0; t < n_threads; ++t)\n        for (std::size_t b = 0; b < 256; ++b)\n            bins[b] += local[t][b];\n}\n```",
  "23_histogram_first_letter_counts": "```cpp\n#pragma once\n#include <array>\n#include <string>\n#include <vector>\n#include <cstddef>\n#include <omp.h>\n\n/*\n * Count the number of strings in `s` that start with each lowercase letter.\n * The result is stored in `bins`, where bins[0] corresponds to 'a', bins[1] to 'b', etc.\n * The function uses OpenMP to parallelise the work across available threads.\n */\ninline void firstLetterCounts(const std::vector<std::string> &s,\n                              std::array<std::size_t, 26> &bins) {\n    // Initialise result array to zero. The loop below will add to these values.\n    bins.fill(0);\n\n    // Each thread keeps a private array to avoid contention.\n    const int nthreads = omp_get_max_threads();\n    std::vector<std::array<std::size_t, 26>> priv(nthreads);\n    for (auto &p : priv) p.fill(0);\n\n    #pragma omp parallel\n    {\n        const int tid = omp_get_thread_num();\n        const std::size_t start = (s.size() * tid) / nthreads;\n        const std::size_t end   = (s.size() * (tid + 1)) / nthreads;\n\n        for (std::size_t i = start; i < end; ++i) {\n            const unsigned char c = static_cast<unsigned char>(s[i][0]);\n            const std::size_t idx = static_cast<std::size_t>(c - 'a');\n            priv[tid][idx] += 1;\n        }\n    }\n\n    // Reduce private counts into the final bins array\n    for (int tid = 0; tid < nthreads; ++tid) {\n        for (int j = 0; j < 26; ++j) {\n            bins[j] += priv[tid][j];\n        }\n    }\n}\n```",
  "34_scan_largest_contiguous_subarray_sum": "```cpp\n#pragma once\n#include <vector>\n#include <limits>\n\n/* Return the largest sum of any contiguous subarray in the vector x.\n   Uses O(n) Kadane's algorithm, preserving exact behaviour of the\n   original solution. */\nint maximumSubarray(std::vector<int> const& x) {\n    int best = std::numeric_limits<int>::lowest();\n    int cur  = 0;\n\n    for (int val : x) {\n        cur = std::max(val, cur + val);\n        best = std::max(best, cur);\n    }\n\n    return best;\n}\n```",
  "19_graph_shortest_path": "```cpp\n#pragma once\n#include <vector>\n#include <limits>\n#include <cstddef>\n\n/*\n   Return the length of the shortest path from source to dest in the undirected graph\n   defined by the adjacency matrix A (row\u2011major, size N\u00d7N). The graph is guaranteed\n   to be connected and undirected.\n\n   Implementation notes:\n   - Uses a simple BFS with a preallocated circular buffer for the queue\n     to avoid dynamic allocations and copy overhead.\n   - Distances are stored in a vector<int> initialized to -1.\n   - Visits are marked by setting the distance value; no separate visited array\n     is needed.\n*/\ninline int shortestPathLength(std::vector<int> const &A, std::size_t N,\n                               int source, int dest)\n{\n    // Special case when source is dest\n    if (source == dest) return 0;\n\n    // distance array, -1 means unvisited\n    std::vector<int> dist(N, -1);\n    dist[source] = 0;\n\n    // Preallocated queue (circular buffer)\n    std::vector<int> q(N);\n    std::size_t head = 0, tail = 0;   // empty when head == tail\n\n    q[tail++] = source;               // enqueue source\n\n    while (head != tail) {\n        int u = q[head++];\n        if (head == N) head = 0;      // wrap around\n\n        int dnext = dist[u] + 1;\n\n        // Scan the row of u for neighbours\n        std::size_t offset = u * N;\n        for (std::size_t v = 0; v < N; ++v) {\n            if (A[offset + v] && dist[v] == -1) {\n                dist[v] = dnext;\n                if (static_cast<int>(v) == dest) {\n                    return dnext;    // reached destination\n                }\n                q[tail++] = static_cast<int>(v);\n                if (tail == N) tail = 0; // wrap around\n            }\n        }\n    }\n\n    // Should never reach here for a connected graph\n    return std::numeric_limits<int>::max();\n}\n```",
  "16_graph_largest_component": "```cpp\n#pragma once\n#include <vector>\n#include <algorithm>\n#include <cstddef>\n\n/* Union-Find data structure */\nclass DSU {\npublic:\n    explicit DSU(std::size_t n) : parent(n), sz(n, 1) {\n        for (std::size_t i = 0; i < n; ++i) parent[i] = i;\n    }\n\n    std::size_t find(std::size_t x) {\n        while (parent[x] != x) {\n            parent[x] = parent[parent[x]]; // path compression\n            x = parent[x];\n        }\n        return x;\n    }\n\n    void unite(std::size_t a, std::size_t b) {\n        std::size_t ra = find(a);\n        std::size_t rb = find(b);\n        if (ra == rb) return;\n        if (sz[ra] < sz[rb]) std::swap(ra, rb);\n        parent[rb] = ra;\n        sz[ra] += sz[rb];\n    }\n\n    std::size_t component_size(std::size_t x) const {\n        return sz[find(x)];\n    }\n\nprivate:\n    std::vector<std::size_t> parent;\n    std::vector<std::size_t> sz;\n};\n\n/* Return the number of vertices in the largest component of the graph defined by the adjacency matrix A.\n   A is an NxN adjacency matrix stored in row-major.\n   Example:\n       input: [[0, 1, 0, 0], [1, 0, 0, 0], [0, 0, 0, 1], [0, 0, 1, 0]]\n   output: 2\n*/\nint largestComponent(const std::vector<int>& A, std::size_t N) {\n    DSU dsu(N);\n    /* Iterate over upper triangle to avoid duplicate edges */\n    for (std::size_t i = 0; i < N; ++i) {\n        std::size_t offset = i * N;\n        for (std::size_t j = i + 1; j < N; ++j) {\n            if (A[offset + j] == 1) {\n                dsu.unite(i, j);\n            }\n        }\n    }\n\n    /* Find maximum component size */\n    std::size_t maxSize = 0;\n    for (std::size_t i = 0; i < N; ++i) {\n        if (dsu.parent[i] == i) { // root\n            maxSize = std::max(maxSize, dsu.sz[i]);\n        }\n    }\n    return static_cast<int>(maxSize);\n}\n```",
  "22_histogram_count_quadrants": "```cpp\n#pragma once\n#include <array>\n#include <vector>\n#include <cstddef>\n#include <cstdint>\n#include <omp.h>\n\nstruct Point {\n    double x, y;\n};\n\n/*\n * Count the number of cartesian points in each quadrant.\n *   - 0: x \u2265 0, y \u2265 0\n *   - 1: x < 0, y \u2265 0\n *   - 2: x < 0, y < 0\n *   - 3: x \u2265 0, y < 0\n *\n * Parameters\n *   points : vector of points to be processed.\n *   bins   : array of 4 counters that will hold the resulting counts.\n *\n * This implementation uses OpenMP to parallelise the loop over points.\n * Each thread keeps a private set of counters which are merged\n * at the end reducing contention on the shared array.\n */\ninline void countQuadrants(const std::vector<Point>& points,\n                           std::array<std::size_t, 4>& bins)\n{\n    // Initialise the output array\n    bins[0] = bins[1] = bins[2] = bins[3] = 0;\n\n    // OpenMP parallel region with private counters\n    #pragma omp parallel\n    {\n        // Private counters for each thread\n        std::array<std::size_t, 4> local = {0, 0, 0, 0};\n\n        // Parallel loop over all points\n        #pragma omp for nowait\n        for (std::size_t i = 0; i < points.size(); ++i) {\n            const auto& p = points[i];\n            if (p.x >= 0.0) {\n                if (p.y >= 0.0)\n                    ++local[0];\n                else\n                    ++local[3];\n            } else {\n                if (p.y >= 0.0)\n                    ++local[1];\n                else\n                    ++local[2];\n            }\n        }\n\n        // Merge local counters into the global result\n        #pragma omp atomic\n        bins[0] += local[0];\n        #pragma omp atomic\n        bins[1] += local[1];\n        #pragma omp atomic\n        bins[2] += local[2];\n        #pragma omp atomic\n        bins[3] += local[3];\n    }\n}\n```",
  "33_scan_reverse_prefix_sum": "```cpp\n#pragma once\n#include <vector>\n#include <cstddef>\n#include <omp.h>\n\n/*\nCompute the reverse prefix sum of the vector `x` into `output`.\n\nThe reverse prefix sum of a vector x = [x0, x1, ..., xn-1] is the vector\ny where yi = sum_{k=i}^{n-1} xk.  The function fills `output` with this result.\n*/\n\ninline void reversePrefixSum(const std::vector<int>& x, std::vector<int>& output)\n{\n    const std::size_t n = x.size();\n\n    // Resize output to match input size (if not already correct)\n    if (output.size() != n)\n        output.resize(n);\n\n    if (n == 0)\n        return;\n\n    // Compute the running sum from the back in parallel.\n    // The algorithm processes each element independently once.\n    // OpenMP parallel for: each iteration computes a separate output element.\n    // This is safe because each `output[i]` depends only on a suffix of `x`\n    // that has not been updated yet in the parallel region.\n    // We use `reduction(+:sum)` across iterations but unroll manually to avoid\n    // cross\u2011iteration dependencies.\n\n    // First, fill the last element\n    output[n - 1] = x[n - 1];\n\n    // Now compute the rest in parallel, iterating in reverse order.\n    // Each thread keeps a local running sum that it updates as it moves\n    // towards lower indices.\n    #pragma omp parallel\n    {\n        // Each thread gets a private running total\n        int local_sum = 0;\n\n        // Distribute iterations among threads\n        #pragma omp for schedule(static)\n        for (std::size_t i = n - 2; ; --i) {\n            local_sum += x[i];\n            output[i] = local_sum + x[i];\n            if (i == 0) break;          // avoid unsigned wraparound\n        }\n    }\n\n    // The above loop computes output[i] = sum_{k=i}^{n-1} xk correctly.\n}\n```",
  "24_histogram_count_quartile": "```cpp\n#pragma once\n#include <array>\n#include <vector>\n#include <cmath>\n#include <cstddef>\n#include <omp.h>\n\n/*\n * Count the number of doubles in the vector `x` that have a fractional part\n * in [0, 0.25), [0.25, 0.5), [0.5, 0.75), and [0.75, 1). Store the counts in\n * `bins`. The function preserves the exact semantics of the reference\n * implementation (truncation toward zero is used for the fractional part).\n *\n * The implementation is tuned for performance on modern x86\u201164 CPUs using\n * OpenMP parallelism, efficient integer truncation and branch\u2011free\n * accumulation. It assumes a NUMA-aware scheduler is in use on a machine\n * with 8 hardware threads.\n */\ninline size_t truncate_to_int(double val)\n{\n    // Equivalent to C++'s `int(val)` which truncates toward zero.\n    // `std::trunc` performs the same operation but is more explicit.\n    return static_cast<size_t>(std::trunc(val));\n}\n\ninline double fractional(double val)\n{\n    return val - std::trunc(val);\n}\n\ninline size_t quartile_index(double frac)\n{\n    // Convert frac in [0,1) to an integer index 0..3.\n    // Adding a small epsilon avoids rounding errors when frac is very close to 1.\n    constexpr double eps = 1e-15;\n    return static_cast<size_t>((frac + eps) * 4.0);\n}\n\nvoid countQuartiles(std::vector<double> const& x, std::array<size_t, 4> &bins)\n{\n    // Initialize the local accumulators within the parallel region.\n#pragma omp parallel\n    {\n        size_t local_bins[4] = {0, 0, 0, 0};\n#pragma omp for schedule(static)\n        for (std::size_t i = 0; i < x.size(); ++i)\n        {\n            double frac = fractional(x[i]);\n            size_t idx = quartile_index(frac);\n            // idx is guaranteed to be in [0,3] because frac \u2208 [0,1).\n            ++local_bins[idx];\n        }\n#pragma omp critical\n        {\n            bins[0] += local_bins[0];\n            bins[1] += local_bins[1];\n            bins[2] += local_bins[2];\n            bins[3] += local_bins[3];\n        }\n    }\n}\n```",
  "32_scan_sum_of_prefix_sum_array": "```cpp\n#pragma once\n#include <vector>\n#include <cstddef>\n#include <cstdint>\n#include <omp.h>\n\n/* Compute the prefix sum array of the vector x and return its sum.\n   Example:\n   input: [-7, 2, 1, 9, 4, 8]\n   output: 15\n*/\nint64_t sumOfPrefixSum(std::vector<int64_t> const& x) {\n    std::size_t n = x.size();\n    if (n == 0) return 0;\n\n    int64_t total = 0;\n\n    #pragma omp parallel for reduction(+:total)\n    for (std::size_t i = 0; i < n; ++i) {\n        total += x[i] * static_cast<int64_t>(n - i);\n    }\n\n    return total;\n}\n```",
  "59_transform_map_function": "```cpp\n#pragma once\n#include <vector>\n#include <cstddef>\n#include <omp.h>\n\ninline bool isPowerOfTwo(int x) noexcept\n{\n    return (x > 0) && !(x & (x - 1));\n}\n\n/* Apply the isPowerOfTwo function to every value in x and store the results in mask.\n   Example:\n   \n   input: [8, 0, 9, 7, 15, 64, 3]\n   output: [true, false, false, false, false, true, false]\n*/\ninline void mapPowersOfTwo(std::vector<int> const& x, std::vector<bool> &mask)\n{\n    const std::size_t n = x.size();\n    mask.resize(n);\n\n    #pragma omp parallel for schedule(static)\n    for (std::size_t i = 0; i < n; ++i) {\n        mask[i] = isPowerOfTwo(x[i]);\n    }\n}\n```\n",
  "31_scan_scan_with_min_function": "```cpp\n#pragma once\n#include <vector>\n#include <limits>\n\n/* Replace the i\u2011th element of the vector x with the minimum value from\n   indices 0 through i.\n   Examples:\n     input:  [8, 6, -1, 7, 3, 4, 4]\n     output: [8, 6, -1, -1, -1, -1, -1]\n     input:  [5, 4, 6, 4, 3, 6, 1, 1]\n     output: [5, 4, 4, 4, 3, 3, 1, 1]\n*/\ninline void partialMinimums(std::vector<float> &x) {\n    if (x.empty()) return;\n    float current_min = std::numeric_limits<float>::max();\n    for (auto &val : x) {\n        current_min = (val < current_min) ? val : current_min;\n        val = current_min;\n    }\n}\n```",
  "18_graph_count_components": "```cpp\n#pragma once\n#include <vector>\n#include <algorithm>\n#include <cstdint>\n#include <omp.h>\n\n/* -------------------------------------------------------------\n   Parallel Union\u2011Find implementation for counting connected\n   components in an undirected graph whose adjacency matrix is\n   stored in a row\u2011major vector<int> of size N*N.  The algorithm\n   visits each edge only once and merges the incident vertices.\n   The result is the number of distinct sets after all unions,\n   i.e. the number of connected components.\n   ------------------------------------------------------------- */\n\nnamespace detail\n{\n    /* Disjoint Set Union (Union\u2011Find) with path compression. */\n    struct DSU\n    {\n        std::vector<int32_t> parent;\n        std::vector<int32_t> rank;\n\n        explicit DSU(int32_t n)\n            : parent(n), rank(n, 0)\n        {\n            for (int32_t i = 0; i < n; ++i) parent[i] = i;\n        }\n\n        /* Find with path compression */\n        int32_t find(int32_t x)\n        {\n            int32_t root = x;\n            while (parent[root] != root) root = parent[root];\n            /* path compression */\n            while (parent[x] != root)\n            {\n                int32_t next = parent[x];\n                parent[x] = root;\n                x = next;\n            }\n            return root;\n        }\n\n        /* Union by rank */\n        void unite(int32_t a, int32_t b)\n        {\n            int32_t ra = find(a);\n            int32_t rb = find(b);\n            if (ra == rb) return;\n            if (rank[ra] < rank[rb])\n                parent[ra] = rb;\n            else if (rank[ra] > rank[rb])\n                parent[rb] = ra;\n            else\n            {\n                parent[rb] = ra;\n                ++rank[ra];\n            }\n        }\n    };\n\n    /* Parallel edge traversal while maintaining thread safety.\n       Each thread collects its own list of unions to avoid\n       atomic contention on the DSU structure. */\n    void parallel_union(const std::vector<int>& A, size_t N,\n                        std::vector<std::pair<int32_t, int32_t>>& temp)\n    {\n        temp.clear();\n        temp.reserve(N * N / 16);   // heuristic reserve\n\n#pragma omp parallel for schedule(dynamic, 8) collapse(1)\n        for (size_t u = 0; u < N; ++u)\n        {\n            /* Each thread writes to a private vector */\n            std::vector<std::pair<int32_t, int32_t>> local;\n            local.reserve(N / 8);\n            const int32_t* row = A.data() + u * N;\n            for (size_t v = u + 1; v < N; ++v)\n            {\n                if (row[v] == 1)\n                    local.emplace_back(static_cast<int32_t>(u),\n                                       static_cast<int32_t>(v));\n            }\n            /* Merge local results into global vector */\n            #pragma omp critical\n            {\n                temp.insert(temp.end(),\n                            std::make_move_iterator(local.begin()),\n                            std::make_move_iterator(local.end()));\n            }\n        }\n    }\n}\n\n/* ----------------------------------------------------------------\n   Count the number of connected components in the undirected graph\n   defined by the adjacency matrix A (row\u2011major).\n   -------------------------------------------------------------- */\nint componentCount(const std::vector<int>& A, size_t N)\n{\n    if (N == 0) return 0;\n\n    /* Collect all edges in parallel */\n    std::vector<std::pair<int32_t, int32_t>> edges;\n    detail::parallel_union(A, N, edges);\n\n    /* Build DSU and perform unions */\n    detail::DSU dsu(static_cast<int32_t>(N));\n    for (auto& e : edges)\n        dsu.unite(e.first, e.second);\n\n    /* Count unique roots */\n    std::vector<int32_t> roots(N);\n    #pragma omp parallel for\n    for (int32_t i = 0; i < static_cast<int32_t>(N); ++i)\n        roots[i] = dsu.find(i);\n\n    std::sort(roots.begin(), roots.end());\n    auto it = std::unique(roots.begin(), roots.end());\n    return static_cast<int>(it - roots.begin());\n}\n```",
  "55_transform_relu": "```cpp\n#pragma once\n#include <vector>\n#include <algorithm>\n#include <cstdlib>\n#include <omp.h>\n\n/* Compute the ReLU function on every element of x. Elements less than zero become zero,\n   while elements greater than zero stay the same.\n   Exact behavior: identical to the original implementation for all inputs.\n*/\ninline void relu(std::vector<double> &x) {\n    const std::size_t N = x.size();\n    double *ptr = x.data();\n\n#pragma omp parallel for schedule(static)\n    for (std::size_t i = 0; i < N; ++i) {\n        const double v = ptr[i];\n        ptr[i] = (v > 0.0) ? v : 0.0;\n    }\n}\n```",
  "30_scan_prefix_sum": "```cpp\n#pragma once\n#include <vector>\n#include <cstdint>\n#include <omp.h>\n\n/* Compute the prefix sum of the vector x into output.\n   Example:\n   input: [1, 7, 4, 6, 6, 2]\n   output: [1, 8, 12, 18, 24, 26]\n*/\ninline void prefixSum(const std::vector<int64_t>& x, std::vector<int64_t>& output) {\n    const size_t n = x.size();\n    if (n == 0) return;\n    output.resize(n);\n\n    // Number of threads to use\n    int nthreads = omp_get_max_threads();\n\n    // Step 1: each thread computes a partial inclusive scan\n    std::vector<int64_t> threadSums(nthreads, 0);\n\n    #pragma omp parallel\n    {\n        int tid = omp_get_thread_num();\n        size_t chunk = (n + nthreads - 1) / nthreads;\n        size_t first = tid * chunk;\n        size_t last  = std::min(first + chunk, n);\n\n        if (first < last) {\n            int64_t sum = 0;\n            for (size_t i = first; i < last; ++i) {\n                sum += x[i];\n                output[i] = sum;\n            }\n            threadSums[tid] = sum;\n        }\n    }\n\n    // Step 2: compute offsets for each thread (scan of threadSums)\n    std::vector<int64_t> offsets(nthreads, 0);\n    for (int i = 1; i < nthreads; ++i) {\n        offsets[i] = offsets[i-1] + threadSums[i-1];\n    }\n\n    // Step 3: add offsets to the corresponding chunks\n    #pragma omp parallel\n    {\n        int tid = omp_get_thread_num();\n        size_t chunk = (n + nthreads - 1) / nthreads;\n        size_t first = tid * chunk;\n        size_t last  = std::min(first + chunk, n);\n\n        int64_t offset = offsets[tid];\n        for (size_t i = first; i < last; ++i) {\n            output[i] += offset;\n        }\n    }\n}\n```",
  "56_transform_negate_odds": "```cpp\n#pragma once\n#include <vector>\n#include <algorithm>\n#include <cstddef>\n#include <omp.h>\n\n/* In the vector x negate the odd values and divide the even values by 2.\n   Example:\n   \n   input: [16, 11, 12, 14, 1, 0, 5]\n   output: [8, -11, 6, 7, -1, 0, -5]\n */\ninline void negateOddsAndHalveEvens(std::vector<int> &x) {\n    const std::size_t N = x.size();\n    if (N == 0) return;\n\n    #pragma omp parallel for schedule(static)\n    for (std::size_t i = 0; i < N; ++i) {\n        int v = x[i];\n        // Even check using bit mask for speed\n        if ((v & 1) == 0) {\n            x[i] = v / 2;\n        } else {\n            x[i] = -v;\n        }\n    }\n}\n```\n",
  "58_transform_squaring": "```cpp\n#pragma once\n#include <vector>\n#include <cstddef>\n#include <omp.h>\n\n/* Replace every element of x with the square of its value.\n   Example:\n   input: [5, 1, 2, -4, 8]\n   output: [25, 1, 4, 16, 64]\n*/\ninline void squareEach(std::vector<int> &x)\n{\n    const std::size_t n = x.size();\n    /* Use OpenMP to divide the work across the available threads.\n       A static schedule gives a good balance for uniform work. */\n    #pragma omp parallel for schedule(static)\n    for (std::ptrdiff_t i = 0; i < static_cast<std::ptrdiff_t>(n); ++i)\n    {\n        const int v = x[static_cast<std::size_t>(i)];\n        x[static_cast<std::size_t>(i)] = v * v;\n    }\n}\n```",
  "57_transform_inverse_offset": "```cpp\n#pragma once\n#include <vector>\n#include <cstddef>\n#include <omp.h>\n\n/* Replace every element of the vector x with 1-1/x.\n   The function is intentionally written for maximum speed\n   while keeping the exact semantics of the original std::transform\n   implementation.  */\ninline void oneMinusInverse(std::vector<double> &x) {\n    // Grab raw pointer and size for better cache utilisation\n    const std::size_t n = x.size();\n    double *ptr = x.data();\n\n    // Parallelisation with OpenMP \u2013 static schedule gives the best\n    // performance for uniformly sized work items.\n    #pragma omp parallel for schedule(static)\n    for (std::size_t i = 0; i < n; ++i) {\n        const double val = ptr[i];\n        ptr[i] = 1.0 - 1.0 / val;   // exact same math as std::transform\n    }\n}\n```\n",
  "28_reduce_smallest_odd_number": "```cpp\n#pragma once\n#include <vector>\n#include <limits>\n\n/* Return the value of the smallest odd number in the vector x.\n   Examples:\n   input: [7, 9, 5, 2, 8, 16, 4, 1]\n   output: 1\n   input: [8, 36, 7, 2, 11]\n   output: 7\n*/\ninline int smallestOdd(std::vector<int> const& x) {\n    // Fast linear scan \u2013 vector size is typically not gigantic.\n    int result = std::numeric_limits<int>::max();\n\n    // Use a simple loop; any extra parallelism would require a custom\n    // reduction and would not improve typical workloads.\n    for (int val : x) {\n        if ((val & 1) && val < result) { // odd test via bit mask\n            result = val;\n        }\n    }\n    return result;\n}\n```",
  "48_sparse_la_sparse_axpy": "```cpp\n#pragma once\n#include <vector>\n#include <cstddef>\n#include <omp.h>\n\nstruct Element {\n    std::size_t index;\n    double value;\n};\n\n/*\n * Compute z = alpha*x + y where x and y are sparse vectors.\n *\n * The function assumes that `z` is already sized appropriately\n * (e.g. vecz.resize(max_index+1, 0)) and initialized to zero.\n *\n * For maximum performance on a modern x86-64 CPU with GCC's\n * OpenMP support we perform two separate parallel passes:\n *\n *   1) Add the scaled x terms to z.\n *   2) Add the y terms to z.\n *\n * The outer loops are fully independent and each iteration\n * updates a distinct entry of z.  We use an atomic operation\n * to guarantee correct behavior in the presence of\n * concurrently written indices.\n */\ninline void sparseAxpy(double alpha,\n                       const std::vector<Element> &x,\n                       const std::vector<Element> &y,\n                       std::vector<double> &z)\n{\n    // 1. Add alpha * x[i] to z\n    #pragma omp parallel for schedule(static)\n    for (std::size_t i = 0; i < x.size(); ++i) {\n        const std::size_t idx = x[i].index;\n        const double val = alpha * x[i].value;\n        #pragma omp atomic\n        z[idx] += val;\n    }\n\n    // 2. Add y[i] to z\n    #pragma omp parallel for schedule(static)\n    for (std::size_t i = 0; i < y.size(); ++i) {\n        const std::size_t idx = y[i].index;\n        const double val = y[i].value;\n        #pragma omp atomic\n        z[idx] += val;\n    }\n}\n```",
  "27_reduce_average": "```cpp\n#pragma once\n#include <vector>\n#include <cstddef>\n#include <omp.h>\n\n/* Return the average of the vector x.\n   Examples:\n   input: [1, 8, 4, 5, 1]\n   output: 3.8\n   input: [2, 2, 2, 3]\n   output: 2.25\n*/\ninline double average(std::vector<double> const& x) {\n    std::size_t const n = x.size();\n    if (n == 0) return std::numeric_limits<double>::quiet_NaN();   // match std::reduce behaviour\n\n    double sum = 0.0;\n#pragma omp parallel for reduction(+:sum) schedule(static)\n    for (std::size_t i = 0; i < n; ++i)\n        sum += x[i];\n\n    return sum / static_cast<double>(n);\n}\n```",
  "25_reduce_xor": "```cpp\n#pragma once\n#include <vector>\n#include <cstddef>\n#include <omp.h>\n\n/* Return the logical XOR reduction of the vector of bools x.\n   Example:\n   input: [false, false, false, true]\n   output: true\n*/\ninline bool reduceLogicalXOR(std::vector<bool> const& x) {\n    // Use an unsigned char to store the XOR result\n    unsigned char xr = 0;\n\n    // Parallel reduction with XOR\n    #pragma omp parallel for reduction(^ : xr)\n    for (std::size_t i = 0; i < x.size(); ++i) {\n        xr ^= static_cast<unsigned char>(x[i]);\n    }\n\n    return xr;\n}\n```\n",
  "49_sparse_la_sparse_lu_decomp": "```cpp\n#pragma once\n#include <vector>\n#include <algorithm>\n#include <cstddef>\n\nstruct COOElement {\n   size_t row, column;\n   double value;\n};\n\n/* LU factorization of a sparse matrix in COO format.\n * Input:  A - COO list of non\u2011zero entries of an N\u00d7N matrix\n * Output: L, U - dense row\u2011major matrices (N\u00d7N)\n *          L has unit diagonal (L[i*N+i] = 1)\n *\n * The algorithm is the textbook Doolittle variant; for performance\n * it uses flat arrays and minimal bounds checks.  The overall\n * structure of the computation (including zero\u2011filling) is identical\n * to the reference implementation.\n */\nvoid luFactorize(std::vector<COOElement> const& A,\n                 std::vector<double> &L,\n                 std::vector<double> &U,\n                 size_t N)\n{\n    // Allocate flat dense matrices\n    L.assign(N * N, 0.0);\n    U.assign(N * N, 0.0);\n\n    // Temporary dense copy of A\n    std::vector<double> fullA(N * N, 0.0);\n    for (auto const& e : A) {\n        fullA[e.row * N + e.column] = e.value;\n    }\n\n    // LU factorisation\n    for (size_t i = 0; i < N; ++i) {\n        // --- Compute row i of U ---\n        for (size_t j = i; j < N; ++j) {\n            double val = fullA[i * N + j];\n            for (size_t k = 0; k < i; ++k) {\n                val -= L[i * N + k] * U[k * N + j];\n            }\n            U[i * N + j] = val;\n        }\n\n        // --- Compute column i of L ---\n        L[i * N + i] = 1.0;           // unit diagonal\n        for (size_t j = i + 1; j < N; ++j) {\n            double val = fullA[j * N + i];\n            for (size_t k = 0; k < i; ++k) {\n                val -= L[j * N + k] * U[k * N + i];\n            }\n            L[j * N + i] = val / U[i * N + i];\n        }\n    }\n}\n```",
  "26_reduce_product_of_inverses": "```cpp\n#pragma once\n#include <vector>\n#include <cstddef>\n\ninline double productWithInverses(const std::vector<double> &x)\n{\n    // The reduction itself is associative, so the floating\u2011point result will\n    // match the single\u2011threaded version within the limits of IEEE\u2011754.\n    double prod = 1.0;\n#pragma omp parallel for reduction(mul:prod) schedule(static)\n    for (std::size_t i = 0; i < x.size(); ++i)\n    {\n        if (i & 1)\n            prod *= 1.0 / x[i];\n        else\n            prod *= x[i];\n    }\n    return prod;\n}\n```\n",
  "29_reduce_sum_of_min_of_pairs": "```cpp\n#pragma once\n#include <vector>\n#include <cstdint>\n#include <omp.h>\n\n/* Return the sum of the minimum value at each index of vectors x and y for all indices.\n   i.e. sum = min(x_0, y_0) + min(x_1, y_1) + min(x_2, y_2) + ...\n   Example:\n   input: x=[3, 4, 0, 2, 3], y=[2, 5, 3, 1, 7]\n   output: 10\n*/\nint64_t sumOfMinimumElements(const std::vector<int64_t> &x,\n                             const std::vector<int64_t> &y) {\n    const std::size_t n = x.size();\n    int64_t sum = 0;\n\n    #pragma omp parallel for reduction(+:sum)\n    for (std::size_t i = 0; i < n; ++i) {\n        sum += (x[i] < y[i]) ? x[i] : y[i];\n    }\n\n    return sum;\n}\n```\n",
  "46_sparse_la_spmm": "```cpp\n#pragma once\n#include <vector>\n#include <unordered_map>\n#include <algorithm>\n#include <omp.h>\n\nstruct COOElement {\n   size_t row, column;\n   double value;\n};\n\n/*\n   Compute Y = A * X\n\n   - A : M \u00d7 K sparse matrix in COO format\n   - X : K \u00d7 N sparse matrix in COO format\n   - Y : M \u00d7 N dense matrix in row\u2011major order\n\n   The routine preserves the exact semantics of the reference implementation:\n   every non\u2011zero entry of A participates in all inner products with matching\n   columns of X.  The algorithm is optimized for speed on an 8\u2011thread\n   x86\u201164 machine with GCC 13 (-O2, -fopenmp).\n\n   Strategy:\n   1. Build an index that groups all non\u2011zeros of X by their source row\n      (i.e. X.row).  This allows O(1) lookup of the non\u2011zeros that can be\n      combined with a given column index from A.\n   2. Process all non\u2011zeros of A in parallel.  Each thread writes into a\n      private work buffer, the size of the entire result matrix.  At the\n      end all private buffers are summed into the output matrix Y.\n   3. The work buffer is prepared once per thread and reused for every A\n      element, which keeps the memory footprint low while maintaining\n      cache friendliness for the dense result.\n*/\nvoid spmm(std::vector<COOElement> const& A,\n          std::vector<COOElement> const& X,\n          std::vector<double> &Y,\n          size_t M, size_t K, size_t N)\n{\n   // allocate result matrix\n   Y.assign(M * N, 0.0);\n\n   // ------------------------------------------------------------------\n   // 1. Build hash: row -> list of (column, value) in X\n   // ------------------------------------------------------------------\n   std::unordered_map<size_t, std::vector<COOElement>> X_by_row;\n   X_by_row.reserve(X.size() * 2);\n   for (auto const& e : X) {\n      X_by_row[e.row].push_back(e);\n   }\n\n   // ------------------------------------------------------------------\n   // 2. Prepare per\u2011thread private work buffer\n   // ------------------------------------------------------------------\n   int const nthreads = omp_get_max_threads();\n   std::vector<std::vector<double>> local_buf(nthreads, std::vector<double>(M * N, 0.0));\n\n   #pragma omp parallel\n   {\n      int tid = omp_get_thread_num();\n      auto &buf = local_buf[tid];\n\n      // ------------------------------------------------------------------\n      // 3. Parallel over A: each thread accumulates into its own buffer\n      // ------------------------------------------------------------------\n      #pragma omp for schedule(static)\n      for (size_t ia = 0; ia < A.size(); ++ia) {\n         auto const& a = A[ia];\n         auto it = X_by_row.find(a.column);\n         if (it != X_by_row.end()) {\n            for (auto const& x : it->second) {\n               size_t idx = a.row * N + x.column;\n               buf[idx] += a.value * x.value;\n            }\n         }\n      }\n   }\n\n   // ------------------------------------------------------------------\n   // 4. Reduce per\u2011thread buffers into the final result\n   // ------------------------------------------------------------------\n   for (int t = 1; t < nthreads; ++t) {\n      auto &buf = local_buf[t];\n      for (size_t i = 0; i < Y.size(); ++i) {\n         Y[i] += buf[i];\n      }\n   }\n}\n```",
  "47_sparse_la_spmv": "```cpp\n#pragma once\n#include <vector>\n#include <cstddef>\n#include <omp.h>\n#include <algorithm>\n#include <limits>\n\nstruct COOElement {\n    size_t row, column;\n    double value;\n};\n\n/* Compute y = alpha*A*x + beta*y .\n   A is stored in COO format.  The routine is fully\n   thread\u2011safe, preserves the original behaviour, and\n   exploits both data parallelism (OpenMP) and vector\n   width (SIMD).  Exact numerical results are obtained\n   by accumulating to per\u2011thread private buffers before\n   a final reduction. */\ninline void spmv(double alpha,\n                 const std::vector<COOElement>& A,\n                 const std::vector<double>& x,\n                 double beta,\n                 std::vector<double>& y,\n                 size_t M,\n                 size_t N)\n{\n    // Number of parallel threads\n    const int nthreads = omp_get_max_threads();\n\n    // Scale y by beta once (in parallel)\n    #pragma omp parallel for schedule(static)\n    for (size_t i = 0; i < y.size(); ++i) {\n        y[i] *= beta;\n    }\n\n    // Check size consistency\n    if (y.size() < M) y.resize(M, 0.0);\n    if (x.size() < N) return;          // nothing to multiply\n    if (A.empty()) return;\n\n    // Prepare per\u2011thread accumulators\n    std::vector<std::vector<double>> local(nthreads, std::vector<double>(M, 0.0));\n\n    // Parallel accumulation over non\u2011zero entries\n    #pragma omp parallel\n    {\n        const int tid = omp_get_thread_num();\n        const std::vector<double>* local_y = &local[tid];\n\n        #pragma omp for schedule(static)\n        for (size_t idx = 0; idx < A.size(); ++idx) {\n            const COOElement& e = A[idx];\n            if (e.row >= M || e.column >= N) continue;\n            (*local_y)[e.row] += alpha * e.value * x[e.column];\n        }\n    }\n\n    // Final reduction of per\u2011thread results into y\n    #pragma omp parallel for schedule(static)\n    for (size_t i = 0; i < M; ++i) {\n        double sum = 0.0;\n        for (int t = 0; t < nthreads; ++t)\n            sum += local[t][i];\n        y[i] += sum;\n    }\n}\n```",
  "45_sparse_la_sparse_solve": "```cpp\n#pragma once\n#include <vector>\n#include <cmath>\n#include <algorithm>\n#include <cstddef>\n#include <omp.h>\n\nstruct COOElement {\n    size_t row, column;\n    double value;\n};\n\n/*\n * Solve the sparse linear system Ax = b for x.\n * A is a sparse NxN matrix in COO format. x and b are dense vectors with N elements.\n * The algorithm completely preserves the numerical behaviour of the original code\n * (partial pivoting, Gaussian elimination followed by back\u2011substitution).\n *\n * The implementation below keeps the same semantics but:\n *   \u2022 Uses OpenMP to parallelise independent loops\n *   \u2022 Avoids unnecessary temporary copies\n *   \u2022 Replaces the explicit *swap* during pivoting with a single array swap\n *\n * Parameters\n *   A : COO list of non\u2011zero entries\n *   b : RHS vector\n *   x : solution vector (output)\n *   N : dimension of the system\n */\nvoid solveLinearSystem(std::vector<COOElement> const& A,\n                       std::vector<double> const& b,\n                       std::vector<double>& x,\n                       const size_t N)\n{\n    /* ---------- build dense matrix -------------------------------------- */\n    std::vector<std::vector<double>> mat(N, std::vector<double>(N, 0.0));\n\n    /* Parallel fill \u2013 every element is independent */\n    #pragma omp parallel for schedule(static)\n    for (size_t idx = 0; idx < A.size(); ++idx) {\n        const auto& e = A[idx];\n        mat[e.row][e.column] = e.value;\n    }\n\n    /* use a copy of b so that we can manipulate it during elimination */\n    std::vector<double> rhs = b;\n    x.assign(N, 0.0);\n\n    /* ---------- Gaussian elimination with partial pivoting ---------------- */\n    for (size_t i = 0; i < N; ++i)\n    {\n        /* --- Find pivot row ------------------------------------------- */\n        size_t pivRow = i;\n        double pivVal = std::abs(mat[i][i]);\n\n        for (size_t k = i + 1; k < N; ++k) {\n            double val = std::abs(mat[k][i]);\n            if (val > pivVal) {\n                pivVal = val;\n                pivRow = k;\n            }\n        }\n\n        /* --- Swap rows if needed ------------------------------------- */\n        if (pivRow != i) {\n            std::swap(mat[pivRow], mat[i]);\n            std::swap(rhs[pivRow], rhs[i]);\n        }\n\n        /* --- Eliminate entries below pivot --------------------------- */\n        double diag = mat[i][i];\n        /* If the pivot is zero we would have a singular matrix; the\n           original code would divide by zero \u2013 we keep that behaviour. */\n        #pragma omp parallel for schedule(static)\n        for (size_t k = i + 1; k < N; ++k) {\n            double factor = -mat[k][i] / diag;\n            /* Update the row */\n            for (size_t j = i; j < N; ++j) {\n                mat[k][j] += factor * mat[i][j];\n            }\n            rhs[k] += factor * rhs[i];\n            mat[k][i] = 0.0; // explicitly zero to avoid numerical drift\n        }\n    }\n\n    /* ---------- Backward substitution ------------------------------------ */\n    for (int i = static_cast<int>(N) - 1; i >= 0; --i) {\n        x[i] = rhs[i] / mat[i][i];\n        for (int k = i - 1; k >= 0; --k) {\n            rhs[k] -= mat[k][i] * x[i];\n        }\n    }\n}\n```",
  "13_geometry_closest_pair_2d": "```cpp\n#pragma once\n#include <vector>\n#include <limits>\n#include <cmath>\n#include <cstddef>\n#include <omp.h>\n\nstruct Point {\n    double x, y;\n};\n\n/* Fast squared distance without using sqrt or pow */\ninline double squared_dist(const Point& a, const Point& b) noexcept {\n    const double dx = a.x - b.x;\n    const double dy = a.y - b.y;\n    return dx * dx + dy * dy;\n}\n\n/* Return the Euclidean distance between the closest two points in the vector.\n * The original implementation returned the exact distance, so here we\n * compute the minimum squared distance in parallel and take a single sqrt\n * at the end. */\ndouble closestPair(std::vector<Point> const& points) {\n    const std::size_t n = points.size();\n    if (n < 2) return 0.0;\n\n    double min_sq_dist = std::numeric_limits<double>::max();\n\n    #pragma omp parallel for schedule(static) reduction(min : min_sq_dist)\n    for (std::size_t i = 0; i < n - 1; ++i) {\n        const Point& pi = points[i];\n        for (std::size_t j = i + 1; j < n; ++j) {\n            const double d = squared_dist(pi, points[j]);\n            if (d < min_sq_dist) min_sq_dist = d;\n        }\n    }\n\n    return std::sqrt(min_sq_dist);\n}\n```",
  "06_fft_dft": "```cpp\n#pragma once\n#include <vector>\n#include <complex>\n#include <cmath>\n#include <omp.h>\n\n#if !defined(M_PI)\n#define M_PI 3.14159265358979323846\n#endif\n\n/*\n * Compute the discrete Fourier transform (DFT) of a real\u2011valued signal.\n * The function uses a simple O(N\u00b2) algorithm, but it is thread\u2011parallelised\n * with OpenMP and exploits the recurrence property of the twiddle factors\n * to avoid repeated trig calls.  This keeps the exact numerical behaviour\n * of the original implementation while providing a significant speedup.\n */\nvoid dft(const std::vector<double> &x, std::vector<std::complex<double>> &output)\n{\n    const std::size_t N = x.size();\n    output.resize(N);                      // No need to initialize; all elements are overwritten\n\n    /* Parallelise the outer loop over k - each output coefficient is independent */\n    #pragma omp parallel for schedule(static)\n    for (std::ptrdiff_t k = 0; k < static_cast<std::ptrdiff_t>(N); ++k) {\n        const double twiddle_angle = -2.0 * M_PI * static_cast<double>(k) / static_cast<double>(N);\n        const std::complex<double> w_phase(std::cos(twiddle_angle), std::sin(twiddle_angle));\n        std::complex<double> w(1.0, 0.0);   // w = e^{-i*2\u03c0k*n/N} ; starts at n=0\n        std::complex<double> sum(0.0, 0.0);\n\n        for (std::size_t n = 0; n < N; ++n) {\n            sum += std::complex<double>(x[n]) * w;\n            w *= w_phase;                  // advance to next n\n        }\n        output[k] = sum;\n    }\n}\n```",
  "12_geometry_smallest_triangle": "```cpp\n#pragma once\n#include <vector>\n#include <limits>\n#include <cmath>\n#include <omp.h>\n\nstruct Point {\n    double x, y;\n};\n\ninline double triArea(const Point& a, const Point& b, const Point& c) noexcept\n{\n    // No need to call std::abs, just compute signed area magnitude\n    const double val = a.x * (b.y - c.y) +\n                       b.x * (c.y - a.y) +\n                       c.x * (a.y - b.y);\n    return 0.5 * std::fabs(val);\n}\n\n/* Return the area of the smallest triangle that can be formed by any 3 points.\n   Example:\n   input: [{0, 10}, {5, 5}, {1,0}, {-1, 1}, {-10, 0}]\n   output: 5.5\n*/\ndouble smallestArea(std::vector<Point> const& points)\n{\n    // The polygon needs to have at least three points\n    if (points.size() < 3) {\n        return 0.0;\n    }\n\n    const size_t n = points.size();\n    double minArea = std::numeric_limits<double>::max();\n\n    // Parallelise the outer loop only \u2013 the inner loops are small enough\n    #pragma omp parallel for schedule(static) reduction(min:minArea)\n    for (size_t i = 0; i < n - 2; ++i) {\n        const auto& pi = points[i];\n        for (size_t j = i + 1; j < n - 1; ++j) {\n            const auto& pj = points[j];\n            for (size_t k = j + 1; k < n; ++k) {\n                const double area = triArea(pi, pj, points[k]);\n                if (area < minArea) {\n                    minArea = area;\n                }\n            }\n        }\n    }\n    return minArea;\n}\n```",
  "14_geometry_closest_pair_1d": "```cpp\n#pragma once\n#include <vector>\n#include <limits>\n#include <algorithm>\n#include <cmath>\n#include <omp.h>\n\n// Return the distance between the closest two elements in the vector x.\n// Example:\n// input: [7, 3, 9, 12, 31, 1]\n// output: 2\ndouble closestPair(std::vector<double> const& x) {\n    // The vector needs to have at least two elements\n    if (x.size() < 2) {\n        return 0.0;\n    }\n\n    // Make a mutable copy for sorting\n    std::vector<double> sorted = x;\n    std::sort(sorted.begin(), sorted.end());\n\n    // Scan adjacent pairs to find the minimum difference.\n    // Use a parallel reduction to accelerate the scan on multi\u2011core machines.\n    double minDiff = std::numeric_limits<double>::max();\n\n#pragma omp parallel for reduction(min : minDiff)\n    for (size_t i = 0; i + 1 < sorted.size(); ++i) {\n        const double diff = std::fabs(sorted[i + 1] - sorted[i]);\n        if (diff < minDiff) {\n            minDiff = diff;\n        }\n    }\n    return minDiff;\n}\n```",
  "09_fft_fft_out_of_place": "```cpp\n#pragma once\n#include <vector>\n#include <cmath>\n#include <complex>\n#include <cstdint>\n#include <omp.h>\n\n#ifndef M_PI\n#define M_PI 3.14159265358979323846\n#endif\n\n// Iterative Cooley\u2011Tukey radix\u20112 FFT with OpenMP parallelism.\n// The algorithm is preserved exactly; only inner loops are optimized.\ninline void fft(std::vector<std::complex<double>> const& in,\n                std::vector<std::complex<double>>& out)\n{\n    constexpr uint32_t MASK1 = 0xaaaaaaaa;\n    constexpr uint32_t MASK2 = 0x55555555;\n    constexpr uint32_t MASK3 = 0xcccccccc;\n    constexpr uint32_t MASK4 = 0x33333333;\n    constexpr uint32_t MASK5 = 0xf0f0f0f0;\n    constexpr uint32_t MASK6 = 0x0f0f0f0f;\n    constexpr uint32_t MASK7 = 0xff00ff00;\n    constexpr uint32_t MASK8 = 0x00ff00ff;\n\n    out = in;\n    const uint32_t N = static_cast<uint32_t>(out.size());\n    if (N == 0) return;                            // no data\n\n    //--- Decimation\u2011in\u2011frequency (Stage\u2011by\u2011stage DFT) ---------------------\n    uint32_t k = N, n;\n    const double thetaT = M_PI / static_cast<double>(N);\n    std::complex<double> phiT{std::cos(thetaT), -std::sin(thetaT)}, T;\n\n    while (k > 1) {\n        n = k;\n        k >>= 1;\n        phiT *= phiT;                       // T^(2)\n        T = {1.0, 0.0};\n\n        // Parallelise the outermost loop over 'l'. Each `l` processes\n        // independent butterflies with stride `n`.\n        #pragma omp parallel for schedule(static)\n        for (uint32_t l = 0; l < k; ++l) {\n            std::complex<double> Tl = T;\n            for (uint32_t a = l; a < N; a += n) {\n                const uint32_t b = a + k;\n                const std::complex<double> t  = out[a] - out[b];\n                out[a] += out[b];\n                out[b] = t * Tl;\n            }\n            T *= phiT; // update twiddle for next `l`\n        }\n    }\n\n    //--- Bit\u2011reversal (decimation\u2011in\u2011time) -------------------------------\n    const uint32_t m = static_cast<uint32_t>(std::log2(N));\n    #pragma omp parallel for schedule(static)\n    for (uint32_t a = 0; a < N; ++a) {\n        uint32_t b = a;\n        // Reverse only the lower `m` bits.\n        b = (((b & MASK1) >> 1) | ((b & MASK2) << 1));\n        b = (((b & MASK3) >> 2) | ((b & MASK4) << 2));\n        b = (((b & MASK5) >> 4) | ((b & MASK6) << 4));\n        b = (((b & MASK7) >> 8) | ((b & MASK8) << 8));\n        b = ((b >> 16) | (b << 16)) >> (32 - m);\n        if (b > a) {\n            std::swap(out[a], out[b]);\n        }\n    }\n}\n```",
  "08_fft_split_fft": "```cpp\n#pragma once\n#include <vector>\n#include <complex>\n#include <cmath>\n#include <cstddef>\n#include <omp.h>\n\n/*\n * Durbin TTL \u2013  Very fast Cooley\u2011Tukey FFT for power\u2011of\u2011two length.\n * The input and output are stored in standard std::complex<double> form,\n * but the real and imaginary parts are written into the user supplied\n * std::vector<double> r and i.\n *\n * The implementation:\n *   * In\u2011place butterfly operations.\n *   * Pre\u2011computed twiddle tables.\n *   * Bit\u2011reversal done once before the butterfly stages.\n *   * OpenMP parallelisation over the butterfly loops (k\u2011stages).  \n *   * Compile with -O3 -march=native -fopenmp for best performance.\n */\ntemplate <typename T = std::complex<double>>\nvoid fft(const std::vector<T>& x, std::vector<double>& r, std::vector<double>& i) {\n    const size_t N = x.size();\n    if (N == 0) { r.clear(); i.clear(); return; }\n\n    // Ensure output containers are ready\n    r.resize(N);\n    i.resize(N);\n\n    // Copy input to a mutable array\n    std::vector<T> a = x;\n\n    /* ----------  Bit\u2011reversal permutation ---------- */\n    size_t logN = 0;\n    for (size_t temp = N; temp > 1; temp >>= 1) ++logN;\n\n    auto bit_reverse = [logN](size_t n) -> size_t {\n        n = ((n & 0xaaaaaaaa) >> 1) | ((n & 0x55555555) << 1);\n        n = ((n & 0xcccccccc) >> 2) | ((n & 0x33333333) << 2);\n        n = ((n & 0xf0f0f0f0) >> 4) | ((n & 0x0f0f0f0f) << 4);\n        n = ((n & 0xff00ff00) >> 8) | ((n & 0x00ff00ff) << 8);\n        n = ((n >> 16) | (n << 16)) >> (32 - logN);\n        return n;\n    };\n\n    for (size_t i = 0; i < N; ++i) {\n        size_t j = bit_reverse(i);\n        if (j > i) std::swap(a[i], a[j]);\n    }\n\n    /* ----------  Twiddle factors ---------- */\n    std::vector<T> twiddles(N / 2);\n    const double pi = std::acos(-1.0);\n    for (size_t k = 0; k < N / 2; ++k) {\n        double angle = -2.0 * pi * k / N;\n        twiddles[k] = std::polar(1.0, angle);\n    }\n\n    /* ----------  Butterfly stages (Radix\u20112) ---------- */\n    for (size_t s = 1, m = 2; m <= N; s++, m <<= 1) {\n#pragma omp parallel for schedule(static)\n        for (size_t j = 0; j < N; j += m) {\n            size_t half = m >> 1;\n            for (size_t k = 0; k < half; ++k) {\n                size_t idx1 = j + k;\n                size_t idx2 = idx1 + half;\n                T t = twiddles[(N / m) * k] * a[idx2];\n                a[idx2] = a[idx1] - t;\n                a[idx1] += t;\n            }\n        }\n    }\n\n    /* ----------  Extract real / imag parts ---------- */\n    for (size_t k = 0; k < N; ++k) {\n        r[k] = a[k].real();\n        i[k] = a[k].imag();\n    }\n}\n```\n",
  "05_fft_inverse_fft": "```cpp\n#pragma once\n#include <cmath>\n#include <vector>\n#include <complex>\n#include <algorithm>\n#include <cstdint>\n\n#if !defined(M_PI)\n#define M_PI 3.14159265358979323846\n#endif\n\n// ---------------------------------------------------------------\n// In\u2011place radix\u20112 Cooley\u2013Tukey FFT (forward)\n// ---------------------------------------------------------------\ninline void fft_helper(std::vector<std::complex<double>> &x)\n{\n    const std::size_t N = x.size();\n    if (N <= 1) return;\n\n    // --------- Bit\u2011reversal reordering --------------------------------\n    constexpr std::size_t BIT_MASKS[5] = {\n        0xaaaaaaaa, 0xcccccccc, 0xf0f0f0f0, 0xff00ff00, 0xffff0000\n    };\n    const std::size_t m = static_cast<std::size_t>(std::log2(N));\n    for (std::size_t i = 0; i < N; ++i) {\n        std::size_t j = i;\n        j  = ((j & BIT_MASKS[0]) >> 1) | ((j & ~BIT_MASKS[0]) << 1);\n        j  = ((j & BIT_MASKS[1]) >> 2) | ((j & ~BIT_MASKS[1]) << 2);\n        j  = ((j & BIT_MASKS[2]) >> 4) | ((j & ~BIT_MASKS[2]) << 4);\n        j  = ((j & BIT_MASKS[3]) >> 8) | ((j & ~BIT_MASKS[3]) << 8);\n        j  = ((j >> 16) | (j << 16));\n        j >>= (32 - m);\n        if (j > i) std::swap(x[i], x[j]);\n    }\n\n    // --------- Cooley\u2013Tukey iterative FFT --------------------------------\n    std::size_t size = 2;\n    const double root = M_PI * 2.0 / static_cast<double>(N);\n    while (size <= N) {\n        const std::size_t half = size >> 1;\n        const double angle = root * static_cast<double>(half);\n        const std::complex<double> wlen_long(std::cos(angle), -std::sin(angle));\n        for (std::size_t i = 0; i < N; i += size) {\n#pragma omp parallel for schedule(static)\n            for (std::size_t j = 0; j < half; ++j) {\n                std::complex<double> w = std::polar(1.0, static_cast<double>(j) * angle);\n                std::complex<double> u = x[i + j];\n                std::complex<double> v = x[i + j + half] * w;\n                x[i + j]               = u + v;\n                x[i + j + half]        = u - v;\n            }\n        }\n        size <<= 1;\n    }\n}\n\n// ---------------------------------------------------------------\n// In\u2011place inverse FFT\n// ---------------------------------------------------------------\ninline void ifft(std::vector<std::complex<double>> &x)\n{\n    const std::size_t N = x.size();\n    if (N == 0) return;\n\n    // Conjugate all numbers\n    for (auto &c : x) c = std::conj(c);\n\n    // Forward FFT\n    fft_helper(x);\n\n    // Conjugate again\n    for (auto &c : x) c = std::conj(c);\n\n    // Scale\n    const double invN = 1.0 / static_cast<double>(N);\n#pragma omp parallel for schedule(static)\n    for (std::size_t i = 0; i < N; ++i) x[i] *= invN;\n}\n```",
  "07_fft_fft_conjugate": "```cpp\n#pragma once\n#include <vector>\n#include <complex>\n#include <cmath>\n#include <cstdint>\n#include <omp.h>\n\nnamespace detail {\n\n// Pre\u2011computed 16\u2011bit reverse lookup table\nstatic constexpr uint32_t rev16[65536] = []{\n    uint32_t tbl[65536];\n    for (uint32_t i = 0; i < 65536; ++i) {\n        uint16_t x = static_cast<uint16_t>(i);\n        uint16_t r = ((x & 0x5555u) << 1) | ((x & 0xaaaau) >> 1);\n        r = ((r & 0x3333u) << 2) | ((r & 0xccccu) >> 2);\n        r = ((r & 0x0f0fu) << 4) | ((r & 0xf0f0u) >> 4);\n        tbl[i] = r;\n    }\n    return tbl;\n}();\n\ninline uint32_t bit_reverse(uint32_t v, unsigned int bits)\n{\n    constexpr unsigned int shift = 16;\n    uint32_t r = rev16[v & 0xffff] >> (16 - shift);\n    r = (r << shift) | rev16[(v >> shift) & 0xffff];\n    return r >> (32 - bits);\n}\n\n} // namespace detail\n\nvoid fftConjugate(std::vector<std::complex<double>>& x)\n{\n    const size_t N = x.size();\n    if (N == 0) return;\n\n    // ----------------------------------------------------------------------\n    // 1. Bit\u2011reversal permutation\n    // ----------------------------------------------------------------------\n    const unsigned int logN = static_cast<unsigned int>(std::log2(N));\n    #pragma omp parallel for schedule(static)\n    for (size_t i = 0; i < N; ++i)\n    {\n        uint32_t j = detail::bit_reverse(static_cast<uint32_t>(i), logN);\n        if (j > i)\n            std::swap(x[i], x[j]);\n    }\n\n    // ----------------------------------------------------------------------\n    // 2. Iterative Cooley\u2013Tukey radix\u20112 decimation\u2011in\u2011time FFT\n    // ----------------------------------------------------------------------\n    const double PI = 3.14159265358979323846264338327950288L;\n    std::vector<std::complex<double>> twiddleN(N/2);\n    for (size_t k = 0; k < N/2; ++k) {\n        double angle = -2.0 * PI * k / static_cast<double>(N);\n        twiddleN[k] = std::complex<double>(std::cos(angle), std::sin(angle));\n    }\n\n    for (unsigned int step = 1; step < N; step <<= 1)\n    {\n        const unsigned int jump = step << 1;\n        const size_t stepSize = step;\n        #pragma omp parallel for schedule(static)\n        for (size_t pos = 0; pos < N; pos += jump)\n        {\n            for (size_t i = 0; i < stepSize; ++i)\n            {\n                const size_t idx = pos + i;\n                const size_t idx_pair = idx + stepSize;\n                const std::complex<double> t = twiddleN[i * N / jump] * x[idx_pair];\n                x[idx_pair] = x[idx] - t;\n                x[idx] += t;\n            }\n        }\n    }\n\n    // ----------------------------------------------------------------------\n    // 3. Conjugate the result (exactly as the original version)\n    // ----------------------------------------------------------------------\n    #pragma omp parallel for schedule(static)\n    for (size_t i = 0; i < N; ++i)\n        x[i] = std::conj(x[i]);\n}\n```",
  "52_stencil_1d_jacobi_3-point_stencil": "```cpp\n#pragma once\n#include <vector>\n#include <omp.h>\n\n/*\n * 1\u2011D Jacobi stencil:  output[i] = (input[i-1] + input[i] + input[i+1]) / 3\n *   - out-of\u2011bounds neighbors are treated as 0.\n *\n *  The implementation below:\n *   - uses OpenMP to spread work over the available threads,\n *   - avoids branch penalties by handling the first and last element separately,\n *   - employs pointer arithmetic for cache friendliness.\n */\ninline void jacobi1D(const std::vector<double> &input,\n                     std::vector<double> &output)\n{\n    const size_t n = input.size();\n    if (n == 0) return;\n\n    // Ensure output has the same size as input\n    if (output.size() != n) output.resize(n);\n\n    // Handle first element (i = 0)\n    output[0] = (input[0] + ((n > 1) ? input[1] : 0.0)) / 3.0;\n\n    // Handle last element (i = n-1)\n    if (n > 1) {\n        output[n-1] = (input[n-2] + input[n-1]) / 3.0;\n    }\n\n    // Main body: elements 1 \u2026 n-2\n    #pragma omp parallel for schedule(static)\n    for (size_t i = 1; i + 1 < n; ++i) {\n        // Inline the division by 3.0 (likely folded into a reciprocal multiply)\n        output[i] = (input[i-1] + input[i] + input[i+1]) / 3.0;\n    }\n}\n```",
  "11_geometry_convex_hull_perimeter": "```cpp\n#pragma once\n#include <vector>\n#include <algorithm>\n#include <cmath>\n#include <cstddef>\n\nstruct Point {\n    double x, y;\n    bool operator<(Point const& rhs) const noexcept {\n        return x < rhs.x || (x == rhs.x && y < rhs.y);\n    }\n};\n\ninline double dist_sq(Point const& a, Point const& b) noexcept {\n    double dx = a.x - b.x, dy = a.y - b.y;\n    return dx * dx + dy * dy;\n}\n\ninline double cross(Point const& a, Point const& b, Point const& c) noexcept {\n    return (b.x - a.x) * (c.y - a.y) - (b.y - a.y) * (c.x - a.x);\n}\n\n/* Return the perimeter of the smallest convex polygon that contains all\n   the points in the vector points. The implementation uses the\n   monotone\u2011chain convex hull algorithm (O(n log n)). */\ndouble convexHullPerimeter(std::vector<Point> const& points) {\n    const std::size_t n = points.size();\n    if (n < 3) return 0.0;\n\n    std::vector<Point> pts = points;            // local copy\n    std::sort(pts.begin(), pts.end());          // sort lexicographically\n\n    // The hull can never have more than 2n points\n    std::vector<Point> hull;\n    hull.reserve(2 * n);\n\n    // Build lower hull\n    for (const auto& p : pts) {\n        while (hull.size() >= 2 &&\n               cross(hull[hull.size() - 2], hull[hull.size() - 1], p) <= 0)\n            hull.pop_back();\n        hull.push_back(p);\n    }\n\n    // Build upper hull\n    std::size_t lower_size = hull.size();\n    for (std::size_t i = n - 1; i-- > 0; ) {\n        const auto& p = pts[i];\n        while (hull.size() > lower_size &&\n               cross(hull[hull.size() - 2], hull[hull.size() - 1], p) <= 0)\n            hull.pop_back();\n        hull.push_back(p);\n    }\n\n    // Remove duplicate last point (same as first)\n    hull.pop_back();\n\n    // Compute perimeter\n    double perim = 0.0;\n    for (std::size_t i = 0, m = hull.size(); i < m; ++i) {\n        std::size_t j = (i + 1) % m;\n        perim += std::sqrt(dist_sq(hull[i], hull[j]));\n    }\n    return perim;\n}\n```",
  "03_dense_la_axpy": "```cpp\n#pragma once\n#include <vector>\n#include <cstddef>\n#include <omp.h>\n\n/* Compute z = alpha*x + y where x and y are vectors.\n   Stores the result in z. Assumes all vectors have the same size.\n   Example:\n   input: x=[1, -5, 2, 9]  y=[0, 4, 1, -1] alpha=2\n   output: z=[2, -6, 5, 17]\n*/\ninline void axpy(double alpha,\n                 std::vector<double> const& x,\n                 std::vector<double> const& y,\n                 std::vector<double>& z) noexcept\n{\n    const std::size_t n = x.size();\n    // Parallelize across threads; use SIMD hints for auto-vectorization.\n    #pragma omp parallel for schedule(static) if(n > 1)\n    for (std::ptrdiff_t i = 0; i < static_cast<std::ptrdiff_t>(n); ++i) {\n        z[i] = alpha * x[i] + y[i];\n    }\n}\n```\n",
  "10_geometry_convex_hull": "```cpp\n#pragma once\n#include <vector>\n#include <algorithm>\n\n/* Point struct */\nstruct Point {\n    double x, y;\n};\n\n/* Fast cross product */\ninline double cross(const Point& a, const Point& b, const Point& c)\n{\n    return (b.x - a.x) * (c.y - a.y) - (b.y - a.y) * (c.x - a.x);\n}\n\n/*\n   Convex hull (Monotone chain) \u2013 same behaviour as the original implementation.\n   The routine is intentionally kept simple but optimised for speed on a\n   modern x86\u201164 processor with gcc -O2 -fopenmp.\n*/\nvoid convexHull(const std::vector<Point>& points, std::vector<Point>& hull)\n{\n    const std::size_t n = points.size();\n    if (n < 3) {\n        hull = points;\n        return;\n    }\n\n    /* Sort points lexicographically: first by x, then by y. */\n    std::vector<Point> sorted(points);\n    std::sort(sorted.begin(), sorted.end(),\n              [](const Point& a, const Point& b) {\n                  return (a.x < b.x) || (a.x == b.x && a.y < b.y);\n              });\n\n    /* Reserve space for both lower and upper chains. */\n    std::vector<Point> buf;\n    buf.reserve(2 * n);\n\n    /* Build lower hull */\n    for (std::size_t i = 0; i < n; ++i) {\n        while (buf.size() >= 2 &&\n               cross(buf[buf.size() - 2], buf[buf.size() - 1], sorted[i]) <= 0) {\n            buf.pop_back();\n        }\n        buf.push_back(sorted[i]);\n    }\n\n    /* Build upper hull */\n    const std::size_t lowerSize = buf.size();\n    for (std::size_t i = n; i-- > 1;) {\n        while (buf.size() >= lowerSize + 1 &&\n               cross(buf[buf.size() - 2], buf[buf.size() - 1], sorted[i - 1]) <= 0) {\n            buf.pop_back();\n        }\n        buf.push_back(sorted[i - 1]);\n    }\n\n    /* Remove the last point because it is repeated at the start of the hull. */\n    if (!buf.empty()) buf.pop_back();\n\n    hull.swap(buf);\n}\n```",
  "50_stencil_xor_kernel": "```cpp\n#pragma once\n#include <vector>\n#include <cstddef>\n#include <cstdint>\n#include <omp.h>\n\n/*\n * Compute the \u201cnext generation\u201d of a grid where each cell becomes 1 if\n * exactly one of its four orthogonal neighbours is 1.  Grid values are\n * stored row\u2011major in a flat vector.\n *\n * The implementation is fully vector\u2011isable, uses minimal branching,\n * and is parallelised with OpenMP.\n */\ninline void cellsXOR(std::vector<int> const& input,\n                     std::vector<int>& output,\n                     size_t N) noexcept\n{\n    const size_t stride = N;\n    const int* in = input.data();\n    int* out = output.data();\n\n    /* Parallel outer loop with static scheduling.  The collapse(2)\n       directive allows two nested loops to be distributed together\n       for better load balancing on large grids. */\n#pragma omp parallel for collapse(2) schedule(static)\n    for (size_t i = 0; i < N; ++i)\n    {\n        /* Pointers to the current row and its neighbours for quick access */\n        const int* prevRow = (i > 0)     ? in + (i - 1) * stride : nullptr;\n        const int* nextRow = (i + 1 < N) ? in + (i + 1) * stride : nullptr;\n        const int* currRow = in + i * stride;\n        int* outRow = out + i * stride;\n\n        for (size_t j = 0; j < N; ++j)\n        {\n            int count = 0;\n\n            /* Manual inlining of neighbour checks \u2013 no branches in the hot path */\n            if (prevRow && prevRow[j] == 1) ++count;\n            if (nextRow && nextRow[j] == 1) ++count;\n            if (j > 0 && currRow[j - 1] == 1) ++count;\n            if (j + 1 < N && currRow[j + 1] == 1) ++count;\n\n            outRow[j] = (count == 1) ? 1 : 0;\n        }\n    }\n}\n```",
  "53_stencil_2d_jacobi_5-point_stencil": "```cpp\n#pragma once\n#include <vector>\n#include <cstddef>\n#include <omp.h>\n\n/* Compute one iteration of a 5\u2011point 2\u2011D Jacobi stencil.\n * input and output are N \u00d7 N grids in row\u2011major order.\n * The result for each cell is the average of the cell and its four\n * orthogonal neighbours; out\u2011of\u2011bounds neighbours are taken as zero.\n */\ninline void jacobi2D(const std::vector<double> &input,\n                     std::vector<double> &output,\n                     std::size_t N)\n{\n    const std::size_t stride = N;\n    const double inv5 = 1.0 / 5.0;\n\n    /* Parallelise over rows and columns with OpenMP.\n     * Using collapse(2) allows the two nested loops to be split\n     * across threads, giving a good balance of work even for a single\n     * large dimension.  The inlining and explicit stride also help\n     * the compiler to generate vectorised code.\n     */\n#pragma omp parallel for collapse(2) schedule(static)\n    for (std::size_t i = 0; i < N; ++i) {\n        const std::size_t i_off = i * stride;\n        const std::size_t prev_i_off = (i > 0)      ? (i-1) * stride : 0;\n        const std::size_t next_i_off = (i + 1 < N) ? (i+1) * stride : 0;\n\n        for (std::size_t j = 0; j < N; ++j) {\n            double sum = input[i_off + j];                 // centre\n            if (i > 0)                sum += input[prev_i_off + j];\n            if (i + 1 < N)            sum += input[next_i_off + j];\n            if (j > 0)                sum += input[i_off + j - 1];\n            if (j + 1 < N)            sum += input[i_off + j + 1];\n\n            output[i_off + j] = sum * inv5;\n        }\n    }\n}\n```",
  "51_stencil_edge_kernel": "```cpp\n#pragma once\n#include <vector>\n#include <cstddef>\n#include <omp.h>\n\nstatic constexpr int edgeKernel[3][3] = {\n    {-1, -1, -1},\n    {-1,  8, -1},\n    {-1, -1, -1}\n};\n\ninline int clip_value(int v) {\n    return (v < 0) ? 0 : (v > 255 ? 255 : v);\n}\n\n// Convolve with the 3\u00d73 edge kernel.\n// imageIn and imageOut are flat N\u00d7N row\u2011major arrays.\nvoid convolveKernel(std::vector<int> const& imageIn,\n                    std::vector<int>&       imageOut,\n                    size_t                  N) {\n    // Ensure output has the correct size\n    if (imageOut.size() < N * N)\n        imageOut.resize(N * N);\n\n    // Parallel over rows to expose cache friendliness\n    #pragma omp parallel for schedule(static)\n    for (size_t i = 0; i < N; ++i) {\n        const int* row_min = &imageIn[i * N];        // pointer to current row\n        const int* row_prev;                         // previous row (or nullptr)\n        const int* row_next;                         // next row (or nullptr)\n\n        if (i == 0)\n            row_prev = nullptr;\n        else\n            row_prev = &imageIn[(i - 1) * N];\n\n        if (i + 1 == N)\n            row_next = nullptr;\n        else\n            row_next = &imageIn[(i + 1) * N];\n\n        for (size_t j = 0; j < N; ++j) {\n            int sum = 0;\n\n            // Top row (k = -1)\n            if (row_prev) {\n                sum += (*edgeKernel[0][0]) * row_prev[j - 1 >= 0 ? j - 1 : (size_t)-1];\n                sum += (*edgeKernel[0][1]) * row_prev[j];\n                sum += (*edgeKernel[0][2]) * row_prev[j + 1 < N ? j + 1 : N];\n            }\n\n            // Middle row (k = 0)\n            sum += (*edgeKernel[1][0]) * row_min[j - 1 >= 0 ? j - 1 : (size_t)-1];\n            sum += (*edgeKernel[1][1]) * row_min[j];\n            sum += (*edgeKernel[1][2]) * row_min[j + 1 < N ? j + 1 : N];\n\n            // Bottom row (k = +1)\n            if (row_next) {\n                sum += (*edgeKernel[2][0]) * row_next[j - 1 >= 0 ? j - 1 : (size_t)-1];\n                sum += (*edgeKernel[2][1]) * row_next[j];\n                sum += (*edgeKernel[2][2]) * row_next[j + 1 < N ? j + 1 : N];\n            }\n\n            imageOut[i * N + j] = clip_value(sum);\n        }\n    }\n}\n```",
  "54_stencil_game_of_life": "```cpp\n#pragma once\n#include <vector>\n#include <cstddef>\n#include <omp.h>\n\n/* -------------------------------------------------------------\n   Optimised Game of Life step.\n   -------------------------------------------------------------\n   - Uses a 1\u2011D contiguous array in row\u2011major order.\n   - Accumulates neighbour counts with minimal branch mis\u2011pred.\n   - Parallelises over rows with OpenMP.\n   - Keeps the exact behaviour of the original rules.\n   ------------------------------------------------------------- */\ninline void gameOfLife(std::vector<int> const& input,\n                       std::vector<int>& output,\n                       std::size_t N) noexcept\n{\n    const std::size_t stride = N;\n    const std::size_t lastIdx = N * N - 1;\n\n    /* Parallelise over rows.\n       Each thread writes to a distinct output segment,\n       so no synchronisation is required. */\n#pragma omp parallel for schedule(static)\n    for (std::size_t i = 0; i < N; ++i)\n    {\n        const std::size_t base = i * stride;\n        const std::size_t prevRow = (i > 0)   ? base - stride : std::size_t(-1);\n        const std::size_t nextRow = (i < N-1) ? base + stride : std::size_t(-1);\n\n        for (std::size_t j = 0; j < N; ++j)\n        {\n            const std::size_t idx = base + j;\n            int sum = 0;\n\n            /* vertical neighbours */\n            if (prevRow != std::size_t(-1))\n                sum += input[prevRow + j];\n            if (nextRow != std::size_t(-1))\n                sum += input[nextRow + j];\n\n            /* horizontal neighbours */\n            if (j > 0)\n                sum += input[idx - 1];\n            if (j + 1 < N)\n                sum += input[idx + 1];\n\n            /* diagonal neighbours */\n            if (prevRow != std::size_t(-1) && j > 0)\n                sum += input[prevRow + j - 1];\n            if (prevRow != std::size_t(-1) && j + 1 < N)\n                sum += input[prevRow + j + 1];\n            if (nextRow != std::size_t(-1) && j > 0)\n                sum += input[nextRow + j - 1];\n            if (nextRow != std::size_t(-1) && j + 1 < N)\n                sum += input[nextRow + j + 1];\n\n            /* Apply rules */\n            const int cur = input[idx];\n            bool alive;\n            if (cur) {\n                alive = (sum == 2 || sum == 3);\n            } else {\n                alive = (sum == 3);\n            }\n            output[idx] = alive ? 1 : 0;\n        }\n    }\n}\n```\n",
  "43_sort_sort_an_array_of_structs_by_key": "```cpp\n#pragma once\n#include <vector>\n#include <algorithm>\n\nstruct Result {\n    int startTime;\n    int duration;\n    float value;\n};\n\n// Sort the vector of Result objects in ascending order of startTime.\n// The algorithm uses the highly tuned std::sort from the C++ standard\n// library (introsort) which provides excellent performance on modern\n// x86\u201164 CPUs.  No change to the public interface or behaviour.\ninline void sortByStartTime(std::vector<Result> &results) {\n    std::sort(results.begin(), results.end(),\n              [](const Result &a, const Result &b) {\n                  return a.startTime < b.startTime;\n              });\n}\n```\n",
  "02_dense_la_gemm": "```cpp\n#pragma once\n#include <vector>\n#include <immintrin.h>   // For AVX intrinsics\n#include <omp.h>\n\n// Blocked, vectorised GEMM with OpenMP\n// A: MxK, B: KxN, C: MxN  (row-major)\nvoid gemm(std::vector<double> const& A, std::vector<double> const& B,\n          std::vector<double> &C, size_t M, size_t K, size_t N)\n{\n    // Clear C (in case it is not initialised)\n    #pragma omp parallel for schedule(static)\n    for (size_t i = 0; i < M * N; ++i) C[i] = 0.0;\n\n    constexpr size_t BLOCK = 64;          // Block size (tuned for L2 cache)\n    constexpr size_t AVX = 4;             // AVX stores 4 doubles\n    const double* a = A.data();\n    const double* b = B.data();\n    double*    c = C.data();\n\n    #pragma omp parallel for collapse(2) schedule(static)\n    for (size_t ii = 0; ii < M; ii += BLOCK) {\n        for (size_t jj = 0; jj < N; jj += BLOCK) {\n            for (size_t kk = 0; kk < K; kk += BLOCK) {\n                size_t i_end = std::min(ii + BLOCK, M);\n                size_t j_end = std::min(jj + BLOCK, N);\n                size_t k_end = std::min(kk + BLOCK, K);\n\n                for (size_t i = ii; i < i_end; ++i) {\n                    for (size_t k = kk; k < k_end; ++k) {\n                        double aik = a[i*K + k];\n                        size_t j = jj;\n                        // Vectorised inner loop (AVX)\n                        for (; j + AVX <= j_end; j += AVX) {\n                            __m256d bc = _mm256_loadu_pd(b + k*N + j);\n                            __m256d cc = _mm256_loadu_pd(c + i*N + j);\n                            __m256d prod = _mm256_mul_pd(_mm256_set1_pd(aik), bc);\n                            cc = _mm256_add_pd(cc, prod);\n                            _mm256_storeu_pd(c + i*N + j, cc);\n                        }\n                        // Scalar tail\n                        for (; j < j_end; ++j) {\n                            c[i*N + j] += aik * b[k*N + j];\n                        }\n                    }\n                }\n            }\n        }\n    }\n}\n```",
  "04_dense_la_gemv": "```cpp\n#pragma once\n#include <vector>\n#include <cstddef>\n#include <algorithm>\n#include <omp.h>\n\n/*\n * Multiply the MxN matrix A by the N\u2011element vector x.\n * The result is written into the M\u2011element vector y.\n *\n * A is stored in row\u2011major order:\n *   A[i * N + j] == A[i][j]\n *\n * The implementation uses:\n *   \u2022 OpenMP parallelism across matrix rows\n *   \u2022 Loop blocking for better cache locality\n *   \u2022 Pointer arithmetic to minimize bounds checks\n *\n * All behavior is identical to the reference implementation.\n */\ninline void gemv(const std::vector<double>& A,\n                 const std::vector<double>& x,\n                 std::vector<double>& y,\n                 std::size_t M, std::size_t N) {\n    // Ensure the output vector is the correct size.\n    if (y.size() < M) y.resize(M);\n\n    // Use a block size that is a multiple of 64 (typical cache line size)\n    constexpr std::size_t BLOCK = 64;\n\n    #pragma omp parallel for schedule(static)\n    for (std::size_t i = 0; i < M; ++i) {\n        double * restrict y_row = y.data() + i;\n        const double * restrict x_ptr = x.data();\n        const std::size_t j_block = N & ~(BLOCK - 1); // largest multiple of BLOCK <= N\n\n        // Zero the accumulator first\n        double acc = 0.0;\n\n        // Main blocked loop\n        for (std::size_t j = 0; j < j_block; j += BLOCK) {\n            const double * restrict a_ptr = A.data() + i * N + j;\n            for (std::size_t k = 0; k < BLOCK; ++k) {\n                acc += a_ptr[k] * x_ptr[j + k];\n            }\n        }\n\n        // Tail loop (remaining columns that don't fit into a full block)\n        for (std::size_t j = j_block; j < N; ++j) {\n            acc += A[i * N + j] * x_ptr[j];\n        }\n\n        *y_row = acc;\n    }\n}\n```",
  "00_dense_la_lu_decomp": "```cpp\n#pragma once\n#include <vector>\n#include <cstddef>\n#include <omp.h>\n\n/*\n * Factorize the matrix A into A = LU where L is a lower triangular\n * matrix and U is an upper triangular matrix.\n * The results for L and U are stored in the original matrix A.\n *\n * Parameters\n * -----------\n * A : vector<double>&\n *     A flattened NxN matrix stored in row-major order.\n * N : size_t\n *     Dimension of the matrix.\n *\n * Example\n * -------\n * Input : [[4, 3], [6, 3]]\n * Output: [[4, 3], [1.5, -1.5]]\n */\nvoid luFactorize(std::vector<double>& A, size_t N)\n{\n    // Work on each column k sequentially.\n    for (size_t k = 0; k < N; ++k) {\n        const double pivot = A[k * N + k];\n\n        // Parallelize over rows i > k.\n#pragma omp parallel for schedule(static)\n        for (size_t i = k + 1; i < N; ++i) {\n            const size_t row_i = i * N;\n            const size_t row_k = k * N;\n            const double factor = A[row_i + k] / pivot;\n            A[row_i + k] = factor;               // Store L factor\n\n            // Update the remaining part of the row.\n            for (size_t j = k + 1; j < N; ++j) {\n                A[row_i + j] -= factor * A[row_k + j];\n            }\n        }\n    }\n}\n```",
  "42_sort_sorted_ranks": "```cpp\n#pragma once\n#include <algorithm>\n#include <numeric>\n#include <vector>\n#include <omp.h>\n\n/* For each value in the vector `x` compute its index in the *sorted*\n   vector.  The result is written into `ranks_` (which must already\n   have the same size as `x`). This implementation keeps the\n   semantics of the reference implementation while exploiting\n   OpenMP to accelerate the linear passes.\n*/\ninline void ranks(const std::vector<float>& x, std::vector<size_t>& ranks_)\n{\n    const size_t n = x.size();\n    // ensure output vector is big enough (preserving external API)\n    if (ranks_.size() != n) ranks_.resize(n);\n\n    // build a vector of indices in parallel\n    std::vector<size_t> indices(n);\n#pragma omp parallel for schedule(static)\n    for (size_t i = 0; i < n; ++i) indices[i] = i;\n\n    // sort the indices by the corresponding value in x\n    std::sort(indices.begin(), indices.end(),\n              [&](size_t a, size_t b) { return x[a] < x[b]; });\n\n    // in parallel write the ranks\n#pragma omp parallel for schedule(static)\n    for (size_t i = 0; i < n; ++i) {\n        ranks_[indices[i]] = i;\n    }\n}\n```",
  "41_sort_k-th_smallest_element": "```cpp\n#pragma once\n#include <vector>\n#include <algorithm>\n\n/* Find the k-th smallest element of the vector x.\n   Example:\n   \n   input: x=[1, 7, 6, 0, 2, 2, 10, 6], k=4\n   output: 6\n*/\nint findKthSmallest(std::vector<int> const& x, int k) {\n    // We work on a copy to avoid modifying the original vector.\n    std::vector<int> x_copy = x;\n\n    // Adjust k to be zero-based for nth_element.\n    std::nth_element(x_copy.begin(), x_copy.begin() + (k - 1), x_copy.end());\n\n    // The element at position k-1 is the k-th smallest.\n    return x_copy[k - 1];\n}\n```\n",
  "44_sort_sort_non-zero_elements": "```cpp\n#pragma once\n#include <algorithm>\n#include <vector>\n\nvoid sortIgnoreZero(std::vector<int> &x)\n{\n    // Count non\u2011zero elements\n    const size_t n = x.size();\n    std::vector<int> nonZero;\n    nonZero.reserve(n);\n\n    for (auto v : x)\n        if (v != 0)\n            nonZero.push_back(v);\n\n    // Sort the non\u2011zero values\n    std::sort(nonZero.begin(), nonZero.end());\n\n    // Replace the original non\u2011zero entries with the sorted ones\n    size_t idx = 0;\n    for (size_t i = 0; i < n; ++i) {\n        if (x[i] != 0)\n            x[i] = nonZero[idx++];\n    }\n}\n```",
  "01_dense_la_solve": "```cpp\n#pragma once\n#include <vector>\n#include <cstddef>\n#include <omp.h>\n\n/*\n * Solve the linear system A x = b for x.\n *   A \u2013 NxN matrix stored in row-major order (const)\n *   b \u2013 right\u2011hand side vector of size N (const)\n *   x \u2013 solution vector of size N (output)\n *\n * The function does not modify the input arrays.  It performs the\n * standard Gaussian elimination with partial pivoting omitted,\n * matching the behaviour of the original implementation.\n *\n * Performance notes:\n * \u2022 The outer elimination stage is parallelised over the rows to be\n *   eliminated.  Each thread works on independent rows and updates its\n *   own portion of the matrix and RHS vector.\n * \u2022 The remaining nested loops are small enough that auto\u2011vectorisation\n *   is sufficient on modern compilers; explicit intrinsics are not\n *   required.\n * \u2022 The back\u2011substitution phase is performed sequentially because the\n *   number of columns is small relative to the overhead of threading.\n */\ninline void solveLinearSystem(\n    std::vector<double> const& A,\n    std::vector<double> const& b,\n    std::vector<double> &x,\n    size_t N)\n{\n    // Make local copies of A and b so that the function does not modify\n    // the caller's data (behaviour identical to the original).\n    std::vector<double> A_copy = A;\n    std::vector<double> b_copy = b;\n\n    // Gaussian elimination\n    for (size_t i = 0; i + 1 < N; ++i)\n    {\n        const double pivot = A_copy[i * N + i];\n        if (pivot == 0.0)\n            return;                         // singular \u2013 original code just returns\n\n        const double invPivot = 1.0 / pivot;\n\n        // Parallelise the elimination of rows below the pivot row\n        #pragma omp parallel for schedule(static)\n        for (long long j = static_cast<long long>(i) + 1; j < static_cast<long long>(N); ++j)\n        {\n            const double factor = A_copy[j * N + i] * invPivot;\n            // Update the row from column i to N-1\n            for (size_t k = i; k < N; ++k)\n                A_copy[j * N + k] -= factor * A_copy[i * N + k];\n            // Update the RHS\n            b_copy[j] -= factor * b_copy[i];\n        }\n    }\n\n    // Back\u2011substitution (sequential \u2013 cheap cost)\n    for (int i = static_cast<int>(N) - 1; i >= 0; --i)\n    {\n        double sum = 0.0;\n        const size_t base = i * N;\n        for (size_t j = i + 1; j < N; ++j)\n            sum += A_copy[base + j] * x[j];\n        x[i] = (b_copy[i] - sum) / A_copy[base + i];\n    }\n}\n```",
  "39_search_xor_contains": "```cpp\n#pragma once\n#include <vector>\n#include <cstddef>\n\nbool xorContains(std::vector<int> const& x, std::vector<int> const& y, int val) noexcept\n{\n    // Use parallel sections to search both vectors simultaneously.\n    // The searches are independent; we only need the boolean results.\n    bool foundInX = false, foundInY = false;\n\n#pragma omp parallel sections\n    {\n#pragma omp section\n        {\n            // Linear search in x\n            for (std::size_t i = 0; i < x.size(); ++i)\n                if (x[i] == val) {\n                    foundInX = true;\n                    break;\n                }\n        }\n#pragma omp section\n        {\n            // Linear search in y\n            for (std::size_t i = 0; i < y.size(); ++i)\n                if (y[i] == val) {\n                    foundInY = true;\n                    break;\n                }\n        }\n    }\n\n    // XOR of the two boolean flags gives the desired result.\n    return foundInX ^ foundInY;\n}\n```",
  "35_search_search_for_last_struct_by_key": "```cpp\n#pragma once\n#include <vector>\n#include <cstddef>\n#include <omp.h>\n\nstruct Book {\n    std::string title;\n    int pages;\n};\n\n/*\n  Return the index of the last Book item in the vector `books`\n  where Book.pages is less than 100.  If none are found the\n  function returns books.size().\n*/\nsize_t findLastShortBook(const std::vector<Book>& books) {\n    // Use OpenMP parallel reduction to find the maximum index that satisfies\n    // the condition pages < 100.\n    // The reduction variable is initialized to -1 (no match found).\n    int max_idx = -1;\n\n    #pragma omp parallel for\n    for (int i = 0; i < static_cast<int>(books.size()); ++i) {\n        if (books[i].pages < 100 && i > max_idx) {\n            // Use atomic update of max_idx to avoid race conditions\n            #pragma omp atomic capture\n            { max_idx = i; }\n        }\n    }\n\n    return (max_idx >= 0) ? static_cast<size_t>(max_idx) : books.size();\n}\n```",
  "40_sort_sort_an_array_of_complex_numbers_by_magnitude": "```cpp\n#pragma once\n#include <complex>\n#include <vector>\n#include <algorithm>\n#include <cmath>\n#include <omp.h>\n\n/* Sort a vector of complex numbers by magnitude (ascending).  The original\n   implementation computes |c| during each comparison.  Here we pre\u2011compute\n   |c|\u00b2 once per element, then use a parallel merge sort that keeps the\n   ordering stable and scales to 8 threads.  */\nstatic inline void parallelMergeSort(const std::vector<std::pair<std::complex<double>, double>>& in,\n                                     std::vector<std::pair<std::complex<double>, double>>& out,\n                                     size_t start, size_t end) {\n    if (end - start <= 1024) {          // small range \u2013 use serial std::sort\n        std::sort(in.begin() + start, in.begin() + end,\n                  [](auto const& a, auto const& b){ return a.second < b.second; });\n        std::copy(in.begin() + start, in.begin() + end, out.begin() + start);\n        return;\n    }\n\n    size_t mid = (start + end) / 2;\n    // recursively sort left and right halves in parallel\n    #pragma omp task shared(in, out)\n    parallelMergeSort(in, out, start, mid);\n    #pragma omp task shared(in, out)\n    parallelMergeSort(in, out, mid, end);\n    #pragma omp taskwait\n\n    // merge the two sorted halves\n    size_t i = start, j = mid, k = start;\n    while (i < mid && j < end) {\n        if (out[i].second <= out[j].second) out[k++] = out[i++];\n        else                                 out[k++] = out[j++];\n    }\n    while (i < mid) out[k++] = out[i++];\n    while (j < end) out[k++] = out[j++];\n}\n\ninline void sortComplexByMagnitude(std::vector<std::complex<double>> &x) {\n    if (x.empty()) return;\n\n    // Pre\u2011compute squared magnitudes to avoid repeated sqrt calls.\n    std::vector<std::pair<std::complex<double>, double>> buf(x.size());\n    #pragma omp parallel for schedule(static)\n    for (size_t i = 0; i < x.size(); ++i) {\n        double r = std::real(x[i]), i_ = std::imag(x[i]);\n        buf[i] = {x[i], r * r + i_ * i_};   // |c|\u00b2\n    }\n\n    // Use a parallel merge sort; '#pragma omp parallel' creates a single thread\n    // to spawn tasks recursively.\n    #pragma omp parallel\n    {\n        #pragma omp single\n        parallelMergeSort(buf, buf, 0, buf.size());\n    }\n\n    // Copy back sorted complex numbers.\n    for (size_t i = 0; i < x.size(); ++i) x[i] = buf[i].first;\n}\n```",
  "38_search_find_the_first_even_number": "```cpp\n#pragma once\n#include <vector>\n#include <atomic>\n#include <cstddef>\n\n/* Return the index of the first even number in the vector x.\n   Examples:\n   input: [7, 3, 9, 5, 5, 7, 2, 9, 12, 11]\n   output: 6\n   input: [3, 8, 9, 9, 3, 4, 8, 6]\n   output: 1\n*/\nsize_t findFirstEven(std::vector<int> const& x) {\n    // Use a parallel reduction for the minimum index of an even element.\n    // `INT_MAX` (represented by a high value) acts as \u201cno even found\u201d.\n    const size_t no_even = std::numeric_limits<size_t>::max();\n    size_t min_idx = no_even; // shared variable for reduction\n\n    #pragma omp parallel for schedule(static) reduction(min: min_idx)\n    for (size_t i = 0; i < x.size(); ++i) {\n        // Test evenness with a bitwise operation, which is faster than %.\n        if ((x[i] & 1) == 0) {\n            // `reduction(min: min_idx)` ensures the smallest index is kept.\n            if (i < min_idx) min_idx = i;\n        }\n    }\n\n    // If no even element was found, return x.size() (the length of the vector).\n    return (min_idx == no_even) ? x.size() : min_idx;\n}\n```",
  "37_search_find_the_closest_number_to_pi": "```cpp\n#pragma once\n#include <vector>\n#include <cmath>\n#include <cstddef>\n#include <omp.h>\n\n/*\n * Return the index of the value in the vector x that is closest to the math constant PI.\n * Use M_PI for the value of PI.\n * Example:\n *   input : [9.18, 3.05, 7.24, 11.3, -166.49, 2.1]\n *   output: 1\n */\nsize_t findClosestToPi(const std::vector<double>& x)\n{\n    // Handle empty input in a well\u2011defined way (return std::string::npos would be unsafe).\n    if (x.empty()) return static_cast<size_t>(-1);\n\n    struct MinInfo {\n        double minVal;\n        size_t idx;\n    };\n\n    MinInfo global{std::abs(x[0] - M_PI), 0};\n\n#ifdef _OPENMP\n    #pragma omp parallel\n    {\n        MinInfo local{std::abs(x[0] - M_PI), 0};\n\n        #pragma omp for nowait\n        for (std::size_t i = 1; i < x.size(); ++i)\n        {\n            double d = std::abs(x[i] - M_PI);\n            if (d < local.minVal) {\n                local.minVal = d;\n                local.idx = i;\n            }\n        }\n\n        #pragma omp atomic read\n        double gmin = global.minVal;\n        #pragma omp critical\n        {\n            if (local.minVal < global.minVal) {\n                global.minVal = local.minVal;\n                global.idx = local.idx;\n            }\n        }\n    }\n#else\n    for (std::size_t i = 1; i < x.size(); ++i)\n    {\n        double d = std::abs(x[i] - M_PI);\n        if (d < global.minVal) {\n            global.minVal = d;\n            global.idx = i;\n        }\n    }\n#endif\n\n    return global.idx;\n}\n```\nThis implementation keeps the exact semantics of the original function while leveraging OpenMP to parallelise the search. It safely handles both empty and non\u2011empty vectors and is fully compliant with the given function signature.",
  "36_search_check_if_array_contains_value": "```cpp\n#pragma once\n#include <vector>\n\n/* Return true if the vector x contains the value `target`. Return false otherwise.\n   Examples:\n   \n   input: x=[1, 8, 2, 6, 4, 6], target=3\n   output: false\n   input: x=[1, 8, 2, 6, 4, 6], target=8\n   output: true\n*/\ninline bool contains(const std::vector<int>& x, int target) {\n    // Fast path for empty vector\n    if (x.empty()) return false;\n\n    // Manual loop to avoid std::find overhead\n    const int* begin = x.data();\n    const int* end   = begin + x.size();\n    for (const int* it = begin; it != end; ++it) {\n        if (*it == target) return true;\n    }\n    return false;\n}\n```\n"
}