{
  "24_histogram_count_quartile": "```cpp\n#pragma once\n#include <array>\n#include <vector>\n#include <cstddef>\n#include <omp.h>\n\n/* Count the number of doubles in the vector `x` that have a fractional part \n   in [0, 0.25), [0.25, 0.5), [0.5, 0.75), and [0.75, 1). Store the counts in `bins`.\n   Examples:\n     input: [7.8, 4.2, 9.1, 7.6, 0.27, 1.5, 3.8]\n     output: [2, 1, 2, 2]\n     input: [1.9, 0.2, 0.6, 10.1, 7.4]\n     output: [2, 1, 1, 1]\n*/\ninline void countQuartiles(const std::vector<double>& x, std::array<size_t, 4>& bins) {\n#pragma omp parallel\n    {\n        std::array<size_t, 4> local = {0, 0, 0, 0};\n#pragma omp for schedule(static)\n        for (size_t i = 0; i < x.size(); ++i) {\n            const double val = x[i];\n            const double frac = val - static_cast<int>(val);\n            if (frac < 0.25)\n                ++local[0];\n            else if (frac < 0.5)\n                ++local[1];\n            else if (frac < 0.75)\n                ++local[2];\n            else\n                ++local[3];\n        }\n#pragma omp critical\n        {\n            for (int j = 0; j < 4; ++j)\n                bins[j] += local[j];\n        }\n    }\n}\n```",
  "15_graph_edge_count": "```cpp\n#pragma once\n#include <vector>\n\nint edgeCount(const std::vector<int>& A, size_t N) {\n    const int* data = A.data();\n    const size_t total = N * N;\n    int count = 0;\n\n    #pragma omp parallel for reduction(+:count) schedule(static)\n    for (size_t i = 0; i < total; ++i) {\n        count += data[i];      // 0 or 1 values; sum yields the number of edges\n    }\n    return count;\n}\n```",
  "30_scan_prefix_sum": "```cpp\n#pragma once\n#include <numeric>\n#include <vector>\n#include <cstddef>\n#include <cstdint>\n#include <omp.h>\n#include <algorithm>\n\n/* Compute the inclusive prefix sum of `x` into `output`.\n * Behaves identically to the original implementation but uses an\n * OpenMP\u2011parallel, block\u2011based scan for improved performance on\n * modern multi\u2011core x86\u201164 systems.\n */\nvoid prefixSum(const std::vector<int64_t>& x, std::vector<int64_t>& output) {\n    const size_t n = x.size();\n    if (n == 0) return;\n\n    /* Use the standard sequential implementation for very small inputs */\n    const size_t minChunk = 1024; /* minimal elements per thread */\n    int hardwareThreads = omp_get_max_threads();\n    size_t potentialThreads = (n + minChunk - 1) / minChunk;\n    int numThreads = static_cast<int>(std::min<size_t>(hardwareThreads, potentialThreads));\n\n    if (numThreads < 2) {\n        std::inclusive_scan(x.begin(), x.end(), output.begin());\n        return;\n    }\n\n    /* Divide the work into contiguous blocks */\n    size_t chunk = (n + numThreads - 1) / numThreads;\n    std::vector<int64_t> blockSums(numThreads, 0);\n\n    /* First pass: compute an inclusive scan inside each block */\n#pragma omp parallel num_threads(numThreads)\n    {\n        int tid = omp_get_thread_num();\n        size_t start = tid * chunk;\n        size_t end   = std::min(start + chunk, n);\n        if (start >= end) {\n            blockSums[tid] = 0;\n            return;\n        }\n\n        int64_t acc = 0;\n        for (size_t i = start; i < end; ++i) {\n            acc += x[i];\n            output[i] = acc;\n        }\n        blockSums[tid] = acc;\n    }\n\n    /* Compute the offset for each block (prefix of block sums) */\n    int64_t offset = 0;\n    for (int i = 0; i < numThreads; ++i) {\n        int64_t sum = blockSums[i];\n        blockSums[i] = offset;\n        offset += sum;\n    }\n\n    /* Second pass: add the appropriate offset to each block */\n#pragma omp parallel for schedule(static) num_threads(numThreads)\n    for (int t = 0; t < numThreads; ++t) {\n        size_t start = t * chunk;\n        size_t end   = std::min(start + chunk, n);\n        if (start >= end) continue;\n        int64_t off = blockSums[t];\n        for (size_t i = start; i < end; ++i) {\n            output[i] += off;\n        }\n    }\n}\n```",
  "59_transform_map_function": "```cpp\n#pragma once\n\n#include <vector>\n#include <cstddef>\n#include <cstdint>\n#include <omp.h>\n\n// Helper: check if an integer is a power of two.\n// Declared inline so the compiler can inline it during the loop below.\ninline bool isPowerOfTwo(int x)\n{\n    return (x > 0) && ((x & (x - 1)) == 0);\n}\n\n// Map each element of `x` to true/false depending on whether it is a power of two.\n// A temporary byte buffer is filled in parallel and then copied to the `vector<bool>`\n// to avoid data\u2011race issues that arise from concurrent writes into the packed\n// representation of std::vector<bool>.\ninline void mapPowersOfTwo(const std::vector<int>& x, std::vector<bool>& mask)\n{\n    const std::size_t N = x.size();\n\n    // Ensure mask has the correct size.\n    if (mask.size() != N)\n        mask.resize(N);\n\n    // Temporary byte buffer to store the results in isolated locations.\n    std::vector<std::uint8_t> tmp(N);\n\n#pragma omp parallel for schedule(static)\n    for (int i = 0; i < static_cast<int>(N); ++i)\n    {\n        const int v = x[i];\n        tmp[i] = (v > 0) && ((v & (v - 1)) == 0);\n    }\n\n    // Copy the results into the packed std::vector<bool>.\n    for (std::size_t i = 0; i < N; ++i)\n        mask[i] = tmp[i];\n}\n```\n",
  "57_transform_inverse_offset": "```cpp\n#pragma once\n#include <vector>\n#include <cstddef>\n#include <omp.h>\n\n// Replace each element `x[i]` with 1 - 1/x[i].\ninline void oneMinusInverse(std::vector<double>& x)\n{\n    const std::size_t n = x.size();\n    if (n == 0)\n        return;\n\n    // Parallel element\u2011wise transformation.\n#pragma omp parallel for schedule(static)\n    for (std::size_t i = 0; i < n; ++i) {\n        const double v = x[i];\n        x[i] = 1.0 - 1.0 / v;\n    }\n}\n```",
  "56_transform_negate_odds": "```cpp\n#pragma once\n\n#include <vector>\n\n/* In the vector x negate the odd values and divide the even values by 2.\n   Example:\n   \n   input: [16, 11, 12, 14, 1, 0, 5]\n   output: [8, -11, 6, 7, -1, 0, -5]\n*/\ninline void negateOddsAndHalveEvens(std::vector<int> &x) {\n    const std::size_t n = x.size();\n    #pragma omp parallel for schedule(static, 256)\n    for (std::size_t i = 0; i < n; ++i) {\n        const int v = x[i];\n        x[i] = (v & 1) ? -v : (v >> 1);   // even: halve; odd: negate\n    }\n}\n```",
  "20_histogram_pixel_histogram": "```cpp\n#pragma once\n#include <vector>\n#include <array>\n#include <algorithm>\n#include <omp.h>\n\nvoid pixelCounts(std::vector<int> const& image, std::array<size_t, 256> &bins) {\n    if (image.empty()) return;\n\n    const int n = static_cast<int>(image.size());\n    const int num_threads = omp_get_max_threads();\n\n    std::vector<std::array<size_t, 256>> localBins(num_threads);\n\n    #pragma omp parallel\n    {\n        int tid = omp_get_thread_num();\n        std::array<size_t, 256> &local = localBins[tid];\n        std::fill(local.begin(), local.end(), static_cast<size_t>(0));\n\n        #pragma omp for schedule(static)\n        for (int i = 0; i < n; ++i) {\n            ++local[image[i]];\n        }\n    }\n\n    for (int t = 0; t < num_threads; ++t) {\n        const std::array<size_t, 256> &local = localBins[t];\n        for (size_t k = 0; k < 256; ++k) {\n            bins[k] += local[k];\n        }\n    }\n}\n```",
  "22_histogram_count_quadrants": "```cpp\n#pragma once\n#include <array>\n#include <vector>\n#include <cstddef>\n#include <omp.h>\n\nstruct Point {\n    double x, y;\n};\n\nvoid countQuadrants(const std::vector<Point>& points, std::array<size_t, 4>& bins)\n{\n#pragma omp parallel\n{\n    std::array<size_t, 4> local{0, 0, 0, 0};\n#pragma omp for schedule(static)\n    for (std::size_t i = 0; i < points.size(); ++i)\n    {\n        const Point& p = points[i];\n        int idx;\n        if (p.x >= 0.0) {\n            if (p.y >= 0.0) idx = 0;   // Quadrant I\n            else           idx = 3;   // Quadrant IV\n        } else { // p.x < 0.0\n            if (p.y >= 0.0) idx = 1;   // Quadrant II\n            else           idx = 2;   // Quadrant III\n        }\n        ++local[idx];\n    }\n#pragma omp critical\n    {\n        bins[0] += local[0];\n        bins[1] += local[1];\n        bins[2] += local[2];\n        bins[3] += local[3];\n    }\n}\n}\n```\n",
  "21_histogram_bin_0-100": "```cpp\n#pragma once\n#include <array>\n#include <vector>\n#include <cstddef>\n#include <omp.h>\n\n// Count how many values of x fall into each 10\u2011unit bin.\ninline void binsBy10Count(const std::vector<double> &x,\n                          std::array<size_t, 10> &bins)\n{\n    const size_t n = x.size();\n    if (n == 0)\n        return;\n\n    #pragma omp parallel\n    {\n        size_t local[10] = {};\n\n        #pragma omp for\n        for (size_t i = 0; i < n; ++i)\n        {\n            const size_t bin = static_cast<size_t>(x[i] / 10.0);\n            local[bin] += 1;\n        }\n\n        #pragma omp critical\n        {\n            for (size_t j = 0; j < 10; ++j)\n                bins[j] += local[j];\n        }\n    }\n}\n```",
  "16_graph_largest_component": "```cpp\n#pragma once\n#include <vector>\n\n/* Return the number of vertices in the largest component of the graph defined by the adjacency matrix A.\n   A is an NxN adjacency matrix stored in row-major.\n   Example:\n     input: [[0, 1, 0, 0], [1, 0, 0, 0], [0, 0, 0, 1], [0, 0, 1, 0]]\n   output: 2\n*/\ninline int largestComponent(std::vector<int> const& A, size_t N) {\n    std::vector<unsigned char> visited(N, 0);\n    int maxCount = 0;\n\n    // Stack large enough for any component\n    std::vector<int> stackArray(N);\n\n    for (size_t i = 0; i < N; ++i) {\n        if (!visited[i]) {\n            int count = 0;\n            size_t top = 0;\n            stackArray[top++] = static_cast<int>(i);\n            visited[i] = 1;\n\n            while (top > 0) {\n                int node = stackArray[--top];\n                ++count;\n\n                const int* row = A.data() + node * N;\n                for (size_t j = 0; j < N; ++j) {\n                    if (row[j] && !visited[j]) {\n                        visited[j] = 1;\n                        stackArray[top++] = static_cast<int>(j);\n                    }\n                }\n            }\n\n            if (count > maxCount)\n                maxCount = count;\n        }\n    }\n\n    return maxCount;\n}\n```",
  "23_histogram_first_letter_counts": "```cpp\n#pragma once\n#include <array>\n#include <string>\n#include <vector>\n#include <omp.h>\n\n/* Count the number of strings in the vector s that start with each letter\n   of the lowercase alphabet and store the result in bins.\n   The function is thread\u2011safe and preserves the exact semantics\n   of the original single\u2011threaded implementation. */\nvoid firstLetterCounts(const std::vector<std::string> &s, std::array<size_t, 26> &bins) {\n    const size_t n = s.size();\n    if (n == 0) return;\n\n    #pragma omp parallel\n    {\n        std::array<size_t, 26> local_bin{};             // zero\u2011initialised per thread\n        #pragma omp for schedule(static)\n        for (size_t i = 0; i < n; ++i) {\n            const char c = s[i][0];\n            const int index = c - 'a';\n            local_bin[index] += 1;\n        }\n        #pragma omp critical\n        {\n            for (int j = 0; j < 26; ++j) {\n                bins[j] += local_bin[j];\n            }\n        }\n    }\n}\n```\n",
  "55_transform_relu": "```cpp\n#pragma once\n#include <vector>\n#include <cstddef>\n#include <omp.h>\n\n/* Compute the ReLU function on every element of x. Elements less than zero become zero,\n   while elements that are zero or greater remain unchanged.\n   Example:\n   input: [-1.8, 24.0, 1.2, 0.0, -5.1, -0.2, 4.5]\n   output: [0, 24.0, 1.2, 0, 0, 0, 4.5]\n*/\ninline void relu(std::vector<double> &x) {\n    const std::size_t n = x.size();\n\n    #pragma omp parallel for schedule(static)\n    for (std::size_t i = 0; i < n; ++i) {\n        const double v = x[i];\n        x[i] = v < 0.0 ? 0.0 : v;\n    }\n}\n```",
  "32_scan_sum_of_prefix_sum_array": "```cpp\n#pragma once\n#include <cstdint>\n#include <vector>\n\ninline int64_t sumOfPrefixSum(const std::vector<int64_t>& x) {\n    const std::size_t n = x.size();\n    int64_t total = 0;\n    #pragma omp parallel for schedule(static) reduction(+:total)\n    for (std::size_t i = 0; i < n; ++i) {\n        total += static_cast<int64_t>(n - i) * x[i];\n    }\n    return total;\n}\n```\n",
  "18_graph_count_components": "```cpp\n#pragma once\n#include <vector>\n#include <cstddef>\n\nint componentCount(const std::vector<int>& A, size_t N) {\n    const int n = static_cast<int>(N);\n    std::vector<int> parent(n);\n    for (int i = 0; i < n; ++i) parent[i] = i;\n\n    auto find = [&](int v) -> int {\n        while (parent[v] != v) {\n            parent[v] = parent[parent[v]];\n            v = parent[v];\n        }\n        return v;\n    };\n\n    const int* a = A.data();\n    for (int i = 0; i < n; ++i) {\n        const int* rowStart = a + i * n + (i + 1);\n        for (int j = i + 1; j < n; ++j, ++rowStart) {\n            if (*rowStart) {\n                int ri = find(i);\n                int rj = find(j);\n                if (ri != rj) {\n                    parent[ri] = rj;\n                }\n            }\n        }\n    }\n\n    int count = 0;\n    for (int i = 0; i < n; ++i) {\n        if (parent[i] == i) {\n            ++count;\n        }\n    }\n    return count;\n}\n```",
  "29_reduce_sum_of_min_of_pairs": "```cpp\n#pragma once\n#include <vector>\n#include <cstdint>\n#include <cstddef>\n\n/* Return the sum of the minimum value at each index of vectors x and y for all indices.\n   i.e. sum = min(x_0, y_0) + min(x_1, y_1) + min(x_2, y_2) + ...\n   Example:\n   input: x=[3, 4, 0, 2, 3], y=[2, 5, 3, 1, 7]\n   output: 10\n*/\ninline int64_t sumOfMinimumElements(const std::vector<int64_t>& x, const std::vector<int64_t>& y) {\n    const std::size_t n = x.size();\n    if (n == 0) return 0;\n\n    const int64_t* px = x.data();\n    const int64_t* py = y.data();\n    int64_t sum = 0;\n\n#pragma omp parallel for reduction(+:sum) schedule(static)\n    for (std::size_t i = 0; i < n; ++i) {\n        sum += (px[i] < py[i] ? px[i] : py[i]);\n    }\n\n    return sum;\n}\n```",
  "17_graph_highest_degree": "```cpp\n#pragma once\n#include <vector>\n#ifdef _OPENMP\n#include <omp.h>\n#endif\n\ninline int maxDegree(std::vector<int> const& A, size_t N) {\n    if (N == 0) return 0;\n    const int* data = A.data();\n    int highestDegree = 0;\n    #pragma omp parallel for reduction(max:highestDegree) schedule(static)\n    for (size_t i = 0; i < N; ++i) {\n        int degree = 0;\n        const int* row = data + i * N;\n        #pragma omp simd reduction(+:degree)\n        for (size_t j = 0; j < N; ++j) {\n            degree += row[j];\n        }\n        if (degree > highestDegree) highestDegree = degree;\n    }\n    return highestDegree;\n}\n```\n",
  "58_transform_squaring": "```cpp\n#pragma once\n#include <vector>\n#include <cstddef>\n#include <immintrin.h>\n#include <omp.h>\n\ninline void squareEach(std::vector<int> &x) {\n    const size_t n = x.size();\n    if (n == 0) return;\n    int *data = x.data();\n\n    // Small arrays don't benefit from threading\n    constexpr size_t PARALLEL_MIN = 1 << 12; // 4096 elements\n\n    if (n <= PARALLEL_MIN) {\n        for (size_t i = 0; i < n; ++i) {\n            int v = data[i];\n            data[i] = v * v;\n        }\n        return;\n    }\n\n    const size_t vec_width = 8; // 8 \u00d7 32\u2011bit integers per AVX register\n\n#pragma omp parallel\n    {\n        int tid       = omp_get_thread_num();\n        int nthreads  = omp_get_num_threads();\n        size_t chunk  = (n + nthreads - 1) / nthreads; // round\u2011up\n        size_t start  = static_cast<size_t>(tid) * chunk;\n        size_t end    = start + chunk;\n        if (end > n) end = n;\n\n        size_t i = start;\n        // Vectorized AVX2 multiplication\n        for (; i + vec_width <= end; i += vec_width) {\n            __m256i v = _mm256_loadu_si256(reinterpret_cast<__m256i const*>(data + i));\n            __m256i r = _mm256_mullo_epi32(v, v);\n            _mm256_storeu_si256(reinterpret_cast<__m256i*>(data + i), r);\n        }\n        // Remaining elements\n        for (; i < end; ++i) {\n            int v = data[i];\n            data[i] = v * v;\n        }\n    }\n}\n```\n",
  "45_sparse_la_sparse_solve": "```cpp\n#pragma once\n#include <vector>\n#include <algorithm>\n#include <cmath>\n#include <cstddef>\n\n#if defined(_OPENMP)\n#  include <omp.h>\n#endif\n\nstruct COOElement {\n    size_t row, column;\n    double value;\n};\n\n/* Solve the sparse linear system Ax=b for x.\n   A is a sparse NxN matrix in COO format. x and b are dense vectors with N elements.\n   Example:\n   input:  A=[{0,0,1}, {0,1,1}, {1,1,-2}]  b=[1,4]\n   output: x=[3,-2]\n*/\nvoid solveLinearSystem(std::vector<COOElement> const& A,\n                       std::vector<double> const& b,\n                       std::vector<double> &x,\n                       size_t N)\n{\n    /* Allocate a dense matrix in row\u2011major order */\n    std::vector<double> mat(N * N, 0.0);\n\n    /* Fill the matrix from the COO representation */\n    for (const auto &e : A) {\n        mat[e.row * N + e.column] = e.value;\n    }\n\n    /* Working copies of the RHS and the solution vector */\n    std::vector<double> b_copy = b;\n    x.assign(N, 0.0);\n\n    /* Forward elimination with partial pivoting */\n    for (size_t i = 0; i < N; ++i)\n    {\n        /* Find pivot row */\n        size_t maxRow   = i;\n        double maxValue = std::abs(mat[i * N + i]);\n\n        for (size_t k = i + 1; k < N; ++k) {\n            double val = std::abs(mat[k * N + i]);\n            if (val > maxValue) {\n                maxValue = val;\n                maxRow   = k;\n            }\n        }\n\n        /* Swap rows iff necessary */\n        if (maxRow != i) {\n            std::swap_ranges(mat.begin() + i * N,\n                             mat.begin() + i * N + N,\n                             mat.begin() + maxRow * N);\n            std::swap(b_copy[i], b_copy[maxRow]);\n        }\n\n        double pivot = mat[i * N + i];\n\n        /* Eliminate all rows below the pivot */\n#       pragma omp parallel for schedule(static)\n        for (size_t k = i + 1; k < N; ++k) {\n            double *row_k = &mat[k * N];\n            double  c     = -row_k[i] / pivot;\n            row_k[i] = 0.0;\n\n            double *row_i = &mat[i * N];\n#           pragma omp simd\n            for (size_t j = i + 1; j < N; ++j) {\n                row_k[j] += c * row_i[j];\n            }\n\n            b_copy[k] += c * b_copy[i];\n        }\n    }\n\n    /* Back substitution */\n    for (int i = static_cast<int>(N) - 1; i >= 0; --i) {\n        double diag = mat[i * N + i];\n        x[i] = b_copy[i] / diag;\n        for (int k = i - 1; k >= 0; --k) {\n            b_copy[k] -= mat[k * N + i] * x[i];\n        }\n    }\n}\n```\n",
  "25_reduce_xor": "```cpp\n#pragma once\n#include <vector>\n#include <cstddef>\n#include <omp.h>\n\n#pragma omp declare reduction(xor: bool : omp_out ^= omp_in) initializer(omp_priv = false)\n\n/* Compute the logical XOR (parity) of all elements in a std::vector<bool>.\n * The function is parallelised using OpenMP. The behaviour is identical\n * to the original sequential implementation. */\ninline bool reduceLogicalXOR(const std::vector<bool>& x) {\n    bool result = false;\n    #pragma omp parallel for reduction(xor: result) schedule(static)\n    for (std::size_t i = 0; i < x.size(); ++i) {\n        result ^= static_cast<bool>(x[i]);   // explicit conversion avoids proxy overloads\n    }\n    return result;\n}\n```\n",
  "06_fft_dft": "```cpp\n#pragma once\n#include <vector>\n#include <complex>\n#include <cmath>\n\n/* Define M_PI if the compiler didn't provide it */\n#ifndef M_PI\n#define M_PI 3.14159265358979323846\n#endif\n\n/**\n * Compute the discrete Fourier transform of x.\n * Result is written to `output`. The function is fully thread-safe\n * and uses an OpenMP parallel for loop for the outer loop.\n *\n * @param x      Input signal (real values)\n * @param output Output array of complex numbers (size is set by the function)\n */\ninline void dft(const std::vector<double>& x, std::vector<std::complex<double>>& output)\n{\n    const std::size_t N = x.size();\n    output.resize(N);\n\n    const double twoPiOverN = 2.0 * M_PI / static_cast<double>(N);\n\n    /* Parallel over the output indices k */\n#pragma omp parallel for schedule(static)\n    for (std::size_t k = 0; k < N; ++k)\n    {\n        /* Pre\u2011compute the exponential step e = e^{-i*2\u03c0k/N} */\n        const double theta   = twoPiOverN * static_cast<double>(k);\n        const double cos_theta = std::cos(theta);\n        const double sin_theta = std::sin(theta);\n        const double e_real =  cos_theta;   /* real part of e */\n        const double e_imag = -sin_theta;   /* imag part of e */\n\n        double c_real = 1.0;  /* current e^{i\u03b8n} */\n        double c_imag = 0.0;\n\n        double sum_real = 0.0;\n        double sum_imag = 0.0;\n\n        for (std::size_t n = 0; n < N; ++n)\n        {\n            const double xn = x[n];\n            sum_real += xn * c_real;\n            sum_imag += xn * c_imag;\n\n            /* Rotate c by one step: c = c * e */\n            const double tmp_real = c_real * e_real - c_imag * e_imag;\n            const double tmp_imag = c_real * e_imag + c_imag * e_real;\n            c_real = tmp_real;\n            c_imag = tmp_imag;\n        }\n\n        output[k] = std::complex<double>(sum_real, sum_imag);\n    }\n}\n```\n",
  "31_scan_scan_with_min_function": "```cpp\n#pragma once\n#include <vector>\n#include <limits>\n#include <algorithm>\n\n// Compute partial minimums: replace each element with the minimum value\n// of all preceding elements (inclusive scan with min).\ninline void partialMinimums(std::vector<float> &x) {\n    const std::size_t n = x.size();\n    if (n == 0) return;\n\n    // Partition the array into blocks that can be processed independently.\n    constexpr std::size_t BLOCK_SIZE = 1 << 12;          // 4096 elements per block\n    const std::size_t num_blocks = (n + BLOCK_SIZE - 1) / BLOCK_SIZE;\n\n    // Temporary storage for the final value of each block.\n    std::vector<float> block_min(num_blocks);\n\n    // First pass: compute the running minimum inside each block.\n    #pragma omp parallel for schedule(static)\n    for (std::size_t b = 0; b < num_blocks; ++b) {\n        const std::size_t start = b * BLOCK_SIZE;\n        const std::size_t end   = std::min(start + BLOCK_SIZE, n);\n        float local_min = std::numeric_limits<float>::max();\n        for (std::size_t i = start; i < end; ++i) {\n            local_min = std::min(local_min, x[i]);\n            x[i] = local_min;\n        }\n        block_min[b] = local_min;\n    }\n\n    // Prefix scan over the block minima.\n    for (std::size_t b = 1; b < num_blocks; ++b) {\n        block_min[b] = std::min(block_min[b - 1], block_min[b]);\n    }\n\n    // Second pass: apply the minimum from all preceding blocks.\n    #pragma omp parallel for schedule(static)\n    for (std::size_t b = 1; b < num_blocks; ++b) {\n        const std::size_t start = b * BLOCK_SIZE;\n        const std::size_t end   = std::min(start + BLOCK_SIZE, n);\n        const float prev_min = block_min[b - 1];\n        for (std::size_t i = start; i < end; ++i) {\n            x[i] = std::min(prev_min, x[i]);\n        }\n    }\n}\n```",
  "28_reduce_smallest_odd_number": "```cpp\n#pragma once\n#include <vector>\n#include <limits>\n#include <omp.h>\n\nint smallestOdd(const std::vector<int>& x) {\n    const size_t N = x.size();\n    const int INF = std::numeric_limits<int>::max();\n    int ans = INF;\n\n    #pragma omp parallel for reduction(min:ans)\n    for (size_t i = 0; i < N; ++i) {\n        const int v = x[i];\n        // Match original behaviour: only positive odd numbers count.\n        if (v > 0 && (v & 1) && v < ans) {\n            ans = v;\n        }\n    }\n    return ans;\n}\n```",
  "27_reduce_average": "```cpp\n#pragma once\n#include <vector>\n#ifdef _OPENMP\n#include <omp.h>\n#endif\n\ninline double average(const std::vector<double> &x) {\n    const std::size_t n = x.size();\n    if (n == 0) {\n        return 0.0 / 0.0;           // retain NaN for empty input as std::reduce does\n    }\n\n    double sum = 0.0;\n#ifdef _OPENMP\n    // Use parallel reduction for large vectors; otherwise fall back to serial\n    constexpr std::size_t PAR_THRESHOLD = 32768;   // tunable threshold\n    if (n > PAR_THRESHOLD) {\n        #pragma omp parallel for reduction(+:sum) schedule(static)\n        for (std::size_t i = 0; i < n; ++i) {\n            sum += x[i];\n        }\n    } else {\n        for (std::size_t i = 0; i < n; ++i) {\n            sum += x[i];\n        }\n    }\n#else\n    for (std::size_t i = 0; i < n; ++i) {\n        sum += x[i];\n    }\n#endif\n\n    return sum / static_cast<double>(n);\n}\n```",
  "26_reduce_product_of_inverses": "```cpp\n#pragma once\n#include <vector>\n#include <cstddef>\n#include <omp.h>\n\ninline double productWithInverses(std::vector<double> const& x)\n{\n    double result = 1.0;\n    const std::size_t n = x.size();\n    const double* data = x.data();\n\n#pragma omp parallel for reduction(*: result) schedule(static)\n    for (std::size_t i = 0; i < n; ++i)\n    {\n        double val = (i & 1) ? (1.0 / data[i]) : data[i];\n        result *= val;\n    }\n\n    return result;\n}\n```",
  "19_graph_shortest_path": "```cpp\n#pragma once\n#include <vector>\n#include <limits>\n#include <atomic>\n#include <omp.h>\n\n/* Return the length of the shortest path from source to dest in the undirected graph defined by the adjacency matrix A.\n   A is an NxN adjacency matrix stored in row\u2011major. Assume the graph is connected. A is undirected.\n   Example:\n     input: [[0, 1, 1, 0], [1, 0, 0, 1], [1, 0, 0, 1], [0, 1, 1, 0]], source=0, dest=3\n   output: 2\n*/\nint shortestPathLength(const std::vector<int>& A, size_t N, int source, int dest) {\n    if (source == dest) return 0;\n\n    // visited array (atomic to avoid data races when parallelised)\n    std::vector<std::atomic<unsigned char>> visited(N);\n    for (size_t i = 0; i < N; ++i) visited[i].store(0, std::memory_order_relaxed);\n    visited[source].store(1, std::memory_order_relaxed);\n\n    std::vector<int> frontier;\n    frontier.push_back(source);\n\n    std::atomic<int> destDistance(-1);\n    int depth = 0;\n\n    const int num_threads = omp_get_max_threads();\n    std::vector<std::vector<int>> localNext(num_threads);\n\n    const int N_int = static_cast<int>(N);\n\n    while (!frontier.empty() && destDistance.load(std::memory_order_relaxed) == -1) {\n        // Clear per\u2011thread buffers\n        for (int t = 0; t < num_threads; ++t) localNext[t].clear();\n\n        ++depth;  // distance of nodes we are about to explore\n\n        const int frontierSize = static_cast<int>(frontier.size());\n\n        #pragma omp parallel for schedule(static)\n        for (int idx = 0; idx < frontierSize; ++idx) {\n            int node = frontier[idx];\n            size_t row_off = static_cast<size_t>(node) * N;\n            const int* row = &A[row_off];\n\n            for (int neigh = 0; neigh < N_int; ++neigh) {\n                if (!row[neigh]) continue;            // no edge\n                if (neigh == dest) {                  // found destination\n                    int expected = -1;\n                    destDistance.compare_exchange_strong(\n                        expected, depth, std::memory_order_relaxed);\n                }\n                unsigned char prev = visited[neigh].exchange(\n                    1, std::memory_order_relaxed);\n                if (!prev) {                          // newly discovered node\n                    int tid = omp_get_thread_num();\n                    localNext[tid].push_back(neigh);\n                }\n            }\n        }\n\n        // Merge per\u2011thread results\n        std::vector<int> next;\n        size_t total = 0;\n        for (int t = 0; t < num_threads; ++t) total += localNext[t].size();\n        next.reserve(total);\n        for (int t = 0; t < num_threads; ++t) {\n            next.insert(next.end(), localNext[t].begin(), localNext[t].end());\n        }\n\n        frontier.swap(next);  // move to the next frontier\n    }\n\n    int result = destDistance.load(std::memory_order_relaxed);\n    return (result != -1) ? result : std::numeric_limits<int>::max();\n}\n```\n",
  "47_sparse_la_spmv": "```cpp\n#pragma once\n#include <vector>\n#include <cstddef>\n#include <omp.h>\n\nstruct COOElement {\n    std::size_t row;\n    std::size_t column;\n    double value;\n};\n\n/* Compute y = alpha*A*x + beta*y where alpha and beta are scalars,\n   x and y are vectors, and A is a sparse matrix stored in COO format.\n   x and y are length N and A is M x N.\n*/\nvoid spmv(double alpha,\n          const std::vector<COOElement> &A,\n          const std::vector<double> &x,\n          double beta,\n          std::vector<double> &y,\n          std::size_t M,\n          std::size_t N) {\n    /* 1. Scale y by beta (parallel) */\n    #pragma omp parallel for schedule(static)\n    for (std::size_t i = 0; i < y.size(); ++i) {\n        y[i] *= beta;\n    }\n\n    /* Fast exit if matrix has no valid entries */\n    if (A.empty() || M == 0) return;\n\n    /* 2. Count valid non\u2011zero entries per row and total number of valid entries */\n    std::vector<std::size_t> row_count(M, 0);\n    std::size_t nnz = 0;\n    for (const auto &e : A) {\n        if (e.row < M && e.column < N) {\n            ++row_count[e.row];\n            ++nnz;\n        }\n    }\n\n    /* If no valid entries, we are finished */\n    if (nnz == 0) return;\n\n    /* 3. Build CSR row pointers */\n    std::vector<std::size_t> row_ptr(M + 1, 0);\n    std::size_t offset = 0;\n    for (std::size_t i = 0; i < M; ++i) {\n        row_ptr[i] = offset;\n        offset += row_count[i];\n    }\n    row_ptr[M] = offset;          // last element\n\n    /* 4. Allocate CSR column indices and values */\n    std::vector<std::size_t> col_idx(nnz);\n    std::vector<double> values(nnz);\n\n    /* 5. Fill CSR arrays while preserving the original order of entries */\n    std::vector<std::size_t> pos(row_ptr.begin(), row_ptr.end()); // current fill position\n    for (const auto &e : A) {\n        if (e.row < M && e.column < N) {\n            std::size_t i = e.row;\n            std::size_t idx = pos[i]++;   // next free spot for row i\n            col_idx[idx] = e.column;\n            values[idx] = e.value;\n        }\n    }\n\n    /* 6. Parallel row\u2011wise SpMV */\n    #pragma omp parallel for schedule(static)\n    for (std::size_t i = 0; i < M; ++i) {\n        for (std::size_t j = row_ptr[i]; j < row_ptr[i + 1]; ++j) {\n            y[i] += alpha * values[j] * x[col_idx[j]];\n        }\n    }\n}\n```",
  "13_geometry_closest_pair_2d": "```cpp\n#pragma once\n#include <vector>\n#include <limits>\n#include <cmath>\n#ifdef _OPENMP\n#include <omp.h>\n#endif\n\nstruct Point {\n    double x, y;\n};\n\n// Backward\u2011compatibility helper; unchanged from the original code.\ninline double distance(Point const& p1, Point const& p2) {\n    double dx = p2.x - p1.x;\n    double dy = p2.y - p1.y;\n    return std::sqrt(dx * dx + dy * dy);\n}\n\ninline double closestPair(std::vector<Point> const& points) {\n    const std::size_t n = points.size();\n    if (n < 2) return 0.0;\n\n    double minDistSq = std::numeric_limits<double>::max();\n    const Point* pData = points.data();\n\n    // Avoid OpenMP overhead for very small vectors.\n    const std::size_t sequentialThreshold = 1024;\n    if (n <= sequentialThreshold) {\n        for (std::size_t i = 0; i + 1 < n; ++i) {\n            const Point& p1 = pData[i];\n            for (std::size_t j = i + 1; j < n; ++j) {\n                const Point& p2 = pData[j];\n                double dx = p2.x - p1.x;\n                double dy = p2.y - p1.y;\n                double distSq = dx * dx + dy * dy;\n                if (distSq < minDistSq) minDistSq = distSq;\n            }\n        }\n    } else {\n#ifdef _OPENMP\n        // Force eight threads as specified by the execution environment.\n        omp_set_num_threads(8);\n#pragma omp parallel for reduction(min:minDistSq) schedule(static)\n#endif\n        for (std::size_t i = 0; i + 1 < n; ++i) {\n            const Point& p1 = pData[i];\n            for (std::size_t j = i + 1; j < n; ++j) {\n                const Point& p2 = pData[j];\n                double dx = p2.x - p1.x;\n                double dy = p2.y - p1.y;\n                double distSq = dx * dx + dy * dy;\n                if (distSq < minDistSq) minDistSq = distSq;\n            }\n        }\n    }\n\n    return std::sqrt(minDistSq);\n}\n```",
  "46_sparse_la_spmm": "```cpp\n#pragma once\n#include <vector>\n#include <cstddef>\n#include <omp.h>\n\nstruct COOElement {\n    size_t row;\n    size_t column;\n    double value;\n};\n\n/*\n * Compute Y = A * X where:\n *   A is an M x K sparse matrix (COO format)\n *   X is a K x N sparse matrix (COO format)\n *   Y is a dense M x N matrix (row\u2011major)\n *\n * This implementation preserves the same numerical result as the naive\n * double\u2011loop version, while improving performance by:\n *   \u2013 grouping X elements by their row index to avoid scanning all pairs\n *   \u2013 grouping A elements by their row index to allow per\u2011row parallelism\n *     without atomic updates.  Each thread works on a distinct output row,\n *     so the update order for each Y cell matches the original algorithm.\n */\ninline void spmm(const std::vector<COOElement> &A,\n                 const std::vector<COOElement> &X,\n                 std::vector<double> &Y,\n                 size_t M, size_t K, size_t N)\n{\n    Y.assign(M * N, 0.0);\n\n    if (A.empty() || X.empty() || M == 0 || N == 0) {\n        return;\n    }\n\n    /*----- Build bucket for X elements grouped by their row index -----*/\n    std::vector<std::vector<size_t>> x_buckets(K);\n    std::vector<size_t> x_counts(K, 0);\n    for (const auto &x : X) {\n        ++x_counts[x.row];\n    }\n    for (size_t r = 0; r < K; ++r) {\n        x_buckets[r].reserve(x_counts[r]);\n    }\n    for (size_t i = 0; i < X.size(); ++i) {\n        const auto &x = X[i];\n        x_buckets[x.row].push_back(i);\n    }\n\n    /*----- Build bucket for A elements grouped by their row index -----*/\n    std::vector<std::vector<size_t>> a_buckets(M);\n    std::vector<size_t> a_counts(M, 0);\n    for (const auto &a : A) {\n        ++a_counts[a.row];\n    }\n    for (size_t r = 0; r < M; ++r) {\n        a_buckets[r].reserve(a_counts[r]);\n    }\n    for (size_t i = 0; i < A.size(); ++i) {\n        const auto &a = A[i];\n        a_buckets[a.row].push_back(i);\n    }\n\n    /*----- Parallel processing over output rows -----*/\n    #pragma omp parallel for schedule(static)\n    for (size_t r = 0; r < M; ++r) {\n        const auto &rowA = a_buckets[r];\n        if (rowA.empty()) continue;\n\n        double *Yrow = Y.data() + r * N;\n\n        for (size_t ai : rowA) {\n            const auto &a = A[ai];\n            const auto &xrow = x_buckets[a.column];\n\n            for (size_t xi : xrow) {\n                const auto &x = X[xi];\n                Yrow[x.column] += a.value * x.value;\n            }\n        }\n    }\n}\n```\n",
  "52_stencil_1d_jacobi_3-point_stencil": "```cpp\n#pragma once\n#include <vector>\n#include <omp.h>\n\nvoid jacobi1D(const std::vector<double> &input, std::vector<double> &output) {\n    const size_t N = input.size();\n    if (N == 0) return;\n\n    const double inv3 = 1.0 / 3.0;\n    const double* in  = input.data();\n    double*       out = output.data();\n\n    if (N == 1) {\n        out[0] = in[0] * inv3;\n        return;\n    }\n\n    if (N == 2) {\n        double sum = (in[0] + in[1]) * inv3;\n        out[0] = sum;\n        out[1] = sum;\n        return;\n    }\n\n    out[0]       = (in[0] + in[1]) * inv3;\n    out[N - 1]   = (in[N - 2] + in[N - 1]) * inv3;\n\n#pragma omp parallel for schedule(static)\n    for (size_t i = 1; i < N - 1; ++i) {\n        out[i] = (in[i - 1] + in[i] + in[i + 1]) * inv3;\n    }\n}\n```\n",
  "14_geometry_closest_pair_1d": "```cpp\n#pragma once\n#include <vector>\n#include <algorithm>\n#include <limits>\n#include <cmath>\n#ifdef _OPENMP\n#include <omp.h>\n#endif\n\ninline double distance(double x1, double x2) {\n    return std::abs(x1 - x2);\n}\n\n/* Return the distance between the closest two elements in the vector x.\n   Example:\n   input: [7, 3, 9, 12, 31, 1]\n   output: 2\n*/\ninline double closestPair(std::vector<double> const& x) {\n    const std::size_t n = x.size();\n    if (n < 2) {\n        return 0.0;\n    }\n\n    // Copy and sort the data\n    std::vector<double> sorted(x.begin(), x.end());\n    std::sort(sorted.begin(), sorted.end());\n\n    double minDist = std::numeric_limits<double>::max();\n\n#ifdef _OPENMP\n#pragma omp parallel for reduction(min:minDist) schedule(static)\n#endif\n    for (std::size_t i = 0; i + 1 < n; ++i) {\n        double d = sorted[i + 1] - sorted[i]; // non\u2011negative\n        if (d < minDist) {\n            minDist = d;\n        }\n    }\n\n    return minDist;\n}\n```",
  "49_sparse_la_sparse_lu_decomp": "```cpp\n#pragma once\n#include <vector>\n#include <cstddef>\n#ifdef _OPENMP\n#include <omp.h>\n#endif\n\nstruct COOElement {\n    size_t row, column;\n    double value;\n};\n\nvoid luFactorize(const std::vector<COOElement> &A,\n                 std::vector<double> &L,\n                 std::vector<double> &U,\n                 size_t N)\n{\n    /* Convert the COO representation to a dense matrix. */\n    std::vector<double> fullA(N * N, 0.0);\n    for (const auto &e : A)\n        fullA[e.row * N + e.column] = e.value;\n\n    /* Ensure that L and U are the correct size and zero\u2011initialised. */\n    L.assign(N * N, 0.0);\n    U.assign(N * N, 0.0);\n\n    /* Doolittle factorisation (no pivoting). */\n    for (size_t i = 0; i < N; ++i) {\n        const size_t iN = i * N;\n        double *Lrow = L.data() + iN;\n        double *Urow = U.data() + iN;\n\n        /* --- Compute the sub\u2011diagonal elements of L --- */\n        for (size_t j = 0; j < i; ++j) {\n            double sum = fullA[iN + j];\n#pragma omp simd\n            for (size_t k = 0; k < j; ++k) {\n                sum -= Lrow[k] * U[k * N + j];\n            }\n            Lrow[j] = sum / U[j * N + j];\n        }\n\n        /* Diagonal of L is unit. */\n        Lrow[i] = 1.0;\n\n        /* --- Compute the remaining entries of U --- */\n#pragma omp parallel for schedule(static)\n        for (size_t j = i; j < N; ++j) {\n            double sum = fullA[iN + j];\n#pragma omp simd\n            for (size_t k = 0; k < i; ++k) {\n                sum -= Lrow[k] * U[k * N + j];\n            }\n            Urow[j] = sum;\n        }\n    }\n}\n```",
  "53_stencil_2d_jacobi_5-point_stencil": "```cpp\n#include <vector>\n#include <cstddef>\n#include <omp.h>\n\n/* Compute one iteration of a 5\u2011point 2\u2011D Jacobi stencil on `input`.\n   The operation is performed in-place on `output`.\n   Boundaries are treated as zeros. */\nvoid jacobi2D(std::vector<double> const& input,\n              std::vector<double> &output,\n              size_t N)\n{\n    // Use raw pointers for maximum speed\n    const double * __restrict__ in  = input.data();\n    double *       __restrict__ out = output.data();\n    const double inv5 = 0.2;   // 1/5\n\n    /* Parallelise over the rows; each thread works on distinct rows\n       and therefore does not race with others. */\n#pragma omp parallel for schedule(static)\n    for (size_t i = 0; i < N; ++i)\n    {\n        const double *row      = in + i * N;\n        double       *outrow   = out + i * N;\n        const double *row_up   = (i > 0)       ? in + (i - 1) * N : nullptr;\n        const double *row_down = (i + 1 < N)   ? in + (i + 1) * N : nullptr;\n\n        for (size_t j = 0; j < N; ++j)\n        {\n            double sum = row[j];\n            if (row_up)   sum += row_up[j];\n            if (row_down) sum += row_down[j];\n            if (j > 0)        sum += row[j - 1];\n            if (j + 1 < N)    sum += row[j + 1];\n            outrow[j] = sum * inv5;\n        }\n    }\n}\n```\n",
  "33_scan_reverse_prefix_sum": "```cpp\n#pragma once\n#include <vector>\n#include <algorithm>\n#ifdef _OPENMP\n#include <omp.h>\n#endif\n\n/* Compute the reverse prefix sum of the vector `x` into `output`.\n   For every index i, output[i] = sum of x[k] for k = i \u2026 x.size()-1.\n   Example:\n       input:  [1, 7, 4, 6, 6, 2]\n       output: [2, 8, 14, 18, 25, 26]\n   The implementation is fully parallel using OpenMP when available\n   and preserves the exact behaviour of the original sequential code.\n*/\nvoid reversePrefixSum(const std::vector<int> &x, std::vector<int> &output)\n{\n    const std::size_t N = x.size();\n    if (N == 0) return;\n\n    int num_threads = 1;\n#ifdef _OPENMP\n    num_threads = omp_get_max_threads();\n#endif\n    if (num_threads > static_cast<int>(N)) num_threads = static_cast<int>(N);\n\n    const std::size_t block_size = (N + num_threads - 1) / num_threads;\n    std::vector<int> block_sum(num_threads, 0);\n\n#ifdef _OPENMP\n#pragma omp parallel num_threads(num_threads)\n#endif\n    {\n        int tid = 0;\n#ifdef _OPENMP\n        tid = omp_get_thread_num();\n#endif\n        const std::size_t bs = static_cast<std::size_t>(tid) * block_size;\n        const std::size_t be = std::min(bs + block_size, N);\n\n        int local_sum = 0;\n        const int *xp   = x.data() + (N - 1 - bs); // start of reversed block\n        int   *outp    = output.data() + bs;       // corresponding output block\n        for (std::size_t i = bs; i < be; ++i) {\n            local_sum += *xp;\n            *outp = local_sum;\n            --xp;\n            ++outp;\n        }\n        block_sum[tid] = local_sum;\n    }\n\n    // Compute offsets for each block (prefix sums of block sums)\n    std::vector<int> offset(num_threads, 0);\n    int prefix = 0;\n    for (int t = 0; t < num_threads; ++t) {\n        offset[t] = prefix;\n        prefix += block_sum[t];\n    }\n\n#ifdef _OPENMP\n#pragma omp parallel for num_threads(num_threads) schedule(static)\n#endif\n    for (int t = 0; t < num_threads; ++t) {\n        const std::size_t bs = static_cast<std::size_t>(t) * block_size;\n        const std::size_t be = std::min(bs + block_size, N);\n        const int off = offset[t];\n        for (std::size_t i = bs; i < be; ++i) {\n            output[i] += off;\n        }\n    }\n}\n```",
  "11_geometry_convex_hull_perimeter": "```cpp\n#pragma once\n#include <vector>\n#include <algorithm>\n#include <cmath>\n\nstruct Point {\n    double x, y;\n};\n\ninline double distance(const Point& p1, const Point& p2) {\n    double dx = p2.x - p1.x;\n    double dy = p2.y - p1.y;\n    return std::sqrt(dx * dx + dy * dy);\n}\n\ninline double dist(const Point& a, const Point& b) {\n    double dx = a.x - b.x;\n    double dy = a.y - b.y;\n    return std::sqrt(dx * dx + dy * dy);\n}\n\ninline double cross(const Point& a, const Point& b, const Point& c) {\n    return (b.x - a.x) * (c.y - a.y) - (b.y - a.y) * (c.x - a.x);\n}\n\ninline bool pointLess(const Point& a, const Point& b) {\n    return a.x < b.x || (a.x == b.x && a.y < b.y);\n}\n\ndouble convexHullPerimeter(const std::vector<Point>& points) {\n    const std::size_t n = points.size();\n    if (n < 3) {\n        return 0.0;\n    }\n\n    std::vector<Point> sorted(points);\n    std::sort(sorted.begin(), sorted.end(), pointLess);\n\n    // Allocate space for hull (maximum 2*n points)\n    std::vector<Point> hull(2 * n);\n    std::size_t k = 0;\n\n    // Build lower hull\n    for (std::size_t i = 0; i < n; ++i) {\n        while (k >= 2 && cross(hull[k - 2], hull[k - 1], sorted[i]) <= 0.0) {\n            --k;\n        }\n        hull[k++] = sorted[i];\n    }\n\n    // Build upper hull\n    const std::size_t t = k + 1;\n    for (std::size_t i = n - 1; i > 0; --i) {\n        while (k >= t && cross(hull[k - 2], hull[k - 1], sorted[i - 1]) <= 0.0) {\n            --k;\n        }\n        hull[k++] = sorted[i - 1];\n    }\n\n    // Remove duplicate last point\n    hull.resize(k - 1);\n\n    // Compute perimeter\n    double perimeter = 0.0;\n    const std::size_t m = hull.size();\n    for (std::size_t i = 0; i + 1 < m; ++i) {\n        double dx = hull[i].x - hull[i + 1].x;\n        double dy = hull[i].y - hull[i + 1].y;\n        perimeter += std::sqrt(dx * dx + dy * dy);\n    }\n    // Close the polygon\n    double dx = hull[m - 1].x - hull[0].x;\n    double dy = hull[m - 1].y - hull[0].y;\n    perimeter += std::sqrt(dx * dx + dy * dy);\n\n    return perimeter;\n}\n```",
  "03_dense_la_axpy": "```cpp\n#pragma once\n#include <vector>\n#include <cstddef>\n#include <cstdint>\n#include <omp.h>\n\ninline void axpy(double alpha,\n                 const std::vector<double>& x,\n                 const std::vector<double>& y,\n                 std::vector<double>& z)\n{\n    const std::size_t n = x.size();\n    const double* px = x.data();\n    const double* py = y.data();\n    double*   pz = z.data();\n\n    /* Parallelize only when the vector is large enough to amortize\n       the threading overhead. */\n    #pragma omp parallel for schedule(static) if (n > 1024)\n    for (int64_t i = 0; i < static_cast<int64_t>(n); ++i)\n    {\n        pz[i] = alpha * px[i] + py[i];\n    }\n}\n```",
  "05_fft_inverse_fft": "```cpp\n#pragma once\n#include <cmath>\n#include <complex>\n#include <vector>\n#include <cstddef>\n#include <omp.h>\n\n#ifndef M_PI\n#define M_PI 3.14159265358979323846\n#endif\n\n/* Perform an in-place radix\u20112 FFT (decimation\u2011in\u2011time). */\nstatic inline void fft_helper(std::vector<std::complex<double>> &x)\n{\n    const size_t N = x.size();\n    if (N <= 1) return;\n\n    std::complex<double>* px = x.data();\n\n    /* Pre\u2011compute the basic twiddle factor \u03c6T = exp(-i\u00b7\u03c0/N) */\n    const long double PI_ld = 3.14159265358979323846264338328L;\n    const long double thetaT = PI_ld / static_cast<long double>(N);\n    std::complex<double> phiT(std::cosl(thetaT), -std::sinl(thetaT));\n\n    size_t k = N;\n    while (k > 1) {\n        const size_t n = k;          /* n = 2\u00b7k */\n        k >>= 1;                     /* k = k/2 */\n        phiT *= phiT;                /* \u03c6T \u2190 \u03c6T\u00b2 */\n\n        /* Parallel over l: each thread works on disjoint butterflies */\n        #pragma omp parallel for schedule(static)\n        for (size_t l = 0; l < k; ++l) {\n            std::complex<double> T(1.0, 0.0);\n            for (size_t a = l; a < N; a += n) {\n                const size_t b = a + k;\n                const std::complex<double> t = px[a] - px[b];\n                px[a] += px[b];\n                px[b] = t * T;\n                T *= phiT;\n            }\n        }\n    }\n\n    /* Bit\u2011reversal re\u2011ordering */\n    const size_t m = static_cast<size_t>(std::log2(static_cast<double>(N)));\n\n    #pragma omp parallel for schedule(static)\n    for (size_t a = 0; a < N; ++a) {\n        unsigned int bi = static_cast<unsigned int>(a);\n        bi = ((bi & 0xaaaaaaaau) >> 1) | ((bi & 0x55555555u) << 1);\n        bi = ((bi & 0xccccccccu) >> 2) | ((bi & 0x33333333u) << 2);\n        bi = ((bi & 0xf0f0f0f0u) >> 4) | ((bi & 0x0f0f0f0fu) << 4);\n        bi = ((bi & 0xff00ff00u) >> 8) | ((bi & 0x00ff00ffu) << 8);\n        bi = (bi >> 16) | (bi << 16);\n        const size_t rev = static_cast<size_t>(bi >> (32u - static_cast<unsigned int>(m)));\n        if (rev > a) {\n            std::complex<double> tmp = px[a];\n            px[a]     = px[rev];\n            px[rev]   = tmp;\n        }\n    }\n}\n\n/* Compute the inverse fourier transform of x in\u2011place. */\nstatic inline void ifft(std::vector<std::complex<double>> &x)\n{\n    const size_t N = x.size();\n    if (N == 0) return;\n\n    std::complex<double>* px = x.data();\n\n    /* Conjugate */\n    #pragma omp parallel for\n    for (size_t i = 0; i < N; ++i)\n        px[i] = std::conj(px[i]);\n\n    /* Forward FFT */\n    fft_helper(x);\n\n    /* Conjugate again and scale */\n    const double invN = 1.0 / static_cast<double>(N);\n    #pragma omp parallel for\n    for (size_t i = 0; i < N; ++i)\n        px[i] = std::conj(px[i]) * invN;\n}\n```",
  "12_geometry_smallest_triangle": "```cpp\n#pragma once\n#include <vector>\n#include <limits>\n#include <cmath>\n#include <omp.h>\n\nstruct Point {\n    double x, y;\n};\n\ninline double triangleArea(const Point& A, const Point& B, const Point& C) {\n    double cross = A.x * (B.y - C.y) + B.x * (C.y - A.y) + C.x * (A.y - B.y);\n    return 0.5 * std::fabs(cross);\n}\n\ninline double smallestArea(const std::vector<Point>& points) {\n    const size_t n = points.size();\n    if (n < 3) {\n        return 0.0;\n    }\n\n    double minArea = std::numeric_limits<double>::max();\n\n#pragma omp parallel for schedule(static, 1) reduction(min:minArea)\n    for (size_t i = 0; i < n - 2; ++i) {\n        const Point* pa = &points[i];\n        double ax = pa->x;\n        double ay = pa->y;\n\n        for (size_t j = i + 1; j < n - 1; ++j) {\n            const Point* pb = &points[j];\n            double bx = pb->x;\n            double by = pb->y;\n\n            double base = ax * by - bx * ay;\n            double dx   = bx - ax;\n            double dy   = ay - by;\n\n            for (size_t k = j + 1; k < n; ++k) {\n                const Point* pc = &points[k];\n                double cx = pc->x;\n                double cy = pc->y;\n\n                double area = 0.5 * std::fabs(base + cy * dx + cx * dy);\n                if (area < minArea) {\n                    minArea = area;\n                }\n            }\n        }\n    }\n\n    return minArea;\n}\n```\n",
  "01_dense_la_solve": "```cpp\n#pragma once\n#include <vector>\n#include <cstddef>\n#include <omp.h>\n\n/* Solve the linear system Ax=b for x.\n   A is an NxN matrix in row-major. x and b have N elements.\n   Example: input: A=[[1,4,2], [1,2,3], [2,1,3]] b=[11, 11, 13] output: x=[3, 1, 2]\n*/\ninline void solveLinearSystem(const std::vector<double> &A,\n                              const std::vector<double> &b,\n                              std::vector<double>       &x,\n                              size_t                     N)\n{\n    if (N == 0) return;                                 // guard against size_t underflow\n\n    // Work on copies so we preserve the originals\n    std::vector<double> A_copy = A;\n    std::vector<double> b_copy = b;\n\n    double *A_data = A_copy.data();\n    double *b_data = b_copy.data();\n    double *x_data = x.data();\n\n    const size_t N_minus_1 = N - 1U;\n\n    /* Gaussian elimination \u2013 forward sweep */\n    for (size_t i = 0; i < N_minus_1; ++i)\n    {\n        const double pivot = A_data[i * N + i];\n        if (pivot == 0.0)          // singular matrix \u2013 leave x untouched\n            return;\n\n        const double inv_pivot = 1.0 / pivot;\n        const double bi        = b_data[i];\n        const size_t  i_offset = i * N;\n        const double *row_i    = A_data + i_offset;   // read\u2011only pivot row\n\n        /* Parallelize the elimination of rows below the pivot row */\n        #pragma omp parallel for schedule(static)\n        for (size_t j = i + 1; j < N; ++j)\n        {\n            const size_t j_offset = j * N;\n            double *row_j = A_data + j_offset;\n\n            const double factor = row_j[i] * inv_pivot;  // A[j][i] / pivot\n            b_data[j] -= factor * bi;\n\n            /* Vectorized row operation */\n            #pragma omp simd\n            for (size_t k = i; k < N; ++k)\n                row_j[k] -= factor * row_i[k];\n        }\n    }\n\n    /* Back substitution \u2013 final sweep */\n    for (int i = static_cast<int>(N) - 1; i >= 0; --i)\n    {\n        const size_t i_offset = static_cast<size_t>(i) * N;\n        double sum = 0.0;\n\n        for (size_t j = i + 1; j < N; ++j)\n            sum += A_data[i_offset + j] * x_data[j];\n\n        x_data[i] = (b_data[i] - sum) / A_data[i_offset + i];\n    }\n}\n```",
  "00_dense_la_lu_decomp": "```cpp\n#pragma once\n#include <vector>\n#include <cstddef>\n#include <omp.h>\n\n/*\n   Factorize the matrix A into A = LU where L is a lower triangular matrix\n   and U is an upper triangular matrix.\n   The results for L and U are stored back into the original matrix A.\n   A is an NxN matrix stored in row-major order.\n   Example:\n     input:  [[4, 3], [6, 3]]\n     output: [[4, 3], [1.5, -1.5]]\n*/\ninline void luFactorize(std::vector<double> &A, size_t N) {\n    double *const data = A.data();\n\n    #pragma omp parallel\n    {\n        for (size_t k = 0; k < N; ++k) {\n            // Pivot row\n            const double * __restrict__ rowk = data + k * N;\n            const double pivot = rowk[k];\n\n            // Parallelize the work for all rows below the pivot\n            #pragma omp for schedule(static)\n            for (size_t i = k + 1; i < N; ++i) {\n                double * __restrict__ rowi = data + i * N;\n                const double factor = rowi[k] / pivot;\n                rowi[k] = factor;\n\n                // Update the remaining columns in row i\n                #pragma omp simd\n                for (size_t j = k + 1; j < N; ++j) {\n                    rowi[j] -= factor * rowk[j];\n                }\n            }\n        }\n    }\n}\n```",
  "07_fft_fft_conjugate": "```cpp\n#pragma once\n#include <vector>\n#include <complex>\n#include <cmath>\n#include <utility>  // std::swap\n#include <cstddef>  // std::size_t\n\nvoid fftConjugate(std::vector<std::complex<double>>& x)\n{\n    const std::size_t Nsize = x.size();\n    if (Nsize == 0) return;\n\n    // Work with 32\u2011bit indices as the original routine does\n    const unsigned int N = static_cast<unsigned int>(Nsize);\n\n    /* ---------- 1. Initial twiddle factor --------------------------------- */\n    static constexpr long double PI = 3.14159265358979323846264338328L;\n    const long double thetaT = PI / static_cast<long double>(N);\n    std::complex<double> phiT(std::cos(thetaT), -std::sin(thetaT));\n\n    /* ---------- 2. In\u2011place FFT (decimation\u2011in\u2011frequency) ----------------- */\n    unsigned int k = N;\n    std::vector<std::complex<double>> Twiddle;   // reused each stage\n\n    while (k > 1) {\n        unsigned int old_k = k;     // size before the shift\n        unsigned int stride = old_k; // step for the inner loop\n        k >>= 1;                   // number of butterfly groups this stage\n\n        phiT *= phiT;               // \u03c6_T = (\u03c6_T)^2\n\n        /* Compute the powers T^l = \u03c6_T^l for l = 0 \u2026 k\u20111 */\n        Twiddle.resize(k);\n        Twiddle[0] = std::complex<double>(1.0, 0.0);\n        for (unsigned int l = 1; l < k; ++l)\n            Twiddle[l] = Twiddle[l - 1] * phiT;\n\n        /* Parallel butterfly stage */\n        const unsigned int step = stride;\n        #pragma omp parallel for schedule(static)\n        for (int l = 0; l < static_cast<int>(k); ++l) {\n            const std::complex<double> Tval = Twiddle[l];\n            for (unsigned int a = static_cast<unsigned int>(l); a < N; a += step) {\n                unsigned int b = a + k;\n                std::complex<double> t = x[a] - x[b];\n                x[a] += x[b];\n                x[b] = t * Tval;\n            }\n        }\n    }\n\n    /* ---------- 3. Bit\u2011reversal decimation ------------------------------ */\n    const unsigned int m = static_cast<unsigned int>(std::log2(static_cast<double>(N)));\n    for (unsigned int a = 0; a < N; ++a) {\n        unsigned int b = a;\n        b = (((b & 0xaaaaaaaaU) >> 1) | ((b & 0x55555555U) << 1));\n        b = (((b & 0xccccccccU) >> 2) | ((b & 0x33333333U) << 2));\n        b = (((b & 0xf0f0f0f0U) >> 4) | ((b & 0x0f0f0f0fU) << 4));\n        b = (((b & 0xff00ff00U) >> 8) | ((b & 0x00ff00ffU) << 8));\n        b = (b >> 16) | (b << 16);\n        if (m > 0) {\n            b >>= (32u - m);\n        } else {\n            b = 0;\n        }\n        if (b > a) {\n            std::swap(x[a], x[b]);\n        }\n    }\n\n    /* ---------- 4. Conjugate each element -------------------------------- */\n    #pragma omp parallel for schedule(static)\n    for (int i = 0; i < static_cast<int>(N); ++i) {\n        x[i] = std::conj(x[i]);\n    }\n}\n```",
  "34_scan_largest_contiguous_subarray_sum": "```cpp\n#pragma once\n#include <vector>\n#include <limits>\n#include <omp.h>\n\nstruct Stats {\n    int sum;\n    int best_prefix;\n    int best_suffix;\n    int best;\n};\n\nconstexpr Stats Identity{\n    0,\n    std::numeric_limits<int>::lowest(),\n    std::numeric_limits<int>::lowest(),\n    std::numeric_limits<int>::lowest()\n};\n\ninline Stats merge_stats(const Stats& left, const Stats& right) noexcept {\n    Stats res;\n    res.sum = left.sum + right.sum;\n    res.best_prefix = left.best_prefix > left.sum + right.best_prefix\n                         ? left.best_prefix\n                         : left.sum + right.best_prefix;\n    res.best_suffix = right.best_suffix > right.sum + left.best_suffix\n                         ? right.best_suffix\n                         : right.sum + left.best_suffix;\n    int cross = left.best_suffix + right.best_prefix;\n    int best = left.best;\n    if (right.best > best) best = right.best;\n    if (cross > best) best = cross;\n    res.best = best;\n    return res;\n}\n\ninline int sequential_max_subarray(const std::vector<int>& x) {\n    const std::size_t n = x.size();\n    if (n == 0) {\n        return std::numeric_limits<int>::lowest();\n    }\n    int curr = x[0];\n    int best = x[0];\n    for (std::size_t i = 1; i < n; ++i) {\n        int val = x[i];\n        curr = val > curr + val ? val : curr + val;\n        if (curr > best) best = curr;\n    }\n    return best;\n}\n\nint maximumSubarray(std::vector<int> const& x) {\n    const std::size_t n = x.size();\n    if (n < 4096) {\n        return sequential_max_subarray(x);\n    }\n\n    const int thread_count = omp_get_max_threads();\n    const std::size_t chunk = (n + thread_count - 1) / static_cast<std::size_t>(thread_count);\n\n    std::vector<Stats> threadStats(thread_count, Identity);\n\n    #pragma omp parallel\n    {\n        const int tid = omp_get_thread_num();\n        const std::size_t start = static_cast<std::size_t>(tid) * chunk;\n        std::size_t end = start + chunk;\n        if (end > n) end = n;\n\n        if (start < end) {\n            int sum = 0;\n            int best_prefix = std::numeric_limits<int>::lowest();\n            int best = std::numeric_limits<int>::lowest();\n            int curr = std::numeric_limits<int>::lowest();\n\n            // First pass: compute sum, best_prefix, and best sub\u2011array within the chunk\n            for (std::size_t i = start; i < end; ++i) {\n                int val = x[i];\n                sum += val;\n                if (i == start) {\n                    curr = best = val;\n                    best_prefix = val;\n                } else {\n                    if (sum > best_prefix) best_prefix = sum;\n                    curr = val > curr + val ? val : curr + val;\n                    if (curr > best) best = curr;\n                }\n            }\n\n            // Second pass: compute best suffix (max suffix sum of the chunk)\n            int suffixSum = 0;\n            int best_suffix = std::numeric_limits<int>::lowest();\n            for (std::size_t i = end; i > start; ) {\n                --i;\n                suffixSum += x[i];\n                if (suffixSum > best_suffix) best_suffix = suffixSum;\n            }\n\n            threadStats[tid] = {sum, best_prefix, best_suffix, best};\n        }\n        // Empty range \u2013 leave the identity\n    }\n\n    Stats merged = Identity;\n    for (int t = 0; t < thread_count; ++t) {\n        merged = merge_stats(merged, threadStats[t]);\n    }\n\n    return merged.best;\n}\n```",
  "51_stencil_edge_kernel": "```cpp\n#pragma once\n#include <vector>\n#include <algorithm>\n#include <omp.h>\n\nvoid convolveKernel(std::vector<int> const& imageIn, std::vector<int> &imageOut, size_t N) {\n    if (N == 0) return;\n\n    const int n = static_cast<int>(N);\n    const size_t stride = N + 2;\n    std::vector<int> padded(stride * stride, 0);\n\n    // Copy image into padded interior\n    for (size_t i = 0; i < N; ++i) {\n        std::copy(imageIn.begin() + i * N,\n                  imageIn.begin() + (i + 1) * N,\n                  padded.begin() + (i + 1) * stride + 1);\n    }\n\n    const int *paddedData = padded.data();\n    int *outData = imageOut.data();\n\n#pragma omp parallel for schedule(static)\n    for (int pi = 1; pi <= n; ++pi) {\n        const int *top = paddedData + (pi - 1) * stride;\n        const int *cur = paddedData + pi * stride;\n        const int *bot = paddedData + (pi + 1) * stride;\n        size_t outRow = (pi - 1) * N;\n\n        for (int pj = 0; pj < n; ++pj) {\n            int idx = pj + 1;\n            int sum = 8 * cur[idx]\n                      - top[idx - 1] - top[idx] - top[idx + 1]\n                      - cur[idx - 1]            - cur[idx + 1]\n                      - bot[idx - 1] - bot[idx] - bot[idx + 1];\n            if (sum < 0) sum = 0;\n            else if (sum > 255) sum = 255;\n            outData[outRow + pj] = sum;\n        }\n    }\n}\n```",
  "48_sparse_la_sparse_axpy": "```cpp\n#pragma once\n#include <vector>\n#include <algorithm>\n#include <cstddef>\n#include <omp.h>\n\nstruct Element {\n    size_t index;\n    double value;\n};\n\ninline void sparseAxpy(double alpha,\n                       const std::vector<Element> &x,\n                       const std::vector<Element> &y,\n                       std::vector<double> &z) {\n    const size_t N = z.size();\n    if (N == 0) return;\n\n    int nThreads = omp_get_max_threads();\n    if (nThreads > 8) nThreads = 8;\n    if (nThreads < 1) nThreads = 1;\n\n    const size_t block = (N + nThreads - 1) / nThreads; // ceil division\n    std::vector<size_t> boundary(nThreads + 1);\n    boundary[0] = 0;\n    for (int t = 1; t <= nThreads; ++t) {\n        boundary[t] = std::min(N, boundary[t - 1] + block);\n    }\n\n    const auto lessIndex = [](const Element &e, size_t v) noexcept { return e.index < v; };\n\n    #pragma omp parallel for num_threads(nThreads) schedule(static)\n    for (int t = 0; t < nThreads; ++t) {\n        const size_t segStart = boundary[t];\n        const size_t segEnd   = boundary[t + 1];\n\n        const std::vector<Element> &xVec = x;\n        const std::vector<Element> &yVec = y;\n        const size_t xSize = xVec.size();\n        const size_t ySize = yVec.size();\n        double *pZ = z.data();\n\n        size_t xi = std::lower_bound(xVec.begin(), xVec.end(), segStart, lessIndex) - xVec.begin();\n        size_t yi = std::lower_bound(yVec.begin(), yVec.end(), segStart, lessIndex) - yVec.begin();\n\n        while (xi < xSize && yi < ySize &&\n               xVec[xi].index < segEnd && yVec[yi].index < segEnd) {\n            if (xVec[xi].index == yVec[yi].index) {\n                pZ[xVec[xi].index] += alpha * xVec[xi].value + yVec[yi].value;\n                ++xi; ++yi;\n            } else if (xVec[xi].index < yVec[yi].index) {\n                pZ[xVec[xi].index] += alpha * xVec[xi].value;\n                ++xi;\n            } else {\n                pZ[yVec[yi].index] += yVec[yi].value;\n                ++yi;\n            }\n        }\n\n        while (xi < xSize && xVec[xi].index < segEnd) {\n            pZ[xVec[xi].index] += alpha * xVec[xi].value;\n            ++xi;\n        }\n\n        while (yi < ySize && yVec[yi].index < segEnd) {\n            pZ[yVec[yi].index] += yVec[yi].value;\n            ++yi;\n        }\n    }\n}\n```\n",
  "04_dense_la_gemv": "```cpp\n#pragma once\n#include <vector>\n#include <cstddef>\n#include <omp.h>\n\n/* Multiply the matrix A by the vector x. Store the results in the vector y.\n   A is an MxN matrix stored in row\u2011major, x has N elements, and y has M elements.\n   Example:\n   input: A=[[1, -1, 2], [0, -3, 1]] x=[2, 1, 0]\n   output: y=[1, -3]\n*/\ninline void gemv(std::vector<double> const& A,\n                 std::vector<double> const& x,\n                 std::vector<double> & y,\n                 size_t M,\n                 size_t N)\n{\n    if (M == 0 || N == 0) return;\n\n    const double* a  = A.data();\n    const double* vx = x.data();\n    double* vy       = y.data();\n\n    #pragma omp parallel for schedule(static)\n    for (size_t i = 0; i < M; ++i) {\n        const double* row = a + i * N;\n        double sum = 0.0;\n        size_t j = 0;\n\n        /* Manual unroll for better vectorization */\n        for (; j + 7 < N; j += 8) {\n            sum += row[j]   * vx[j];\n            sum += row[j+1] * vx[j+1];\n            sum += row[j+2] * vx[j+2];\n            sum += row[j+3] * vx[j+3];\n            sum += row[j+4] * vx[j+4];\n            sum += row[j+5] * vx[j+5];\n            sum += row[j+6] * vx[j+6];\n            sum += row[j+7] * vx[j+7];\n        }\n        for (; j < N; ++j) {\n            sum += row[j] * vx[j];\n        }\n\n        vy[i] = sum;\n    }\n}\n```\n",
  "41_sort_k-th_smallest_element": "```cpp\n#pragma once\n#include <vector>\n#include <algorithm>\n\n/* Find the k-th smallest element of the vector x.\n   Example:\n   \n   input: x=[1, 7, 6, 0, 2, 2, 10, 6], k=4\n   output: 6\n*/\ninline int findKthSmallest(const std::vector<int>& x, int k) {\n    // Create a local copy of the input as we need to modify it\n    std::vector<int> x_copy = x;\n\n    // Partially sort to position the k-th smallest element at index k-1\n    std::nth_element(x_copy.begin(), x_copy.begin() + k - 1, x_copy.end());\n\n    // Return the k-th smallest value\n    return x_copy[k - 1];\n}\n```",
  "02_dense_la_gemm": "```cpp\n#pragma once\n#include <vector>\n#include <emmintrin.h>   // SSE2 intrinsics\n#include <omp.h>         // OpenMP declarations\n\ninline void gemm(const std::vector<double> &A,\n                 const std::vector<double> &B,\n                 std::vector<double>       &C,\n                 const size_t M, const size_t K, const size_t N)\n{\n    const double *a = A.data();          // A[i*K + k]\n    const double *b = B.data();          // B[k*N + j]\n    double       *c = C.data();          // C[i*N + j]\n\n    // Parallelise over the outermost dimension (rows of A / C)\n    #pragma omp parallel for schedule(static)\n    for (size_t i = 0; i < M; ++i) {\n        const double *a_row = a + i * K;\n        double       *c_row = c + i * N;\n\n        // Compute the dot product of the i\u2011th row of A and each column of B\n        for (size_t k = 0; k < K; ++k) {\n            double ak = a_row[k];               // A[i*K + k]\n            const double *b_row = b + k * N;    // B[k*N + j]\n\n            size_t j = 0;\n            // Unrolled 2\u2011element vectorised inner loop (SSE2, 128\u2011bit)\n            for (; j + 1 < N; j += 2) {\n                __m128d a_vec  = _mm_set1_pd(ak);          // broadcast ak\n                __m128d b_vec  = _mm_loadu_pd(b_row + j);   // load B row\n                __m128d c_vec  = _mm_loadu_pd(c_row + j);   // load C row\n                c_vec = _mm_add_pd(c_vec, _mm_mul_pd(a_vec, b_vec));\n                _mm_storeu_pd(c_row + j, c_vec);\n            }\n            // Handle the remaining element (if N is odd)\n            for (; j < N; ++j) {\n                c_row[j] += ak * b_row[j];\n            }\n        }\n    }\n}\n```",
  "42_sort_sorted_ranks": "#pragma once\n#include <algorithm>\n#include <numeric>\n#include <vector>\n\n#if __cplusplus >= 201703L\n#include <execution>\n#endif\n\ninline void ranks(std::vector<float> const& x, std::vector<size_t> &ranks_) {\n    std::vector<size_t> indices(x.size());\n    std::iota(indices.begin(), indices.end(), 0);\n\n    auto comp = [&x](size_t i1, size_t i2) { return x[i1] < x[i2]; };\n\n#if __cplusplus >= 201703L\n    std::sort(std::execution::par_unseq, indices.begin(), indices.end(), comp);\n#else\n    std::sort(indices.begin(), indices.end(), comp);\n#endif\n\n#pragma omp parallel for schedule(static)\n    for (size_t i = 0; i < indices.size(); ++i) {\n        ranks_[indices[i]] = i;\n    }\n}\n",
  "39_search_xor_contains": "```cpp\n#pragma once\n#include <vector>\n#include <algorithm>\n#include <cstddef>\n#include <omp.h>\n\n/* Return true if `val` is found in exactly one of the two vectors.\n   The implementation uses a parallel scan for large vectors to\n   amortize the overhead of creating an OpenMP team, while small\n   vectors are scanned sequentially for speed. */\ninline bool xorContains(const std::vector<int> &x,\n                        const std::vector<int> &y,\n                        int val) noexcept\n{\n    bool foundX = false;\n    bool foundY = false;\n\n    constexpr std::size_t PARALLEL_THRESHOLD = 4096; // adjust as needed\n\n    if (x.size() > PARALLEL_THRESHOLD || y.size() > PARALLEL_THRESHOLD) {\n        #pragma omp parallel sections\n        {\n#pragma omp section\n            {\n                const int *p = x.data();\n                const int *const end = p + x.size();\n                for (; p != end; ++p) {\n                    if (*p == val) {\n                        foundX = true;\n                        break;\n                    }\n                }\n            }\n#pragma omp section\n            {\n                const int *p = y.data();\n                const int *const end = p + y.size();\n                for (; p != end; ++p) {\n                    if (*p == val) {\n                        foundY = true;\n                        break;\n                    }\n                }\n            }\n        }\n    } else {\n        for (int v : x) {\n            if (v == val) {\n                foundX = true;\n                break;\n            }\n        }\n        for (int v : y) {\n            if (v == val) {\n                foundY = true;\n                break;\n            }\n        }\n    }\n\n    return foundX ^ foundY;\n}\n```",
  "50_stencil_xor_kernel": "```cpp\n#pragma once\n#include <vector>\n\nvoid cellsXOR(std::vector<int> const& input, std::vector<int> &output, size_t N) {\n    const int *inp = input.data();\n    int *out = output.data();\n\n    #pragma omp parallel for schedule(static)\n    for (size_t i = 0; i < N; ++i) {\n        const int *prev = (i > 0) ? inp + (i - 1) * N : nullptr;\n        const int *next = (i + 1 < N) ? inp + (i + 1) * N : nullptr;\n        const int *cur  = inp + i * N;\n        int *outRow      = out + i * N;\n\n        #pragma omp simd\n        for (size_t j = 0; j < N; ++j) {\n            int cnt = 0;\n            if (prev && prev[j] == 1) ++cnt;\n            if (next && next[j] == 1) ++cnt;\n            if (j > 0 && cur[j - 1] == 1) ++cnt;\n            if (j + 1 < N && cur[j + 1] == 1) ++cnt;\n            outRow[j] = (cnt == 1) ? 1 : 0;\n        }\n    }\n}\n```",
  "36_search_check_if_array_contains_value": "```cpp\n#pragma once\n#include <vector>\n#include <cstddef>\n#include <omp.h>\n\n/* Return true if the vector x contains the value `target`. Return false otherwise.\n   Examples:\n\n   input: x=[1, 8, 2, 6, 4, 6], target=3\n   output: false\n   input: x=[1, 8, 2, 6, 4, 6], target=8\n   output: true\n*/\ninline bool contains(std::vector<int> const& x, int target) {\n    const size_t n = x.size();\n    if (n == 0) return false;\n    const int* data = x.data();\n\n    // Threshold to decide when parallelization gives advantage (tuned for 8 threads)\n    constexpr size_t parallelThreshold = 65536;\n\n    if (n <= parallelThreshold) {\n        // Small vectors \u2013 single\u2011threaded search is faster.\n        for (size_t i = 0; i < n; ++i) {\n            if (data[i] == target) return true;\n        }\n        return false;\n    }\n\n    bool found = false;\n#pragma omp parallel for reduction(||:found) schedule(static)\n    for (size_t i = 0; i < n; ++i) {\n        if (data[i] == target) found = true;\n    }\n    return found;\n}\n```",
  "38_search_find_the_first_even_number": "```cpp\n#pragma once\n#include <vector>\n\ninline size_t findFirstEven(std::vector<int> const& x) {\n    const size_t n = x.size();\n    size_t idx = n;                     // sentinel for \u201cnot found\u201d\n#pragma omp parallel for schedule(static) reduction(min:idx)\n    for (size_t i = 0; i < n; ++i) {\n        if ((x[i] & 1) == 0) {          // even number found\n            idx = (i < idx) ? i : idx; // keep the first (smallest) index\n        }\n    }\n    return idx;\n}\n```",
  "35_search_search_for_last_struct_by_key": "```cpp\n#pragma once\n#include <vector>\n#include <string>\n#include <cstdint>\n\nstruct Book {\n    std::string title;\n    int pages;\n};\n\ninline size_t findLastShortBook(const std::vector<Book>& books) {\n    const size_t n = books.size();\n    int64_t lastIdx = -1;                       // sentinel for \u201cnot found\u201d\n\n    #pragma omp parallel for reduction(max:lastIdx)\n    for (size_t i = 0; i < n; ++i) {\n        if (books[i].pages < 100) {\n            lastIdx = static_cast<int64_t>(i);\n        }\n    }\n\n    return (lastIdx >= 0) ? static_cast<size_t>(lastIdx) : n;\n}\n```\n",
  "40_sort_sort_an_array_of_complex_numbers_by_magnitude": "```cpp\n#pragma once\n#include <complex>\n#include <vector>\n#include <algorithm>\n#include <numeric>\n\n#ifdef _OPENMP\n#include <omp.h>\n#endif\n\ninline void sortComplexByMagnitude(std::vector<std::complex<double>>& x) {\n    const std::size_t n = x.size();\n    if (n < 2) return;\n\n    std::vector<double> sqmag(n);\n    #pragma omp parallel for if(n > 1024) schedule(static)\n    for (std::size_t i = 0; i < n; ++i) {\n        const double r = x[i].real();\n        const double i = x[i].imag();\n        sqmag[i] = r * r + i * i;\n    }\n\n    std::vector<std::size_t> idx(n);\n    std::iota(idx.begin(), idx.end(), 0);\n    std::sort(idx.begin(), idx.end(),\n              [&](std::size_t a, std::size_t b){ return sqmag[a] < sqmag[b]; });\n\n    std::vector<std::complex<double>> tmp;\n    tmp.reserve(n);\n    for (std::size_t i = 0; i < n; ++i) {\n        tmp.push_back(std::move(x[idx[i]]));\n    }\n    x.swap(tmp);\n}\n```",
  "37_search_find_the_closest_number_to_pi": "```cpp\n#pragma once\n#include <vector>\n#include <cmath>\n#include <limits>\n#include <omp.h>\n\ninline size_t findClosestToPi(std::vector<double> const& x) {\n    const double pi = M_PI;\n    const size_t n = x.size();\n\n    // Base case using the first element\n    size_t bestIdx  = 0;\n    double bestDiff = std::fabs(x[0] - pi);\n\n    // For very small vectors, avoid the overhead of creating threads\n    constexpr size_t PAR_THRESHOLD = 8192;\n\n    if (n <= PAR_THRESHOLD) {\n        for (size_t i = 1; i < n; ++i) {\n            double diff = std::fabs(x[i] - pi);\n            if (diff < bestDiff) {\n                bestDiff = diff;\n                bestIdx = i;\n            }\n        }\n        return bestIdx;\n    }\n\n#pragma omp parallel\n    {\n        size_t localIdx = 0;\n        double localDiff = std::numeric_limits<double>::infinity();\n\n        #pragma omp for\n        for (size_t i = 1; i < n; ++i) {\n            double d = std::fabs(x[i] - pi);\n            if (d < localDiff) {\n                localDiff = d;\n                localIdx = i;\n            }\n        }\n\n#pragma omp critical\n        {\n            if (localDiff < bestDiff || (localDiff == bestDiff && localIdx < bestIdx)) {\n                bestDiff = localDiff;\n                bestIdx = localIdx;\n            }\n        }\n    }\n\n    return bestIdx;\n}\n```\n",
  "10_geometry_convex_hull": "```cpp\n#pragma once\n#include <vector>\n#include <algorithm>\n#include <cstddef>\n\nstruct Point {\n    double x, y;\n};\n\ninline double cross(const Point& a, const Point& b, const Point& c) {\n    return (b.x - a.x) * (c.y - a.y) - (b.y - a.y) * (c.x - a.x);\n}\n\nvoid convexHull(const std::vector<Point>& points, std::vector<Point>& hull) {\n    const std::size_t n = points.size();\n    if (n < 3) {\n        hull = points;\n        return;\n    }\n\n    // Indices sorted by point coordinates\n    std::vector<std::size_t> idx(n);\n    for (std::size_t i = 0; i < n; ++i) idx[i] = i;\n\n    std::sort(idx.begin(), idx.end(), [&](std::size_t ia, std::size_t ib) {\n        const Point& a = points[ia];\n        const Point& b = points[ib];\n        return (a.x < b.x) || (a.x == b.x && a.y < b.y);\n    });\n\n    // Temporary array for building the hull\n    std::vector<Point> tmp(2 * n);\n    std::size_t k = 0;\n\n    // Build lower hull\n    for (std::size_t i = 0; i < n; ++i) {\n        const Point& p = points[idx[i]];\n        while (k >= 2 && cross(tmp[k - 2], tmp[k - 1], p) <= 0.0) {\n            --k;\n        }\n        tmp[k++] = p;\n    }\n\n    std::size_t t = k + 1;\n    // Build upper hull (excluding the last point of the lower hull)\n    for (std::size_t i = n - 1; i > 0; --i) {\n        const Point& p = points[idx[i - 1]];\n        while (k >= t && cross(tmp[k - 2], tmp[k - 1], p) <= 0.0) {\n            --k;\n        }\n        tmp[k++] = p;\n    }\n\n    // Move result to outputvector and trim unused capacity\n    hull.swap(tmp);\n    hull.resize(k - 1);\n}\n```",
  "09_fft_fft_out_of_place": "```cpp\n#pragma once\n#include <vector>\n#include <cmath>\n#include <complex>\n\n#if !defined(M_PI)\n# define M_PI 3.14159265358979323846\n#endif\n\nvoid fft(std::vector<std::complex<double>> const& x,\n         std::vector<std::complex<double>> &output)\n{\n    output = x;\n    const unsigned int N = static_cast<unsigned int>(output.size());\n    if (N <= 1) return;                 // trivial case\n\n    unsigned int k = N;\n    double thetaT = 3.14159265358979323846264338328L / N;\n    std::complex<double> phiT(std::cos(thetaT), -std::sin(thetaT));\n\n    // Twiddle table (up to N/2 entries)\n    std::vector<std::complex<double>> Twiddle;\n    Twiddle.resize(N >> 1);\n\n    while (k > 1) {\n        unsigned int n = k;      // segment size before halving\n        k >>= 1;                 // half segment size for next stage\n        phiT = phiT * phiT;       // advance twiddle base\n\n        // Pre\u2011compute twiddle factors for this stage\n        Twiddle[0] = std::complex<double>(1.0, 0.0);\n        for (unsigned int l = 1; l < k; ++l)\n            Twiddle[l] = Twiddle[l - 1] * phiT;\n\n        // Parallel butterfly computation\n        #pragma omp parallel for schedule(static)\n        for (unsigned int l = 0; l < k; ++l) {\n            const std::complex<double> T = Twiddle[l];\n            for (unsigned int a = l; a < N; a += n) {\n                const unsigned int b = a + k;\n                const std::complex<double> t = output[a] - output[b];\n                output[a] += output[b];\n                output[b] = t * T;\n            }\n        }\n    }\n\n    // Bit\u2011reversal reordering\n    unsigned int m = 0;\n    unsigned int tmp = N;\n    while ((tmp >>= 1)) ++m;      // floor(log2(N))\n    for (unsigned int a = 0; a < N; ++a) {\n        unsigned int b = a;\n        b = (((b & 0xaaaaaaaau) >> 1) | ((b & 0x55555555u) << 1));\n        b = (((b & 0xccccccccu) >> 2) | ((b & 0x33333333u) << 2));\n        b = (((b & 0xf0f0f0f0u) >> 4) | ((b & 0x0f0f0f0fu) << 4));\n        b = (((b & 0xff00ff00u) >> 8) | ((b & 0x00ff00ffu) << 8));\n        b = ((b >> 16) | (b << 16)) >> (32u - m);\n        if (b > a) {\n            const std::complex<double> t = output[a];\n            output[a] = output[b];\n            output[b] = t;\n        }\n    }\n}\n```",
  "43_sort_sort_an_array_of_structs_by_key": "```cpp\n#pragma once\n#include <vector>\n#include <algorithm>\n#include <cstddef>\n#include <queue>\n#include <omp.h>\n\nstruct Result {\n    int startTime;\n    int duration;\n    float value;\n};\n\nstatic inline bool cmpStart(const Result& a, const Result& b) {\n    return a.startTime < b.startTime;\n}\n\nstatic constexpr std::size_t SERIAL_THRESHOLD = 16 * 1024;  // 16 KiB\n\ninline void sortByStartTime(std::vector<Result>& results) {\n    const std::size_t n = results.size();\n    if (n <= 1) return;\n\n    const std::size_t numThreads = omp_get_max_threads();\n    if (numThreads <= 1 || n <= SERIAL_THRESHOLD) {\n        std::sort(results.begin(), results.end(), cmpStart);\n        return;\n    }\n\n    // Divide the vector into chunks for each thread\n    std::size_t chunkSize = (n + numThreads - 1) / numThreads;  // ceil division\n    std::vector<std::pair<std::size_t, std::size_t>> segments;\n    segments.reserve(numThreads);\n    for (std::size_t i = 0; i < numThreads; ++i) {\n        std::size_t start = i * chunkSize;\n        if (start >= n) break;\n        std::size_t end = std::min(start + chunkSize, n);\n        segments.emplace_back(start, end);\n    }\n\n    // Parallel sorting of each segment\n    #pragma omp parallel\n    {\n        const std::size_t tid = omp_get_thread_num();\n        if (tid < segments.size()) {\n            const std::size_t start = segments[tid].first;\n            const std::size_t end   = segments[tid].second;\n            std::sort(results.begin() + start, results.begin() + end, cmpStart);\n        }\n    }\n\n    // K\u2011way merge of the sorted segments\n    std::vector<Result> buffer(n);\n\n    struct Node {\n        const Result* cur;\n        const Result* end;\n    };\n    struct NodeCmp {\n        bool operator()(const Node& a, const Node& b) const {\n            return a.cur->startTime > b.cur->startTime;  // min\u2011heap\n        }\n    };\n    std::priority_queue<Node, std::vector<Node>, NodeCmp> pq;\n\n    for (const auto& seg : segments) {\n        const std::size_t start = seg.first;\n        const std::size_t end   = seg.second;\n        if (start < end)\n            pq.push({&results[start], &results[end]});\n    }\n\n    std::size_t outIdx = 0;\n    while (!pq.empty()) {\n        const Node cur = pq.top();\n        pq.pop();\n        buffer[outIdx++] = *cur.cur;\n        const Result* next = cur.cur + 1;\n        if (next != cur.end)\n            pq.push({next, cur.end});\n    }\n\n    results.swap(buffer);\n}\n```\n",
  "54_stencil_game_of_life": "```cpp\n#pragma once\n#include <vector>\n#include <cstddef>\n#ifdef _OPENMP\n#include <omp.h>\n#endif\n\n// Simulate one generation of Conway's Game of Life.\n// `input` is the current state, `output` will receive the next state.\n// Both grids are stored in row\u2011major order as a flat vector of 0/1 integers.\nvoid gameOfLife(std::vector<int> const& input, std::vector<int> &output, size_t N) {\n    const int* in  = input.data();   // read\u2011only data\n    int*      out = output.data();   // write\u2011only data\n    const size_t n = N;\n\n    #pragma omp parallel\n    {\n        /* --------------------------------------------------------------------\n           Interior cells (except first & last row and column)\n           -------------------------------------------------------------------- */\n        if (n >= 3) {\n            #pragma omp for schedule(static) nowait\n            for (size_t i = 1; i < n - 1; ++i) {\n                const int *rowPrev = in + (i - 1) * n;\n                const int *rowCurr = in +  i     * n;\n                const int *rowNext = in + (i + 1) * n;\n                for (size_t j = 1; j < n - 1; ++j) {\n                    size_t idx  = i * n + j;\n                    int   curr  = rowCurr[j];\n                    int   sum   = rowPrev[j - 1] + rowPrev[j] + rowPrev[j + 1]\n                                + rowCurr[j - 1] + rowCurr[j + 1]\n                                + rowNext[j - 1] + rowNext[j] + rowNext[j + 1];\n                    out[idx] = (curr && (sum == 2 || sum == 3)) || (!curr && sum == 3);\n                }\n            }\n        }\n\n        /* --------------------------------------------------------------------\n           Top row (i == 0)\n           -------------------------------------------------------------------- */\n        if (n >= 1) {\n            const int *bottom = in + n;          // row 1\n            #pragma omp for schedule(static) nowait\n            for (size_t j = 0; j < n; ++j) {\n                size_t idx  = j;\n                int   sum   = 0;\n                if (n > 1) {\n                    sum += bottom[j];\n                    if (j > 0)   sum += bottom[j - 1];\n                    if (j + 1 < n) sum += bottom[j + 1];\n                }\n                if (j + 1 < n) sum += in[j + 1];\n                if (j > 0)     sum += in[j - 1];\n                int   curr = in[j];\n                out[idx]   = (curr && (sum == 2 || sum == 3)) || (!curr && sum == 3);\n            }\n        }\n\n        /* --------------------------------------------------------------------\n           Bottom row (i == n-1)\n           -------------------------------------------------------------------- */\n        if (n > 1) {\n            const int *top      = in;                 // row 0\n            const int *bottomRow = in + (n - 1) * n; // last row\n            #pragma omp for schedule(static) nowait\n            for (size_t j = 0; j < n; ++j) {\n                size_t idx  = (n - 1) * n + j;\n                int   sum   = 0;\n                sum += top[j];\n                if (j > 0)         sum += top[j - 1];\n                if (j + 1 < n)     sum += top[j + 1];\n                if (j + 1 < n)     sum += bottomRow[j + 1];\n                if (j > 0)         sum += bottomRow[j - 1];\n                int   curr = bottomRow[j];\n                out[idx]   = (curr && (sum == 2 || sum == 3)) || (!curr && sum == 3);\n            }\n        }\n\n        /* --------------------------------------------------------------------\n           Left column (j == 0, excluding corners)\n           -------------------------------------------------------------------- */\n        if (n > 2) {\n            #pragma omp for schedule(static) nowait\n            for (size_t i = 1; i < n - 1; ++i) {\n                size_t idx  = i * n;\n                const int *rowPrev = in + (i - 1) * n;\n                const int *rowCurr = in +  i      * n;\n                const int *rowNext = in + (i + 1) * n;\n                int   sum   = rowPrev[0] + rowPrev[1] + rowCurr[1]\n                            + rowNext[0] + rowNext[1];\n                int   curr = rowCurr[0];\n                out[idx]   = (curr && (sum == 2 || sum == 3)) || (!curr && sum == 3);\n            }\n        }\n\n        /* --------------------------------------------------------------------\n           Right column (j == n-1, excluding corners)\n           -------------------------------------------------------------------- */\n        if (n > 2) {\n            #pragma omp for schedule(static) nowait\n            for (size_t i = 1; i < n - 1; ++i) {\n                size_t idx  = i * n + (n - 1);\n                const int *rowPrev = in + (i - 1) * n;\n                const int *rowCurr = in +  i      * n;\n                const int *rowNext = in + (i + 1) * n;\n                int   sum   = rowPrev[n - 2] + rowPrev[n - 1]\n                            + rowCurr[n - 2] + rowNext[n - 2] + rowNext[n - 1];\n                int   curr = rowCurr[n - 1];\n                out[idx]   = (curr && (sum == 2 || sum == 3)) || (!curr && sum == 3);\n            }\n        }\n        /* implicit barrier at the exit of the parallel region */\n    }\n}\n```",
  "08_fft_split_fft": "```cpp\n#pragma once\n#include <vector>\n#include <complex>\n#include <cmath>\n#include <omp.h>\n\n/* FFT implementation:\n   - Uses the same algorithmic steps as the original code.\n   - Parallelises the innermost butterfly loop, the bit reversal step\n     and the final copy to the output vectors.\n   - Keeps the exact numerical behaviour of the original code. */\ninline void fft(std::vector<std::complex<double>> const& x,\n                std::vector<double>& r, std::vector<double>& i)\n{\n    const unsigned int N = static_cast<unsigned int>(x.size());\n    if (N == 0) return;\n\n    /* ------------------------------------------------------------------\n       Step 1: Copy input to a local working array\n       ------------------------------------------------------------------ */\n    std::vector<std::complex<double>> data(x);\n\n    /* ------------------------------------------------------------------\n       Step 2: Initialise twiddle\u2011factor root of unity\n       ------------------------------------------------------------------ */\n    const double PI = std::acos(-1.0);\n    const double thetaT = PI / static_cast<double>(N);\n    std::complex<double> phiT(std::cos(thetaT), -std::sin(thetaT));\n\n    /* ------------------------------------------------------------------\n       Step 3: In\u2011place decimation\u2011in\u2011frequency butterfly stages\n       ------------------------------------------------------------------ */\n    unsigned int k = N;\n    std::vector<std::complex<double>> T_vec(N / 2 + 1);   // twiddle factors\n\n    while (k > 1)\n    {\n        unsigned int n = k;           // old stride\n        k >>= 1;                      // new stride\n        phiT *= phiT;                  // squared root for next stage\n\n        // Pre\u2011compute T_vec[l] = phiT^l  for l = 0 .. k-1\n        T_vec[0] = std::complex<double>(1.0, 0.0);\n        for (unsigned int l = 1; l < k; ++l)\n            T_vec[l] = T_vec[l-1] * phiT;\n\n        // Bitmask to obtain l = a mod (2k)  (2k is always a power\u2011of\u20112)\n        const unsigned int mask = (2u * k) - 1;\n\n        #pragma omp parallel for schedule(static)\n        for (unsigned int a = 0; a < N; ++a)\n        {\n            const unsigned int l = a & mask;\n            if (l < k)                          // only the first k lanes\n            {\n                const unsigned int b = a + k;\n                const std::complex<double> t = data[a] - data[b];\n                data[a] += data[b];\n                data[b]  = t * T_vec[l];\n            }\n        }\n    }\n\n    /* ------------------------------------------------------------------\n       Step 4: Bit\u2011reversal permutation\n       ------------------------------------------------------------------ */\n    const unsigned int m = static_cast<unsigned int>(std::log2(static_cast<double>(N)));\n\n    #pragma omp parallel for schedule(static)\n    for (unsigned int a = 0; a < N; ++a)\n    {\n        unsigned int b = a;\n        b = (((b & 0xaaaaaaaau) >> 1) | ((b & 0x55555555u) << 1));\n        b = (((b & 0xccccccccu) >> 2) | ((b & 0x33333333u) << 2));\n        b = (((b & 0xf0f0f0f0u) >> 4) | ((b & 0x0f0f0f0fu) << 4));\n        b = (((b & 0xff00ff00u) >> 8) | ((b & 0x00ff00ffu) << 8));\n        b = ((b >> 16) | (b << 16)) >> (32 - m);\n        if (b > a)\n        {\n            const std::complex<double> t = data[a];\n            data[a] = data[b];\n            data[b] = t;\n        }\n    }\n\n    /* ------------------------------------------------------------------\n       Step 5: Copy real and imaginary parts to output vectors\n       ------------------------------------------------------------------ */\n    #pragma omp parallel for schedule(static)\n    for (unsigned int j = 0; j < N; ++j)\n    {\n        r[j] = data[j].real();\n        i[j] = data[j].imag();\n    }\n}\n```",
  "44_sort_sort_non-zero_elements": "```cpp\n#pragma once\n#include <vector>\n#include <algorithm>\n#include <omp.h>\n#if defined(__has_include) && __has_include(<execution>)\n#include <execution>\n#define HAVE_PARALLEL_EXEC 1\n#else\n#define HAVE_PARALLEL_EXEC 0\n#endif\n\nvoid sortIgnoreZero(std::vector<int> &x) {\n    const size_t n = x.size();\n    if (n == 0) return;\n\n    // Count non\u2011zero elements per thread\n    const int maxThreads = omp_get_max_threads();\n    std::vector<size_t> threadCounts(maxThreads, 0);\n\n    #pragma omp parallel\n    {\n        const int tid    = omp_get_thread_num();\n        const int numThr = omp_get_num_threads();\n        const size_t chunk = (n + numThr - 1) / numThr;\n        const size_t start = static_cast<size_t>(tid) * chunk;\n        size_t end = start + chunk;\n        if (end > n) end = n;\n\n        size_t localCount = 0;\n        for (size_t i = start; i < end; ++i) {\n            if (x[i] != 0) ++localCount;\n        }\n        threadCounts[tid] = localCount;\n    }\n\n    // Compute offsets and total number of non\u2011zero values\n    std::vector<size_t> offsets(maxThreads, 0);\n    size_t totalNonZero = 0;\n    for (int i = 0; i < maxThreads; ++i) {\n        offsets[i] = totalNonZero;\n        totalNonZero += threadCounts[i];\n    }\n\n    if (totalNonZero == 0) return;\n\n    // Gather non\u2011zero elements into a contiguous array\n    std::vector<int> nzElements(totalNonZero);\n\n    #pragma omp parallel\n    {\n        const int tid    = omp_get_thread_num();\n        const int numThr = omp_get_num_threads();\n        const size_t chunk = (n + numThr - 1) / numThr;\n        const size_t start = static_cast<size_t>(tid) * chunk;\n        size_t end = start + chunk;\n        if (end > n) end = n;\n\n        size_t offset = offsets[tid];\n        for (size_t i = start; i < end; ++i) {\n            if (x[i] != 0) {\n                nzElements[offset++] = x[i];\n            }\n        }\n    }\n\n    // Sort the collected non\u2011zero values (possibly in parallel)\n#if HAVE_PARALLEL_EXEC\n    std::sort(std::execution::par, nzElements.begin(), nzElements.end());\n#else\n    std::sort(nzElements.begin(), nzElements.end());\n#endif\n\n    // Write sorted values back into the original vector\n    #pragma omp parallel\n    {\n        const int tid    = omp_get_thread_num();\n        const int numThr = omp_get_num_threads();\n        const size_t chunk = (n + numThr - 1) / numThr;\n        const size_t start = static_cast<size_t>(tid) * chunk;\n        size_t end = start + chunk;\n        if (end > n) end = n;\n\n        size_t offset = offsets[tid];\n        for (size_t i = start; i < end; ++i) {\n            if (x[i] != 0) {\n                x[i] = nzElements[offset++];\n            }\n        }\n    }\n}\n```"
}