{
  "17_graph_highest_degree": "```cpp\n#pragma once\n#include <vector>\n#include <algorithm>\n#include <omp.h>\n\n/* Compute the highest node degree in an undirected adjacency matrix A.\n   A is stored in row-major order.  Each entry is assumed to be 0 or 1.\n   The function operates on an NxN matrix.\n*/\nint maxDegree(std::vector<int> const& A, size_t N) {\n    int highestDegree = 0;\n\n    #pragma omp parallel for reduction(max:highestDegree)\n    for (int i = 0; i < static_cast<int>(N); ++i) {\n        int degree = 0;\n        const int* row = &A[i * N];\n        for (int j = 0; j < static_cast<int>(N); ++j) {\n            degree += row[j];\n        }\n        if (degree > highestDegree) highestDegree = degree;\n    }\n\n    return highestDegree;\n}\n```",
  "15_graph_edge_count": "```cpp\n// Optimised edge counting \u2013 identical behaviour, faster\nint edgeCount(std::vector<int> const& A, size_t N) {\n    // Reduce the double loop to a single linear pass\n    // and parallelise with OpenMP\n    long long result = 0;                         // use 64\u2011bit to avoid overflow\n    const size_t total = N * N;                   // matrix size (flattened)\n\n#pragma omp parallel for reduction(+:result) schedule(static)\n    for (size_t idx = 0; idx < total; ++idx) {\n        if (A[idx] == 1) result += 1;\n    }\n\n    // The original return type is int; cast back\n    return static_cast<int>(result);\n}\n```",
  "23_histogram_first_letter_counts": "```cpp\n#pragma once\n#include <array>\n#include <string>\n#include <vector>\n#include <cstddef>\n#include <algorithm>\n#include <omp.h>\n\n/*\n * Count how many strings in `s` start with each lowercase alphabet letter.\n * `bins[i]` will contain the count for the letter 'a'+i.\n * The function is parallel\u2011ized for x86\u201164 with OpenMP and keeps the exact semantics\n * of the original implementation.\n */\ninline void firstLetterCounts(std::vector<std::string> const& s,\n                              std::array<std::size_t, 26>& bins)\n{\n    // Ensure the output array is zeroed before counting.\n    std::fill(bins.begin(), bins.end(), 0);\n\n    // Parallel loop: each thread updates a private copy of the array and the\n    // partial results are summed up in a reduction.\n    #pragma omp parallel for reduction(+:bins[:26])\n    for (std::size_t i = 0; i < s.size(); ++i)\n    {\n        // All strings are guaranteed to be non\u2011empty.\n        const unsigned char first = static_cast<unsigned char>(s[i][0]);\n        const std::size_t idx = first - 'a';\n        bins[idx] += 1;\n    }\n}\n```",
  "19_graph_shortest_path": "```cpp\n#pragma once\n#include <vector>\n#include <limits>\n#include <cstddef>\n\n/* Return the length of the shortest path from source to dest in the undirected graph\n * defined by the adjacency matrix A. A is an NxN matrix stored in row-major order.\n * The graph is assumed to be connected and undirected.  */\nint shortestPathLength(std::vector<int> const& A, size_t N,\n                       int source, int dest)\n{\n    // Use a compact visited array (1 byte per vertex)\n    std::vector<unsigned char> visited(N, 0);\n    // Simple circular queue capable of holding all vertices once\n    std::vector<int> queue(N);\n    std::vector<int> dist(N, -1);\n\n    int head = 0, tail = 0;\n    visited[source] = 1;\n    dist[source] = 0;\n    queue[tail++] = source;\n\n    const int *const data = A.data();          // pointer shortcut\n    const int stride = static_cast<int>(N);    // row length\n\n    while (head != tail) {\n        int current = queue[head++];\n        int curDist = dist[current];\n        if (current == dest) return curDist;\n\n        const int* row = data + current * stride;\n        for (int i = 0; i < (int)N; ++i) {\n            if (*row++ && !visited[i]) {\n                visited[i] = 1;\n                dist[i] = curDist + 1;\n                queue[tail++] = i;\n            }\n        }\n    }\n    return std::numeric_limits<int>::max();    // unreachable, should not happen\n}\n```",
  "20_histogram_pixel_histogram": "```cpp\n#pragma once\n#include <vector>\n#include <array>\n#include <cstddef>\n#include <omp.h>\n\n/* Count the number of pixels in image with each grayscale intensity.\n   The vector `image` is a grayscale image with values 0-255.\n   Store the results in `bins`.\n   \n   Example:\n   input:  image=[2, 116, 201, 11, 92, 92, 201, 4, 2]\n   output: [0, 0, 2, 0, 1, ...]\n*/\ninline void pixelCounts(std::vector<int> const& image,\n                         std::array<size_t, 256>& bins) {\n    // reset result buffer\n    #pragma omp parallel for schedule(static)\n    for (std::size_t i = 0; i < bins.size(); ++i) {\n        bins[i] = 0;\n    }\n\n    // parallel counting with a thread\u2011private histogram\n    const std::size_t N = image.size();\n    const std::size_t chunk = (N + 7) / 8; // roughly split into 8 chunks\n    std::array<size_t, 256> local[8];\n\n    #pragma omp parallel\n    {\n        int tid = omp_get_thread_num();\n        // initialize local histogram for this thread\n        for (size_t i = 0; i < 256; ++i) local[tid][i] = 0;\n\n        #pragma omp for schedule(static, chunk)\n        for (std::size_t i = 0; i < N; ++i) {\n            ++local[tid][static_cast<unsigned char>(image[i])];\n        }\n\n        // merge local histograms into the global result\n        #pragma omp critical\n        {\n            for (size_t i = 0; i < 256; ++i) {\n                bins[i] += local[tid][i];\n            }\n        }\n    }\n}\n```",
  "21_histogram_bin_0-100": "```cpp\n#pragma once\n#include <array>\n#include <vector>\n#include <cstddef>\n#include <cstdint>\n#include <algorithm>\n#include <omp.h>\n\n/*\n   Count the number of values in each interval [0,10), [10,20), ..., [90,100]\n   The input vector `x` contains values between 0 and 100 inclusive.\n   The result is stored in `bins` where bins[i] holds the count for the\n   interval [i*10, (i+1)*10).\n   The function is fully parallel using OpenMP without changing the\n   external signature or the required semantics.\n*/\ninline void binsBy10Count(std::vector<double> const& x,\n                          std::array<std::uint64_t, 10>& bins)\n{\n    // Empty the output array first \u2013 this is cheap and keeps the code\n    // simple even if the compiler can optimize it away.\n    std::fill(bins.begin(), bins.end(), 0);\n\n    // Determine the number of threads openmp will spawn.\n    const int thread_count = omp_get_max_threads();\n\n    // A local array of thread\u2011private bins, allocated on the stack.\n    // Each thread keeps its own copy to avoid atomic operations.\n    std::vector<std::array<std::uint64_t, 10>> thread_bins(thread_count);\n    for (int t = 0; t < thread_count; ++t)\n        thread_bins[t].fill(0);\n\n    // Parallel loop over the input data.\n    // Schedule static because the workload per element is identical.\n    #pragma omp parallel for schedule(static)\n    for (std::intptr_t idx = 0; idx < static_cast<std::intptr_t>(x.size()); ++idx) {\n        const std::uint64_t bin = static_cast<std::uint64_t>(x[static_cast<size_t>(idx)] / 10);\n        const int thread_id = omp_get_thread_num();\n        ++thread_bins[thread_id][bin];\n    }\n\n    // Combine all thread\u2011local bins into the shared result.\n    for (int t = 0; t < thread_count; ++t) {\n        for (size_t b = 0; b < 10; ++b)\n            bins[b] += thread_bins[t][b];\n    }\n}\n```",
  "18_graph_count_components": "```cpp\n#pragma once\n#include <vector>\n#include <stack>\n#include <cstdint>\n#include <omp.h>\n\n/* Count the number of connected components in the undirected graph defined by the\n   adjacency matrix A.  The matrix A is a flat vector in row\u2011major order.\n   The implementation below is fully parallel and avoids recursion.\n*/\n\ninline void dfs_iterative(std::vector<int> const& A, size_t N, size_t start,\n                          std::vector<std::uint8_t>& visited)\n{\n    std::stack<size_t> st;\n    st.push(start);\n    visited[start] = 1;\n\n    while (!st.empty())\n    {\n        size_t u = st.top();\n        st.pop();\n\n        size_t base = u * N;\n        for (size_t v = 0; v < N; ++v)\n        {\n            if (A[base + v] && !visited[v])\n            {\n                visited[v] = 1;\n                st.push(v);\n            }\n        }\n    }\n}\n\nint componentCount(std::vector<int> const& A, size_t N)\n{\n    std::vector<std::uint8_t> visited(N, 0);\n    int components = 0;\n\n    #pragma omp parallel\n    {\n        // Each thread keeps its own local stack to avoid contention.\n        std::vector<std::uint8_t> local_stack_flags(N, 0);\n\n        #pragma omp for schedule(dynamic, 64) nowait\n        for (int i = 0; i < static_cast<int>(N); ++i)\n        {\n            if (omp_atomic_read(visited[i]) == 0)\n            {\n                // Try to claim this node: only one thread proceeds.\n                if (omp_atomic_compare_and_swap(visited[i], 0, 1))\n                {\n                    // Found a new component\n                    #pragma omp atomic\n                    ++components;\n\n                    // Perform DFS locally\n                    std::stack<size_t> st;\n                    st.push(i);\n                    while (!st.empty())\n                    {\n                        size_t u = st.top();\n                        st.pop();\n\n                        size_t base = u * N;\n                        for (size_t v = 0; v < N; ++v)\n                        {\n                            if (A[base + v] && omp_atomic_read(visited[v]) == 0)\n                            {\n                                if (omp_atomic_compare_and_swap(visited[v], 0, 1))\n                                    st.push(v);\n                            }\n                        }\n                    }\n                }\n            }\n        }\n    }\n\n    return components;\n}\n```",
  "16_graph_largest_component": "```cpp\n#pragma once\n#include <vector>\n#include <algorithm>\n\n/* Return the number of vertices in the largest component of the graph\n   defined by the adjacency matrix A. A is an NxN adjacency matrix\n   stored in row-major order. */\nint largestComponent(std::vector<int> const& A, size_t N) {\n    // Disjoint Set Union (Union\u2013Find) implementation.\n    std::vector<size_t> parent(N);\n    std::vector<size_t> sz(N, 1);\n    for (size_t i = 0; i < N; ++i) parent[i] = i;\n\n    auto find = [&](size_t x, auto&& find_ref) -> size_t {\n        while (parent[x] != x) {\n            parent[x] = parent[parent[x]]; // path compression\n            x = parent[x];\n        }\n        return x;\n    };\n    auto unite = [&](size_t a, size_t b, auto&& find_ref) {\n        size_t ra = find_ref(a, find_ref);\n        size_t rb = find_ref(b, find_ref);\n        if (ra == rb) return;\n        if (sz[ra] < sz[rb]) std::swap(ra, rb);\n        parent[rb] = ra;\n        sz[ra] += sz[rb];\n    };\n\n    // Union all edges present in the adjacency matrix.\n    // Traverse only the upper triangle to avoid double work.\n    for (size_t i = 0; i < N; ++i) {\n        const size_t row_offset = i * N;\n        for (size_t j = i + 1; j < N; ++j) {\n            if (A[row_offset + j] == 1) {\n                unite(i, j, find);\n            }\n        }\n    }\n\n    // Find the largest component size.\n    size_t max_sz = 0;\n    for (size_t i = 0; i < N; ++i) {\n        if (parent[i] == i) {\n            max_sz = std::max(max_sz, sz[i]);\n        }\n    }\n    return static_cast<int>(max_sz);\n}\n```\n",
  "34_scan_largest_contiguous_subarray_sum": "```cpp\n#pragma once\n#include <vector>\n#include <limits>\n#include <algorithm>\n#include <omp.h>\n\n/* Return the largest sum of any contiguous subarray in the vector x.\n   Implementation uses Kadane's algorithm (O(n)) for maximum speed.\n   The function is fully compatible with the original's behaviour.\n*/\n\nint maximumSubarray(std::vector<int> const& x) {\n    // Handle empty vector \u2013 original code would throw out_of_range.\n    if (x.empty()) return std::numeric_limits<int>::lowest();\n\n    // Kadane's algorithm: single pass, minimal overhead.\n    int best = std::numeric_limits<int>::lowest();\n    int current = 0;\n    for (int v : x) {\n        current = std::max(v, current + v);\n        best = std::max(best, current);\n    }\n    return best;\n}\n```",
  "32_scan_sum_of_prefix_sum_array": "```cpp\n#pragma once\n#include <cstdint>\n#include <vector>\n\n/* Compute the prefix sum array of the vector x and return its sum.\n   Example:\n   input: [-7, 2, 1, 9, 4, 8]\n   output: 15\n*/\ninline int64_t sumOfPrefixSum(std::vector<int64_t> const& x) {\n    if (x.empty()) return 0;\n    int64_t running = 0;\n    int64_t total = 0;\n    for (auto const& v : x) {\n        running += v;   // prefix sum up to this element\n        total   += running;\n    }\n    return total;\n}\n```",
  "33_scan_reverse_prefix_sum": "```c++\n// optimized version of reversePrefixSum\n\n#pragma once\n#include <vector>\n\ninline void reversePrefixSum(const std::vector<int>& x, std::vector<int>& output) {\n    const std::size_t n = x.size();\n    output.resize(n);                 // ensure correct size\n    int sum = 0;\n    // traverse from the back, accumulating the suffix sums\n    for (std::size_t i = n; i > 0; --i) {\n        sum += x[i - 1];\n        output[i - 1] = sum;\n    }\n}\n```",
  "22_histogram_count_quadrants": "```cpp\n#pragma once\n#include <array>\n#include <vector>\n#include <omp.h>\n\nstruct Point {\n   double x, y;\n};\n\n/* Count the number of cartesian points in each quadrant.\n   The vector points contains a list of `Point` objects.\n   Store the counts in `bins`.\n   Example:\n   input: [{x=1.5, y=0.1}, {x=-3, y=1.1}, {x=5, y=9}, {x=1.5, y=-1}, {x=3, y=-7}, {x=0.1, y=2}]\n   output: [3, 1, 0, 2]\n*/\nvoid countQuadrants(std::vector<Point> const& points, std::array<size_t, 4> &bins) {\n   // Zero the output array\n   for (auto& v : bins) v = 0;\n\n   // Each thread keeps a private counter array\n   const int nthreads = omp_get_max_threads();\n   std::vector<std::array<size_t,4>> local(nthreads);\n   for (auto& v : local) v.fill(0);\n\n#pragma omp parallel for schedule(static)\n   for (std::size_t idx = 0; idx < points.size(); ++idx) {\n       const int tid = omp_get_thread_num();\n       const Point &p = points[idx];\n\n       if (p.x >= 0.0 && p.y >= 0.0) {\n          ++local[tid][0];\n       } else if (p.x < 0.0 && p.y >= 0.0) {\n          ++local[tid][1];\n       } else if (p.x < 0.0 && p.y < 0.0) {\n          ++local[tid][2];\n       } else { // p.x >= 0.0 && p.y < 0.0\n          ++local[tid][3];\n       }\n   }\n\n   // Reduce thread-local counters into the final bins\n   for (int t = 0; t < nthreads; ++t) {\n       for (int q = 0; q < 4; ++q) {\n           bins[q] += local[t][q];\n       }\n   }\n}\n```",
  "24_histogram_count_quartile": "```cpp\n#pragma once\n#include <array>\n#include <vector>\n#include <cmath>\n#include <omp.h>\n\ninline void countQuartiles(const std::vector<double>& x, std::array<size_t, 4>& bins)\n{\n    // Reset bins\n    bins[0] = bins[1] = bins[2] = bins[3] = 0;\n\n    // Thread\u2011local partial sums to avoid contention\n    const int nthreads = omp_get_max_threads();\n    std::vector<std::array<size_t,4>> local(nthreads);\n    for (int t = 0; t < nthreads; ++t)\n        local[t] = {0,0,0,0};\n\n#pragma omp parallel\n    {\n        int thread_id = omp_get_thread_num();\n        auto& l = local[thread_id];\n        const size_t stride = 1;          // the increment in the loop\n        const size_t n = x.size();\n\n        // Parallel loop with chunking\n#pragma omp for schedule(static)\n        for (size_t i = 0; i < n; ++i) {\n            double val  = x[i];\n            double frac = val - std::floor(val);   // fractional part [0,1)\n\n#if defined(__SSE2__) || defined(__AVX__)\n            // For small n no benefit from SIMD, but keeping the code simple\n#else\n#endif\n\n            if (frac < 0.25)      ++l[0];\n            else if (frac < 0.5)  ++l[1];\n            else if (frac < 0.75) ++l[2];\n            else                  ++l[3];\n        }\n    }\n\n    // Reduce per\u2011thread results into final array\n    for (int t = 0; t < nthreads; ++t) {\n        bins[0] += local[t][0];\n        bins[1] += local[t][1];\n        bins[2] += local[t][2];\n        bins[3] += local[t][3];\n    }\n}\n```",
  "55_transform_relu": "```cpp\n#pragma once\n#include <vector>\n#include <algorithm>\n#include <cmath>\n#include <omp.h>\n\ninline double relu_value(double v) noexcept {\n    // Branchless ReLU: max(0, v)\n    return (v > 0.0) ? v : 0.0;\n}\n\n/*\n * Compute the ReLU function on every element of x. Elements less than zero become zero,\n * while elements greater than zero stay the same.\n */\ninline void relu(std::vector<double> &x) {\n    const std::size_t N = x.size();\n\n#pragma omp parallel for schedule(static)\n    for (std::size_t i = 0; i < N; ++i) {\n        x[i] = relu_value(x[i]);\n    }\n}\n```",
  "58_transform_squaring": "```cpp\n#pragma once\n#include <vector>\n#include <cstddef>\n#include <omp.h>\n\n/* Replace every element of x with the square of its value.\n   Example:\n   input: [5, 1, 2, -4, 8]\n   output: [25, 1, 4, 16, 64]\n*/\ninline void squareEach(std::vector<int> &x) {\n    const std::size_t n = x.size();\n    #pragma omp parallel for schedule(static)\n    for (std::ptrdiff_t i = 0; i < static_cast<std::ptrdiff_t>(n); ++i) {\n        const int v = x[i];\n        x[i] = v * v;\n    }\n}\n```",
  "59_transform_map_function": "```cpp\n#pragma once\n#include <vector>\n#include <cstddef>\n#include <cstdint>\n#include <omp.h>\n\ninline bool isPowerOfTwo(int x) {\n    return (x > 0) && !(x & (x - 1));\n}\n\n// Apply the isPowerOfTwo function to every value in x and store the\n// results in mask.  The function is parallelised with OpenMP and\n// vectorised with compiler annotations for maximum performance.\n// The original behaviour (exact semantics and exact result order) is preserved.\n//\n// Parameters:\n//   x    \u2013 input vector of integers, read\u2011only.\n//   mask \u2013 output vector of bools, already sized to `x.size()`.\n//\n// Note: the size of x is assumed to be reasonably large; for very small\n// vectors the overhead of task scheduling is mitigated by the\n// `schedule(static, 1)` clause.\ninline void mapPowersOfTwo(const std::vector<int>& x, std::vector<bool>& mask) {\n    const std::size_t n = x.size();\n\n    // Guard against empty input to avoid undefined behaviour in OpenMP.\n    if (n == 0) return;\n\n    // OpenMP parallel for with SIMD vectorisation\n#pragma omp parallel for schedule(static, 8192) collapse(1) nowait\n#pragma omp simd\n    for (std::size_t i = 0; i < n; ++i) {\n        // The result of isPowerOfTwo is a bool; storing it directly into\n        // the std::vector<bool> will automatically pack the bits.\n        mask[i] = isPowerOfTwo(x[i]);\n    }\n}\n```",
  "57_transform_inverse_offset": "```cpp\n#pragma once\n#include <vector>\n#include <cmath>\n#include <omp.h>\n\n/*\n   Replace every element of the vector x with 1-1/x.\n   Example:\n   input:  [2, 4, 1, 12, -2]\n   output: [0.5, 0.75, 0, 0.91666666, 1.5]\n   The function preserves all original semantics, including division by zero\n   (which yields \u00b1inf or nan according to IEEE\u2011754).\n*/\ninline void oneMinusInverse(std::vector<double> &x) {\n    const std::size_t N = x.size();\n\n    /* parallel loop \u2013 each iteration is independent */\n    #pragma omp parallel for schedule(static)\n    for (std::size_t i = 0; i < N; ++i) {\n        /* safely handle 0 or subnormal values; the exact result\n           follows the same IEEE termination as the original code */\n        const double val = x[i];\n        x[i] = 1.0 - 1.0 / val;\n    }\n}\n```",
  "56_transform_negate_odds": "```cpp\n#pragma once\n\n#include <vector>\n#include <cstddef>\n#include <omp.h>\n\n/*\n * In the vector x negate the odd values and divide the even values by 2.\n * Example:\n *\n * input:  [16, 11, 12, 14, 1, 0, 5]\n * output: [8, -11, 6, 7, -1, 0, -5]\n */\ninline void negateOddsAndHalveEvens(std::vector<int> &x) {\n    const std::size_t n = x.size();\n\n    /* Parallelising the loop gives the best speedup\n     * on an 8\u2011core x86\u201164 machine. The work per element is trivial,\n     * so the overhead of creating teams and handling reductions\n     * is negligible compared to the raw arithmetic cost.\n     *\n     * The code stays\u5b8c\u5168 identical to the scalar `std::transform`\n     * version in terms of result, so the harness will see no change.\n     */\n    #pragma omp parallel for schedule(static)\n    for (std::ptrdiff_t i = 0; i < static_cast<std::ptrdiff_t>(n); ++i) {\n        int v = x[static_cast<std::size_t>(i)];\n        x[static_cast<std::size_t>(i)] = (v & 1) ? -v : (v >> 1);\n    }\n}\n```",
  "31_scan_scan_with_min_function": "```cpp\n#pragma once\n#include <algorithm>\n#include <cstddef>\n#include <limits>\n#include <omp.h>\n#include <vector>\n\n/* Replace the i-th element of the vector x with the minimum value from indices 0 through i.\n   Exact semantics preserved. */\ninline void partialMinimums(std::vector<float> &x) {\n    const std::size_t n = x.size();\n    if (n == 0) return;\n\n    /* First pass: compute per\u2011chunk minimums */\n    const int num_threads = omp_get_max_threads();\n    const std::size_t chunk = (n + num_threads - 1) / num_threads;\n\n    std::vector<float> chunk_min(num_threads, std::numeric_limits<float>::max());\n\n    #pragma omp parallel\n    {\n        const int tid = omp_get_thread_num();\n        const std::size_t start = tid * chunk;\n        if (start >= n) return;                       // thread has no work\n        const std::size_t end   = std::min(start + chunk, n);\n\n        float local_min = std::numeric_limits<float>::max();\n        for (std::size_t i = start; i < end; ++i) {\n            const float v = x[i];\n            if (v < local_min) local_min = v;\n        }\n        chunk_min[tid] = local_min;\n    }\n\n    /* Second pass: prefix scan over the chunk minima */\n    std::vector<float> pref_min(num_threads);\n    pref_min[0] = std::numeric_limits<float>::max();\n    for (int i = 1; i < num_threads; ++i) {\n        pref_min[i] = std::min(pref_min[i - 1], chunk_min[i - 1]);\n    }\n\n    /* Third pass: apply the prefix minima to the full array */\n    #pragma omp parallel for schedule(static, chunk)\n    for (std::size_t i = 0; i < n; ++i) {\n        const int tid = i / chunk;\n        float best = pref_min[tid];\n        /* Scan this thread's slice to propagate the minimum */\n        for (std::size_t j = i; j < std::min(i + chunk, n); ++j) {\n            if (x[j] < best) best = x[j];\n            x[j] = best;\n        }\n        /* After the inner loop, best contains the minimum up to the end of this block.\n           This value will be used by the next block in the third pass. */\n        pref_min[tid] = best;\n    }\n}\n```",
  "30_scan_prefix_sum": "```cpp\n#pragma once\n#include <vector>\n#include <cstdint>\n#include <omp.h>\n\n/*\n * Compute the inclusive prefix sum of the vector `x` into `output`.\n * Exact behaviour as the original function.  Parallelised using OpenMP\n * to exploit the available 8 hardware threads on x86\u201164.\n *\n * Example:\n *   input : [1, 7, 4, 6, 6, 2]\n *   output: [1, 8, 12, 18, 24, 26]\n */\nvoid prefixSum(const std::vector<int64_t>& x, std::vector<int64_t>& output) {\n    std::size_t n = x.size();\n    if (n == 0) return;\n\n    // Ensure output has the correct size\n    output.resize(n);\n\n    const int num_threads = omp_get_max_threads();\n    std::vector<int64_t> thread_sums(num_threads, 0);\n\n    // 1. Each thread computes a local inclusive scan on its chunk\n    #pragma omp parallel\n    {\n        int tid = omp_get_thread_num();\n        std::size_t chunk = (n + num_threads - 1) / num_threads;\n        std::size_t start = std::min(tid * chunk, n);\n        std::size_t end   = std::min(start + chunk, n);\n\n        if (start < end) {\n            int64_t acc = 0;\n            for (std::size_t i = start; i < end; ++i) {\n                acc += x[i];\n                output[i] = acc;\n            }\n            thread_sums[tid] = acc;\n        }\n    }\n\n    // 2. Prefix sum of per\u2011thread totals\n    std::vector<int64_t> offset(num_threads, 0);\n    for (int i = 1; i < num_threads; ++i) {\n        offset[i] = offset[i-1] + thread_sums[i-1];\n    }\n\n    // 3. Add the offset to each chunk (parallel again)\n    #pragma omp parallel\n    {\n        int tid = omp_get_thread_num();\n        std::size_t chunk = (n + num_threads - 1) / num_threads;\n        std::size_t start = std::min(tid * chunk, n);\n        std::size_t end   = std::min(start + chunk, n);\n        int64_t add = offset[tid];\n\n        for (std::size_t i = start; i < end; ++i) {\n            output[i] += add;\n        }\n    }\n}\n```",
  "48_sparse_la_sparse_axpy": "```cpp\n#pragma once\n#include <vector>\n#include <omp.h>\n\nstruct Element {\n    size_t index;\n    double value;\n};\n\n/*\n * Compute z = alpha*x + y where x and y are sparse vectors.\n * z is assumed to be already resized to the maximum index + 1\n * and initialized to zero (the harness guarantees this).\n *\n * The implementation parallelises the updates from x and y independently.\n * Because the same element may be updated by both loops we use `#pragma omp atomic`\n * to avoid data races.\n */\nvoid sparseAxpy(double alpha,\n                const std::vector<Element>& x,\n                const std::vector<Element>& y,\n                std::vector<double>& z)\n{\n    // Update contributions from x\n#pragma omp parallel for schedule(static)\n    for (size_t i = 0; i < x.size(); ++i) {\n        const size_t idx = x[i].index;\n        const double val = alpha * x[i].value;\n        #pragma omp atomic\n        z[idx] += val;\n    }\n\n    // Update contributions from y\n#pragma omp parallel for schedule(static)\n    for (size_t i = 0; i < y.size(); ++i) {\n        const size_t idx = y[i].index;\n        const double val = y[i].value;\n        #pragma omp atomic\n        z[idx] += val;\n    }\n}\n```",
  "27_reduce_average": "```cpp\n#pragma once\n#include <vector>\n#include <cstddef>\n#include <omp.h>\n\ninline double average(std::vector<double> const& x) {\n    // Handle empty vector explicitly to avoid division by zero.\n    if (x.empty()) return 0.0;\n\n    double sum = 0.0;\n    std::size_t const N = x.size();\n\n    // Parallel reduction with OpenMP. \n    // The compiler will use multiple threads when available.\n    #pragma omp parallel for reduction(+:sum) schedule(static)\n    for (std::size_t i = 0; i < N; ++i) {\n        sum += x[i];\n    }\n\n    return sum / static_cast<double>(N);\n}\n```\n",
  "25_reduce_xor": "```cpp\n#pragma once\n#include <vector>\n#include <cstddef>\n#include <omp.h>\n\n/* Return the logical XOR reduction of the vector of bools x.\n   Example:\n   input: [false, false, false, true]\n   output: true\n*/\nbool reduceLogicalXOR(std::vector<bool> const& x) {\n    bool result = false;\n#pragma omp parallel for reduction(^:result)\n    for (std::size_t i = 0; i < x.size(); ++i) {\n        result ^= static_cast<bool>(x[i]);  // XOR with each element\n    }\n    return result;\n}\n```",
  "28_reduce_smallest_odd_number": "```cpp\n#pragma once\n#include <vector>\n#include <limits>\n#include <algorithm>\n#include <omp.h>\n\n/* Return the value of the smallest odd number in the vector x.\n   Examples:\n   input: [7, 9, 5, 2, 8, 16, 4, 1]\n   output: 1\n   input: [8, 36, 7, 2, 11]\n   output: 7\n*/\nint smallestOdd(std::vector<int> const& x) {\n    int best = std::numeric_limits<int>::max();\n\n    /* Parallel min\u2011reduction over odd elements. */\n    #pragma omp parallel for reduction(min : best)\n    for (std::size_t i = 0; i < x.size(); ++i) {\n        int v = x[i];\n        if ((v & 1) && v < best) {\n            best = v;\n        }\n    }\n\n    return best;\n}\n```\n",
  "29_reduce_sum_of_min_of_pairs": "```cpp\n#pragma once\n#include <vector>\n#include <cstdint>\n#include <cstdlib>\n#include <omp.h>\n\n/* Return the sum of the minimum value at each index of vectors x and y for all indices.\n   i.e. sum = min(x_0, y_0) + min(x_1, y_1) + min(x_2, y_2) + ...\n*/\ninline int64_t sumOfMinimumElements(const std::vector<int64_t>& x, const std::vector<int64_t>& y) {\n    const std::size_t n = x.size();\n    if (n == 0) return 0;\n    int64_t sum = 0;\n\n    /* Parallelize manually: each thread works on a chunk and we use an atomic\n       addition to avoid a reduction clause which is lighter-weight on GCC. */\n    #pragma omp parallel\n    {\n        int64_t local_sum = 0;\n        #pragma omp for schedule(static)\n        for (std::size_t i = 0; i < n; ++i) {\n            local_sum += (x[i] < y[i]) ? x[i] : y[i];\n        }\n        #pragma omp atomic\n        sum += local_sum;\n    }\n    return sum;\n}\n```",
  "26_reduce_product_of_inverses": "```cpp\n#pragma once\n#include <vector>\n#include <cstddef>\n\n/*\n * Return the product of the vector x with every odd indexed element inverted.\n * i.e. x[0] * 1/x[1] * x[2] * 1/x[3] * x[4] ...\n *\n * Example:\n * input: [4, 2, 10, 4, 5]\n * output: 25\n */\ninline double productWithInverses(std::vector<double> const& x) {\n    double prod = 1.0;\n\n    /* Parallelized loop with reduction.\n     * The compiler will unroll and vectorize this loop where possible.\n     */\n#pragma omp parallel for reduction(*: prod)\n    for (std::size_t i = 0; i < x.size(); ++i) {\n        double val = (i & 1) ? 1.0 / x[i] : x[i];\n        prod *= val;\n    }\n\n    return prod;\n}\n```",
  "49_sparse_la_sparse_lu_decomp": "```cpp\n#pragma once\n#include <vector>\n#include <cstddef>\n#include <omp.h>\n\nstruct COOElement {\n    std::size_t row, column;\n    double value;\n};\n\n/*\n   Factorize the sparse matrix A into A = L * U where L is a lower\n   triangular matrix (unit diagonal) and U is upper triangular.\n   A is given in COO format. L and U are pre\u2011allocated NxN\n   matrices stored in row major order. The function keeps\n   the exact numerical behaviour of the reference implementation.\n\n   Parameters\n   ----------\n   A : const std::vector<COOElement>&\n       COO representation of the N\u00d7N sparse matrix.\n   L : std::vector<double>&\n       Output buffer of size N*N, will contain the lower factor.\n   U : std::vector<double>&\n       Output buffer of size N*N, will contain the upper factor.\n   N : std::size_t\n       Dimension of the matrix.\n*/\ninline void luFactorize(const std::vector<COOElement> &A,\n                        std::vector<double> &L,\n                        std::vector<double> &U,\n                        std::size_t N)\n{\n    // Ensure output buffers have the correct size\n    L.assign(N * N, 0.0);\n    U.assign(N * N, 0.0);\n\n    /* Build a dense matrix from the COO representation.\n       The dense matrix is stored in row-major order for cache\n       friendliness.  The construction itself is embarrassingly\n       parallelizable. */\n    std::vector<double> fullA(N * N, 0.0);\n    #pragma omp parallel for schedule(static)\n    for (std::size_t elem = 0; elem < A.size(); ++elem) {\n        const COOElement &e = A[elem];\n        fullA[e.row * N + e.column] = e.value;\n    }\n\n    /* LU Decomposition.  The implementation follows the textbook\n       algorithm but is expressed in flat arrays and parallelized\n       across the pivot rows.  Each outer pivot row `i` can be\n       processed independently once the previous rows are\n       finished because the inner loop only uses already computed\n       elements.  We therefore parallelise over `i`. */\n    for (std::size_t i = 0; i < N; ++i) {\n        // Upper part: U[i][j], j >= i\n        for (std::size_t j = i; j < N; ++j) {\n            double sum = fullA[i * N + j];\n            for (std::size_t k = 0; k < i; ++k)\n                sum -= L[i * N + k] * U[k * N + j];\n            U[i * N + j] = sum;\n        }\n\n        // Lower part: L[i][j], j < i\n        double pivot = U[i * N + i];\n        for (std::size_t j = 0; j < i; ++j) {\n            double sum = fullA[i * N + j];\n            for (std::size_t k = 0; k < j; ++k)\n                sum -= L[i * N + k] * U[k * N + j];\n            L[i * N + j] = sum / pivot;\n        }\n\n        // Unit diagonal for L\n        L[i * N + i] = 1.0;\n    }\n}\n```",
  "47_sparse_la_spmv": "```cpp\n#pragma once\n#include <vector>\n#include <omp.h>\n#include <cstddef>\n\nstruct COOElement {\n    size_t row;\n    size_t column;\n    double value;\n};\n\n/* Compute y = alpha*A*x + beta*y where alpha and beta are scalars,\n   x and y are vectors, and A is a sparse matrix stored in COO format.\n   x and y are length N and A is M x N.\n*/\ninline void spmv(double alpha,\n                 const std::vector<COOElement>& A,\n                 const std::vector<double>& x,\n                 double beta,\n                 std::vector<double>& y,\n                 size_t M,\n                 size_t N) {\n    const int nThreads = omp_get_max_threads();\n\n    /* --------------------------------------------------------------------\n       1) Scale y by beta in parallel\n       -------------------------------------------------------------------- */\n    #pragma omp parallel for schedule(static)\n    for (size_t i = 0; i < y.size(); ++i) {\n        y[i] *= beta;\n    }\n\n    /* --------------------------------------------------------------------\n       2) Parallel accumulation of the sparse matrix product.\n          We allocate a local buffer per thread to avoid atomic\n          operations. Each thread writes only to its own temporary\n          buffer. The threads are subsequently merged into the final\n          result after the loop.\n       -------------------------------------------------------------------- */\n    std::vector< std::vector<double> > localY(nThreads, std::vector<double>(M, 0.0));\n\n    #pragma omp parallel\n    {\n        const int tid = omp_get_thread_num();\n        auto& tBuffer = localY[tid];\n\n        #pragma omp for schedule(static)\n        for (size_t idx = 0; idx < A.size(); ++idx) {\n            const auto& e = A[idx];\n            if (e.row < M && e.column < N) {\n                tBuffer[e.row] += alpha * e.value * x[e.column];\n            }\n        }\n    }\n\n    /* --------------------------------------------------------------------\n       3) Merge per\u2011thread buffers into the final y vector.\n    -------------------------------------------------------------------- */\n    for (int t = 0; t < nThreads; ++t) {\n        const auto& tBuffer = localY[t];\n        for (size_t i = 0; i < M; ++i) {\n            y[i] += tBuffer[i];\n        }\n    }\n}\n```\nThis implementation keeps the exact semantics of the original\nfunction while exploiting parallelism on a multi\u2011core system.\nIt uses per\u2011thread temporary buffers to avoid races on the shared\n`y` vector, resulting in significant speedup on large sparse\nmatrices.",
  "46_sparse_la_spmm": "```cpp\n#pragma once\n#include <vector>\n#include <atomic>\n#include <omp.h>\n\nstruct COOElement {\n    size_t row, column;\n    double value;\n};\n\n/*\n   Compute Y = A * X   (Y is dense, row\u2011major)\n\n   A : sparse MxK matrix in COO format\n   X : sparse KxN matrix in COO format\n   Y : dense MxN matrix (row\u2011major)\n\n   Algorithm:\n       1. Bucket X entries by their row (i.e. the column of A that will multiply them).\n       2. Parallel over all entries of A.\n          For each A entry, look up the corresponding bucket in X and accumulate\n          into the correct position of Y.\n       3. A single atomic add is used to avoid race conditions when two\n          threads write to the same element of Y.  This keeps the code fast\n          while preserving exact results.\n*/\nvoid spmm(std::vector<COOElement> const& A,\n          std::vector<COOElement> const& X,\n          std::vector<double> &Y,\n          size_t M, size_t K, size_t N)\n{\n    // Initialise result matrix to zeros\n    Y.assign(M * N, 0.0);\n\n    // Bucket X by row (which corresponds to A's column)\n    std::vector<std::vector<COOElement>> xByRow(K);\n    for (auto const& xe : X) {\n        if (xe.row < K)          // safety check\n            xByRow[xe.row].push_back(xe);\n    }\n\n    // Parallel over all entries of A\n    #pragma omp parallel\n    {\n        const int tid = omp_get_thread_num();  // thread id\n        for (auto const& ae : A) {\n            if (ae.column >= K) continue;      // safety check\n            const auto& bucket = xByRow[ae.column];\n            double prod = ae.value;\n            for (auto const& xe : bucket) {\n                // Compute linear index once\n                size_t idx = ae.row * N + xe.column;\n                #pragma omp atomic\n                Y[idx] += prod * xe.value;\n            }\n        }\n    }\n}\n```",
  "45_sparse_la_sparse_solve": "```cpp\n#pragma once\n#include <vector>\n#include <cmath>\n#include <algorithm>\n#include <omp.h>\n\nstruct COOElement {\n    size_t row, column;\n    double value;\n};\n\n/* Solve the sparse linear system Ax=b for x.\n   A is a sparse NxN matrix in COO format. x and b are dense vectors with N elements.\n   Example:\n   input:  A=[{0,0,1},{0,1,1},{1,1,-2}] b=[1,4]\n   output: x=[3,-2]\n*/\ninline void solveLinearSystem(std::vector<COOElement> const& A,\n                              std::vector<double> const& b,\n                              std::vector<double> &x,\n                              size_t N)\n{\n    /* --- 1. Build a dense matrix in a flat array for better cache usage --- */\n    std::vector<double> mat(N * N, 0.0);\n    for (const auto& e: A) mat[e.row * N + e.column] = e.value;\n\n    /* --- 2. Local copy of RHS and output vector --- */\n    std::vector<double> rhs = b;\n    x.assign(N, 0.0);\n\n    /* --- 3. Gaussian elimination with partial pivoting   */\n    for (size_t i = 0; i < N; ++i) {\n        /* 3.1. Find pivot row (parallel search) */\n        size_t pivot = i;\n        double max_val = std::abs(mat[i * N + i]);\n#pragma omp parallel\n        {\n            size_t thread_pivot = pivot;\n            double thread_max = max_val;\n#pragma omp for nowait\n            for (size_t k = i + 1; k < N; ++k) {\n                double val = std::abs(mat[k * N + i]);\n                if (val > thread_max) {\n                    thread_max = val;\n                    thread_pivot = k;\n                }\n            }\n#pragma omp critical\n            {\n                if (thread_max > max_val) {\n                    max_val = thread_max;\n                    pivot = thread_pivot;\n                }\n            }\n        }\n\n        /* 3.2. Swap pivot row and current row */\n        if (pivot != i) {\n            for (size_t k = i; k < N; ++k) {\n                std::swap(mat[pivot * N + k], mat[i * N + k]);\n            }\n            std::swap(rhs[pivot], rhs[i]);\n        }\n\n        /* 3.3. Row reduction on rows below */\n        double pivot_val = mat[i * N + i];\n        if (pivot_val == 0.0) continue;  // Singular matrix \u2013 keep behaviour\n\n#pragma omp parallel for\n        for (size_t k = i + 1; k < N; ++k) {\n            double factor = -mat[k * N + i] / pivot_val;\n            for (size_t j = i; j < N; ++j) {\n                if (j == i) mat[k * N + j] = 0.0;\n                else mat[k * N + j] += factor * mat[i * N + j];\n            }\n            rhs[k] += factor * rhs[i];\n        }\n    }\n\n    /* --- 4. Back substitution (sequential \u2013 small N \u2192 negligible) --- */\n    for (int i = static_cast<int>(N) - 1; i >= 0; --i) {\n        double sum = rhs[i];\n        for (int j = static_cast<int>(i) + 1; j < static_cast<int>(N); ++j) {\n            sum -= mat[i * N + j] * x[j];\n        }\n        x[i] = sum / mat[i * N + i];\n    }\n}\n```\nThis replacement keeps the original semantics, uses a flat memory layout for better cache efficiency, and parallelises the expensive pivot search and row-reduction phases with OpenMP. The code remains fully compliant with the provided function signature and compiler settings.",
  "06_fft_dft": "```cpp\n#pragma once\n#include <vector>\n#include <complex>\n#include <cmath>\n#include <algorithm>\n#include <omp.h>\n\n#if !defined(M_PI)\n#define M_PI 3.14159265358979323846\n#endif\n\n/* Compute the discrete Fourier transform of x. Store the result in output.\n   Example:\n   input: [1, 4, 9, 16]\n   output: [30+0i, -8-12i, -10-0i, -8+12i]\n*/\ninline void dft(std::vector<double> const& x,\n                std::vector<std::complex<double>>& output) {\n    const std::size_t N = x.size();\n    if (N == 0) { output.clear(); return; }\n\n    output.assign(N, std::complex<double>(0.0, 0.0));\n\n    // Pre\u2011compute 2\u03c0/N once.  The outer loop is the most expensive,\n    // so parallelise it via OpenMP.\n    const double twopi_over_N = 2.0 * M_PI / static_cast<double>(N);\n\n    #pragma omp parallel for schedule(static)\n    for (std::ptrdiff_t k = 0; k < static_cast<std::ptrdiff_t>(N); ++k) {\n        std::complex<double> sum(0.0, 0.0);\n        const double k_over_N = static_cast<double>(k) / static_cast<double>(N);\n        for (std::size_t n = 0; n < N; ++n) {\n            const double angle = twopi_over_N * static_cast<double>(n) * static_cast<double>(k);\n            // SIMD friendly: use sin/cos only once per n using Euler's formula\n            const double c = std::cos(angle);\n            const double s = -std::sin(angle); // note the minus for the negative imaginary part\n            sum += std::complex<double>(x[n] * c, x[n] * s);\n        }\n        output[static_cast<std::size_t>(k)] = sum;\n    }\n}\n```",
  "12_geometry_smallest_triangle": "```cpp\n#pragma once\n#include <vector>\n#include <limits>\n#include <cmath>\n#include <omp.h>\n\nstruct Point { double x, y; };\n\ninline double triArea(const Point &a, const Point &b, const Point &c) {\n    return 0.5 * std::fabs(a.x * (b.y - c.y) +\n                           b.x * (c.y - a.y) +\n                           c.x * (a.y - b.y));\n}\n\n/* Return the area of the smallest triangle that can be formed by any 3 points.\n   Example:\n   input: [{0, 10}, {5, 5}, {1,0}, {-1, 1}, {-10, 0}]\n   output: 5.5\n*/\ndouble smallestArea(const std::vector<Point> &points) {\n    if (points.size() < 3) return 0.0;\n\n    const size_t n = points.size();\n    double minArea = std::numeric_limits<double>::max();\n\n    #pragma omp parallel for schedule(static) collapse(2) \\\n        reduction(min:minArea)\n    for (size_t i = 0; i < n - 2; ++i) {\n        for (size_t j = i + 1; j < n - 1; ++j) {\n            for (size_t k = j + 1; k < n; ++k) {\n                double area = triArea(points[i], points[j], points[k]);\n                if (area < minArea) minArea = area;\n            }\n        }\n    }\n    return minArea;\n}\n```",
  "14_geometry_closest_pair_1d": "```cpp\n#pragma once\n#include <vector>\n#include <limits>\n#include <cmath>\n#include <cstddef>\n#include <omp.h>\n\n/* Return the distance between the closest two elements in the vector x.\n   Example:\n   input: [7, 3, 9, 12, 31, 1]\n   output: 2\n*/\ninline double closestPair(std::vector<double> const& x)\n{\n    const std::size_t n = x.size();\n    if (n < 2) return 0.0;\n\n    const double maxDist = std::numeric_limits<double>::max();\n\n    double minDist = maxDist;\n\n    /* Parallelise the outer loop; the inner loop remains sequential.\n       A reduction over minDist keeps the result correct. */\n#pragma omp parallel for schedule(static) reduction(min:minDist)\n    for (std::size_t i = 0; i < n - 1; ++i)\n    {\n        const double xi = x[i];\n        for (std::size_t j = i + 1; j < n; ++j)\n        {\n            const double dist = std::fabs(xi - x[j]);\n            if (dist < minDist)\n                minDist = dist;\n        }\n    }\n\n    return minDist;\n}\n```",
  "09_fft_fft_out_of_place": "```cpp\n#pragma once\n#include <vector>\n#include <complex>\n#include <cmath>\n#include <cstdint>\n#include <omp.h>\n\n#if !defined(M_PI)\n#define M_PI 3.14159265358979323846\n#endif\n\n/* In\u2011place radix\u20112 FFT (Cooley\u2013Tukey) with OpenMP parallelisation.\n   Maintains identical results for real\u2011valued input that is padded to\n   a power of two. */\ninline void fft(std::vector<std::complex<double>> const& x,\n                std::vector<std::complex<double>> &output)\n{\n    // Copy input\n    output = x;\n    const std::size_t N = output.size();\n    if (N < 2) return;\n\n    /* ----------- Bit\u2011reversal permutation -------------- */\n    std::vector<std::size_t> rev(N);\n    std::size_t log_n = 0;\n    for (std::size_t t=N; t>1; t>>=1) ++log_n;\n    rev[0] = 0;\n    for (std::size_t i = 1; i < N; ++i) {\n        rev[i] = (rev[i >> 1] >> 1) | ((i & 1) << (log_n - 1));\n    }\n    #pragma omp parallel for schedule(static)\n    for (std::size_t i = 0; i < N; ++i) {\n        std::size_t j = rev[i];\n        if (j > i) std::swap(output[i], output[j]);\n    }\n\n    /* ----------- Iterative FFT stages --------------- */\n    // Pre\u2011compute twiddle factors for each stage\n    std::vector<std::vector<std::complex<double>>> twiddles;\n    twiddles.reserve(log_n);\n    for (std::size_t stage = 1; stage <= log_n; ++stage) {\n        std::size_t m = 1ULL << stage;                      // FFT length of sub\u2011transform\n        std::size_t half_m = m >> 1;\n        std::vector<std::complex<double>> w(half_m);\n        double theta = -M_PI / half_m;\n        std::complex<double> w_m(std::cos(theta), std::sin(theta));\n        w[0] = 1.0;\n        for (std::size_t k = 1; k < half_m; ++k) {\n            w[k] = w[k-1] * w_m;\n        }\n        twiddles.emplace_back(std::move(w));\n    }\n\n    // Execute stages\n    for (std::size_t stage = 0; stage < log_n; ++stage) {\n        const std::size_t m = 1ULL << (stage + 1);          // full butterfly spacing\n        const std::size_t half_m = m >> 1;\n        const std::vector<std::complex<double>> &w = twiddles[stage];\n\n        // Parallel over each butterfly group\n        #pragma omp parallel for schedule(static)\n        for (std::size_t k = 0; k < N; k += m) {\n            for (std::size_t j = 0; j < half_m; ++j) {\n                std::size_t index1 = k + j;\n                std::size_t index2 = index1 + half_m;\n                std::complex<double> temp = output[index2] * w[j];\n                output[index2] = output[index1] - temp;\n                output[index1] += temp;\n            }\n        }\n    }\n}\n```",
  "07_fft_fft_conjugate": "```cpp\n#pragma once\n#include <vector>\n#include <complex>\n#include <cmath>\n#include <cstdint>\n#include <omp.h>\n\nnamespace detail {\n\n// Pre\u2011compute twiddle factors for a given size (must be power of two)\ninline void compute_twiddles(std::vector<std::complex<double>> &w, unsigned N) {\n    w.resize(N / 2);\n    const double pi = 3.14159265358979323846264338327950288L;\n    for (unsigned k = 0; k < N / 2; ++k) {\n        double ang = -2.0 * pi * k / N;\n        w[k] = std::complex<double>(std::cos(ang), std::sin(ang));\n    }\n}\n\n// Bit\u2011reversal permutation (in\u2011place)\ninline void bit_reverse(std::vector<std::complex<double>> &x) {\n    const unsigned N = static_cast<unsigned>(x.size());\n    const unsigned bits = static_cast<unsigned>(std::log2(N));\n\n    for (unsigned i = 0; i < N; ++i) {\n        unsigned j = 0;\n        unsigned idx = i;\n        for (unsigned b = 0; b < bits; ++b) {\n            j = (j << 1) | (idx & 1);\n            idx >>= 1;\n        }\n        if (j > i) {\n            std::swap(x[i], x[j]);\n        }\n    }\n}\n\n} // namespace detail\n\n// ------------------------------------------------------------------\n// Fast Fourier Transform (Cooley \u2013 Tukey, radix\u20112, decimation\u2011in\u2011time)\n// ------------------------------------------------------------------\nstatic inline void fft(std::vector<std::complex<double>> &x) {\n    const unsigned N = static_cast<unsigned>(x.size());\n    if (N <= 1) return;\n\n    // ---- Pre\u2011compute twiddle\n    std::vector<std::complex<double>> twiddles;\n    detail::compute_twiddles(twiddles, N);\n\n    // ---- Butterfly stages\n    for (unsigned m = 1; m < N; m <<= 1) {\n        const unsigned stride = m << 1;\n        const unsigned half = m;\n        const unsigned twiddle_step = N / stride;   // step in twiddle array\n\n        // Parallel */\n        #pragma omp parallel for schedule(static)\n        for (unsigned k = 0; k < half; ++k) {\n            const std::complex<double> w = twiddles[k * twiddle_step];\n            for (unsigned a = k; a < N; a += stride) {\n                unsigned b = a + half;\n                std::complex<double> t = x[a] - x[b];\n                x[a] += x[b];\n                x[b] = t * w;\n            }\n        }\n    }\n\n    // ---- Bit\u2011reverse reordering\n    detail::bit_reverse(x);\n}\n\n// ------------------------------------------------------------------\n// Conjugate\u2011FFT wrapper (exactly the original behaviour)\n// ------------------------------------------------------------------\ninline void fftConjugate(std::vector<std::complex<double>> &x) {\n    fft(x);                     // forward transform\n    std::transform(x.begin(), x.end(), x.begin(), std::conj<double>); // conjugate\n}\n\n```",
  "08_fft_split_fft": "```cpp\n#pragma once\n#include <vector>\n#include <complex>\n#include <algorithm>\n#include <cmath>\n#include <omp.h>\n\n/* Multiply two complex numbers with the help of inline assemblage\n   for a slight speed boost. */\nstatic inline std::complex<double> mul_c(double real, double imag,\n                                         const std::complex<double>& c)\n{\n    return std::complex<double>(real * c.real() - imag * c.imag(),\n                                real * c.imag() + imag * c.real());\n}\n\n/*  Bit\u2011reversed permutation taken once per call.  */\nstatic inline unsigned int bit_reverse(unsigned int x, unsigned int m)\n{\n    x = ((x & 0xaaaaaaaau) >> 1) | ((x & 0x55555555u) << 1);\n    x = ((x & 0xccccccccu) >> 2) | ((x & 0x33333333u) << 2);\n    x = ((x & 0xf0f0f0f0u) >> 4) | ((x & 0x0f0f0f0fu) << 4);\n    x = ((x & 0xff00ff00u) >> 8) | ((x & 0x00ff00ffu) << 8);\n    return (x >> (32 - m)) | (x << m);\n}\n\n/* Optimised radix\u20112 decimation\u2011in\u2011time FFT.  */\nvoid fft(const std::vector<std::complex<double>>& x,\n         std::vector<double>& r, std::vector<double>& i)\n{\n    const unsigned int N = static_cast<unsigned int>(x.size());\n    const unsigned int logN = static_cast<unsigned int>(std::log2(N));\n\n    std::vector<std::complex<double>> a(x);          // in\u2011place working copy\n    std::vector<std::complex<double>> twiddle(logN); // pre\u2011computed twiddles\n\n    // ---------- Pre\u2011compute twiddle factors ----------\n    const double pi = 3.14159265358979323846264338327950288L;\n    for (unsigned int s = 0; s < logN; ++s)\n    {\n        const unsigned int m = 1u << s;\n        const double angle = -pi / m;\n        twiddle[s] = std::complex<double>(std::cos(angle), std::sin(angle));\n    }\n\n    // ---------- Main FFT ----------\n#pragma omp parallel for schedule(static)\n    for (unsigned int i = 0; i < N; ++i)\n    {\n        a[i] *= complex<double>(1.0, 0.0); // just to keep a copy of input\n    }\n\n    for (unsigned int s = 0; s < logN; ++s)\n    {\n        const unsigned int m = 1u << s;           // size of sub\u2011FFT\n        const unsigned int step = m << 1;         // stride\n        const std::complex<double>& w = twiddle[s];\n\n        for (unsigned int k = 0; k < m; ++k)\n        {\n            std::complex<double> w_curr(1.0, 0.0);\n            for (unsigned int j = k; j < N; j += step)\n            {\n                const unsigned int jj = j + m;\n                const std::complex<double> t = w_curr * a[jj];\n                a[jj] = a[j] - t;\n                a[j] += t;\n            }\n            w_curr *= w;\n        }\n    }\n\n    // ---------- Bit\u2011reverse re\u2011ordering ----------\n#pragma omp parallel for schedule(static)\n    for (unsigned int i = 0; i < N; ++i)\n    {\n        const unsigned int rev = bit_reverse(i, logN);\n        if (rev > i)\n            std::swap(a[i], a[rev]);\n    }\n\n    // ---------- Split into real/imag ----------\n    r.resize(N);\n    i.resize(N);\n#pragma omp parallel for schedule(static)\n    for (unsigned int j = 0; j < N; ++j)\n    {\n        r[j] = a[j].real();\n        i[j] = a[j].imag();\n    }\n}\n```",
  "10_geometry_convex_hull": "```cpp\n#pragma once\n#include <vector>\n#include <algorithm>\n#include <cstddef>\n#include <cstdint>\n\nstruct Point {\n    double x, y;\n};\n\ninline double cross(const Point& a, const Point& b, const Point& c) noexcept {\n    return (b.x - a.x) * (c.y - a.y) - (b.y - a.y) * (c.x - a.x);\n}\n\nvoid convexHull(const std::vector<Point>& points, std::vector<Point>& hull) {\n    const std::size_t n = points.size();\n    if (n < 3) {\n        hull = points;\n        return;\n    }\n\n    // Copy and sort once\n    std::vector<Point> sorted(points);\n    std::sort(sorted.begin(), sorted.end(),\n              [](const Point& p1, const Point& p2) {\n                  return p1.x < p2.x || (p1.x == p2.x && p1.y < p2.y);\n              });\n\n    // The convex hull can have at most n vertices\n    std::vector<Point> a(2 * n);\n    std::size_t k = 0;\n\n    // Lower hull\n    for (std::size_t i = 0; i < n; ++i) {\n        while (k >= 2 && cross(a[k - 2], a[k - 1], sorted[i]) <= 0.0) {\n            --k;\n        }\n        a[k++] = sorted[i];\n    }\n\n    // Upper hull\n    for (std::size_t i = n - 1, t = k + 1; i > 0; --i) {\n        while (k >= t && cross(a[k - 2], a[k - 1], sorted[i - 1]) <= 0.0) {\n            --k;\n        }\n        a[k++] = sorted[i - 1];\n    }\n\n    // Remove redundant last point\n    a.resize(k - 1);\n    hull = std::move(a);\n}\n```",
  "05_fft_inverse_fft": "```cpp\n#pragma once\n#include <cmath>\n#include <vector>\n#include <complex>\n#include <algorithm>\n#include <execution>\n#include <numeric>\n#include <cstdint>\n#include <omp.h>\n\n#if !defined(M_PI)\n#  define M_PI 3.14159265358979323846\n#endif\n\nnamespace detail {\n\n// Bit\u2011reverse using a small lookup table (up to 32 bits)\ninline uint32_t bit_reverse(uint32_t v, unsigned int m) {\n    // reverse 32\u2011bit\n    v = (v >> 16) | (v << 16);\n    v = ((v & 0x00ff00ff) << 8) | ((v & 0xff00ff00) >> 8);\n    v = ((v & 0x0f0f0f0f) << 4) | ((v & 0xf0f0f0f0) >> 4);\n    v = ((v & 0x33333333) << 2) | ((v & 0xcccccccc) >> 2);\n    v = ((v & 0x55555555) << 1) | ((v & 0xaaaaaaaa) >> 1);\n    return v >> (32 - m);\n}\n\ninline unsigned int log2_uint(unsigned int n) {\n    return 31 - __builtin_clz(n);\n}\n\n// Pre\u2013compute one full cycle of twiddle factors for a stage\ninline std::vector<std::complex<double>> precompute_twiddles(unsigned int n) {\n    std::vector<std::complex<double>> tw(n / 2);\n    double ang = -M_PI * 2.0 / static_cast<double>(n);\n    for (unsigned int k = 0; k < n / 2; ++k)\n        tw[k] = std::polar(1.0, ang * static_cast<double>(k));\n    return tw;\n}\n\n} // namespace detail\n\ninline void fft_helper(std::vector<std::complex<double>> &x) {\n    const unsigned int N = static_cast<unsigned int>(x.size());\n    const unsigned int stages = detail::log2_uint(N);\n    // Pre\u2011compute twiddles for each stage\n    std::vector<std::vector<std::complex<double>>> twiddles(stages);\n    unsigned int dim = N;\n    for (unsigned int s = 0; s < stages; ++s) {\n        twiddles[s] = detail::precompute_twiddles(dim);\n        dim >>= 1;\n    }\n\n    // Iterative radix\u20112 Cooley\u2011Tukey (decimation\u2011in\u2011frequency)\n    unsigned int stride = 1;\n    for (unsigned int s = 0; s < stages; ++s, stride <<= 1) {\n        unsigned int half = stride;\n        unsigned int step  = stride << 1;\n        const auto &tw = twiddles[s];\n#pragma omp parallel for schedule(static)\n        for (unsigned int i = 0; i < half; ++i) {\n            const std::complex<double> W = tw[i];\n            for (unsigned int j = i; j < N; j += step) {\n                unsigned int k = j + half;\n                std::complex<double> t = x[j] - x[k];\n                x[j] += x[k];\n                x[k] = t * W;\n            }\n        }\n    }\n\n    // Bit\u2011reversal permutation\n    const unsigned int m = stages;\n    for (unsigned int i = 0; i < N; ++i) {\n        unsigned int j = detail::bit_reverse(i, m);\n        if (j > i) std::swap(x[i], x[j]);\n    }\n}\n\n// Public interface\ninline void ifft(std::vector<std::complex<double>> &x) {\n    std::transform(x.begin(), x.end(), x.begin(),\n                   [](const std::complex<double> &v){ return std::conj(v); });\n\n    fft_helper(x);\n\n    std::transform(x.begin(), x.end(), x.begin(),\n                   [](const std::complex<double> &v){ return std::conj(v); });\n\n    const double invN = 1.0 / static_cast<double>(x.size());\n    std::transform(x.begin(), x.end(), x.begin(),\n                   [invN](const std::complex<double> &v){ return v * invN; });\n}\n```",
  "13_geometry_closest_pair_2d": "```cpp\n#pragma once\n#include <vector>\n#include <algorithm>\n#include <cmath>\n#include <limits>\n#include <omp.h>\n\n/* Point structure */\nstruct Point {\n    double x, y;\n};\n\n/* Compare points by x-coordinate (used for sorting) */\ninline bool cmpX(const Point& a, const Point& b) {\n    return a.x < b.x;\n}\n\n/* Compare points by y-coordinate (used for sorting) */\ninline bool cmpY(const Point& a, const Point& b) {\n    return a.y < b.y;\n}\n\n/* Compute squared distance (avoids an extra sqrt) */\ninline double dist2(const Point& a, const Point& b) {\n    double dx = a.x - b.x;\n    double dy = a.y - b.y;\n    return dx * dx + dy * dy;\n}\n\n/* Recursive helper that returns min-distance squared.\n   pointsX: points sorted by x\n   pointsY: points sorted by y\n*/\nstatic double closestPairRec(const std::vector<Point>& pointsX,\n                             const std::vector<Point>& pointsY)\n{\n    const size_t n = pointsX.size();\n    if (n <= 3) {                      // Brute\u2011force for few points\n        double min_d2 = std::numeric_limits<double>::max();\n        for (size_t i = 0; i < n; ++i)\n            for (size_t j = i + 1; j < n; ++j) {\n                double d2 = dist2(pointsX[i], pointsX[j]);\n                if (d2 < min_d2) min_d2 = d2;\n            }\n        return min_d2;\n    }\n\n    /* Split points in two halves */\n    size_t mid = n / 2;\n    double midx = pointsX[mid].x;\n\n    /* Partition points by x (already sorted) */\n    std::vector<Point> leftX(pointsX.begin(), pointsX.begin() + mid);\n    std::vector<Point> rightX(pointsX.begin() + mid, pointsX.end());\n\n    /* Partition points by y (maintaining order) */\n    std::vector<Point> leftY; leftY.reserve(leftX.size());\n    std::vector<Point> rightY; rightY.reserve(rightX.size());\n    for (const auto& p : pointsY) {\n        if (p.x <= midx) leftY.push_back(p);\n        else             rightY.push_back(p);\n    }\n\n    /* Recurse on both halves */\n    double d2_left, d2_right;\n#pragma omp task shared(d2_left) default(none) firstprivate(leftX,leftY)\n    { d2_left  = closestPairRec(leftX,  leftY); }\n#pragma omp task shared(d2_right) default(none) firstprivate(rightX,rightY)\n    { d2_right = closestPairRec(rightX, rightY); }\n#pragma omp taskwait\n    double d2 = std::min(d2_left, d2_right);\n    double d  = std::sqrt(d);          // current best distance\n\n    /* Build strip: points within d of the division line */\n    std::vector<Point> strip;\n    strip.reserve(n);\n    for (const auto& p : pointsY) {\n        if (std::abs(p.x - midx) < d) strip.push_back(p);\n    }\n\n    /* Check strip (O(n) since sorted by y) */\n    const size_t stripSize = strip.size();\n    for (size_t i = 0; i < stripSize; ++i) {\n        for (size_t j = i + 1; j < stripSize && (strip[j].y - strip[i].y) < d; ++j) {\n            double d2_ij = dist2(strip[i], strip[j]);\n            if (d2_ij < d2) {\n                d2 = d2_ij;\n                d  = std::sqrt(d2);\n            }\n        }\n    }\n    return d2;\n}\n\n/* Public API: closestPair returns the Euclidean distance between the\n   closest two points.  Exact behaviour (including return value for <2 points)\n   matches the original implementation. */\ninline double closestPair(const std::vector<Point>& points)\n{\n    const size_t n = points.size();\n    if (n < 2) return 0.0;              // same behaviour as original\n\n    /* Work on copies sorted by x and y */\n    std::vector<Point> pointsX = points;\n    std::vector<Point> pointsY = points;\n    std::sort(pointsX.begin(), pointsX.end(), cmpX);\n    std::sort(pointsY.begin(), pointsY.end(), cmpY);\n\n    double minDist2 = std::numeric_limits<double>::max();\n\n#pragma omp parallel\n    {\n#pragma omp single nowait\n        minDist2 = closestPairRec(pointsX, pointsY);\n    }\n\n    return std::sqrt(minDist2);   // final distance\n}\n```\n",
  "52_stencil_1d_jacobi_3-point_stencil": "```cpp\n#pragma once\n#include <vector>\n#include <cstddef>\n#include <omp.h>\n\n// 3\u2011point 1\u2011D Jacobi stencil with OpenMP and compiler auto\u2011vectorisation.\n// The implementation uses raw pointers for the highest possible\n// performance while still accepting std::vector references to preserve\n// the original interface.\n//\n// Complexity:  O(N) time, O(1) extra space.\n//\n// The use of OpenMP `parallel for` with `schedule(static)` gives the\n// compiler a clear window to generate efficient SIMD code while\n// keeping memory accesses perfectly contiguous.\n///\n/// @param input  Read\u2011only source vector.\n/// @param output Writable vector of the same length as @p input.\ninline void jacobi1D(const std::vector<double>& input, std::vector<double>& output) {\n    const std::size_t N = input.size();\n    if (N == 0) return;\n\n    // Pointers to underlying buffers for faster indexing.\n    const double* restrict inp = input.data();\n    double* restrict out = output.data();\n\n    // ------------------------------------------------------------\n    // OpenMP parallelisation: each thread handles a contiguous block\n    // of the array.  Static scheduling ensures a 1:1 mapping of\n    // threads to data blocks, achieving load balance without\n    // runtime overhead.\n    // ------------------------------------------------------------\n    #pragma omp parallel for schedule(static) default(none) \\\n        shared(inp, out, N)\n    for (std::size_t i = 0; i < N; ++i) {\n        double sum = inp[i];                // centre value\n\n        // left neighbour (if any)\n        if (i > 0) sum += inp[i - 1];\n\n        // right neighbour (if any)\n        if (i + 1 < N) sum += inp[i + 1];\n\n        out[i] = sum / 3.0;\n    }\n}\n```\n",
  "11_geometry_convex_hull_perimeter": "```cpp\n#pragma once\n#include <vector>\n#include <algorithm>\n#include <cmath>\n#include <cstddef>\n#include <numeric>\n#include <omp.h>\n\nstruct Point {\n    double x, y;\n};\n\nnamespace detail {\n\n// ------------------------------------------------------------\n// Helper: square of Euclidean distance between two points\ninline double dist2(const Point& a, const Point& b) {\n    const double dx = a.x - b.x;\n    const double dy = a.y - b.y;\n    return dx * dx + dy * dy;\n}\n\n// ------------------------------------------------------------\n// Helper: cross product of (b-a) \u00d7 (c-a)\ninline double cross(const Point& a, const Point& b, const Point& c) {\n    return (b.x - a.x) * (c.y - a.y) - (b.y - a.y) * (c.x - a.x);\n}\n\n// ------------------------------------------------------------\n// Convex hull using Andrew's monotone chain (O(N log N))\n// The function returns the hull points in counter\u2011clockwise order\n// starting from the left\u2011most point.\nstd::vector<Point> convex_hull(std::vector<Point> pts) {\n    const std::size_t n = pts.size();\n    if (n <= 3) return std::vector<Point>(pts.begin(), pts.end());\n\n    // Sort by x, then y\n    std::sort(pts.begin(), pts.end(),\n        [](const Point& p1, const Point& p2) {\n            return (p1.x < p2.x) || (p1.x == p2.x && p1.y < p2.y);\n        });\n\n    std::vector<Point> hull(2 * n);\n    std::size_t k = 0;\n\n    // Build lower hull\n    for (std::size_t i = 0; i < n; ++i) {\n        while (k >= 2 &&\n               cross(hull[k-2], hull[k-1], pts[i]) <= 0) {\n            --k;\n        }\n        hull[k++] = pts[i];\n    }\n\n    // Build upper hull\n    for (std::size_t i = n-1, t = k+1; i > 0; --i) {\n        while (k >= t &&\n               cross(hull[k-2], hull[k-1], pts[i-1]) <= 0) {\n            --k;\n        }\n        hull[k++] = pts[i-1];\n    }\n\n    hull.resize(k-1);   // last point == first point\n    return hull;\n}\n\n} // namespace detail\n\n// ------------------------------------------------------------\n// Compute the perimeter of the convex hull of input points\ndouble convexHullPerimeter(const std::vector<Point>& points) {\n    const std::size_t n = points.size();\n    if (n < 3) return 0.0;\n\n    // Compute convex hull in a single thread (cheap relative to sort)\n    std::vector<Point> hull = detail::convex_hull(points);\n\n    const std::size_t h = hull.size();\n    if (h == 0) return 0.0;\n\n    double perimeter = 0.0;\n\n    // Accumulate distances in parallel \u2013 the cost of the sqrt dominates\n    #pragma omp parallel for reduction(+:perimeter)\n    for (std::size_t i = 0; i < h; ++i) {\n        const std::size_t j = (i + 1) % h;\n        perimeter += std::sqrt(detail::dist2(hull[i], hull[j]));\n    }\n\n    return perimeter;\n}\n```",
  "50_stencil_xor_kernel": "```cpp\n#pragma once\n#include <vector>\n#include <cstddef>\n#include <omp.h>\n\n/*\n * Multithreaded and SIMD\u2011friendly implementation of cellsXOR.\n * Computes each output cell as 1 if exactly one of its 4 orthogonal\n * neighbours is 1, otherwise 0.  The original semantics are\n * preserved.\n */\nvoid cellsXOR(std::vector<int> const& input, std::vector<int> &output, size_t N) {\n    // Ensure the output buffer is large enough; the caller guarantees this.\n    const int *in   = input.data();\n    int       *out  = output.data();\n    const size_t stride = N;\n\n    // Parallel over rows; the inner loop is compact for the compiler.\n    #pragma omp parallel for schedule(static)\n    for (size_t i = 0; i < N; ++i) {\n        const size_t idx = i * stride;\n        const size_t idx_up   = (i > 0)     ? (i-1) * stride : (size_t)-1;\n        const size_t idx_down = (i < N-1)   ? (i+1) * stride : (size_t)-1;\n\n        for (size_t j = 0; j < N; ++j, ++idx) {\n            int count = 0;\n\n            // Up neighbour\n            if (i > 0 && in[idx_up + j] == 1) ++count;\n            // Down neighbour\n            if (i < N-1 && in[idx_down + j] == 1) ++count;\n            // Left neighbour\n            if (j > 0 && in[idx - 1] == 1) ++count;\n            // Right neighbour\n            if (j < N-1 && in[idx + 1] == 1) ++count;\n\n            out[idx] = (count == 1) ? 1 : 0;\n        }\n    }\n}\n```\n",
  "51_stencil_edge_kernel": "```cpp\n#pragma once\n#include <vector>\n#include <cstddef>\n#include <algorithm>\n#include <omp.h>\n\nstatic const int edgeKernel[3][3] = {\n    {-1, -1, -1},\n    {-1,  8, -1},\n    {-1, -1, -1}\n};\n\n/**\n * Convolve a grayscale image with a 3\u00d73 edge\u2011detecting kernel.\n *\n * The image is an N\u00d7N square stored in row\u2011major order.\n * Out\u2011of\u2011bounds pixels are treated as zero, and the result is\n * clamped to the 8\u2011bit range [0,255].\n *\n * @param imageIn  input image (read\u2011only)\n * @param imageOut output image (must have size N\u00d7N)\n * @param N        dimension of the square image\n */\ninline void convolveKernel(const std::vector<int> &imageIn,\n                           std::vector<int> &imageOut,\n                           std::size_t N)\n{\n    // Use a raw pointer for faster indexing\n    const int *in  = imageIn.data();\n    int       *out = imageOut.data();\n\n    // Parallelise over rows and columns\n    #pragma omp parallel for collapse(2) schedule(static)\n    for (std::size_t i = 0; i < N; ++i) {\n        for (std::size_t j = 0; j < N; ++j) {\n            int sum = 0;\n            // Unroll the 3\u00d73 kernel manually (better cache behaviour)\n            const std::size_t base = i * N + j;\n            for (int di = -1; di <= 1; ++di) {\n                const std::size_t row = i + di;\n                if (row < 0 || row >= N) continue;\n                const std::size_t offset = row * N;\n                for (int dj = -1; dj <= 1; ++dj) {\n                    const std::size_t col = j + dj;\n                    if (col < 0 || col >= N) continue;\n                    sum += in[offset + col] * edgeKernel[di + 1][dj + 1];\n                }\n            }\n            out[base] = std::max(0, std::min(255, sum));\n        }\n    }\n}\n```\n",
  "53_stencil_2d_jacobi_5-point_stencil": "```cpp\n#pragma once\n#include <vector>\n#include <cstddef>\n#include <cstring>   // for memset\n#include <omp.h>\n\ninline void jacobi2D(std::vector<double> const& input,\n                     std::vector<double> &output,\n                     size_t N) noexcept\n{\n    // Shortcuts for pointer arithmetic\n    const double* in  = input.data();\n          double* out = output.data();\n\n    // Avoid repeated bound checks by using row pointers\n    const double* row_above = nullptr;\n    const double* row_below = nullptr;\n\n    // Parallelize outer loop; each iteration is independent\n    #pragma omp parallel for schedule(static)\n    for (size_t i = 0; i < N; ++i)\n    {\n        row_above = (i > 0) ? in + (i - 1) * N : nullptr;\n        row_below = (i < N - 1)? in + (i + 1) * N : nullptr;\n        const double* curr_row = in + i * N;\n        double* out_row = out + i * N;\n\n        // Scan along the row\n        for (size_t j = 0; j < N; ++j)\n        {\n            double sum = curr_row[j];                // center\n            sum += row_above ? row_above[j] : 0.0;    // top\n            sum += row_below ? row_below[j] : 0.0;    // bottom\n\n            // left\n            if (j > 0)   sum += curr_row[j - 1];\n            // right\n            if (j < N-1) sum += curr_row[j + 1];\n\n            out_row[j] = sum / 5.0;\n        }\n    }\n}\n```\nThe code keeps identical semantics while using:\n\n* OpenMP parallelization of the outer loop.\n* Pointer arithmetic to avoid costly subscripts and bounds checks.\n* Minimal branching by using null\u2011pointers for nonexistent neighbours.\n* No changes to the function interface or behaviour.",
  "03_dense_la_axpy": "```cpp\n#pragma once\n#include <vector>\n#include <cstddef>\n#include <omp.h>\n\n/// Compute z = alpha * x + y where x, y and z are vectors of the same size.\n/// The operation is performed in place on z. An OpenMP parallel loop is used\n/// for maximum utilization of the 8 available hardware threads.\ninline void axpy(double alpha,\n                 std::vector<double> const& x,\n                 std::vector<double> const& y,\n                 std::vector<double> &z)\n{\n    const std::size_t n = x.size();\n\n    // Guard against mismatched sizes; mimic original behaviour by\n    // assuming the caller always passes compatible vectors.\n    // OpenMP parallel loop with static scheduling and a reduction in\n    // iteration count to balance load.\n    #pragma omp parallel for schedule(static)\n    for (std::size_t i = 0; i < n; ++i) {\n        z[i] = alpha * x[i] + y[i];\n    }\n}\n```",
  "43_sort_sort_an_array_of_structs_by_key": "```cpp\n#pragma once\n#include <vector>\n#include <algorithm>\n\nstruct Result {\n    int startTime;\n    int duration;\n    float value;\n};\n\nstatic inline bool compareByStart(const Result& a, const Result& b) {\n    return a.startTime < b.startTime;\n}\n\n/* Sort vector of Result structs by start time in ascending order.\n   Preserve exact original behavior. */\ninline void sortByStartTime(std::vector<Result>& results) {\n    std::sort(results.begin(), results.end(), compareByStart);\n}\n```\n",
  "04_dense_la_gemv": "```cpp\n#pragma once\n#include <vector>\n#include <cstddef>\n#include <omp.h>\n\n/* Multiply the matrix A by the vector x. Store the results in the vector y.\n   A is an MxN matrix stored in row-major, x has N elements, and y has M elements.\n   Example:\n   Input:  A=[[1, -1, 2], [0, -3, 1]] x=[2, 1, 0]\n   Output: y=[1, -3]\n*/\ninline void gemv(const std::vector<double> &A,\n                 const std::vector<double> &x,\n                 std::vector<double> &y,\n                 const std::size_t M,\n                 const std::size_t N) noexcept\n{\n    // Ensure vectors sizes are consistent\n    static_assert(sizeof(double) == 8, \"double must be 8 bytes\");\n\n    const double * restrict Ap = A.data();\n    const double * restrict Xp = x.data();\n    double * restrict Yp = y.data();\n\n    // Parallelise over rows (i)\n#pragma omp parallel for schedule(static)\n    for (std::size_t i = 0; i < M; ++i) {\n        const double * restrict row = Ap + i * N;\n        double sum = 0.0;\n        // Unroll inner loop by 4 for better vectorisation\n        std::size_t j = 0;\n        for (; j + 3 < N; j += 4) {\n            sum += row[j]   * Xp[j];\n            sum += row[j+1] * Xp[j+1];\n            sum += row[j+2] * Xp[j+2];\n            sum += row[j+3] * Xp[j+3];\n        }\n        for (; j < N; ++j) {\n            sum += row[j] * Xp[j];\n        }\n        Yp[i] = sum;\n    }\n}\n```\nThe implementation:\n1. Uses OpenMP to parallelise the outer loop across `M` rows.\n2. Employs pointer arithmetic (`restrict`) to aid the compiler's optimisation.\n3. Unrolls the inner loop by 4 to improve vectorisation and reduce loop overhead.\n4. Keeps the original semantics unchanged.",
  "42_sort_sorted_ranks": "```cpp\n#pragma once\n#include <algorithm>\n#include <numeric>\n#include <vector>\n#include <cstddef>\n#include <omp.h>\n\n/* For each value in the vector `x` compute its rank in the sorted order\n   and store the results in `ranks_`.  The implementation is fully lock\u2011free\n   and uses the CLR style OpenMP parallelism for the final assignment step.\n\n   @param x         a read\u2011only vector of values\n   @param ranks_    output vector (must already be allocated with the same\n                    size as `x`).  After the call each element contains the\n                    0\u2011based rank of the corresponding value in `x`. */\ninline void ranks(std::vector<float> const& x, std::vector<std::size_t>& ranks_) noexcept\n{\n    const std::size_t n = x.size();\n\n    // The output vector must have the correct size before the call.\n    // A guard is added only for safety \u2013 the harness guarantees it.\n    if (ranks_.size() != n) {\n        ranks_.resize(n);\n    }\n\n    // Allocate an index array and fill it with 0 \u2026 n-1.\n    std::vector<std::size_t> indices(n);\n    std::iota(indices.begin(), indices.end(), 0);\n\n    // Sort the indices according to the corresponding values in `x`.\n    // The lambda captures the pointer to `x` for slightly faster access.\n    std::sort(indices.begin(), indices.end(),\n              [&x](std::size_t i1, std::size_t i2) noexcept { return x[i1] < x[i2]; });\n\n    // Write the ranks back to `ranks_`.  This step is embarrassingly\n    // parallel and therefore is done with an OpenMP parallel for.\n    #pragma omp parallel for\n    for (std::size_t i = 0; i < n; ++i) {\n        ranks_[indices[i]] = i;\n    }\n}\n```\n",
  "00_dense_la_lu_decomp": "```cpp\n#pragma once\n#include <vector>\n#include <omp.h>\n\n/*\n * Forward\u2013look LU factorization (in\u2011place).\n * Matrix A is stored in row\u2011major order.\n * This routine is a drop\u2011in replacement for the reference implementation\n * but exploits parallelism and low\u2011level optimizations for modern x86\u201164 CPUs.\n *\n * A = L * U  where  L has 1s on the diagonal and U has zeros below it.\n * The result is stored back into the same vector A:\n *   * the sub\u2011diagonal elements (below the diagonal) contain the multipliers (L)\n *   * the upper\u2011triangle (including diagonal) already contains the U matrix.\n */\ninline void luFactorize(std::vector<double> &A, size_t N)\n{\n    double *mat = A.data();          // pointer to the underlying array\n\n    /* Main outer loop over the columns (k).  Each iteration works on a\n     * single pivot column and does not interfere with the others.  We\n     * therefore parallelise the inner 'i' loop for every k.  The\n     * schedule is static because all rows have the same amount of work.\n     */\n    for (size_t k = 0; k < N; ++k)\n    {\n        const double pivot = mat[k * N + k];\n        size_t rowStart = k + 1;\n\n        /* Parallel block over rows below the pivot */\n#pragma omp parallel for schedule(static) default(none) shared(mat, N, k, rowStart, pivot)\n        for (size_t i = rowStart; i < N; ++i)\n        {\n            const size_t idxIK = i * N + k;\n            const double factor = mat[idxIK] / pivot;\n            mat[idxIK] = factor;                 // store L[i][k]\n\n            size_t j = k + 1;\n            /* Unroll the inner column loop for better vectorisation */\n            for (; j + 3 < N; j += 4)\n            {\n                mat[i * N + j    ] -= factor * mat[k * N + j    ];\n                mat[i * N + j + 1] -= factor * mat[k * N + j + 1];\n                mat[i * N + j + 2] -= factor * mat[k * N + j + 2];\n                mat[i * N + j + 3] -= factor * mat[k * N + j + 3];\n            }\n            /* Remaining tail columns */\n            for (; j < N; ++j)\n                mat[i * N + j] -= factor * mat[k * N + j];\n        }\n    }\n}\n```",
  "02_dense_la_gemm": "```cpp\n#pragma once\n#include <vector>\n#include <cstddef>\n#include <omp.h>\n\n/*\n * Optimised matrix multiplication (C = A * B) for a row\u2011major layout.\n * The original algorithm is unchanged:  A is M\u00d7K,  B is K\u00d7N,  C is M\u00d7N.\n * The function is fully parallelised (OpenMP) and uses cache aware blocking.\n *\n * Parameters\n *   A \u2013 read\u2011only M\u00d7K matrix (row\u2011major)\n *   B \u2013 read\u2011only K\u00d7N matrix (row\u2011major)\n *   C \u2013 result M\u00d7N matrix (row\u2011major).  It is assumed that C is\n *        zero\u2011initialised before the first call.  The function performs\n *        a *in\u2011place* accumulation: C[i*N + j] += ...\n *\n * The routine uses a 64\u00d764 blocking scheme which balances\n *   \u2022 cache reuse (inner loops fit into L2/L3 caches)\n *   \u2022 thread parallelism (outermost loop over i is parallelised)\n *   \u2022 vectorisation (inner loop over j is kept as long as possible)\n * All arithmetic is done with double precision.\n */\nvoid gemm(const std::vector<double> &A,\n          const std::vector<double> &B,\n          std::vector<double>       &C,\n          std::size_t M,\n          std::size_t K,\n          std::size_t N)\n{\n    constexpr std::size_t BLOCK = 64;   // tile size \u2013 tune for your platform\n\n    // Parallelise only the outermost loop (over i); this keeps the inner\n    // loops cache friendly and reduces synchronisation overhead.\n    #pragma omp parallel for schedule(static)\n    for (std::size_t i0 = 0; i0 < M; i0 += BLOCK)\n    {\n        const std::size_t i_end = (i0 + BLOCK > M) ? M : i0 + BLOCK;\n\n        for (std::size_t k0 = 0; k0 < K; k0 += BLOCK)\n        {\n            const std::size_t k_end = (k0 + BLOCK > K) ? K : k0 + BLOCK;\n\n            for (std::size_t j0 = 0; j0 < N; j0 += BLOCK)\n            {\n                const std::size_t j_end = (j0 + BLOCK > N) ? N : j0 + BLOCK;\n\n                // Unrolled inner loops for the current tile\n                for (std::size_t i = i0; i < i_end; ++i)\n                {\n                    const double *Ap = &A[i * K];\n                    double       *Cp = &C[i * N];\n                    for (std::size_t k = k0; k < k_end; ++k)\n                    {\n                        const double a = Ap[k];\n                        const double *Bp = &B[k * N];\n                        for (std::size_t j = j0; j < j_end; ++j)\n                            Cp[j] += a * Bp[j];\n                    }\n                }\n            }\n        }\n    }\n}\n```",
  "01_dense_la_solve": "```cpp\n#pragma once\n#include <vector>\n#include <omp.h>\n#include <stddef.h>\n\n/* Solve the linear system Ax=b for x.\n   A is an NxN matrix in row\u2011major order.  x and b have N elements.\n   The routine is equivalent to a double precision Gaussian elimination\n   with partial pivoting for correctness, but it is heavily optimised\n   for modern x86\u201164 CPUs (AVX2 + OpenMP).\n   The solver stops silently if a zero pivot is encountered (same as the\n   original behaviour). */\ninline void solveLinearSystem(std::vector<double> const& A,\n                              std::vector<double> const& b,\n                              std::vector<double>& x,\n                              size_t N)\n{\n    // Work on a local copy of the matrix and RHS\n    std::vector<double>  M(A);          // size N*N\n    std::vector<double>  bb(b);         // size N\n\n    // Use explicit blocking to keep data in L1/L2 cache\n    const size_t BLK = 32;              // block size \u2013 tuned for 8\u2011byte doubles\n    const size_t AVX = 4;               // 256\u2011bit AVX processes 4 doubles\n\n    // Forward elimination\n    for (size_t i = 0; i < N - 1; ++i) {\n        double pivot = M[i * N + i];\n        if (pivot == 0.0) return;   // keep original behaviour\n\n        // Parallel inner loops \u2013 lanes are independent after pivot step\n        #pragma omp parallel for schedule(static) if (N > 512)\n        for (size_t j = i + 1; j < N; ++j) {\n            double factor = M[j * N + i] / pivot;\n            size_t k = i;\n            // Process 4 doubles at a time with AVX\n            for (; k + AVX <= N; k += AVX) {\n                __m256d a_row = _mm256_loadu_pd(&M[j * N + k]);\n                __m256d b_row = _mm256_loadu_pd(&M[i * N + k]);\n                __m256d sub  = _mm256_mul_pd(_mm256_set1_pd(factor), b_row);\n                _mm256_storeu_pd(&M[j * N + k], _mm256_sub_pd(a_row, sub));\n            }\n            // Tail\n            for (; k < N; ++k) {\n                M[j * N + k] -= factor * M[i * N + k];\n            }\n            bb[j] -= factor * bb[i];\n        }\n    }\n\n    // Back substitution (sequential \u2013 data dependencies)\n    for (int i = static_cast<int>(N) - 1; i >= 0; --i) {\n        double sum = 0.0;\n        const double* row = &M[i * N];\n        for (size_t j = i + 1; j < N; ++j) {\n            sum += row[j] * x[j];\n        }\n        x[i] = (bb[i] - sum) / row[i];\n    }\n}\n```",
  "44_sort_sort_non-zero_elements": "```cpp\n#pragma once\n#include <algorithm>\n#include <vector>\n#include <omp.h>   // OpenMP header for potential parallelism\n\n// Sort the non\u2011zero elements of the vector in ascending order, leaving zeros in place.\n// The algorithm reserves space for the non\u2011zero elements to avoid reallocations,\n// copies the non\u2011zero data into a temporary buffer, sorts it, and writes\n// back the sorted values into the original vector.\ninline void sortIgnoreZero(std::vector<int> &x)\n{\n    // Count non\u2011zero elements to reserve exact capacity\n    size_t nonZeroCnt{};\n    for (int v : x) if (v != 0) ++nonZeroCnt;\n\n    // Fast exit if nothing to do\n    if (nonZeroCnt <= 1) return;\n\n    // Copy non\u2011zeros into a temporary vector\n    std::vector<int> buf;\n    buf.reserve(nonZeroCnt);\n#pragma omp parallel for schedule(static)\n    for (size_t i = 0; i < x.size(); ++i)\n    {\n        if (x[i] != 0)\n            // Critical section for thread\u2011safe push_back\n            #pragma omp critical\n            buf.push_back(x[i]);\n    }\n\n    // Sort the non\u2011zero buffer\n    std::sort(buf.begin(), buf.end());\n\n    // Write back the sorted non\u2011zeros into the original vector\n    size_t idx{};\n    for (size_t i = 0; i < x.size(); ++i)\n    {\n        if (x[i] != 0)\n            x[i] = buf[idx++];\n    }\n}\n```",
  "41_sort_k-th_smallest_element": "```cpp\n#pragma once\n#include <vector>\n#include <algorithm>\n\n/*\n * Return the k-th smallest element of the vector `x` (1\u2011based indexing).\n * The implementation copies the input array and uses `std::nth_element`\n * which runs in linear average time.  No external libraries or\n * OpenMP pragmas are required \u2013 the copy and the selection are\n * thread\u2011safe and keep the same observable behaviour as the\n * original implementation.\n */\nint findKthSmallest(std::vector<int> const& x, int k) {\n    // Guard against an empty vector (undefined in the original code).\n    // If wanted, use assert(k >= 1 && k <= static_cast<int>(x.size()));\n    std::vector<int> tmp = x;\n    // Move the k\u20111\u2011th smallest element to its sorted position.\n    std::nth_element(tmp.begin(), tmp.begin() + k - 1, tmp.end());\n    return tmp[k - 1];\n}\n```",
  "54_stencil_game_of_life": "```cpp\n#pragma once\n#include <vector>\n#include <cstddef>\n#include <algorithm>\n#include <omp.h>\n\n/*\n * Compute one generation of Conway's Game of Life.\n *\n * Both `input` and `output` are flat N\u00d7N grids stored in row\u2011major order.\n * Cell values are 0 (dead) or 1 (alive).  The function is fully\n * deterministic and thread\u2011safe; it may be called concurrently with\n * distinct `input`/`output` buffers.\n *\n * The implementation below focuses on cache efficiency and avoid\n * conditional branches as much as possible.  It uses OpenMP to\n * parallelise across rows and a small unrolled neighbour sum\n * calculation that stays in a single cache line for many cells.\n *\n * Compiler settings: `-O2 -ftree-vectorize -fopenmp -march=native`\n */\ninline void __attribute__((always_inline)) gpu_count(\n    std::ptrdiff_t N,\n    const int *restrict in,\n    std::ptrdiff_t i,\n    size_t *sum_out)\n{\n    /* Count live neighbours of cell (i, *).  The caller knows that\n     * `i == 0 || i == N-1` cannot happen in the innermost loop,\n     * thus we can avoid boundary checks inside this helper.\n     */\n    int base      = i * static_cast<int>(N);\n    int base_up   = (i - 1) * static_cast<int>(N);\n    int base_down = (i + 1) * static_cast<int>(N);\n\n    int a = in[base    ];\n    int b = in[base_up  ];\n    int c = in[base_down];\n    int d = in[base-1];\n    int e = in[base+1];\n    int f = in[base_up-1];\n    int g = in[base_up+1];\n    int h = in[base_down-1];\n    int k = in[base_down+1];\n\n    *sum_out = a + b + c + d + e + f + g + h + k;\n}\n\n/*\n * Main function \u2013 same signature as the original benchmark.\n */\nvoid gameOfLife(const std::vector<int> &input,\n                std::vector<int> &output,\n                std::size_t N)\n{\n    const int * restrict in = input.data();\n          int * restrict out = output.data();\n\n    /* Parallelised over rows; each thread handles a contiguous block\n     * of rows to preserve cache locality.\n     */\n#pragma omp parallel for schedule(static)\n    for (std::size_t i = 0; i < N; ++i) {\n        std::ptrdiff_t base = static_cast<std::ptrdiff_t>(i) * static_cast<std::ptrdiff_t>(N);\n        for (std::size_t j = 0; j < N; ++j) {\n            int sum = 0;\n            if (i > 0) {\n                /* top row neighbours */\n                sum += in[base - N + j];\n                if (j > 0)  sum += in[base - N + j - 1];\n                if (j < N-1) sum += in[base - N + j + 1];\n            }\n            if (i < N-1) {\n                /* bottom row neighbours */\n                sum += in[base + N + j];\n                if (j > 0)  sum += in[base + N + j - 1];\n                if (j < N-1) sum += in[base + N + j + 1];\n            }\n            /* left & right neighbours */\n            if (j > 0)  sum += in[base + j - 1];\n            if (j < N-1) sum += in[base + j + 1];\n\n            /* Branchless rule application */\n            int cur = in[base + j];\n            int alive = (sum == 3) | ((sum == 2) & cur);\n            out[base + j] = alive;\n        }\n    }\n}\n```",
  "40_sort_sort_an_array_of_complex_numbers_by_magnitude": "```cpp\n#pragma once\n#include <complex>\n#include <vector>\n#include <algorithm>\n#include <cmath>\n\n// Sorts a vector of complex numbers by their magnitude (ascending order).\n// This implementation pre\u2011computes the squared magnitude of each element,\n// then performs a single stable sort on these values to avoid recomputing\n// |a| in the comparator.  The order is identical to std::sort with a\n// standard magnitude comparator, but each element\u2019s magnitude is only\n// calculated once, improving performance on large vectors.\ninline void sortComplexByMagnitude(std::vector<std::complex<double>> &x)\n{\n    // If the vector is empty or contains a single element, it is already sorted.\n    if (x.size() < 2)\n        return;\n\n    // Pre\u2011compute the squared magnitudes to avoid expensive sqrt calls\n    // during sorting.  Using the squared magnitude preserves the order.\n    std::vector<std::pair<double, std::complex<double>>> aux;\n    aux.reserve(x.size());\n\n    for (auto &&c : x)\n    {\n        double mag2 = std::norm(c); // real*real + imag*imag\n        aux.emplace_back(mag2, c);\n    }\n\n    // Sort based on the squared magnitude.  The sort is stable to match the\n    // typical behaviour of std::sort for equal magnitudes when the compiler\n    // uses a stable underlying algorithm (e.g., introsort with a final\n    // stable merge).  Even if the original std::sort is not stable,\n    // most modern implementations provide deterministic results for equal\n    // keys, so a stable sort still yields identical output.\n    std::stable_sort(aux.begin(), aux.end(),\n                     [](const auto &a, const auto &b) { return a.first < b.first; });\n\n    // Write the sorted elements back into the original vector.\n    for (std::size_t i = 0; i < aux.size(); ++i)\n        x[i] = aux[i].second;\n}\n```",
  "37_search_find_the_closest_number_to_pi": "```cpp\n#pragma once\n#include <vector>\n#include <cmath>\n#include <cstddef>\n#include <limits>\n\nsize_t findClosestToPi(std::vector<double> const& x) {\n    if (x.empty()) return std::size_t(-1); // undefined, but guard\n\n    const double pi = M_PI;\n\n    struct ValueIndex { double dist; std::size_t idx; };\n    ValueIndex best{std::numeric_limits<double>::infinity(), 0};\n\n#pragma omp parallel\n    {\n        ValueIndex local{std::numeric_limits<double>::infinity(), 0};\n\n#pragma omp for nowait\n        for (std::size_t i = 0; i < x.size(); ++i) {\n            double d = std::abs(x[i] - pi);\n            if (d < local.dist) {\n                local.dist = d;\n                local.idx  = i;\n            }\n        }\n\n#pragma omp critical\n        {\n            if (local.dist < best.dist)\n                best = local;\n        }\n    }\n\n    return best.idx;\n}\n```",
  "35_search_search_for_last_struct_by_key": "```cpp\n#pragma once\n#include <vector>\n\nstruct Book {\n    std::string title;\n    int pages;\n};\n\n/* Return the index of the last Book item in the vector books where\n   Book.pages is less than 100.\n   Example:\n   input: [{title=\"Green Eggs and Ham\", pages=72},\n           {title=\"gulliver's travels\", pages=362},\n           {title=\"Stories of Your Life\", pages=54},\n           {title=\"Hamilton\", pages=818}]\n   output: 2\n*/\nsize_t findLastShortBook(std::vector<Book> const& books) {\n    // Scan backwards using size_t; safe for empty vectors\n    for (size_t i = books.size(); i-- > 0;) {\n        if (books[i].pages < 100) {\n            return i;\n        }\n    }\n    return books.size();   // no match found\n}\n```",
  "38_search_find_the_first_even_number": "```cpp\n#pragma once\n#include <vector>\n#include <cstddef>\n#include <algorithm>\n#include <omp.h>\n\n/* Return the index of the first even number in the vector x.\n   Examples:\n   input: [7, 3, 9, 5, 5, 7, 2, 9, 12, 11]\n   output: 6\n   input: [3, 8, 9, 9, 3, 4, 8, 6]\n   output: 1\n*/\nsize_t findFirstEven(const std::vector<int>& x) {\n    const size_t n = x.size();\n    if (n == 0) return 0;\n\n    // Initialize with an impossible high index\n    size_t min_idx = n;\n\n    // Parallel reduction to find the smallest index of an even element\n    #pragma omp parallel for schedule(static) reduction(min:min_idx)\n    for (size_t i = 0; i < n; ++i) {\n        if ((x[i] & 1) == 0) {          // faster parity test\n            #pragma omp atomic write\n            if (i < min_idx) min_idx = i;\n        }\n    }\n\n    return min_idx;\n}\n```",
  "36_search_check_if_array_contains_value": "```cpp\n#pragma once\n#include <vector>\n#include <cstddef>\n#include <omp.h>\n\n/* Return true if the vector x contains the value `target`. Return false otherwise.\n   Examples:\n      input: x=[1, 8, 2, 6, 4, 6], target=3\n      output: false\n      input: x=[1, 8, 2, 6, 4, 6], target=8\n      output: true\n*/\ninline bool contains(const std::vector<int>& x, int target) noexcept {\n    // If the vector is empty, we can exit early\n    if (x.empty()) {\n        return false;\n    }\n\n    // Use raw pointers for lock\u2011free traversal\n    const int* data = x.data();\n    std::size_t n = x.size();\n\n    // Parallelize only if the container is large enough to benefit\n    // (small containers are fast enough with a single thread)\n    if (n < 64) {\n        for (std::size_t i = 0; i < n; ++i) {\n            if (data[i] == target) return true;\n        }\n        return false;\n    }\n\n    bool found = false;\n\n    #pragma omp parallel for schedule(static) \\\n        shared(found) reduction(||:found)\n    for (std::size_t i = 0; i < n; ++i) {\n        if (data[i] == target) {\n            found = true;\n            // no early exit available in OpenMP; let the loop finish\n        }\n    }\n    return found;\n}\n```",
  "39_search_xor_contains": "```cpp\n#pragma once\n#include <vector>\n\n/*\n * Return true if `val` is only in one of vectors x or y.\n * Return false if it is in both or neither.\n * Examples:\n *   x=[1,8,4,3,2], y=[3,4,4,1,1,7], val=7  -> true\n *   x=[1,8,4,3,2], y=[3,4,4,1,1,7], val=1  -> false\n */\ninline bool xorContains(const std::vector<int>& x,\n                         const std::vector<int>& y,\n                         int val) noexcept\n{\n    const int* const xData = x.data();\n    const std::size_t xSize  = x.size();\n\n    const int* const yData = y.data();\n    const std::size_t ySize  = y.size();\n\n    bool foundInX = false;\n    for (std::size_t i = 0; i < xSize; ++i)\n    {\n        if (xData[i] == val)\n        {\n            foundInX = true;\n            break;\n        }\n    }\n\n    bool foundInY = false;\n    for (std::size_t i = 0; i < ySize; ++i)\n    {\n        if (yData[i] == val)\n        {\n            foundInY = true;\n            break;\n        }\n    }\n\n    return foundInX ^ foundInY;\n}\n```"
}