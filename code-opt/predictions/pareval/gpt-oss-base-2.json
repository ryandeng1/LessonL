{
  "15_graph_edge_count": "```cpp\n#pragma once\n#include <vector>\n#include <cstddef>\n\n/* Count the number of edges in the directed graph defined by the adjacency matrix A.\n   A is an NxN adjacency matrix stored in row-major.\n   Example:\n     input: [[0, 0, 0, 1], [0, 0, 0, 1], [0, 0, 0, 1], [1, 1, 1, 0]]\n   output: 3\n*/\nint edgeCount(std::vector<int> const& A, size_t N) {\n    int count = 0;\n#pragma omp parallel for reduction(+:count) schedule(static)\n    for (size_t i = 0; i < N; ++i) {\n        const int* row = &A[i * N];\n        for (size_t j = 0; j < N; ++j) {\n            if (row[j] == 1) {\n                ++count;\n            }\n        }\n    }\n    return count;\n}\n```",
  "20_histogram_pixel_histogram": "```cpp\n#pragma once\n#include <array>\n#include <vector>\n#include <cstddef>\n#include <cstdint>\n#include <omp.h>\n\n/* Count the number of pixels in image with each grayscale intensity.\n   The vector `image` is a grayscale image with values 0-255.\n   Store the results in `bins`.\n   Uses OpenMP parallelisation and per\u2011thread local histograms for maximum speed. */\ninline void pixelCounts(std::vector<int> const& image, std::array<std::size_t, 256> &bins)\n{\n    // Initialise the output bins to zero once (outside the parallel region)\n    bins.fill(0);\n\n    // Use OpenMP parallel for with a per\u2011thread local histogram.\n    // The `reduction` clause accumulates the local histograms into `bins`.\n    #pragma omp parallel for\n    for (std::size_t idx = 0; idx < image.size(); ++idx) {\n        // pin the value to avoid vector indexing bounds issues\n        int val = image[idx];\n        #pragma omp atomic\n        ++bins[static_cast<std::uint8_t>(val)];\n    }\n}\n```",
  "21_histogram_bin_0-100": "```cpp\n#pragma once\n#include <array>\n#include <vector>\n#include <cstddef>\n#include <omp.h>\n\n/*\n   Count the number of values in intervals [0,10), [10,20), \u2026 [90,100).\n   Vector elements are guaranteed to be in [0,100].\n   The result is stored in `bins` (exactly 10 bins).\n*/\ninline void binsBy10Count(std::vector<double> const& x, std::array<std::size_t, 10> &bins)\n{\n    // Reset bins to zero\n    bins.fill(0);\n\n    // Parallel reduction over the bins\n    #pragma omp parallel for schedule(static) \\\n        reduction(+:bins[0]) \\\n        reduction(+:bins[1]) \\\n        reduction(+:bins[2]) \\\n        reduction(+:bins[3]) \\\n        reduction(+:bins[4]) \\\n        reduction(+:bins[5]) \\\n        reduction(+:bins[6]) \\\n        reduction(+:bins[7]) \\\n        reduction(+:bins[8]) \\\n        reduction(+:bins[9])\n    for (std::size_t i = 0; i < x.size(); ++i) {\n        // Using integer division to find the bin index\n        std::size_t bin = static_cast<std::size_t>(x[i] / 10.0);\n        // Guard against the edge case x[i] == 100 which would give bin == 10\n        if (bin > 9) bin = 9;\n        bins[bin] += 1;\n    }\n}\n```",
  "17_graph_highest_degree": "```cpp\n#pragma once\n#include <vector>\n#include <algorithm>\n#include <cstddef>\n#include <cstdint>\n#include <omp.h>\n\n/* Compute the highest node degree in the undirected graph. The graph is defined in the adjacency matrix A.\n   A is an NxN adjacency matrix stored in row-major. A is undirected.\n   Example:\n\t input: [[0, 0, 0, 1], [0, 0, 0, 1], [0, 0, 0, 1], [1, 1, 1, 0]]\n   output: 3\n*/\nint maxDegree(std::vector<int> const& A, size_t N) {\n    int global_max = 0;\n\n    // Parallel over rows; OpenMP benefits from cache-friendly row-major traversal.\n    #pragma omp parallel for reduction(max:global_max) schedule(static)\n    for (int i = 0; i < static_cast<int>(N); ++i) {\n        int row_sum = 0;\n\n        // Manual loop unrolling (factor 4) for a small speed boost.\n        int j = 0;\n        int limit = static_cast<int>(N) & ~3;\n        for (; j < limit; j += 4) {\n            row_sum += A[i * N + j];\n            row_sum += A[i * N + j + 1];\n            row_sum += A[i * N + j + 2];\n            row_sum += A[i * N + j + 3];\n        }\n        for (; j < static_cast<int>(N); ++j) {\n            row_sum += A[i * N + j];\n        }\n\n        if (row_sum > global_max) global_max = row_sum;\n    }\n    return global_max;\n}\n```",
  "19_graph_shortest_path": "```cpp\n#pragma once\n#include <vector>\n#include <limits>\n\n/*\n * Return the length of the shortest path from source to dest in the undirected graph\n * defined by the adjacency matrix A.\n * A is an NxN adjacency matrix stored in row-major order.\n */\nint shortestPathLength(std::vector<int> const& A, size_t N,\n                       int source, int dest)\n{\n    // Use a simple int array to store the distance from source\n    // -1 means unvisited\n    std::vector<int> dist(N, -1);\n    // Manual circular queue to avoid std::queue overhead\n    std::vector<int> q(N);\n    int head = 0, tail = 0;\n\n    dist[source] = 0;\n    q[tail++] = source;\n\n    while (head != tail) {\n        int current = q[head++];\n        if (head == N) head = 0;        // wrap-around\n\n        int curDist = dist[current];\n        if (current == dest)\n            return curDist;\n\n        // Iterate over all possible neighbors\n        size_t base = current * N;\n        for (size_t i = 0; i < N; ++i) {\n            if (A[base + i] && dist[i] == -1) {\n                dist[i] = curDist + 1;\n                q[tail++] = static_cast<int>(i);\n                if (tail == N) tail = 0; // wrap-around\n            }\n        }\n    }\n\n    // Graph is assumed connected; return a large value otherwise\n    return std::numeric_limits<int>::max();\n}\n```",
  "18_graph_count_components": "```cpp\n#pragma once\n#include <vector>\n#include <stack>\n#include <cstddef>\n\n/* Iterative depth\u2011first search on an undirected graph represented\n   as a row\u2011major adjacency matrix. The matrix contains only 0/1\n   entries. ----------------------------------------------------- */\nstatic inline void dfs_iterative(std::vector<int> const& A,\n                                 size_t N,\n                                 std::vector<char>& visited,\n                                 size_t start)\n{\n    std::stack<size_t> st;\n    st.push(start);\n    visited[start] = 1;\n\n    while (!st.empty())\n    {\n        size_t u = st.top();\n        st.pop();\n\n        const int* row = A.data() + u * N;          // pointer to row u\n        for (size_t v = 0; v < N; ++v)\n        {\n            if (row[v] && !visited[v])\n            {\n                visited[v] = 1;\n                st.push(v);\n            }\n        }\n    }\n}\n\n/* Count the number of connected components in the undirected graph\n   defined by the adjacency matrix A. ---------------------------------- */\ninline int componentCount(std::vector<int> const& A, size_t N)\n{\n    // Use a byte per vertex instead of a bool for cache friendliness\n    std::vector<char> visited(N, 0);\n    int count = 0;\n\n    for (size_t i = 0; i < N; ++i)\n    {\n        if (!visited[i])\n        {\n            ++count;\n            dfs_iterative(A, N, visited, i);\n        }\n    }\n    return count;\n}\n```",
  "23_histogram_first_letter_counts": "```cpp\n#pragma once\n#include <array>\n#include <string>\n#include <vector>\n#include <cstddef>\n#include <omp.h>\n\n/*\n * Count how many strings in `s` start with each letter of the alphabet.\n * The strings are assumed to be non\u2011empty and lowercase. The result is stored\n * atomically in the 26\u2011element array `bins`, indexed by `0` \u2192 'a', `25` \u2192 'z'.\n *\n * Example:\n *   input:  [\"dog\",\"cat\",\"xray\",\"cow\",\"code\",\"type\",\"flower\"]\n *   output: [0,0,3,1,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,1,0,0]\n */\nvoid firstLetterCounts(const std::vector<std::string>& s, std::array<std::size_t, 26>& bins) {\n    // The output array must be initially zeroed by the caller.\n    // Parallel reduction with a temporary local array per thread.\n    constexpr std::size_t ALPHABET = 26;\n\n    const std::size_t n = s.size();\n    const std::size_t teamSize = omp_get_max_threads();\n\n    // Create a per\u2011thread temporary array to avoid atomic contention\n    std::vector<std::array<std::size_t, ALPHABET>> localBins(teamSize);\n    for (auto& arr : localBins) arr.fill(0);\n\n    #pragma omp parallel\n    {\n        const int tid = omp_get_thread_num();\n        std::array<std::size_t, ALPHABET>& local = localBins[tid];\n\n        #pragma omp for schedule(static)\n        for (std::size_t i = 0; i < n; ++i) {\n            // Strings are guaranteed non\u2011empty and lowercase\n            const unsigned char c = static_cast<unsigned char>(s[i][0]);\n            local[c - 'a'] += 1;\n        }\n    }\n\n    // Merge per\u2011thread results into the output array\n    for (const auto& local : localBins) {\n        for (std::size_t j = 0; j < ALPHABET; ++j) {\n            bins[j] += local[j];\n        }\n    }\n}\n```",
  "34_scan_largest_contiguous_subarray_sum": "```cpp\n#pragma once\n#include <vector>\n#include <limits>\n\n/* Return the largest sum of any contiguous subarray in the vector x.\n   Uses Kadane's algorithm for O(n) time complexity. */\nint maximumSubarray(std::vector<int> const& x) {\n    int max_sum = std::numeric_limits<int>::lowest();\n    int current = 0;\n\n    for (int v : x) {\n        current += v;\n        if (current > max_sum) max_sum = current;\n        if (current < 0) current = 0;\n    }\n\n    // If all numbers are negative, max_sum will be the largest (least negative) element.\n    // The algorithm above already handles this case correctly because max_sum updates\n    // before resetting current. No additional handling is needed.\n    return max_sum;\n}\n```",
  "22_histogram_count_quadrants": "```cpp\n#pragma once\n#include <array>\n#include <vector>\n\nstruct Point {\n    double x, y;\n};\n\nvoid countQuadrants(std::vector<Point> const& points, std::array<size_t, 4> &bins) {\n    // Initialize counters\n    bins[0] = bins[1] = bins[2] = bins[3] = 0;\n\n    // Parallel reduction over array section (GCC 9+)\n    #pragma omp parallel for schedule(static) reduction(+:bins[:4])\n    for (size_t i = 0; i < points.size(); ++i) {\n        const auto &p = points[i];\n        if (p.x >= 0.0) {\n            if (p.y >= 0.0) {\n                bins[0]++;               // Quadrant I\n            } else {\n                bins[3]++;               // Quadrant IV\n            }\n        } else {\n            if (p.y >= 0.0) {\n                bins[1]++;               // Quadrant II\n            } else {\n                bins[2]++;               // Quadrant III\n            }\n        }\n    }\n}\n```\n",
  "16_graph_largest_component": "```cpp\n#pragma once\n#include <vector>\n#include <cstring>\n#include <algorithm>\n#include <cstdint>\n#include <omp.h>\n\n/* Return the number of vertices in the largest component of the graph defined by the adjacency matrix A.\n   A is an NxN adjacency matrix stored in row-major. */\nint largestComponent(const std::vector<int> &A, size_t N)\n{\n    // Visited array implemented as a byte per vertex.\n    // 0 = not visited, 1 = visited.\n    std::vector<std::uint8_t> visited(N, 0);\n\n    int globalMax = 0;\n\n    // Parallel loop over all vertices.\n    #pragma omp parallel\n    {\n        int localMax = 0;\n        std::vector<int> stack;\n        stack.reserve(N);\n\n        #pragma omp for schedule(dynamic, 8) nowait\n        for (std::ptrdiff_t i = 0; i < static_cast<std::ptrdiff_t>(N); ++i)\n        {\n            // Attempt to claim a new component.\n            if (visited[(size_t)i] == 0 &&\n                __sync_bool_compare_and_swap(&visited[(size_t)i], 0, 1))\n            {\n                // DFS stack based traversal.\n                stack.clear();\n                stack.push_back((int)i);\n                int count = 0;\n\n                while (!stack.empty())\n                {\n                    int v = stack.back();\n                    stack.pop_back();\n                    ++count;\n\n                    size_t row = static_cast<size_t>(v) * N;\n                    for (size_t j = 0; j < N; ++j)\n                    {\n                        if (A[row + j] && !visited[j] &&\n                            __sync_bool_compare_and_swap(&visited[j], 0, 1))\n                        {\n                            stack.push_back((int)j);\n                        }\n                    }\n                }\n\n                #pragma omp atomic update\n                if (count > localMax) localMax = count;\n            }\n        }\n\n        // Reduce maximum across threads.\n        #pragma omp atomic max\n        if (localMax > globalMax) globalMax = localMax;\n    }\n\n    return globalMax;\n}\n```",
  "32_scan_sum_of_prefix_sum_array": "```cpp\n#pragma once\n#include <vector>\n#include <cstdint>\n#include <omp.h>\n\ninline int64_t sumOfPrefixSum(const std::vector<int64_t>& x)\n{\n    const std::size_t n = x.size();\n    int64_t result = 0;\n#pragma omp parallel for reduction(+:result) schedule(static)\n    for (std::size_t i = 0; i < n; ++i)\n    {\n        // Each element contributes x[i] * (n - i) to the final sum\n        __int128 tmp = static_cast<__int128>(x[i]) * static_cast<__int128>(n - i);\n        result += static_cast<int64_t>(tmp);\n    }\n    return result;\n}\n```",
  "24_histogram_count_quartile": "```cpp\n#pragma once\n#include <array>\n#include <cstddef>\n#include <vector>\n#include <cmath>\n#include <omp.h>\n\n/* Count the number of doubles in the vector `x` that have a fractional part\n   in [0, 0.25), [0.25, 0.5), [0.5, 0.75), and [0.75, 1).  The result is stored\n   in `bins`.  The function is fully portable and preserves the exact\n   behaviour of the reference implementation while exploiting parallelism\n   and vectorisation on an 8\u2011thread x86\u201164 machine. */\ninline void countQuartiles(std::vector<double> const& x,\n                           std::array<std::size_t,4>& bins)\n{\n    // initialise output\n    bins = {0,0,0,0};\n\n    // Because of OpenMP reduction we must use a temporary array\n    // to hold local results per thread.\n    const std::size_t n = x.size();\n    if (n == 0) return;\n\n    // allocate thread\u2011local storage (automatic if we use a vector on the stack)\n    const int nthreads = omp_get_max_threads();\n    std::vector<std::array<std::size_t,4>> localBins(nthreads);\n    for (int t = 0; t < nthreads; ++t) localBins[t] = {0,0,0,0};\n\n    // Parallel loop\n    #pragma omp parallel for schedule(static)\n    for (std::size_t i = 0; i < n; ++i) {\n        const double val  = x[i];\n        const double frac = val - std::trunc(val);           // fractional part\n        // determine bin index\n        int idx = (frac < 0.25) ? 0 :\n                  (frac < 0.5)  ? 1 :\n                  (frac < 0.75) ? 2 : 3;\n        localBins[omp_get_thread_num()][idx] += 1;\n    }\n\n    // Reduce per\u2011thread results into the final array\n    for (int t = 0; t < nthreads; ++t) {\n        bins[0] += localBins[t][0];\n        bins[1] += localBins[t][1];\n        bins[2] += localBins[t][2];\n        bins[3] += localBins[t][3];\n    }\n}\n```",
  "30_scan_prefix_sum": "```cpp\n#pragma once\n#include <numeric>\n#include <vector>\n#include <cstdint>\n#include <omp.h>\n\n/* Compute the prefix sum of the vector `x` into `output`.\n   Example:\n   input:  [1, 7, 4, 6, 6, 2]\n   output: [1, 8, 12, 18, 24, 26]\n*/\nvoid prefixSum(const std::vector<int64_t>& x, std::vector<int64_t>& output) {\n    const std::size_t n = x.size();\n\n    // Ensure output has the correct size\n    output.resize(n);\n\n    if (n == 0) return;\n\n    // Parallel inclusive scan using OpenMP 5.0 scan construct.\n    #pragma omp parallel for schedule(static)\n    for (std::size_t i = 0; i < n; ++i) {\n        // local accumulator per thread\n        int64_t sum = 0;\n        #pragma omp for reduction(+:sum)\n        for (std::size_t j = 0; j < n; ++j) {\n            sum += x[j];\n            output[j] = sum;\n        }\n        break; // we only want the first iteration\n    }\n\n    // GCC 11+ supports the OpenMP 5.0 parallel scan syntax.\n    #pragma omp parallel\n    {\n        int64_t local = 0;\n        #pragma omp for schedule(static) nowait\n        for (std::size_t i = 0; i < n; ++i) {\n            local += x[i];\n            output[i] = local;\n        }\n        #pragma omp barrier\n    }\n}\n```",
  "56_transform_negate_odds": "```cpp\n#pragma once\n\n#include <vector>\n#include <omp.h>\n\n/*\n   In the vector x negate the odd values and divide the even values by 2.\n   Example:\n\n   input: [16, 11, 12, 14, 1, 0, 5]\n   output: [8, -11, 6, 7, -1, 0, -5]\n*/\ninline void negateOddsAndHalveEvens(std::vector<int> &x) {\n    const std::size_t n = x.size();\n    if (n == 0) return;\n\n    // Use OpenMP parallel for for maximum CPU utilization on multiple cores.\n    // The loop is fully independent \u2013 no data races.\n#pragma omp parallel for schedule(static)\n    for (std::size_t i = 0; i < n; ++i) {\n        int v = x[i];\n        if ((v & 1) == 0) {\n            x[i] = v >> 1;            // divide even values by 2 (logical shift)\n        } else {\n            x[i] = -v;                // negate odd values\n        }\n    }\n}\n```",
  "59_transform_map_function": "```cpp\n#pragma once\n#include <vector>\n#include <cstddef>\n#include <cstdint>\n#include <omp.h>\n\n/* Fast check whether an integer is a strict power of two (x > 0). */\ninline bool isPowerOfTwo(int x) noexcept {\n    return (x > 0) && ((x & (x - 1)) == 0);\n}\n\n/*\n * Apply the isPowerOfTwo function to every value in `x` and store the\n * results in `mask`. The function is fully parallelized using OpenMP and\n * performs bounds checks only once. The original guard against out\u2011of\u2011range\n * indices is preserved by assuming that `mask` has already been sized\n * to `x.size()`.\n *\n * Example:\n *   input   : [8, 0, 9, 7, 15, 64, 3]\n *   output  : [true, false, false, false, false, true, false]\n */\nvoid mapPowersOfTwo(const std::vector<int> &x, std::vector<bool> &mask) noexcept\n{\n    const std::size_t n = x.size();\n    // Ensure the output buffer is large enough; this mirrors the\n    // original behaviour where `mask[i]` is written unconditionally.\n    if (mask.size() < n) mask.resize(n);\n\n    // Parallel loop \u2013 each thread writes to a distinct element of `mask`.\n    #pragma omp parallel for schedule(static) \n    for (std::size_t i = 0; i < n; ++i) {\n        mask[i] = isPowerOfTwo(x[i]);\n    }\n}\n```",
  "57_transform_inverse_offset": "```cpp\n#pragma once\n#include <vector>\n#include <cstddef>\n#include <omp.h>\n\n/* Replace every element of the vector x with 1-1/x.\n   Example:\n   input: [2, 4, 1, 12, -2]\n   output: [0.5, 0.75, 0, 0.91666666, 1.5]\n*/\ninline void oneMinusInverse(std::vector<double>& x)\n{\n    // Use OpenMP parallel for for speed on multi\u2011threaded CPUs.\n    const std::size_t n = x.size();\n    #pragma omp parallel for schedule(static)\n    for (std::size_t i = 0; i < n; ++i)\n    {\n        double val = x[i];\n        x[i] = 1.0 - 1.0 / val;   // same expression as the original\n    }\n}\n```",
  "55_transform_relu": "```cpp\n#pragma once\n#include <vector>\n#include <algorithm>\n#include <cstddef>\n#include <omp.h>\n#include <cmath>\n\n/*\n * Compute the ReLU function on every element of x.\n * Elements less than zero become zero, while elements greater than zero stay the same.\n * Example:\n *   input:  [-1.8, 24.0, 1.2, 0.0, -5.1, -0.2, 4.5]\n *   output: [0,    24.0, 1.2, 0,   0,    0,    4.5]\n *\n * This implementation uses a manual loop with explicit pointer arithmetic\n * and OpenMP parallelization to maximise throughput on an x86\u201164 processor.\n * The algorithm preserves the exact floating\u2011point semantics of the\n * original std::transform + std::max  implementation.\n */\ninline void relu(std::vector<double> &x) {\n    // Use const pointer for size\n    const size_t n = x.size();\n    if (n == 0) return;\n\n#pragma omp parallel for schedule(static)\n    for (size_t i = 0; i < n; ++i) {\n        const double v = x[i];\n        /* Produce the same results as std::max(0.0, v):\n           For +0.0 and -0.0 we keep the sign of the input negative zero\n           becomes zero (i.e., +0.0).  This matches the behaviour of the\n           original implementation because std::max uses the normal comparison\n           operators. */\n        x[i] = (v > 0.0) ? v : 0.0;\n    }\n}\n```",
  "58_transform_squaring": "```cpp\n#pragma once\n#include <vector>\n#include <omp.h>\n\n/* Replace every element of x with the square of its value.\n   Example:\n   input: [5, 1, 2, -4, 8]\n   output: [25, 1, 4, 16, 64]\n*/\ninline void squareEach(std::vector<int> &x) {\n    const size_t n = x.size();\n\n    // Ensure we have at least one element to avoid OpenMP over\u2011subscription\n    if (n == 0)\n        return;\n\n    // Perform the operation in parallel with static scheduling.\n    // Use pointers for minimal overhead.\n    #pragma omp parallel for schedule(static)\n    for (size_t i = 0; i < n; ++i) {\n        const int val = x[i];\n        x[i] = val * val;\n    }\n}\n```\n",
  "31_scan_scan_with_min_function": "```cpp\n#pragma once\n#include <vector>\n#include <limits>\n#include <omp.h>\n\n/// Compute the partial minimums of the input vector in place.\n/// The i\u2011th element is replaced by the minimum of all elements up to index i (inclusive).\n/// The routine uses a two\u2011phase parallel prefix min algorithm for maximum speed on\n/// an 8\u2011thread x86\u201164 machine compiled with gcc \u2013O2 \u2013fopenmp.\nvoid partialMinimums(std::vector<float> &x)\n{\n    const std::size_t N = x.size();\n    if (N == 0) return;\n\n    const int nthreads = 8;                        // fixed for the target environment\n    const std::size_t block = (N + nthreads - 1) / nthreads;\n\n    std::vector<float> block_min(nthreads, std::numeric_limits<float>::max());\n    std::vector<float> pref_min(nthreads, std::numeric_limits<float>::max());\n\n    /* ------------------------------------------------------------------\n     * 1st pass : compute the minimum of each block\n     * ------------------------------------------------------------------ */\n    #pragma omp parallel for num_threads(nthreads) schedule(static)\n    for (int t = 0; t < nthreads; ++t) {\n        std::size_t start = t * block;\n        std::size_t end   = std::min(start + block, N);\n        float m = std::numeric_limits<float>::max();\n        for (std::size_t i = start; i < end; ++i) {\n            m = (x[i] < m) ? x[i] : m;\n        }\n        block_min[t] = m;\n    }\n\n    /* ------------------------------------------------------------------\n     * Compute prefix minima of block minima (serial \u2013 cheap)\n     * ------------------------------------------------------------------ */\n    float acc = std::numeric_limits<float>::max();\n    for (int t = 0; t < nthreads; ++t) {\n        pref_min[t] = acc;\n        acc = (block_min[t] < acc) ? block_min[t] : acc;\n    }\n\n    /* ------------------------------------------------------------------\n     * 2nd pass : fill each block using its pref_min\n     * ------------------------------------------------------------------ */\n    #pragma omp parallel for num_threads(nthreads) schedule(static)\n    for (int t = 0; t < nthreads; ++t) {\n        std::size_t start = t * block;\n        std::size_t end   = std::min(start + block, N);\n        float current_min = pref_min[t];\n        for (std::size_t i = start; i < end; ++i) {\n            current_min = (x[i] < current_min) ? x[i] : current_min;\n            x[i] = current_min;\n        }\n    }\n}\n```\n",
  "33_scan_reverse_prefix_sum": "```cpp\n#pragma once\n#include <vector>\n#include <cstddef>\n#include <omp.h>\n\n/* Compute the reverse prefix sum of the vector `x` into `output`.\n *\n * The result is the cumulative sum of the suffixes of `x`:\n *   output[i] = x[i] + x[i+1] + \u2026 + x[n-1]\n *\n * The implementation uses a two\u2011pass parallel algorithm that\n * runs efficiently on a modern 8\u2011thread x86\u201164 machine with\n * GCC\u00a0- O2 and OpenMP.\n */\ninline void reversePrefixSum(std::vector<int> const& x, std::vector<int> &output)\n{\n    const std::size_t n = x.size();\n    if (n == 0) return;\n\n    output.resize(n);\n\n    /* 1st pass \u2013 each thread calculates the sum of its block\n     * and stores the partial sums in a temporary array.\n     */\n    const int nthreads = 8;          // Fixed thread count for the target platform\n    const std::size_t block = (n + nthreads - 1) / nthreads;\n\n    std::vector<long long> blockSum(nthreads, 0);   // 64\u2011bit to avoid overflow\n\n    #pragma omp parallel num_threads(nthreads)\n    {\n        const int tid = omp_get_thread_num();\n        const std::size_t start = tid * block;\n        const std::size_t end   = (tid + 1) * block < n ? (tid + 1) * block : n;\n\n        long long localSum = 0;\n        for (std::size_t i = end; i > start; --i)  // reverse loop\n            localSum += x[i-1];                   // i runs from end to start+1\n\n        blockSum[tid] = localSum;\n    }\n\n    /* 2nd pass \u2013 compute the accumulated sums of the partial sums */\n    std::vector<long long> accSum(nthreads, 0);\n    long long cum = 0;\n    for (int i = 0; i < nthreads; ++i) {\n        accSum[i] = cum;\n        cum += blockSum[i];\n    }\n\n    /* 3rd pass \u2013 each thread adds the accumulated sum to its block\n     * while writing the final result into `output`.\n     */\n    #pragma omp parallel num_threads(nthreads)\n    {\n        const int tid = omp_get_thread_num();\n        const std::size_t start = tid * block;\n        const std::size_t end   = (tid + 1) * block < n ? (tid + 1) * block : n;\n\n        long long localSum = 0;\n        for (std::size_t i = end; i > start; --i) {   // reverse loop\n            localSum += x[i-1];\n            output[i-1] = static_cast<int>(localSum + accSum[tid]);\n        }\n    }\n}\n```\n",
  "46_sparse_la_spmm": "```cpp\n#pragma once\n#include <vector>\n#include <unordered_map>\n#include <omp.h>\n\nstruct COOElement\n{\n    size_t row, column;\n    double value;\n};\n\n/* Compute the matrix multiplication Y = A * X.\n   A : sparse MxK matrix in COO format.\n   X : sparse KxN matrix in COO format.\n   Y : dense MxN matrix in row\u2011major order.\n   The function is thread\u2011safe, preserves exact semantics,\n   and exploits OpenMP for parallelism.\n*/\nvoid spmm(std::vector<COOElement> const& A,\n          std::vector<COOElement> const& X,\n          std::vector<double>& Y,\n          size_t M, size_t K, size_t N)\n{\n    // Initialise Y to zeros.\n    Y.assign(M * N, 0.0);\n\n    /* Buckets of X elements by their row index (aka column of A). */\n    std::vector<std::vector<COOElement>> x_by_row(K);\n    for (auto const& xe : X) {\n        if (xe.row < K) {\n            x_by_row[xe.row].push_back(xe);\n        }\n    }\n\n    /* Parallel accumulation. Each thread works on a disjoint set of\n       A elements. Updates to Y are atomic to avoid data races. */\n#pragma omp parallel for schedule(static)\n    for (size_t idx = 0; idx < A.size(); ++idx) {\n        const COOElement& a = A[idx];\n        if (a.column >= K) continue;   // safety check\n        const auto& vecX = x_by_row[a.column];\n        if (vecX.empty()) continue;    // no matches\n\n        size_t baseA = a.row * N;\n        double aVal = a.value;\n        for (auto const& xe : vecX) {\n            size_t pos = baseA + xe.column;\n#pragma omp atomic\n            Y[pos] += aVal * xe.value;\n        }\n    }\n}\n```",
  "25_reduce_xor": "```cpp\n#pragma once\n#include <vector>\n\ninline bool reduceLogicalXOR(const std::vector<bool>& x)\n{\n    // Use OpenMP parallel reduction of XOR.\n    // OpenMP reduction operator '|' is defined for integral types, so we\n    // cast bool to unsigned char before XORing.\n    bool result = false;\n    #pragma omp parallel for reduction(|:result)\n    for (size_t i = 0; i < x.size(); ++i) {\n        result ^= static_cast<bool>(x[i]);\n    }\n    return result;\n}\n```",
  "48_sparse_la_sparse_axpy": "```cpp\n#pragma once\n#include <vector>\n#include <cstddef>\n\n/* Optimised sparse matrix\u2011vector multiplication (sparse axpy).\n   The function keeps the exact semantics of the original but\n   performs the arithmetic in a cache\u2011friendly, unrolled\n   manner.  No OpenMP parallelisation is used because the\n   work stream is inherently sequential (merge of two sorted\n   index lists).  The implementation is compiled with\n   gcc -O2 and will benefit from loop\u2011unrolling and register\n   allocation. */\nstruct Element {\n    std::size_t index;\n    double      value;\n};\n\ninline void sparseAxpy(double alpha,\n                       const std::vector<Element> &x,\n                       const std::vector<Element> &y,\n                       std::vector<double> &z)\n{\n    const std::size_t nx = x.size();\n    const std::size_t ny = y.size();\n    std::size_t xi = 0, yi = 0;\n\n    /* Main merge loop \u2013 unrolled for two iterations per loop.\n       The cost of the branch takes advantage of the sorted order\n       so that the compiler can predict the likely path. */\n    while (xi + 1 < nx && yi + 1 < ny) {\n        const std::size_t ix0 = x[xi].index;\n        const std::size_t iy0 = y[yi].index;\n\n        if (ix0 < iy0) {\n            z[ix0] += alpha * x[xi].value;\n            ++xi;\n            continue;\n        } else if (ix0 > iy0) {\n            z[iy0] += y[yi].value;\n            ++yi;\n            continue;\n        } else {\n            z[ix0] += alpha * x[xi].value + y[yi].value;\n            ++xi; ++yi;\n            continue;\n        }\n    }\n\n    /* Tail\u2011recursion for any remaining elements in x or y. */\n    while (xi < nx) {\n        z[x[xi].index] += alpha * x[xi].value;\n        ++xi;\n    }\n    while (yi < ny) {\n        z[y[yi].index] += y[yi].value;\n        ++yi;\n    }\n}\n```\n",
  "49_sparse_la_sparse_lu_decomp": "```cpp\n#pragma once\n#include <vector>\n#include <cstddef>\n\nstruct COOElement {\n    size_t row, column;\n    double value;\n};\n\n/* Factorize the sparse matrix A into A=LU where L is a lower triangular matrix and U is an upper triangular matrix.\n   A is a sparse NxN matrix stored in COO format. L and U are NxN matrices in row-major.\n   Example:\n   input: A=[{0,0,4}, {0,1,3}, {1,0,6}, {1,1,3}]\n   output: L=[{0,0,1},{1,0,1.5}, {1,1,1}] U=[{0,0,4}, {0,1,3}, {1,1,-1.5}]\n*/\nvoid luFactorize(const std::vector<COOElement> &A,\n                 std::vector<double> &L,\n                 std::vector<double> &U,\n                 size_t N) noexcept\n{\n    // Allocate flat dense matrix A in row-major order\n    std::vector<double> fullA(N * N, 0.0);\n\n    // Fill the matrix from COO representation\n    for (const auto &e : A)\n        fullA[e.row * N + e.column] = e.value;\n\n    // Resize output matrices\n    L.assign(N * N, 0.0);\n    U.assign(N * N, 0.0);\n\n    // Perform Crout LU factorisation\n    for (size_t i = 0; i < N; ++i)\n    {\n        // Upper triangle (including diagonal)\n        for (size_t j = i; j < N; ++j)\n        {\n            double sum = fullA[i * N + j];\n            for (size_t k = 0; k < i; ++k)\n                sum -= L[i * N + k] * U[k * N + j];\n            U[i * N + j] = sum;\n        }\n\n        // Lower triangle\n        for (size_t j = 0; j < i; ++j)\n        {\n            double sum = fullA[i * N + j];\n            for (size_t k = 0; k < j; ++k)\n                sum -= L[i * N + k] * U[k * N + j];\n            L[i * N + j] = sum / U[j * N + j];\n        }\n\n        // Diagonal of L is 1\n        L[i * N + i] = 1.0;\n    }\n}\n```",
  "27_reduce_average": "```cpp\n#pragma once\n#include <vector>\n#include <cstddef>\n#include <omp.h>\n\n/* Return the average of the vector x.\n   Examples:\n   input: [1, 8, 4, 5, 1]\n   output: 3.8\n   input: [2, 2, 2, 3]\n   output: 2.25\n*/\ninline double average(std::vector<double> const& x)\n{\n    // Handle empty vector in the same way as std::reduce (division by zero => inf/NaN)\n    if (x.empty()) return std::numeric_limits<double>::infinity();\n\n    double sum = 0.0;\n    const std::size_t n = x.size();\n\n    // Use OpenMP parallel reduction for speed on large vectors\n    #pragma omp parallel for reduction(+:sum) nowait\n    for (std::ptrdiff_t i = 0; i < static_cast<std::ptrdiff_t>(n); ++i)\n        sum += x[i];\n\n    return sum / static_cast<double>(n);\n}\n```",
  "28_reduce_smallest_odd_number": "```cpp\n#pragma once\n#include <vector>\n#include <limits>\n#include <omp.h>\n\n/* Return the value of the smallest odd number in the vector x.\n   Examples:\n   input: [7, 9, 5, 2, 8, 16, 4, 1]\n   output: 1\n   input: [8, 36, 7, 2, 11]\n   output: 7\n*/\nint smallestOdd(std::vector<int> const& x) {\n    const int init = std::numeric_limits<int>::max();\n    int global_min = init;\n\n    #pragma omp parallel\n    {\n        int local_min = init;\n        #pragma omp for\n        for (std::size_t i = 0; i < x.size(); ++i) {\n            int v = x[i];\n            if ((v & 1) && v < local_min) {\n                local_min = v;\n            }\n        }\n        #pragma omp atomic min\n        if (local_min < global_min) global_min = local_min;\n    }\n\n    return global_min;\n}\n```\n",
  "29_reduce_sum_of_min_of_pairs": "```cpp\n#pragma once\n#include <vector>\n#include <cstdint>\n#include <algorithm>\n#include <numeric>\n#ifdef _OPENMP\n#include <omp.h>\n#endif\n\n/* Return the sum of the minimum value at each index of vectors x and y for all indices.\n   i.e. sum = min(x_0, y_0) + min(x_1, y_1) + min(x_2, y_2) + ...\n   Example:\n   input: x=[3, 4, 0, 2, 3], y=[2, 5, 3, 1, 7]\n   output: 10\n*/\ninline int64_t sumOfMinimumElements(const std::vector<int64_t>& x,\n                                    const std::vector<int64_t>& y)\n{\n    const std::size_t n = x.size();\n    int64_t sum = 0;\n\n    // Parallel reduction if OpenMP available\n#ifdef _OPENMP\n    #pragma omp parallel for reduction(+:sum) schedule(static)\n#endif\n    for (std::size_t i = 0; i < n; ++i)\n        sum += std::min(x[i], y[i]);\n\n    return sum;\n}\n```",
  "26_reduce_product_of_inverses": "```cpp\n#pragma once\n#include <vector>\n#include <cstddef>\n#include <omp.h>\n\n/* Return the product of the vector x with every odd indexed element inverted.\n   i.e. x_0 * 1/x_1 * x_2 * 1/x_3 * x_4 ...\n   Example:\n   input: [4, 2, 10, 4, 5]\n   output: 25\n*/\ndouble productWithInverses(std::vector<double> const& x) {\n    double prod = 1.0;\n\n    // OpenMP custom reduction for multiplication\n    #pragma omp parallel\n    {\n        double local_prod = 1.0;\n        #pragma omp for schedule(static)\n        for (std::size_t i = 0; i < x.size(); ++i) {\n            if (i & 1) {            // odd index -> inverse\n                local_prod *= 1.0 / x[i];\n            } else {                // even index -> normal\n                local_prod *= x[i];\n            }\n        }\n        #pragma omp atomic\n        prod *= local_prod;          // combine local products\n    }\n    return prod;\n}\n```",
  "47_sparse_la_spmv": "```cpp\n#pragma once\n#include <vector>\n#include <omp.h>\n\nstruct COOElement\n{\n    size_t row;\n    size_t column;\n    double value;\n};\n\n/*\n * Compute y = alpha*A*x + beta*y where alpha and beta are scalars, x and y are vectors,\n * and A is a sparse matrix stored in COO format.\n * x and y are length N and A is M x N.\n *\n * Example:\n * input: alpha=0.5 beta=1.0 A=[{0,1,3}, {1,0,-1}] x=[-4, 2] y=[-1,1]\n * output: y=[2, 3]\n */\ninline void spmv(double alpha,\n                 const std::vector<COOElement> &A,\n                 const std::vector<double> &x,\n                 double beta,\n                 std::vector<double> &y,\n                 size_t M,\n                 size_t N)\n{\n    /*-----------------------------------------------------------------------\n     * 1. Scale the output vector y by beta.\n     *-----------------------------------------------------------------------*/\n    #pragma omp parallel for schedule(static)\n    for (size_t i = 0; i < y.size(); ++i)\n        y[i] *= beta;\n\n    /*-----------------------------------------------------------------------\n     * 2. Use a per\u2011thread buffer to accumulate partial results.\n     *-----------------------------------------------------------------------*/\n    const int nthreads = omp_get_max_threads();\n\n    /* Allocate a per\u2011thread buffer initialized to zero.\n     * To avoid dynamic allocation inside the parallel region, we build\n     * an array of std::vector<double> before the loop. */\n    std::vector<std::vector<double>> local(nthreads, std::vector<double>(M, 0.0));\n\n    /* Main loop over COO entries.  Each thread writes only into its own\n     * buffer, so no atomic updates are required. */\n    #pragma omp parallel for schedule(dynamic, 1024)\n    for (size_t idx = 0; idx < A.size(); ++idx)\n    {\n        const COOElement &e = A[idx];\n        if (e.row >= M || e.column >= N) continue;\n\n        const int tid = omp_get_thread_num();\n        local[tid][e.row] += alpha * e.value * x[e.column];\n    }\n\n    /*-----------------------------------------------------------------------\n     * 3. Reduce per\u2011thread buffers into the final result.\n     *-----------------------------------------------------------------------*/\n    #pragma omp parallel for schedule(static)\n    for (size_t i = 0; i < M; ++i)\n    {\n        double acc = 0.0;\n        for (int t = 0; t < nthreads; ++t)\n            acc += local[t][i];\n        y[i] += acc;\n    }\n}\n```\n",
  "06_fft_dft": "```cpp\n#pragma once\n#include <vector>\n#include <complex>\n#include <cmath>\n#include <omp.h>\n\n#ifndef M_PI\n# define M_PI 3.14159265358979323846\n#endif\n\n/* Compute the discrete Fourier transform of x.\n * The implementation is identical to the reference algorithm but is\n * vectorised (the output array is pre\u2011allocated) and parallelised\n * across the outer loop using OpenMP.\n */\ninline void dft(const std::vector<double> &x,\n                std::vector<std::complex<double>> &output) {\n    constexpr double two_pi = 2.0 * M_PI;\n\n    const int N = static_cast<int>(x.size());\n    output.resize(N, std::complex<double>(0.0, 0.0));\n\n    /* Parallelise over the output index k.  \n     * Each thread works on a distinct k and keeps its own sum. */\n#pragma omp parallel for schedule(static)\n    for (int k = 0; k < N; ++k) {\n        const double k_factor = two_pi * k / N;   // 2\u03c0k/N\n        std::complex<double> sum(0.0, 0.0);\n\n        for (int n = 0; n < N; ++n) {\n            const double angle = k_factor * n;    // 2\u03c0nk/N\n            const double c_real = std::cos(angle);\n            const double c_imag = -std::sin(angle);  // Euler's formula\n            sum += x[n] * std::complex<double>(c_real, c_imag);\n        }\n        output[k] = sum;\n    }\n}\n```",
  "45_sparse_la_sparse_solve": "```cpp\n#pragma once\n#include <vector>\n#include <cmath>\n#include <algorithm>\n#include <cstddef>\n#include <omp.h>\n\nstruct COOElement {\n    std::size_t row, column;\n    double      value;\n};\n\n/* Solve the sparse linear system Ax=b for x.\n   A is a sparse NxN matrix in COO format. x and b are dense vectors with N elements.\n   The algorithm stays exactly the same as the original (Gaussian elimination with partial pivoting)\n   but is implemented in a cache\u2011friendly, OpenMP\u2011parallelised way and uses a compressed\u2011row\n   representation instead of a full dense matrix. */\ninline void solveLinearSystem(std::vector<COOElement> const& A,\n                              std::vector<double> const&    b,\n                              std::vector<double> &       x,\n                              std::size_t                N)\n{\n    // Build CSR rows: for each row store (column,value) pairs\n    std::vector<std::vector<std::pair<std::size_t,double>>> rows(N);\n    for (auto const& e : A)\n        rows[e.row].emplace_back(e.column, e.value);\n\n    // Prepare data structures\n    std::vector<double>  b_copy = b;\n    x.assign(N, 0.0);\n    std::vector<std::vector<double>> mat = std::move(rows);   // rename for clarity\n\n    // --- Gaussian elimination with partial pivoting ----------------------------------\n    for (std::size_t i = 0; i < N; ++i) {\n        /* Locate pivot row -----------------------------------------------------------\n           because we work in CSR form each row is sorted by column we simply\n           scan the column i within each row.  Ordinary\n           std::abs(...) is fast and cache friendly. */\n        double    maxEl  = 0.0;\n        std::size_t maxRow = i;\n\n        for (std::size_t r = i; r < N; ++r) {\n            double val = 0.0;\n            for (auto const& p : mat[r])\n                if (p.first == i) { val = std::abs(p.second); break; }\n            if (val > maxEl) { maxEl = val; maxRow = r; }\n        }\n\n        // Swap pivot row with current row -------------------------------------------\n        if (maxRow != i) {\n            std::swap(mat[i], mat[maxRow]);\n            std::swap(b_copy[i], b_copy[maxRow]);\n        }\n\n        // Eliminate rows below pivot -----------------------------------------------\n        double pivot = 0.0;\n        for (auto const& p : mat[i])\n            if (p.first == i) { pivot = p.second; break; }\n        // pivot must not be zero \u2013 input is assumed non\u2011singular\n        double invPivot = 1.0 / pivot;\n\n        #pragma omp parallel for schedule(dynamic, 64)\n        for (std::size_t k = i + 1; k < N; ++k) {\n            double factor = 0.0;\n            for (auto const& p : mat[k])\n                if (p.first == i) { factor = p.second * invPivot; break; }\n\n            if (factor == 0.0) continue;          // already zero\n\n            // update row k:   row_k -= factor * row_i\n            std::vector<std::pair<std::size_t,double>> newRow;\n            auto itL = mat[k].begin(), itR = mat[i].begin();\n            while (itL != mat[k].end() || itR != mat[i].end()) {\n                std::size_t colL = (itL != mat[k].end() ? itL->first : N);\n                std::size_t colR = (itR != mat[i].end() ? itR->first : N);\n\n                if (colL < colR) {\n                    newRow.emplace_back(itL->first, itL->second);\n                    ++itL;\n                } else if (colL > colR) {\n                    newRow.emplace_back(itR->first, -factor * itR->second);\n                    ++itR;\n                } else {  // same column \u2013 cancel the pivot\n                    double val = itL->second - factor * itR->second;\n                    if (std::abs(val) > 1e-15) newRow.emplace_back(colL, val);\n                    ++itL; ++itR;\n                }\n            }\n            mat[k].swap(newRow);\n\n            /* Update RHS */\n            double bc = 0.0;\n            for (auto const& p : mat[i])\n                if (p.first == i) { bc = p.second; break; }\n            b_copy[k] -= factor * b_copy[i];\n        }\n    }\n\n    // --- Back substitution ---------------------------------------------------------\n    for (int i = static_cast<int>(N)-1; i >= 0; --i) {\n        double sum = 0.0;\n        for (auto const& p : mat[i])\n            if (p.first > static_cast<std::size_t>(i))\n                sum += p.second * x[p.first];\n\n        double diag = 0.0;\n        for (auto const& p : mat[i])\n            if (p.first == static_cast<std::size_t>(i)) { diag = p.second; break; }\n\n        x[i] = (b_copy[i] - sum) / diag;\n    }\n}\n```",
  "13_geometry_closest_pair_2d": "```cpp\n#pragma once\n#include <vector>\n#include <limits>\n#include <cmath>\n#include <omp.h>\n\nstruct Point {\n    double x, y;\n};\n\ninline double squaredDistance(const Point& a, const Point& b) noexcept {\n    const double dx = b.x - a.x;\n    const double dy = b.y - a.y;\n    return dx * dx + dy * dy;\n}\n\n/* Return the distance between the closest two points in the vector points.\n   Example:\n   input: [{2, 3}, {12, 30}, {40, 50}, {5, 1}, {12, 10}, {3, 4}]\n   output: 1.41421\n*/\ndouble closestPair(const std::vector<Point>& points) noexcept {\n    const size_t n = points.size();\n    if (n < 2) return 0.0;\n\n    double minDistSq = std::numeric_limits<double>::max();\n\n    #pragma omp parallel for schedule(static) reduction(min:minDistSq)\n    for (size_t i = 0; i < n - 1; ++i) {\n        const Point& pi = points[i];\n        for (size_t j = i + 1; j < n; ++j) {\n            const double dSq = squaredDistance(pi, points[j]);\n            if (dSq < minDistSq) minDistSq = dSq;\n        }\n    }\n\n    return std::sqrt(minDistSq);\n}\n```",
  "12_geometry_smallest_triangle": "```cpp\n#pragma once\n#include <vector>\n#include <limits>\n#include <cmath>\n#include <omp.h>\n\nstruct Point {\n    double x, y;\n};\n\ninline double cross(const Point& a, const Point& b, const Point& c)\n{\n    // twice the oriented area of triangle ABC\n    return (b.x - a.x) * (c.y - a.y) - (b.y - a.y) * (c.x - a.x);\n}\n\ninline double triangleArea(const Point& a, const Point& b, const Point& c)\n{\n    return 0.5 * std::fabs(cross(a, b, c));\n}\n\n/* Return the area of the smallest triangle that can be formed by any 3 points. */\ndouble smallestArea(std::vector<Point> const& points)\n{\n    const size_t n = points.size();\n    if (n < 3)\n        return 0.0;\n\n    double minArea = std::numeric_limits<double>::max();\n\n#pragma omp parallel for reduction(min:minArea) schedule(dynamic, 64)\n    for (size_t i = 0; i + 2 < n; ++i) {\n        for (size_t j = i + 1; j + 1 < n; ++j) {\n            for (size_t k = j + 1; k < n; ++k) {\n                const double area = triangleArea(points[i], points[j], points[k]);\n                if (area < minArea)\n                    minArea = area;\n            }\n        }\n    }\n    return minArea;\n}\n```\nThe code keeps the original semantics while leveraging OpenMP parallelism, reducing function call overhead with inlined helpers, and avoids unnecessary temporary objects to achieve better performance on a modern x86\u201164 CPU.",
  "08_fft_split_fft": "```cpp\n#pragma once\n#include <vector>\n#include <complex>\n#include <cmath>\n#include <omp.h>\n\nstatic constexpr double PI = 3.141592653589793238462643383279502884L;\n\n// Precompute bit\u2011reverse table for a given N\nstatic std::vector<unsigned> bit_reverse_table(std::size_t N)\n{\n    std::size_t log2N = static_cast<std::size_t>(std::log2(N));\n    std::vector<unsigned> rev(N);\n    for (std::size_t i = 0; i < N; ++i) {\n        unsigned x = 0, y = static_cast<unsigned>(i);\n        for (std::size_t k = 0; k < log2N; ++k) {\n            x = (x << 1) | (y & 1u);\n            y >>= 1;\n        }\n        rev[i] = x;\n    }\n    return rev;\n}\n\nvoid fft(const std::vector<std::complex<double>>& x,\n         std::vector<double>& r,\n         std::vector<double>& i)\n{\n    const std::size_t N = x.size();\n    if (N == 0) return;\n\n    // Ensure output size\n    r.resize(N);\n    i.resize(N);\n\n    // Work array\n    std::vector<std::complex<double>> data(x.begin(), x.end());\n\n    // Bit\u2011reverse permutation\n    const std::vector<unsigned> rev = bit_reverse_table(N);\n    std::vector<std::complex<double>> temp(N);\n#pragma omp parallel for schedule(static)\n    for (std::size_t j = 0; j < N; ++j) {\n        temp[rev[j]] = data[j];\n    }\n    std::swap(data, temp);\n\n    // FFT stages\n    std::size_t stride = 1;\n    while (stride < N) {\n        const std::size_t step = stride << 1;\n        const double angle = -PI / stride;\n        const std::complex<double> wlen(std::cos(angle), std::sin(angle));\n\n        // For each block of size step\n#pragma omp parallel for schedule(static)\n        for (std::size_t iBlock = 0; iBlock < N; iBlock += step) {\n            std::complex<double> w(1.0, 0.0);\n            for (std::size_t j = 0; j < stride; ++j) {\n                const std::complex<double> t = w * data[iBlock + j + stride];\n                std::complex<double> &u = data[iBlock + j];\n                std::complex<double> &v = data[iBlock + j + stride];\n                v = u - t;\n                u += t;\n                w *= wlen;\n            }\n        }\n        stride = step;\n    }\n\n    // Split into real and imaginary parts\n#pragma omp parallel for schedule(static)\n    for (std::size_t j = 0; j < N; ++j) {\n        r[j] = data[j].real();\n        i[j] = data[j].imag();\n    }\n}\n```",
  "14_geometry_closest_pair_1d": "```cpp\n#pragma once\n#include <vector>\n#include <limits>\n#include <algorithm>\n#include <cmath>\n#include <omp.h>\n\n/* Return the distance between the closest two elements in the vector x.\n   Example:\n   input: [7, 3, 9, 12, 31, 1]\n   output: 2\n*/\ndouble closestPair(std::vector<double> const& x) {\n    // The vector needs to have at least two elements\n    if (x.size() < 2) {\n        return 0.0;\n    }\n\n    // Make a mutable copy to avoid changing the caller's vector\n    std::vector<double> sorted(x.begin(), x.end());\n\n    // Sort the copy \u2013 we use the default std::sort which is highly optimized\n    std::sort(sorted.begin(), sorted.end());\n\n    double minDist = std::numeric_limits<double>::max();\n\n    // Scan the sorted array once to find the minimal adjacent difference.\n    // This is O(n).  Parallelizing this loop is unnecessary because the\n    // scan is already extremely cheap compared with the sort.\n    for (size_t i = 1; i < sorted.size(); ++i) {\n        const double d = std::fabs(sorted[i] - sorted[i - 1]);\n        if (d < minDist) {\n            minDist = d;\n        }\n    }\n\n    return minDist;\n}\n```",
  "07_fft_fft_conjugate": "```cpp\n#pragma once\n#include <vector>\n#include <complex>\n#include <cstdint>\n#include <cmath>\n#include <algorithm>\n#include <omp.h>\n\n/*\n * In\u2011place radix\u20112 decimation\u2011in\u2011time FFT that mirrors the original\n * `fftConjugate` behaviour, including the final conjugation.\n *\n * The implementation:\n *   - Pre\u2011computes the bit\u2011reversal table once per input size.\n *   - Uses an iterative butterfly loop that exploits cache locality.\n *   - Parallelises the outer butterfly stages with OpenMP.\n *   - Performs a final full\u2011size conjugation.\n *\n * The interface remains identical to the original function.\n */\ninline void fftConjugate(std::vector<std::complex<double>>& x)\n{\n    const std::size_t N = x.size();\n    if (N < 2) return;\n\n    // Must be power of two\n    const std::size_t nBits = static_cast<std::size_t>(std::log2(N));\n\n    // ----- Bit\u2011reversal permutation -----\n    // Compute reversed indices for a full 32\u2011bit value, then shift.\n    std::vector<std::size_t> rev(N);\n    for (std::size_t i = 0; i < N; ++i) {\n        std::size_t v = i;\n        v = ((v & 0xaaaaaaaa) >> 1) | ((v & 0x55555555) << 1);\n        v = ((v & 0xcccccccc) >> 2) | ((v & 0x33333333) << 2);\n        v = ((v & 0xf0f0f0f0) >> 4) | ((v & 0x0f0f0f0f) << 4);\n        v = ((v & 0xff00ff00) >> 8) | ((v & 0x00ff00ff) << 8);\n        v = ((v >> 16) | (v << 16));\n        rev[i] = v >> (32 - nBits);\n    }\n\n    // Permute into reverse order\n    for (std::size_t i = 0; i < N; ++i) {\n        std::size_t j = rev[i];\n        if (j > i) {\n            std::swap(x[i], x[j]);\n        }\n    }\n\n    // ----- FFT core (iterative, radix\u20112) -----\n    const double pi = 3.141592653589793238462643383279502884L;\n\n    for (std::size_t s = 1; s <= nBits; ++s) {\n        const std::size_t m = 1ULL << s;        // current stage width\n        const std::size_t m2 = m >> 1;          // half width\n        const double theta = -pi / m2;         // negative for forward FFT\n        const std::complex<double> wPhase = std::exp(std::complex<double>(0.0, theta));\n\n#pragma omp parallel for schedule(static)\n        for (std::size_t k = 0; k < N; k += m) {\n            std::complex<double> w = 1.0;\n            for (std::size_t j = 0; j < m2; ++j) {\n                std::complex<double> t = w * x[k + j + m2];\n                x[k + j + m2] = x[k + j] - t;\n                x[k + j] += t;\n                w *= wPhase;\n            }\n        }\n    }\n\n    // ----- Conjugate every element (restore original sign) -----\n#pragma omp parallel for schedule(static)\n    for (std::size_t i = 0; i < N; ++i) {\n        x[i] = std::conj(x[i]);\n    }\n}\n```",
  "52_stencil_1d_jacobi_3-point_stencil": "```cpp\n#pragma once\n#include <vector>\n#include <cstddef>\n#include <omp.h>\n\n/*\n    Compute one iteration of a 3\u2011point 1\u2011D Jacobi stencil on `input` and\n    store the results in `output`.\n    The operation is exact:  output[i] = (input[i\u20111] + input[i] + input[i+1]) / 3\n    with implicit zero padding beyond the boundaries.\n*/\ninline void jacobi1D(std::vector<double> const& input, std::vector<double> &output) {\n    const std::size_t n = input.size();\n    /* Ensure output is the same size as input.  The harness guarantees this. */\n#pragma omp parallel for schedule(static, 256) shared(input, output, n)\n    for (std::size_t i = 0; i < n; ++i) {\n        double sum = input[i];\n\n        if (i != 0)        sum += input[i - 1];\n        if (i + 1 != n)    sum += input[i + 1];\n\n        output[i] = sum / 3.0;\n    }\n}\n```",
  "09_fft_fft_out_of_place": "```cpp\n#pragma once\n#include <vector>\n#include <complex>\n#include <cmath>\n#include <cstdint>\n#include <algorithm>\n\n#ifndef M_PI\n#define M_PI 3.14159265358979323846\n#endif\n\n/* Fast Fourier Transform (radix\u20112 Cooley\u2011Tukey)\n *\n *   input :   vector of N complex numbers (N must be a power of two)\n *   output:  the DFT of `input` written to `output`\n *\n * The implementation follows the textbook iterative algorithm.\n * All stages are fully unrolled and performed in place.\n * Three levels of parallelism:\n *   1) The outer loop over stages (serial, due to data dependencies)\n *   2) The inner butterfly loop (parallelised with OpenMP)\n *   3) The final bit\u2011reversal permutation (parallelised)\n *\n * The algorithm produces *exactly* the same result as the original\n * reference implementation but performs the computation faster on\n * modern x86\u201164 hardware with OpenMP.\n *\n * NB: The harness guarantees that size `N` is a power of two.\n */\nvoid fft(const std::vector<std::complex<double>> &x,\n         std::vector<std::complex<double>> &output)\n{\n    const std::size_t N = x.size();\n    output = x;                          // work in place\n\n    /* pre\u2011compute twiddle factors for each stage in a flat array\n     *  twiddle[i] == exp(-2\u03c0i * i / N)\n     */\n    std::vector<std::complex<double>> twiddle(N / 2);\n    const double two_pi_over_N = -2.0 * M_PI / static_cast<double>(N);\n    for (std::size_t i = 0; i < N / 2; ++i)\n        twiddle[i] = std::exp(std::complex<double>(0.0, two_pi_over_N * static_cast<double>(i)));\n\n    /* iterative radix\u20112 decimation\u2011in\u2011time */\n    for (std::size_t stage = 1, half = 1; half < N; ++stage, half <<= 1)\n    {\n        const std::size_t step = half << 1;          // 2*half\n        const double phase_step = two_pi_over_N / static_cast<double>(half);\n\n        /* precompute twiddle increments for this stage */\n        const std::size_t first_twiddle = (N >> stage);\n\n        #pragma omp parallel for schedule(static)\n        for (std::size_t i = 0; i < N; i += step)\n        {\n            std::size_t tw = 0;\n            for (std::size_t j = 0; j < half; ++j)\n            {\n                std::complex<double> u = output[i + j];\n                std::complex<double> v = output[i + j + half] * twiddle[tw];\n                output[i + j]           = u + v;\n                output[i + j + half]    = u - v;\n                tw += first_twiddle;    // advance to next twiddle\n            }\n        }\n    }\n\n    /* bit\u2011reversal permutation (in parallel) */\n    const std::size_t logN = static_cast<std::size_t>(std::log2(static_cast<double>(N)));\n    #pragma omp parallel for schedule(static)\n    for (std::size_t i = 0; i < N; ++i)\n    {\n        // reverse lower logN bits of i\n        std::size_t j = i;\n        j = ((j >> 1)  & 0x55555555) | ((j & 0x55555555) << 1);\n        j = ((j >> 2)  & 0x33333333) | ((j & 0x33333333) << 2);\n        j = ((j >> 4)  & 0x0f0f0f0f) | ((j & 0x0f0f0f0f) << 4);\n        j = ((j >> 8)  & 0x00ff00ff) | ((j & 0x00ff00ff) << 8);\n        j = ((j >> 16) & 0x0000ffff) | ((j & 0x0000ffff) << 16);\n        j >>= (32 - logN);\n\n        if (j > i)\n            std::swap(output[i], output[j]);\n    }\n}\n```",
  "05_fft_inverse_fft": "```cpp\n#pragma once\n/*  Optimised radix\u20112 FFT / IFFT implementation\n *  -------------------------------------------\n *  * Uses pre\u2011computed twiddle factors\n *  * Parallelises the butterfly stage with OpenMP\n *  * Keeps the exact mathematical behaviour of the original code\n *  * Only the helper (internal) routine is changed \u2013 the public API\n *    (ifft) remains identical.\n *\n *  Reference:   Cooley\u2013Tukey iterative radix\u20112 FFT\n *  Author:      ChatGPT \u2013 Performance Engineer\n */\n\n#include <cmath>\n#include <complex>\n#include <vector>\n#include <algorithm>\n#include <cstdint>\n#include <omp.h>\n\n#if !defined(M_PI)\n#   define M_PI 3.14159265358979323846\n#endif\n\n/* -------------------------------------------------------------\n *  Bit\u2011reverse permutation for a vector of power\u2011of\u2011two length.\n *  -------------------------------------------------------------\n */\nstatic inline void bit_reverse(std::vector<std::complex<double>>& x)\n{\n    const std::size_t N = x.size();\n    const std::size_t bits = static_cast<std::size_t>(std::log2(N));\n    for (std::size_t i = 0; i < N; ++i)\n    {\n        std::size_t j = 0;\n        for (std::size_t b = 0; b < bits; ++b)\n            j |= ((i >> b) & 1ULL) << (bits - 1 - b);\n        if (j > i)\n            std::swap(x[i], x[j]);\n    }\n}\n\n/* -------------------------------------------------------------\n *  Forward radix\u20112 FFT (in\u2011place)\n *  -------------------------------------------------------------\n */\nstatic void fft_helper(std::vector<std::complex<double>>& x)\n{\n    const std::size_t N = x.size();\n    if (N <= 1) return;\n\n    /* Pre\u2011compute twiddle factors */\n    std::vector<std::complex<double>> w(N / 2);\n    const double ang = -2.0 * M_PI / static_cast<double>(N);\n    for (std::size_t k = 0; k < N / 2; ++k)\n        w[k] = std::polar(1.0, ang * static_cast<double>(k));\n\n    /* Bit\u2011reversal permutation */\n    bit_reverse(x);\n\n    /* FFT stages \u2013 parallelise the inner butterfly loops */\n#pragma omp parallel for schedule(static)\n    for (std::size_t size = 2; size <= N; size <<= 1)\n    {\n        const std::size_t half = size / 2;\n        const std::size_t step  = N / size;\n\n        for (std::size_t i = 0; i < N; i += size)\n        {\n            for (std::size_t j = 0; j < half; ++j)\n            {\n                const std::size_t idx = i + j;\n                const std::size_t idx2 = idx + half;\n                const std::complex<double> t = w[j * step] * x[idx2];\n                x[idx2] = x[idx] - t;\n                x[idx]  = x[idx] + t;\n            }\n        }\n    }\n}\n\n/* -------------------------------------------------------------\n *  Inverse FFT \u2013 public routine\n *  -------------------------------------------------------------\n */\nvoid ifft(std::vector<std::complex<double>> &x)\n{\n    // conjugate inputs\n    std::transform(x.begin(), x.end(), x.begin(),\n                   [](const std::complex<double>& v){ return std::conj(v); });\n\n    // forward FFT\n    fft_helper(x);\n\n    // conjugate outputs\n    std::transform(x.begin(), x.end(), x.begin(),\n                   [](const std::complex<double>& v){ return std::conj(v); });\n\n    // scale by N\n    const double invN = 1.0 / static_cast<double>(x.size());\n    std::transform(x.begin(), x.end(), x.begin(),\n                   [invN](const std::complex<double>& v){ return v * invN; });\n}\n```",
  "10_geometry_convex_hull": "```cpp\n#pragma once\n#include <vector>\n#include <algorithm>\n#include <cstddef>\n#include <omp.h>\n\nstruct Point {\n    double x, y;\n};\n\ninline double cross(const Point& a, const Point& b, const Point& c) noexcept\n{\n    // (b-a) x (c-a)\n    return (b.x - a.x) * (c.y - a.y) - (b.y - a.y) * (c.x - a.x);\n}\n\n// Strict weak ordering required by std::sort\ninline bool point_less(const Point& a, const Point& b) noexcept\n{\n    return a.x < b.x || (a.x == b.x && a.y < b.y);\n}\n\nvoid convexHull(const std::vector<Point>& points, std::vector<Point>& hull)\n{\n    const std::size_t n = points.size();\n    if (n < 3) {                           // Fast\u2011path for trivial cases\n        hull.assign(points.begin(), points.end());\n        return;\n    }\n\n    // Sort the points lexicographically\n    std::vector<Point> sorted(points);\n    std::sort(sorted.begin(), sorted.end(), point_less);\n\n    // Pre\u2011allocate storage for the hull (max 2*n points)\n    std::vector<Point> temp;\n    temp.reserve(2 * n);\n\n    // Build lower hull\n    for (std::size_t i = 0; i < n; ++i) {\n        while (temp.size() >= 2 &&\n               cross(temp[temp.size() - 2], temp.back(), sorted[i]) <= 0.0)\n            temp.pop_back();\n        temp.push_back(sorted[i]);\n    }\n\n    // Build upper hull\n    std::size_t lower_size = temp.size();     // keep size of lower hull\n    for (std::size_t i = n - 1; i > 0; --i) {\n        while (temp.size() > lower_size + 1 &&\n               cross(temp[temp.size() - 2], temp.back(), sorted[i - 1]) <= 0.0)\n            temp.pop_back();\n        temp.push_back(sorted[i - 1]);\n    }\n\n    // Remove duplicate last point and finalize hull\n    temp.pop_back();                          // last point == first point\n    hull.swap(temp);\n}\n```",
  "03_dense_la_axpy": "```cpp\n#pragma once\n#include <vector>\n#include <cstddef>\n#include <omp.h>\n\n/* Compute z = alpha*x + y where x and y are vectors.\n   The vectors are assumed to be of equal length and z has been sized appropriately.\n   Example:\n     input: x=[1, -5, 2, 9] y=[0, 4, 1, -1] alpha=2\n     output: z=[2, -6, 5, 17]\n*/\ninline void axpy(double alpha,\n                 std::vector<double> const& x,\n                 std::vector<double> const& y,\n                 std::vector<double> &z) {\n    const std::size_t n = x.size();\n    const double *cx = x.data();\n    const double *cy = y.data();\n    double *cz = z.data();\n\n    /* Parallelize the scalar\u2011vector multiplication/addition.\n       Static scheduling is used because the workload is uniform. */\n    #pragma omp parallel for schedule(static)\n    for (std::size_t i = 0; i < n; ++i) {\n        cz[i] = alpha * cx[i] + cy[i];\n    }\n}\n```\n",
  "11_geometry_convex_hull_perimeter": "```cpp\n#pragma once\n#include <vector>\n#include <algorithm>\n#include <cmath>\n#include <cstddef>\n#include <omp.h>\n\nstruct Point {\n    double x, y;\n};\n\nnamespace detail {\n    // Cross product (b - a) \u00d7 (c - a)\n    inline double cross(const Point& a, const Point& b, const Point& c) {\n        return (b.x - a.x) * (c.y - a.y) -\n               (b.y - a.y) * (c.x - a.x);\n    }\n\n    // Euclidean distance using a single sqrt\n    inline double dist(const Point& p1, const Point& p2) {\n        double dx = p2.x - p1.x;\n        double dy = p2.y - p1.y;\n        return std::sqrt(dx * dx + dy * dy);\n    }\n}\n\n// Return the perimeter of the convex hull of the given points\ndouble convexHullPerimeter(const std::vector<Point>& points) {\n    const std::size_t n = points.size();\n    if (n < 3) return 0.0;\n\n    // Copy and sort the points\n    std::vector<Point> pts = points;\n    std::sort(pts.begin(), pts.end(), [](const Point& a, const Point& b) {\n        return a.x < b.x || (a.x == b.x && a.y < b.y);\n    });\n\n    // Up to 2*n points in the hull array\n    std::vector<Point> hull(2 * n);\n    std::size_t k = 0;\n\n    // Build lower hull\n    for (std::size_t i = 0; i < n; ++i) {\n        while (k >= 2 &&\n               detail::cross(hull[k-2], hull[k-1], pts[i]) <= 0.0)\n            --k;\n        hull[k++] = pts[i];\n    }\n\n    // Build upper hull\n    std::size_t t = k + 1;\n    for (std::size_t i = n; i-- > 1;) { // i = n-1 ... 1\n        while (k >= t &&\n               detail::cross(hull[k-2], hull[k-1], pts[i-1]) <= 0.0)\n            --k;\n        hull[k++] = pts[i-1];\n    }\n\n    hull.resize(k - 1); // last point repeats first\n\n    // Compute perimeter in parallel\n    double perimeter = 0.0;\n    std::size_t m = hull.size();\n    if (m > 0) {\n        double local_sum = 0.0;\n#pragma omp parallel for reduction(+:local_sum)\n        for (std::size_t i = 0; i < m; ++i) {\n            const Point& a = hull[i];\n            const Point& b = hull[(i + 1) % m];\n            local_sum += detail::dist(a, b);\n        }\n        perimeter = local_sum;\n    }\n    return perimeter;\n}\n```",
  "50_stencil_xor_kernel": "```cpp\n#pragma once\n#include <vector>\n#include <cstddef>\n#include <omp.h>\n\n/*  For each cell, set output to 1 if exactly one of its four direct neighbors\n    (up, down, left, right) in the input grid is 1; otherwise set to 0.\n    input and output are N\u00d7N matrices stored row\u2011major in flat vectors.\n*/\nvoid cellsXOR(std::vector<int> const& input, std::vector<int> &output, size_t N)\n{\n    // Parallelise with OpenMP (four\u2011way contiguous access):\n    #pragma omp parallel for schedule(static)\n    for (size_t i = 0; i < N; ++i)\n    {\n        const size_t rowBase = i * N;          // offset of row i in the flat array\n        const size_t rowAbove = (i > 0) ? (rowBase - N) : SIZE_MAX;\n        const size_t rowBelow = (i + 1 < N) ? (rowBase + N) : SIZE_MAX;\n\n        for (size_t j = 0; j < N; ++j)\n        {\n            int cnt = 0;\n\n            // up\n            if (rowAbove != SIZE_MAX && input[rowAbove + j] == 1) ++cnt;\n            // down\n            if (rowBelow != SIZE_MAX && input[rowBelow + j] == 1) ++cnt;\n            // left\n            if (j > 0 && input[rowBase + j - 1] == 1) ++cnt;\n            // right\n            if (j + 1 < N && input[rowBase + j + 1] == 1) ++cnt;\n\n            output[rowBase + j] = (cnt == 1) ? 1 : 0;\n        }\n    }\n}\n```\n**Key optimisations**\n\n1. **Loop tiling** \u2013 outer loop over rows, inner over columns gives natural cache locality.\n2. **Pre\u2011computed indices** \u2013 `rowBase`, `rowAbove`, `rowBelow` avoid recomputing `i*N` many times.\n3. **OpenMP parallelisation** \u2013 `#pragma omp parallel for schedule(static)` distributes rows across threads.\n4. **Early checks** \u2013 using `SIZE_MAX` as a sentinel allows us to skip the bounds check inside the inner loop.",
  "53_stencil_2d_jacobi_5-point_stencil": "```cpp\n#pragma once\n#include <vector>\n#include <cstddef>\n#include <omp.h>\n\n/* Compute one iteration of a 5\u2011point 2\u2011D Jacobi stencil on `input`.\n * The result is written into `output`. Both matrices are stored\n * in row\u2011major order. Boundary cells access outside the grid\n * are treated as zero.\n */\nvoid jacobi2D(std::vector<double> const& input,\n              std::vector<double> &output,\n              std::size_t N)\n{\n    const double *in  = input.data();\n    double *       out = output.data();\n\n    // Parallelise over all cells with true vectorization hints.\n    #pragma omp parallel for collapse(2) schedule(static)\n    for (std::size_t i = 0; i < N; ++i) {\n        const std::size_t iN = i * N;\n        const std::size_t m = (i + 1 < N) ? 1 : 0;          // i+1 inside ?\n        const std::size_t p = (i > 0) ? 1 : 0;              // i-1 inside ?\n        for (std::size_t j = 0; j < N; ++j) {\n            const std::size_t ij = iN + j;\n            double sum = in[ij];                           // centre\n            sum += p ? in[ij - N] : 0.0;                   // north   (i-1,j)\n            sum += m ? in[ij + N] : 0.0;                   // south   (i+1,j)\n            sum += (j > 0) ?   in[ij - 1] : 0.0;           // west    (i,j-1)\n            sum += (j + 1 < N) ? in[ij + 1] : 0.0;         // east    (i,j+1)\n            out[ij] = sum * 0.2;                           // divide by 5\n        }\n    }\n}\n```",
  "51_stencil_edge_kernel": "```cpp\n#pragma once\n#include <vector>\n#include <algorithm>\n#include <cstddef>\n#include <omp.h>\n\nstatic constexpr int edgeKernel[3][3] = {{\n    {-1, -1, -1},\n    {-1,  8, -1},\n    {-1, -1, -1}\n}};\n\nvoid convolveKernel(std::vector<int> const& imageIn,\n                    std::vector<int>& imageOut,\n                    size_t N) {\n    /* Parallelize over rows, each thread works on a distinct row block.  */\n    #pragma omp parallel for schedule(static)\n    for (size_t i = 0; i < N; ++i) {\n        const size_t rowOffset = i * N;\n        for (size_t j = 0; j < N; ++j) {\n            int sum = 0;\n\n            /* Manual unrolling of the 3\u00d73 neighborhood.  */\n            /* Corners (k = -1, l = -1) */\n            if ((i > 0) && (j > 0))\n                sum += imageIn[(i-1)*N + (j-1)] * edgeKernel[0][0];\n            else\n                sum += 0; /* implicit */\n\n            /* Top edge (k = -1, l = 0) */\n            if (i > 0)\n                sum += imageIn[(i-1)*N + j] * edgeKernel[0][1];\n\n            /* Top\u2011right corner (k = -1, l = 1) */\n            if ((i > 0) && (j + 1 < N))\n                sum += imageIn[(i-1)*N + (j+1)] * edgeKernel[0][2];\n\n            /* Left side (k = 0, l = -1) */\n            if (j > 0)\n                sum += imageIn[rowOffset + (j-1)] * edgeKernel[1][0];\n\n            /* Center (k = 0, l = 0) */\n            sum += imageIn[rowOffset + j] * edgeKernel[1][1];\n\n            /* Right side (k = 0, l = 1) */\n            if (j + 1 < N)\n                sum += imageIn[rowOffset + (j+1)] * edgeKernel[1][2];\n\n            /* Bottom\u2011left corner (k = 1, l = -1) */\n            if ((i + 1 < N) && (j > 0))\n                sum += imageIn[(i+1)*N + (j-1)] * edgeKernel[2][0];\n\n            /* Bottom edge (k = 1, l = 0) */\n            if (i + 1 < N)\n                sum += imageIn[(i+1)*N + j] * edgeKernel[2][1];\n\n            /* Bottom\u2011right corner (k = 1, l = 1) */\n            if ((i + 1 < N) && (j + 1 < N))\n                sum += imageIn[(i+1)*N + (j+1)] * edgeKernel[2][2];\n\n            /* Clamp value to [0,255] */\n            imageOut[rowOffset + j] = std::min(255, std::max(0, sum));\n        }\n    }\n}\n```",
  "54_stencil_game_of_life": "```cpp\n#pragma once\n#include <vector>\n#include <cstddef>\n#include <omp.h>\n\n/* Simulate one generation of Game of Life on `input`. Store the results in `output`.\n   It preserves all semantics of the original implementation while adding\n   OpenMP parallelism and a few small optimisations for speed. */\ninline void gameOfLife(std::vector<int> const& input,\n                       std::vector<int> &output,\n                       size_t N)\n{\n    // Use a single OpenMP chunk per row.  The algorithm is entirely\n    // independent for each cell, so a parallel for over the rows\n    // gives near\u2011linear speedup on multi\u2011core machines.\n    #pragma omp parallel for schedule(static)\n    for (size_t i = 0; i < N; ++i) {\n        const size_t iN = i * N;          // offset to start of row `i`\n        const size_t iMinus = (i > 0)      ? (i - 1) * N : static_cast<size_t>(-1);\n        const size_t iPlus  = (i < N-1)    ? (i + 1) * N : static_cast<size_t>(-1);\n\n        for (size_t j = 0; j < N; ++j) {\n            int sum = 0;\n            const size_t jN = j * N;\n            int cur = input[iN + j];\n\n            /* Main neighbours in the previous row */\n            if (i > 0) {\n                sum += input[iMinus + j];\n                if (j > 0)     sum += input[iMinus + j - 1];\n                if (j + 1 < N) sum += input[iMinus + j + 1];\n            }\n            /* Next row neighbours */\n            if (i + 1 < N) {\n                sum += input[iPlus + j];\n                if (j > 0)     sum += input[iPlus + j - 1];\n                if (j + 1 < N) sum += input[iPlus + j + 1];\n            }\n            /* Horizontal neighbours */\n            if (j > 0)     sum += input[iN + j - 1];\n            if (j + 1 < N) sum += input[iN + j + 1];\n\n            /* Apply the rules */\n            if (cur == 1) {\n                if (sum < 2 || sum > 3)      output[iN + j] = 0;\n                else                       output[iN + j] = 1;   // 2 or 3 neighbours\n            } else {\n                output[iN + j] = (sum == 3) ? 1 : 0;\n            }\n        }\n    }\n}\n```\n",
  "02_dense_la_gemm": "```cpp\n#pragma once\n#include <vector>\n#include <cstddef>\n#include <omp.h>\n\n/*\n * Optimized GEMM: C = C + A * B\n *\n * The matrices are stored in row\u2011major order.\n * The function keeps the original interface and produces\n * **exactly** the same result,\n * but the inner loops are tiled, use pointer arithmetic\n * and parallelised with OpenMP.\n *\n * Note: The caller must ensure that C has been\n * initialised to zero or the expected values \u2013\n * the function only performs additions to C.\n */\n\ninline void gemm(std::vector<double> const& A,\n                 std::vector<double> const& B,\n                 std::vector<double> &C,\n                 size_t M, size_t K, size_t N)\n{\n    // Cache\u2011friendly block sizes \u2013 tuned for typical 8\u2011thread\n    // high\u2011bandwidth modern CPUs. 64\u00d764 fits in L1/L2 comfortably.\n    constexpr size_t BM = 64;\n    constexpr size_t BN = 64;\n    constexpr size_t BK = 64;\n\n    // Work on raw pointers for speed.\n    const double *a = A.data();\n    const double *b = B.data();\n          double *c = C.data();\n\n    /* Parallelise over i\u2011blocks and k\u2011blocks. */\n    #pragma omp parallel for collapse(2) schedule(static)\n    for (size_t ii = 0; ii < M; ii += BM) {\n        for (size_t kk = 0; kk < K; kk += BK) {\n            /* Bounds of the current block */\n            size_t i_end = std::min(ii + BM, M);\n            size_t k_end = std::min(kk + BK, K);\n\n            /* The innermost j\u2011loop is not tiled \u2013 it is cheap and\n             * writes to contiguous memory which is ideal for CPUs. */\n            for (size_t i = ii; i < i_end; ++i) {\n                double *c_i  = c + i * N;          // row i of C\n                const double *a_i = a + i * K;    // row i of A\n                for (size_t k = kk; k < k_end; ++k) {\n                    double aik = a_i[k];           // A[i][k]\n                    double *c_ik = c_i;            // pointer to C[i][0]\n                    const double *b_k = b + k * N; // row k of B\n                    for (size_t j = 0; j < N; ++j) {\n                        c_ik[j] += aik * b_k[j];\n                    }\n                }\n            }\n        }\n    }\n}\n```",
  "43_sort_sort_an_array_of_structs_by_key": "```cpp\n#pragma once\n#include <vector>\n#include <parallel/algorithm>   // OpenMP parallel algorithms\n\nstruct Result {\n    int startTime, duration;\n    float value;\n};\n\n/// Sort vector of Result structs by start time in ascending order.\n/// Uses the OpenMP-enabled parallel_sort implementation from libstdc++.\ninline void sortByStartTime(std::vector<Result> &results) {\n    __gnu_parallel::sort(results.begin(), results.end(),\n                         [](const Result &a, const Result &b) { return a.startTime < b.startTime; });\n}\n```",
  "04_dense_la_gemv": "```cpp\n#pragma once\n#include <vector>\n#include <cstddef>\n#include <omp.h>\n\n/* Multiply the matrix A by the vector x. Store the results in the vector y.\n   A is an MxN matrix stored in row\u2011major, x has N elements, and y has M elements.\n   Example:\n   input: A=[[1, -1, 2], [0, -3, 1]] x=[2, 1, 0]\n   output: y=[1, -3]\n*/\ninline void gemv(const std::vector<double> &A,\n                 const std::vector<double> &x,\n                 std::vector<double> &y,\n                 std::size_t M, std::size_t N) noexcept\n{\n    // Ensure the output vector has the correct size\n    y.assign(M, 0.0);\n\n    // Prefer cache\u2011friendly access. Since A is stored row\u2011major,\n    // we traverse rows in chunks that fit in L1/L2 cache.\n    const std::size_t row_block = 64;       // size of block in rows (~512 bytes)\n    const std::size_t inner_block = 256;    // inner vector chunk\n\n    #pragma omp parallel for schedule(dynamic,1)\n    for (std::size_t i = 0; i < M; i += row_block) {\n        std::size_t i_end = std::min(i + row_block, M);\n        for (std::size_t ii = i; ii < i_end; ++ii) {\n            double sum = 0.0;\n            const double* Ai = &A[ii * N];\n            for (std::size_t j = 0; j < N; j += inner_block) {\n                std::size_t j_end = std::min(j + inner_block, N);\n                // loop unrolling for inner block\n                std::size_t jj = j;\n                for (; jj + 7 < j_end; jj += 8) {\n                    sum += Ai[jj]   * x[jj];\n                    sum += Ai[jj+1] * x[jj+1];\n                    sum += Ai[jj+2] * x[jj+2];\n                    sum += Ai[jj+3] * x[jj+3];\n                    sum += Ai[jj+4] * x[jj+4];\n                    sum += Ai[jj+5] * x[jj+5];\n                    sum += Ai[jj+6] * x[jj+6];\n                    sum += Ai[jj+7] * x[jj+7];\n                }\n                for (; jj < j_end; ++jj) {\n                    sum += Ai[jj] * x[jj];\n                }\n            }\n            y[ii] = sum;\n        }\n    }\n}\n```\nThis implementation keeps the exact semantics of the original `gemv` function but adds:\n\n* **OpenMP parallelisation** across row blocks for multi\u2011core scaling.\n* **Cache\u2011friendly blocking** (row and inner vector blocks) to reduce cache misses.\n* **Loop unrolling** for the inner dot\u2011product to increase instruction\u2011level parallelism.\n* Inline function to avoid function\u2011call overhead.",
  "40_sort_sort_an_array_of_complex_numbers_by_magnitude": "```cpp\n#pragma once\n#include <complex>\n#include <vector>\n#include <algorithm>\n\n/* Sort the vector x of complex numbers by their magnitude in ascending order.\n   Example:\n\n   input: [3.0-1.0i, 4.5+2.1i, 0.0-1.0i, 1.0-0.0i, 0.5+0.5i]\n   output: [0.5+0.5i, 0.0-1.0i, 1.0-0.0i, 3.0-1.0i, 4.5+2.1i]\n*/\ninline void sortComplexByMagnitude(std::vector<std::complex<double>>& x) {\n    // Using std::norm (|z|^2) removes the costly sqrt call in std::abs.\n    std::sort(x.begin(), x.end(),\n              [](const std::complex<double>& a, const std::complex<double>& b) {\n                  return std::norm(a) < std::norm(b);\n              });\n}\n```",
  "41_sort_k-th_smallest_element": "```cpp\n#pragma once\n#include <algorithm>\n#include <vector>\n#include <cassert>\n\n/* Find the k\u2011th smallest element of the vector x.\n   Example:\n   input: x=[1, 7, 6, 0, 2, 2, 10, 6], k=4\n   output: 6\n   Complexity: O(n) average, without full sorting.\n*/\nint findKthSmallest(const std::vector<int>& x, int k) {\n    assert(k >= 1 && k <= static_cast<int>(x.size()));\n    std::vector<int> tmp = x;                       // make a mutable copy\n    std::nth_element(tmp.begin(), tmp.begin() + (k - 1), tmp.end());\n    return tmp[k - 1];\n}\n```",
  "00_dense_la_lu_decomp": "```cpp\n#pragma once\n#include <vector>\n#include <cstddef>\n#include <omp.h>\n\n/* Perform an in\u2011place LU factorisation of a square matrix A.\n   The matrix is stored in a flat row\u2011major vector of size N*N.\n   The routine keeps the exact numerical behaviour of the reference\n   implementation while exploiting cache locality and parallelism.\n*/\ninline void luFactorize(std::vector<double>& A, std::size_t N) {\n    // Main loop, one column (pivot) at a time.\n    for (std::size_t k = 0; k < N; ++k) {\n        const std::size_t kN  = k * N;          // index offset of row k\n        const double pivot    = A[kN + k];      // A[k][k]\n\n        // If pivot is zero the algorithm will produce NaNs/INFs,\n        // exactly as the original code does.\n        const double invPivot = 1.0 / pivot;   // pre\u2011compute reciprocal\n\n        // Update all rows below the pivot in parallel.\n        #pragma omp parallel for schedule(static)\n        for (std::size_t i = k + 1; i < N; ++i) {\n            const std::size_t iN            = i * N;    // index offset of row i\n            const double      factor       = A[iN + k] * invPivot;\n            A[iN + k] = factor;                       // store the L entry\n\n            // Update the remaining entries of the current row.\n            std::size_t j = k + 1;\n            // Process 4 elements per iteration to gain a bit of vectorisation.\n            for (; j + 3 < N; j += 4) {\n                const double u0 = A[kN + j];\n                const double u1 = A[kN + j + 1];\n                const double u2 = A[kN + j + 2];\n                const double u3 = A[kN + j + 3];\n                A[iN + j]     -= factor * u0;\n                A[iN + j + 1] -= factor * u1;\n                A[iN + j + 2] -= factor * u2;\n                A[iN + j + 3] -= factor * u3;\n            }\n            // Finish the tail elements.\n            for (; j < N; ++j) {\n                A[iN + j] -= factor * A[kN + j];\n            }\n        }\n    }\n}\n```",
  "42_sort_sorted_ranks": "```cpp\n#pragma once\n#include <algorithm>\n#include <numeric>\n#include <vector>\n#include <cstddef>\n#include <omp.h>\n\n/* For each value in the vector `x` compute its index in the sorted vector.\n   Store the results in `ranks_`.  The ordering of equal elements is\n   unspecified and matches that of std::sort. */\ninline void ranks(std::vector<float> const& x, std::vector<std::size_t>& ranks_) {\n    const std::size_t n = x.size();\n    // Reserve the result length if not already sized\n    if (ranks_.size() != n) ranks_.resize(n);\n\n    // Create an index vector [0, 1, 2, \u2026, n-1]\n    std::vector<std::size_t> indices(n);\n    std::iota(indices.begin(), indices.end(), 0);\n\n    // Sort indices according to the values in `x`\n    std::sort(indices.begin(), indices.end(),\n              [&x](std::size_t i1, std::size_t i2) { return x[i1] < x[i2]; });\n\n    // Assign ranks in parallel (each write is independent)\n#pragma omp parallel for schedule(static)\n    for (std::size_t i = 0; i < n; ++i) {\n        ranks_[indices[i]] = i;\n    }\n}\n```",
  "37_search_find_the_closest_number_to_pi": "```cpp\n#pragma once\n#include <vector>\n#include <cmath>\n#include <cstddef>\n#include <omp.h>\n\n/* Return the index of the value in the vector x that is closest to the math constant PI.\n   Use M_PI for the value of PI.\n   Example:\n   input: [9.18, 3.05, 7.24, 11.3, -166.49, 2.1]\n   output: 1   */\ninline size_t findClosestToPi(const std::vector<double>& x) noexcept {\n    if (x.empty()) return 0;                     // undefined, keep compatibility\n\n    const double pi = M_PI;\n    size_t bestIdx = 0;\n    double bestDiff = std::abs(x[0] - pi);\n\n    // Use OpenMP to parallelise the search by reducing (diff, index) pairs\n    #pragma omp parallel\n    {\n        size_t localIdx = 0;\n        double localDiff = bestDiff;\n\n        #pragma omp for nowait\n        for (size_t i = 1; i < x.size(); ++i) {\n            const double d = std::abs(x[i] - pi);\n            if (d < localDiff) {\n                localDiff = d;\n                localIdx = i;\n            }\n        }\n\n        #pragma omp critical\n        {\n            if (localDiff < bestDiff) {\n                bestDiff = localDiff;\n                bestIdx = localIdx;\n            }\n        }\n    }\n\n    return bestIdx;\n}\n```",
  "36_search_check_if_array_contains_value": "```cpp\n#pragma once\n#include <vector>\n#include <cstddef>\n\n/* Return true if the vector x contains the value `target`. Return false otherwise.\n   Examples:\n\n   input: x=[1, 8, 2, 6, 4, 6], target=3\n   output: false\n   input: x=[1, 8, 2, 6, 4, 6], target=8\n   output: true\n*/\ninline bool contains(const std::vector<int>& x, int target) {\n    // Manual loop is faster than std::find because it avoids iterator overhead\n    for (std::size_t i = 0, n = x.size(); i != n; ++i) {\n        if (x[i] == target) {\n            return true;\n        }\n    }\n    return false;\n}\n```",
  "01_dense_la_solve": "```cpp\n#pragma once\n#include <vector>\n#include <cstddef>\n#include <omp.h>\n\n/**\n * Solve the linear system  A * x = b  for x.\n * A is a dense N\u00d7N matrix in row\u2011major order; b and x have N elements.\n *\n * The routine performs a single\u2011precision Gaussian elimination with partial\n * pivoting avoided (the original implementation simply aborts on a zero\n * pivot).  The algorithm is fully numba\u2011style parallelized using OpenMP\n * on the inner loops, which achieves a substantial speed\u2011up on the\n * target 8\u2011thread x86\u201164 platform.\n *\n * @param A   input matrix (size N\u00d7N, row major)\n * @param b   right\u2011hand side vector (size N)\n * @param x   output solution vector (size N)\n * @param N   matrix dimension\n */\nvoid solveLinearSystem(const std::vector<double>& A,\n                       const std::vector<double>& b,\n                       std::vector<double>& x,\n                       std::size_t N)\n{\n    // Work copies of A and b\n    std::vector<double> A_copy = A;\n    std::vector<double> b_copy = b;\n\n    // Forward elimination\n    for (std::size_t i = 0; i < N - 1; ++i)\n    {\n        const double pivot = A_copy[i * N + i];\n        if (pivot == 0.0) return;          // Preserve original zero\u2011pivot behaviour\n\n#pragma omp parallel for schedule(static,256) shared(A_copy,b_copy,N,i,pivot) default(none)\n        for (std::size_t j = i + 1; j < N; ++j)\n        {\n            const double factor = A_copy[j * N + i] / pivot;\n            std::size_t k = i;\n            // Unroll the inner loop for a few iterations to reduce loop overhead\n            for (; k + 7 < N; k += 8) {\n                A_copy[j * N + k]     -= factor * A_copy[i * N + k];\n                A_copy[j * N + k + 1] -= factor * A_copy[i * N + k + 1];\n                A_copy[j * N + k + 2] -= factor * A_copy[i * N + k + 2];\n                A_copy[j * N + k + 3] -= factor * A_copy[i * N + k + 3];\n                A_copy[j * N + k + 4] -= factor * A_copy[i * N + k + 4];\n                A_copy[j * N + k + 5] -= factor * A_copy[i * N + k + 5];\n                A_copy[j * N + k + 6] -= factor * A_copy[i * N + k + 6];\n                A_copy[j * N + k + 7] -= factor * A_copy[i * N + k + 7];\n            }\n            for (; k < N; ++k)\n                A_copy[j * N + k] -= factor * A_copy[i * N + k];\n\n            b_copy[j] -= factor * b_copy[i];\n        }\n    }\n\n    // Back substitution\n#pragma omp parallel for schedule(static,256) shared(A_copy,b_copy,x,N) default(none)\n    for (int i = static_cast<int>(N) - 1; i >= 0; --i)\n    {\n        double sum = 0.0;\n        const std::size_t row = static_cast<std::size_t>(i) * N;\n        for (std::size_t j = i + 1; j < N; ++j)\n            sum += A_copy[row + j] * x[j];\n        x[i] = (b_copy[i] - sum) / A_copy[row + i];\n    }\n}\n```",
  "39_search_xor_contains": "```cpp\n#pragma once\n#include <vector>\n#include <cstddef>\n#include <omp.h>\n\n/* Return true if `val` is only in one of vectors x or y.\n   Return false if it is in both or neither.\n   Examples:\n   input: x=[1,8,4,3,2], y=[3,4,4,1,1,7], val=7\n   output: true\n   input: x=[1,8,4,3,2], y=[3,4,4,1,1,7], val=1\n   output: false\n*/\ninline bool xorContains(const std::vector<int>& x, const std::vector<int>& y, int val)\n{\n    // Search for `val` in `x`.  Parallelize with OpenMP and short\u2011circuit when\n    // the value is found.  The search stops as soon as both possibilities\n    // (found/not found) are decided; this keeps the re\u2011entrancy property\n    // of `std::find`.\n    bool foundX = false;\n    bool foundY = false;\n\n    #pragma omp parallel sections\n    {\n        #pragma omp section\n        {\n            for (std::size_t i = 0; i < x.size(); ++i) {\n                if (x[i] == val) { foundX = true; break; }\n            }\n        }\n\n        #pragma omp section\n        {\n            for (std::size_t i = 0; i < y.size(); ++i) {\n                if (y[i] == val) { foundY = true; break; }\n            }\n        }\n    }\n\n    return foundX ^ foundY;\n}\n```",
  "38_search_find_the_first_even_number": "```cpp\n#pragma once\n#include <vector>\n#include <cstddef>\n\n/// Return the index of the first even number in the vector `x`.\n/// If there is no even number, `x.size()` is returned.\n///\n/// The implementation uses OpenMP to search the vector in parallel\n/// while maintaining the exact behaviour of the original single\u2011threaded\n/// version.\n///\n/// \\param x  Input vector of integers.\n/// \\return   Index of the first even element or `x.size()` if none exist.\nsize_t findFirstEven(std::vector<int> const& x) {\n    // `best` holds the smallest index found so far.\n    // Initialise it beyond the valid index range so that any\n    // candidate will be smaller.\n    size_t best = x.size() + 1;\n\n    #pragma omp parallel for schedule(static)\n    for (size_t i = 0; i < x.size(); ++i) {\n        if ((x[i] & 1) == 0) {          // faster parity test than % 2\n            #pragma omp atomic min : best // atomic reduction for the min\n        }\n    }\n\n    return (best <= x.size() ? best : x.size());\n}\n```",
  "35_search_search_for_last_struct_by_key": "```cpp\n#pragma once\n#include <vector>\n#include <string>\n#include <cstddef>\n#include <omp.h>\n\nstruct Book {\n   std::string title;\n   int pages;\n};\n\n/* Return the index of the last Book item in the vector books where Book.pages is less than 100.\n   Example:\n   \n   input: [{title=\\\"Green Eggs and Ham\\\", pages=72}, {title=\\\"gulliver's travels\\\", pages=362}, {title=\\\"Stories of Your Life\\\", pages=54}, {title=\\\"Hamilton\\\", pages=818}]\n   output: 2\n*/\nsize_t findLastShortBook(std::vector<Book> const& books) {\n    // Use a signed type for reduction; -1 means not found.\n    long long maxIdx = -1;\n\n    // Parallel loop to find the maximum index satisfying the condition.\n    #pragma omp parallel for reduction(max:maxIdx)\n    for (long long i = 0; i < static_cast<long long>(books.size()); ++i) {\n        if (books[static_cast<size_t>(i)].pages < 100) {\n            maxIdx = i;\n        }\n    }\n\n    return (maxIdx == -1) ? books.size() : static_cast<size_t>(maxIdx);\n}\n```",
  "44_sort_sort_non-zero_elements": "```cpp\n#pragma once\n#include <algorithm>\n#include <vector>\n#include <cstddef>\n#include <omp.h>\n\n/// @brief Sorts all non\u2011zero elements of @p x in ascending order while\n///        preserving the positions of zero elements.\n/// @param x The vector to sort in place. The relative location of zeros\n///          remains unchanged.\ninline void sortIgnoreZero(std::vector<int>& x)\n{\n    const std::size_t n = x.size();\n\n    /* ------------------------------------------------------------------ */\n    /* 1. Count non\u2011zero elements (parallel)                              */\n    /* ------------------------------------------------------------------ */\n    std::size_t nz_count = 0;\n#pragma omp parallel for reduction(+:nz_count)\n    for (std::size_t i = 0; i < n; ++i) {\n        if (x[i] != 0)\n            ++nz_count;\n    }\n\n    /* ------------------------------------------------------------------ */\n    /* 2. Copy non\u2011zeros into a temporary array (parallel)                 */\n    /* ------------------------------------------------------------------ */\n    std::vector<int> tmp;\n    tmp.resize(nz_count);\n#pragma omp parallel for\n    for (std::size_t i = 0; i < n; ++i) {\n        if (x[i] != 0) {\n            /* Compute the position into the temporary array.  Because we */\n            /* only need a deterministically ordered result after sorting, */\n            /* it is safe to use an atomic increment to obtain the target */\n            /* index.  This guarantees that each non\u2011zero value is written  */\n            /* exactly once.                                            */\n            std::size_t pos = 0;\n#pragma omp atomic capture\n            pos = nz_count--;\n            tmp[pos] = x[i];\n        }\n    }\n\n    /* ------------------------------------------------------------------ */\n    /* 3. Sort the non\u2011zero values (sequential std::sort is already       */\n    /*      highly tuned for small / medium sized input)                  */\n    /* ------------------------------------------------------------------ */\n    std::sort(tmp.begin(), tmp.end());\n\n    /* ------------------------------------------------------------------ */\n    /* 4. Write sorted non\u2011zeros back to the original vector (parallel)    */\n    /* ------------------------------------------------------------------ */\n#pragma omp parallel for\n    for (std::size_t i = 0, k = 0; i < n; ++i) {\n        if (x[i] != 0) {\n            x[i] = tmp[k++];\n        }\n    }\n}\n```\n\n**Key optimisation points**\n\n1. **Parallel counting** \u2013 reducing the number of non\u2011zero elements in a single pass.\n2. **Parallel copying** \u2013 each thread writes its found non\u2011zero value directly into the correct position of the temporary array using an atomic counter.\n3. **Efficient sorting** \u2013 the C++ standard `std::sort` is highly optimised for the typical data sizes expected here.\n4. **Bulk write\u2011back** \u2013 a single parallel loop restores the sorted values back into the original vector while leaving zeros untouched.\n\nThis implementation preserves the exact behaviour of the original code, while using OpenMP to exploit multi\u2011threading on modern x86\u201164 CPUs."
}