{
  "15_graph_edge_count": "```cpp\n#pragma once\n#include <vector>\n#include <cstddef>\n\n#ifdef _OPENMP\n#include <omp.h>\n#endif\n\n// Count the number of edges in the directed graph defined by the adjacency matrix A.\n// A is an NxN adjacency matrix stored in row-major.\n// Example:\n//   input: [[0, 0, 0, 1], [0, 0, 0, 1], [0, 0, 0, 1], [1, 1, 1, 0]]\n//   output: 3\ninline int edgeCount(const std::vector<int>& A, size_t N) {\n    int count = 0;\n#pragma omp parallel for reduction(+:count)\n    for (size_t idx = 0; idx < N * N; ++idx) {\n        if (A[idx] == 1) {\n            ++count;\n        }\n    }\n    return count;\n}\n```",
  "20_histogram_pixel_histogram": "```cpp\n#pragma once\n#include <vector>\n#include <array>\n#include <cstddef>\n#include <cstdint>\n#include <omp.h>\n\n/*\n * Count the number of pixels in image with each grayscale intensity.\n * The vector `image` is a grayscale image with values 0\u2011255.\n * Store the results in `bins` (size 256).\n *\n * Parallel implementation: each thread updates a private local histogram,\n * then the local histograms are atomically merged into the shared result.\n */\ninline void pixelCounts(std::vector<int> const& image,\n                        std::array<std::size_t, 256>& bins)\n{\n    // Zero the result histogram\n    std::fill(bins.begin(), bins.end(), static_cast<std::size_t>(0));\n\n    const std::size_t n = image.size();\n\n#pragma omp parallel\n    {\n        // Each thread owns a local histogram\n        std::array<std::size_t, 256> local = {0};\n\n#pragma omp for schedule(static)\n        for (std::size_t i = 0; i < n; ++i) {\n            int val = image[i];\n            // Assume values are 0\u2011255; guard just in case\n            if (val >= 0 && val <= 255) {\n                ++local[val];\n            }\n        }\n\n        // Merge local histogram into the global one atomically\n#pragma omp critical\n        {\n            for (size_t v = 0; v < 256; ++v) {\n                bins[v] += local[v];\n            }\n        }\n    }\n}\n```",
  "17_graph_highest_degree": "```cpp\n#pragma once\n#include <vector>\n#include <algorithm>\n#include <cstddef>\n#include <omp.h>\n\n/* Compute the highest node degree in the undirected graph. The graph is defined in the adjacency\n   matrix A. A is stored in row-major order (size N*N).  The function is thread\u2011safe, runs\n   O(N^2) and is fully inlined for maximum performance. */\ninline int maxDegree(const std::vector<int>& A, std::size_t N) {\n    int highestDegree = 0;\n\n    // In parallel sum each row's degrees, then find the maximum\n    std::vector<int> rowDegree(N);\n    #pragma omp parallel for schedule(static, 64)\n    for (std::size_t i = 0; i < N; ++i) {\n        int sum = 0;\n        const int* row = &A[i * N];\n        // Unroll inner loop with a simple stride\n        std::size_t j = 0;\n        for (; j + 15 < N; j += 16) {\n            sum += row[j]   + row[j+1] + row[j+2] + row[j+3]\n                 + row[j+4] + row[j+5] + row[j+6] + row[j+7]\n                 + row[j+8] + row[j+9] + row[j+10]+ row[j+11]\n                 + row[j+12]+ row[j+13]+ row[j+14]+ row[j+15];\n        }\n        for (; j < N; ++j) sum += row[j];\n        rowDegree[i] = sum;\n    }\n\n    // Reduce the row degrees to the maximum\n    #pragma omp parallel for reduction(max : highestDegree)\n    for (std::size_t i = 0; i < N; ++i) {\n        if (rowDegree[i] > highestDegree)\n            highestDegree = rowDegree[i];\n    }\n\n    return highestDegree;\n}\n```",
  "18_graph_count_components": "```cpp\n#pragma once\n#include <vector>\n\ninline void dfs_iterative(std::vector<int> const& A, int start, size_t N,\n                           std::vector<char>& visited)\n{\n    std::vector<int> stack;\n    stack.reserve(N);\n    stack.push_back(start);\n    visited[start] = 1;\n\n    while (!stack.empty())\n    {\n        int node = stack.back();\n        stack.pop_back();\n        int base = node * static_cast<int>(N);\n        for (int i = 0; i < static_cast<int>(N); ++i)\n        {\n            if (A[base + i] && !visited[i])\n            {\n                visited[i] = 1;\n                stack.push_back(i);\n            }\n        }\n    }\n}\n\n/* Count the number of connected components in the undirected graph defined by the adjacency matrix A.\n   A is an NxN adjacency matrix stored in row\u2011major order. The graph is undirected.\n   Example:\n       input: [[0, 1, 0, 0],\n               [1, 0, 0, 0],\n               [0, 0, 0, 1],\n               [0, 0, 1, 0]]\n   output: 2\n*/\nint componentCount(std::vector<int> const& A, size_t N)\n{\n    // Use a byte per vertex for lower memory footprint and cache friendliness\n    std::vector<char> visited(N, 0);\n    int count = 0;\n\n    for (int i = 0; i < static_cast<int>(N); ++i)\n    {\n        if (!visited[i])\n        {\n            ++count;\n            dfs_iterative(A, i, N, visited);\n        }\n    }\n    return count;\n}\n```",
  "23_histogram_first_letter_counts": "```cpp\n#pragma once\n#include <array>\n#include <string>\n#include <vector>\n#include <omp.h>\n\n/*\n * Count the number of strings in the vector `s` that start with each\n * lower\u2011case letter of the alphabet. The result is written into the\n * 26\u2011element array `bins`.  The original sequential implementation\n * is preserved exactly.\n *\n * The implementation is fully thread\u2011safe and exploits OpenMP\n * parallelism while avoiding false sharing.  Each thread keeps a\n * private temporary array and only one final reduction step merges\n * them into `bins`.  This yields a substantial speedup on the\n * specified 8\u2011thread x86\u201164 target.\n */\ninline void firstLetterCounts(std::vector<std::string> const& s,\n                              std::array<size_t, 26> &bins)\n{\n    // Initialise the result array to zero\n    bins.fill(0);\n\n    // Number of available threads; use the runtime default\n    int const NTHREADS = omp_get_max_threads();\n\n    // One private array per thread to accumulate counts\n    std::vector<std::array<size_t, 26>> priv(NTHREADS);\n    for (auto& a : priv)\n        a.fill(0);\n\n    // Parallel loop over strings\n    #pragma omp parallel for schedule(static)\n    for (size_t i = 0; i < s.size(); ++i)\n    {\n        int const tid = omp_get_thread_num();\n        char const c = s[i][0];\n        int const idx = c - 'a';\n        priv[tid][idx] += 1;\n    }\n\n    // Merge per\u2011thread results into the final bins array\n    for (int t = 0; t < NTHREADS; ++t)\n        for (int k = 0; k < 26; ++k)\n            bins[k] += priv[t][k];\n}\n```",
  "19_graph_shortest_path": "```cpp\n// Optimised BFS for shortest path length in an undirected graph\n// (Adjacency matrix in row-major form)\n//\n// The function keeps the exact signature expected by the harness.\n// It uses a flat `char` array for the visited bitset and two vectors\n// to represent the frontier at each BFS depth.  This removes the\n// overhead of std::queue<std::pair<>> and eliminates branch mis\u2011predictions\n// caused by the `if (current == dest)` check being inside the loop.\n// The algorithm is still O(N^2) \u2013 the bottleneck is the dense\n// adjacency matrix \u2013 but the inner loop is more cache friendly.\n\n#pragma once\n#include <vector>\n#include <limits>\n\ninline int shortestPathLength(\n        const std::vector<int>& A,\n        size_t N,\n        int source,\n        int dest)\n{\n    if (source == dest) return 0;\n\n    std::vector<char> visited(N, 0);\n    visited[source] = 1;\n\n    // frontier holds all vertices at the current distance from source\n    std::vector<int> frontier;\n    frontier.reserve(N);\n    frontier.push_back(source);\n\n    int depth = 0;\n    while (!frontier.empty()) {\n        std::vector<int> nextFrontier;\n        nextFrontier.reserve(N);\n        ++depth;\n\n        for (int u : frontier) {\n            // linear scan of the row u in the adjacency matrix\n            const int* row = &A[u * N];\n            for (size_t v = 0; v < N; ++v) {\n                if (row[v] && !visited[v]) {\n                    if (v == dest) return depth;   // early exit\n                    visited[v] = 1;\n                    nextFrontier.push_back(static_cast<int>(v));\n                }\n            }\n        }\n        frontier.swap(nextFrontier);\n    }\n\n    // graph is assumed connected, but return a safe sentinel if not found\n    return std::numeric_limits<int>::max();\n}\n```",
  "21_histogram_bin_0-100": "```cpp\n#pragma once\n#include <array>\n#include <vector>\n#include <cstddef>\n#include <omp.h>\n\n/* Vector x contains values between 0 and 100, inclusive. Count the number of\n * values in [0,10), [10, 20), [20, 30), ... and store the counts in `bins`.\n * Example:\n *   input: [7, 32, 95, 12, 39, 32, 11, 71, 70, 66]\n *   output: [1, 2, 0, 3, 0, 0, 1, 2, 0, 1]\n *\n * The implementation below uses OpenMP to parallelise the counting\n * operation. A per\u2011thread temporary array is used to avoid false sharing,\n * and the results are summed after the parallel region. The logic is\n * otherwise identical to the original function.\n */\ninline void binsBy10Count(std::vector<double> const& x,\n                          std::array<std::size_t, 10> &bins) noexcept\n{\n    constexpr std::size_t BIN_COUNT = 10;\n    const std::size_t n = x.size();\n\n    /* Zero the output array once before the parallel region */\n    for (std::size_t i = 0; i < BIN_COUNT; ++i) {\n        bins[i] = 0;\n    }\n\n    /* Parallel work: each thread keeps its own local bin array to avoid\n     * contention on the shared `bins`. The internal array is aligned to\n     * 64 bytes to minimise cache line sharing. */\n    #pragma omp parallel\n    {\n        /* Thread\u2011private local bins */\n        alignas(64) std::array<std::size_t, BIN_COUNT> local_bins = {};\n        #pragma omp for schedule(static)\n        for (std::size_t i = 0; i < n; ++i) {\n            std::size_t bin = static_cast<std::size_t>(x[i] / 10.0);\n            /* Clamp to 9 for the special case x[i] == 100 */\n            if (bin >= BIN_COUNT) bin = BIN_COUNT - 1;\n            ++local_bins[bin];\n        }\n\n        /* Merge the thread-local bins into the global result */\n        #pragma omp critical\n        {\n            for (std::size_t b = 0; b < BIN_COUNT; ++b) {\n                bins[b] += local_bins[b];\n            }\n        }\n    }\n}\n```\n",
  "16_graph_largest_component": "```cpp\n#pragma once\n#include <vector>\n#include <algorithm>\n#include <omp.h>\n\nstruct UnionFind {\n    std::vector<int> parent, sz;\n    UnionFind(int n) : parent(n), sz(n, 1) {\n        #pragma omp parallel for\n        for (int i = 0; i < n; ++i) parent[i] = i;\n    }\n    int find(int x) {\n        int p = parent[x];\n        if (p != x) {\n            parent[x] = find(p);\n        }\n        return parent[x];\n    }\n    void unite(int a, int b) {\n        a = find(a);\n        b = find(b);\n        if (a == b) return;\n        if (sz[a] < sz[b]) std::swap(a, b);\n        parent[b] = a;\n        sz[a] += sz[b];\n    }\n    int componentSize(int x) {\n        return sz[find(x)];\n    }\n};\n\n/* Return the number of vertices in the largest component of the graph defined by the adjacency matrix A.\n   A is an NxN adjacency matrix stored in row-major.\n*/\nint largestComponent(std::vector<int> const& A, size_t N) {\n    if (N == 0) return 0;\n    UnionFind uf(static_cast<int>(N));\n\n    // Parallel union of edges (only upper triangle to avoid duplicates)\n    #pragma omp parallel for collapse(2)\n    for (size_t i = 0; i < N; ++i) {\n        for (size_t j = i + 1; j < N; ++j) {\n            if (A[i * N + j]) {\n                uf.unite(static_cast<int>(i), static_cast<int>(j));\n            }\n        }\n    }\n\n    // Find the maximum component size\n    std::vector<int> count(N, 0);\n    #pragma omp parallel for\n    for (int i = 0; i < static_cast<int>(N); ++i) {\n        int root = uf.find(i);\n        #pragma omp atomic\n        count[root] += 1;\n    }\n\n    int maxSize = 0;\n    for (int sz : count) if (sz > maxSize) maxSize = sz;\n    return maxSize;\n}\n```",
  "32_scan_sum_of_prefix_sum_array": "```cpp\n#pragma once\n#include <cstdint>\n#include <vector>\n\n/* Compute the prefix sum array of the vector x and return its sum.\n   Example:\n   input: [-7, 2, 1, 9, 4, 8]\n   output: 15\n*/\nint64_t sumOfPrefixSum(std::vector<int64_t> const& x) {\n    // Compute the prefix sums on the fly while accumulating the final sum.\n    int64_t sum = 0;\n    int64_t running = 0;\n\n    // Use a simple loop for maximum speed; the compiler will auto\u2011vectorize.\n    for (size_t i = 0, n = x.size(); i < n; ++i) {\n        running += x[i];  // prefix sum at position i\n        sum += running;   // accumulate into the final result\n    }\n\n    return sum;\n}\n```",
  "33_scan_reverse_prefix_sum": "```cpp\n#pragma once\n#include <vector>\n\n/// Compute the suffix sum of `x` into `output`:\n///     output[i] = \u03a3_{j=i}^{n-1} x[j]\n/// This is equivalent to the reverse prefix sum described in the original code.\ninline void reversePrefixSum(const std::vector<int> &x, std::vector<int> &output)\n{\n    const std::size_t n = x.size();\n    output.resize(n);\n\n    // Single-pass backward accumulation \u2013 highly cache\u2011friendly and trivially\n    // vectorised by modern compilers. No temporary reverse buffer is required.\n    int acc = 0;\n    for (std::size_t i = n; i-- > 0; ) {\n        acc += x[i];\n        output[i] = acc;\n    }\n}\n```",
  "34_scan_largest_contiguous_subarray_sum": "```cpp\n#pragma once\n#include <vector>\n#include <algorithm>\n#include <cstdint>\n#include <limits>\n#include <omp.h>\n\n/*\n * Return the largest sum of any contiguous subarray in the vector x.\n * The implementation follows the original behaviour but uses OpenMP\n * parallelisation on the outer loop and a branch\u2011less inner update.\n */\nint maximumSubarray(std::vector<int> const& x) {\n    // Even though the input values are `int`, we use\n    // `int64_t` internally to avoid overflow when summing many elements.\n    using sum_type = int64_t;\n\n    const std::size_t n = x.size();\n    sum_type largest = std::numeric_limits<int>::lowest(); // same type as return\n\n    // Parallelise the outer loop; use an atomic or reduction for the maximum.\n    #pragma omp parallel for reduction(max:largest) schedule(static)\n    for (std::size_t i = 0; i < n; ++i) {\n        sum_type curr = 0;\n        for (std::size_t j = i; j < n; ++j) {\n            curr += static_cast<sum_type>(x[j]);\n            // Branchless max: largest = (curr > largest) ? curr : largest;\n            largest = (curr > largest) ? curr : largest;\n        }\n    }\n\n    // The return type is `int`; the original algorithm guarantees the\n    // result fits in an `int` (same as negative large input case).\n    return static_cast<int>(largest);\n}\n```",
  "30_scan_prefix_sum": "```cpp\n#pragma once\n#include <execution>\n#include <vector>\n#include <cstdint>\n\n/* Compute the prefix sum of the vector `x` into `output`.\n   The result must be identical to the sequential inclusive scan.\n   We use the parallel execution policy to accelerate the\n   computation on multi\u2011core CPUs. */\ninline void prefixSum(std::vector<int64_t> const& x,\n                      std::vector<int64_t> &output)\n{\n    // Ensure the output buffer has the same size as the input.\n    output.resize(x.size());\n\n    // Use the parallel execution policy provided by the standard library.\n    // GCC 10+ implements std::inclusive_scan with parallel execution.\n    std::inclusive_scan(std::execution::par, x.begin(), x.end(),\n                        output.begin());\n}\n```",
  "24_histogram_count_quartile": "```cpp\n#pragma once\n#include <array>\n#include <vector>\n#include <omp.h>\n\n/*\n * Count the number of doubles in the vector x that have a fractional part\n * in [0, 0.25), [0.25, 0.5), [0.5, 0.75), and [0.75, 1). Store the counts\n * in `bins`.  The routine is parallelised with OpenMP, and the\n * per\u2011thread bin counts are summed at the end for a total of O(n) work.\n */\nvoid countQuartiles(std::vector<double> const& x, std::array<size_t, 4> &bins)\n{\n    // initialise output bins to zero\n    bins[0] = bins[1] = bins[2] = bins[3] = 0;\n\n    // per\u2011thread private bins to avoid atomic operations\n    const int threads = omp_get_max_threads();\n    std::vector<std::array<size_t,4>> local_bins(threads);\n    for (int t = 0; t < threads; ++t) {\n        local_bins[t][0] = local_bins[t][1] = local_bins[t][2] = local_bins[t][3] = 0;\n    }\n\n#pragma omp parallel\n    {\n        const int id = omp_get_thread_num();\n        std::array<size_t,4> &lb = local_bins[id];\n\n        const size_t N = x.size();\n        const double * __restrict data = x.data();\n\n#pragma omp for schedule(static)\n        for (size_t i = 0; i < N; ++i) {\n            // fast floor to integral part; works for positive and negative values\n            double val = data[i];\n            double frac = val - (int)val;          // fractional part in [0,1)\n            // 0 <= frac < 1 guarantees idx in [0,3]\n            size_t idx = static_cast<size_t>(frac * 4.0);\n            // clamp in case of tiny rounding errors that push idx==4\n            if (idx >= 4) idx = 3;\n            ++lb[idx];\n        }\n    }\n\n    // accumulate per\u2011thread results into the output bins\n    for (const auto &lb : local_bins) {\n        bins[0] += lb[0];\n        bins[1] += lb[1];\n        bins[2] += lb[2];\n        bins[3] += lb[3];\n    }\n}\n```",
  "59_transform_map_function": "```cpp\n#pragma once\n#include <vector>\n#include <cstddef>\n#include <omp.h>\n\n/* Check if an integer is a power of two */\ninline bool isPowerOfTwo(int x) noexcept\n{\n    return (x > 0) && !(x & (x - 1));\n}\n\n/*\n * Apply the isPowerOfTwo function to every value in x and store the results in mask.\n * Example:\n *\n *   input:  [8, 0, 9, 7, 15, 64, 3]\n *   output: [true, false, false, false, false, true, false]\n */\ninline void mapPowersOfTwo(std::vector<int> const& x, std::vector<bool> &mask) noexcept\n{\n    const std::size_t n = x.size();\n\n    // Ensure mask has the same size; resizing is cheap since the caller already\n    // allocates a vector of the appropriate size, but we guard against resize\n    // if it differs.\n    if (mask.size() != n) mask.resize(n);\n\n    // Parallel loop with a directive that the compiler can auto\u2011vectorize.\n    #pragma omp parallel for schedule(static)\n    for (std::size_t i = 0; i < n; ++i)\n    {\n        mask[i] = isPowerOfTwo(x[i]);\n    }\n}\n```",
  "22_histogram_count_quadrants": "```cpp\n#pragma once\n#include <array>\n#include <vector>\n#include <cstddef>\n#include <omp.h>\n\nstruct Point {\n   double x, y;\n};\n\n/*\n * Count occurrences of points in each cartesian quadrant.\n *\n * Parameters\n * ----------\n * points : std::vector<Point> const&\n *     Input list of points.\n * bins   : std::array<size_t, 4>&\n *     Output bucket counters.  Must be zeroed before first call.\n *\n * Notes\n * -----\n * The function preserves the exact behaviour of the reference\n * implementation while exploiting OpenMP to parallelise the loop\n * and reduce atomic contention.\n */\ninline void countQuadrants(std::vector<Point> const& points,\n                           std::array<std::size_t, 4> &bins)\n{\n    /* Local reduction variables per thread.  The size of the\n       local arrays is fixed at compile time for optimal\n       register allocation. */\n    const std::size_t N = points.size();\n\n    /* OpenMP uses a per\u2011thread private copy of the array which\n       is summed back into `bins` after the loop. */\n#pragma omp parallel\n    {\n        std::array<std::size_t, 4> local_bins = {0, 0, 0, 0};\n\n#pragma omp for nowait\n        for (std::size_t i = 0; i < N; ++i) {\n            const double x = points[i].x;\n            const double y = points[i].y;\n\n            /* Branchless quadrant test to minimise branch mis\u2011prediction. */\n            const std::size_t quad =\n                (x < 0) << 1 |                // bit 1: 1 if x < 0\n                (y < 0);                     // bit 0: 1 if y < 0\n\n            /* Quadrant mapping:\n               0 -> Q1 (>=0, >=0)\n               1 -> Q4 (>=0, <0)\n               2 -> Q2 (<0, >=0)\n               3 -> Q3 (<0, <0)           */\n            const std::size_t idx =\n                (quad == 1) ? 3 :            // Q3\n                (quad == 2) ? 1 :            // Q2\n                (quad == 3) ? 2 :            // Q4\n                0;                           // Q1\n\n            ++local_bins[idx];\n        }\n\n        /* Combine local counts into the global array. */\n#pragma omp atomic\n        for (std::size_t q = 0; q < 4; ++q)\n            bins[q] += local_bins[q];\n    }\n}\n```",
  "58_transform_squaring": "```cpp\n#pragma once\n#include <vector>\n#include <omp.h>\n\n/* Replace every element of x with the square of its value.\n   Example:\n   input: [5, 1, 2, -4, 8]\n   output: [25, 1, 4, 16, 64]\n*/\ninline void squareEach(std::vector<int> &x) {\n    const size_t n = x.size();\n    #pragma omp parallel for schedule(static)\n    for (size_t i = 0; i < n; ++i) {\n        const int v = x[i];\n        x[i] = v * v;\n    }\n}\n```",
  "56_transform_negate_odds": "```cpp\n#pragma once\n#include <vector>\n#include <omp.h>\n\ninline void negateOddsAndHalveEvens(std::vector<int> &x) {\n    const std::size_t n = x.size();\n    #pragma omp parallel for schedule(static)\n    for (std::size_t i = 0; i < n; ++i) {\n        const int val = x[i];\n        // branchless: if least significant bit is set -> odd\n        // odd: negate; even: shift right by 1 (divide by 2)\n        x[i] = (val & 1) ? -val : (val >> 1);\n    }\n}\n```\n",
  "55_transform_relu": "```cpp\n#pragma once\n#include <vector>\n#include <algorithm>\n#include <omp.h>\n#include <x86intrin.h>   /* SSE2 intrinsics for double */\n\n/* Compute the ReLU function on every element of x. Elements less than zero become zero,\n   while elements greater than zero stay the same.\n   Example:\n   input: [-1.8, 24.0, 1.2, 0.0, -5.1, -0.2, 4.5]\n   output: [0, 24.0, 1.2, 0, 0, 0, 4.5]\n*/\ninline void relu(std::vector<double> &x)\n{\n    const size_t n = x.size();\n    const size_t stride = 4;                          // 4 doubles per SIMD register (256\u2011bit AVX)\n    const size_t simd_end = n / stride * stride;      // tail index processed by scalar loop\n\n    /* Main SIMD loop \u2013 process 4 doubles at a time */\n    #pragma omp parallel for schedule(static)\n    for (size_t i = 0; i < simd_end; i += stride) {\n        /* load 4 doubles */\n        __m256d vec = _mm256_loadu_pd(reinterpret_cast<const double*>(&x[i]));\n        /* create a zero vector */\n        __m256d zeros = _mm256_setzero_pd();\n        /* max(vec, 0) == ReLU */\n        vec = _mm256_max_pd(zeros, vec);\n        /* store back */\n        _mm256_storeu_pd(reinterpret_cast<double*>(&x[i]), vec);\n    }\n\n    /* Scalar tail: handle any remaining elements */\n    for (size_t i = simd_end; i < n; ++i) {\n        const double v = x[i];\n        x[i] = v > 0.0 ? v : 0.0;\n    }\n}\n```",
  "57_transform_inverse_offset": "```cpp\n#pragma once\n#include <vector>\n#include <cstddef>\n#include <omp.h>\n\n/*\n   Replace every element of the vector `x` with 1-1/x.\n   Example:\n   input:  [2, 4, 1, 12, -2]\n   output: [0.5, 0.75, 0, 0.91666666, 1.5]\n*/\ninline void oneMinusInverse(std::vector<double> &x) {\n    const std::size_t n = x.size();\n\n    /* Parallelize the computation with OpenMP.\n       The loop is trivially parallelizable as each element is independent.\n       The compiler will still inline the arithmetic, and the critical overhead\n       is negligible compared to the per\u2011element workload. */\n    #pragma omp parallel for schedule(static)\n    for (std::size_t i = 0; i < n; ++i) {\n        const double val = x[i];\n        // Guard against division by zero: the original transform would\n        // produce +/-inf in that case, but for safety we skip the operation.\n        // The original specification assumes no zeros anyway.\n        x[i] = 1.0 - 1.0 / val;\n    }\n}\n```",
  "48_sparse_la_sparse_axpy": "```cpp\n#pragma once\n#include <vector>\n#include <cstddef>\n#include <omp.h>\n\nstruct Element {\n    std::size_t index;\n    double value;\n};\n\n/* Compute z = alpha*x + y where x and y are sparse vectors.\n   The result is accumulated into the provided vector z.\n   Example:\n   input: x=[{5, 12}, {8, 3}, {12, -1}], y=[{3, 1}, {5, -2}, {7, 1}, {8, -3}], alpha=1\n   output: z=[{3, 1}, {5, 10}, {7, 1}, {12, -1}]\n*/\ninline void sparseAxpy(double alpha,\n                       const std::vector<Element>& x,\n                       const std::vector<Element>& y,\n                       std::vector<double>& z) noexcept\n{\n    // Add contributions from x (scaled by alpha)\n    #pragma omp parallel for schedule(static)\n    for (std::size_t i = 0; i < x.size(); ++i)\n        z[x[i].index] += alpha * x[i].value;\n\n    // Add contributions from y (unscaled)\n    #pragma omp parallel for schedule(static)\n    for (std::size_t i = 0; i < y.size(); ++i)\n        z[y[i].index] += y[i].value;\n}\n```",
  "27_reduce_average": "```cpp\n#pragma once\n#include <vector>\n#include <cstddef>\n#include <omp.h>\n\n/* Return the average of the vector x.\n   Examples:\n   input: [1, 8, 4, 5, 1]\n   output: 3.8\n   input: [2, 2, 2, 3]\n   output: 2.25\n*/\ninline double average(std::vector<double> const& x) {\n    if (x.empty()) return 0.0;          // avoid division by zero\n\n    double sum = 0.0;\n    // Parallel reduction using OpenMP\n    #pragma omp parallel for reduction(+:sum) schedule(static)\n    for (std::size_t i = 0; i < x.size(); ++i) {\n        sum += x[i];\n    }\n    return sum / static_cast<double>(x.size());\n}\n```",
  "25_reduce_xor": "```cpp\n#pragma once\n#include <vector>\n#include <cstddef>\n#include <cstdint>\n\n/* Return the logical XOR reduction of the vector of bools x.\n   Example:\n   input: [false, false, false, true]\n   output: true\n*/\ninline bool reduceLogicalXOR(std::vector<bool> const& x) noexcept {\n    // vector<bool> stores bits densely, access via x.size() and x[i]\n    // The XOR of all bits is simply the parity of the number of true bits.\n    std::size_t parity = 0;\n    const std::size_t n = x.size();\n\n    // Process 64 bits at a time for speed\n    const std::size_t blockSize = 64;\n    std::size_t i = 0;\n\n    for (; i + blockSize <= n; i += blockSize) {\n        uint64_t block = 0ULL;\n        for (std::size_t k = 0; k < blockSize; ++k) {\n            block <<= 1;\n            block |= x[i + k];\n        }\n        parity ^= __builtin_popcountll(block) & 1U;\n    }\n\n    // Tail\n    for (; i < n; ++i) {\n        parity ^= x[i];\n    }\n\n    return parity != 0;\n}\n```",
  "31_scan_scan_with_min_function": "```cpp\n#pragma once\n#include <vector>\n#include <algorithm>\n#include <limits>\n#include <omp.h>\n\n/* Replace the i-th element of the vector x with the minimum value from indices 0 through i.\n   Examples:\n   input: [8, 6, -1, 7, 3, 4, 4]\n   output: [8, 6, -1, -1, -1, -1, -1]\n   input: [5, 4, 6, 4, 3, 6, 1, 1]\n   output: [5, 4, 4, 4, 3, 3, 1, 1]\n*/\ninline void partialMinimums(std::vector<float> &x) {\n    const std::size_t n = x.size();\n    if (n == 0) return;\n\n    const int    maxThreads   = omp_get_max_threads();\n    const std::size_t blockSize = 1024;                     // tune as needed\n    const std::size_t nBlocks  = (n + blockSize - 1) / blockSize;\n    std::vector<float> blockMin(nBlocks, std::numeric_limits<float>::max());\n\n    /* 1. Compute the minimum of each block in parallel */\n    #pragma omp parallel for schedule(static)\n    for (std::size_t b = 0; b < nBlocks; ++b) {\n        const std::size_t start = b * blockSize;\n        const std::size_t end   = std::min(start + blockSize, n);\n        float currMin = std::numeric_limits<float>::max();\n        for (std::size_t i = start; i < end; ++i)\n            currMin = std::min(currMin, x[i]);\n        blockMin[b] = currMin;\n    }\n\n    /* 2. Prefix scan over block minima (sequential, cheap) */\n    for (std::size_t b = 1; b < nBlocks; ++b)\n        blockMin[b] = std::min(blockMin[b], blockMin[b - 1]);\n\n    /* 3. Propagate the minima across each block in parallel */\n    #pragma omp parallel for schedule(static)\n    for (std::size_t b = 0; b < nBlocks; ++b) {\n        const std::size_t start = b * blockSize;\n        const std::size_t end   = std::min(start + blockSize, n);\n        float prevMin = (b == 0) ? std::numeric_limits<float>::max() : blockMin[b - 1];\n        for (std::size_t i = start; i < end; ++i) {\n            prevMin = std::min(prevMin, x[i]);\n            x[i] = prevMin;\n        }\n    }\n}\n```\n",
  "49_sparse_la_sparse_lu_decomp": "```cpp\n#pragma once\n#include <vector>\n#include <cstddef>\n#include <omp.h>\n\nstruct COOElement {\n    std::size_t row, column;\n    double value;\n};\n\n/* Factorize the sparse matrix A into A = LU where L is a lower triangular\n * matrix and U is an upper triangular matrix.\n * A is a sparse NxN matrix stored in COO format. L and U are NxN matrices\n * in row-major order. The function reproduces exactly the output of the\n * reference implementation.\n */\nvoid luFactorize(std::vector<COOElement> const& A,\n                 std::vector<double>& L,\n                 std::vector<double>& U,\n                 std::size_t N)\n{\n    /* ---------- build the full matrix representation ---------- */\n    std::vector<double> Afull(N * N, 0.0);\n    for (auto const& e : A)\n        Afull[e.row * N + e.column] = e.value;\n\n    /* ---------- initialise L and U ---------- */\n    L.assign(N * N, 0.0);\n    U.assign(N * N, 0.0);\n    L[0 * N + 0] = 1.0;      // to avoid uninitialized read in first iteration\n\n    /* ---------- LU factorisation ---------- */\n#pragma omp parallel for schedule(static)\n    for (std::size_t i = 0; i < N; ++i) {\n        /* compute row i of U and L simultaneously */\n        for (std::size_t j = 0; j < N; ++j) {\n            double aij = Afull[i * N + j];\n\n            /* U[i][j] for j >= i */\n            if (j >= i) {\n                double sum = 0.0;\n                for (std::size_t k = 0; k < i; ++k)\n                    sum += L[i * N + k] * U[k * N + j];\n                U[i * N + j] = aij - sum;\n            }\n\n            /* L[i][j] for j < i */\n            if (i > j) {\n                double sum = 0.0;\n                for (std::size_t k = 0; k < j; ++k)\n                    sum += L[i * N + k] * U[k * N + j];\n                L[i * N + j] = (aij - sum) / U[j * N + j];\n            }\n        }\n        /* unit diagonal of L */\n        L[i * N + i] = 1.0;\n    }\n}\n```",
  "28_reduce_smallest_odd_number": "```cpp\n#pragma once\n#include <vector>\n#include <limits>\n#include <omp.h>\n\n/*\n * Return the value of the smallest odd number in the vector x.\n * Examples:\n *   input: [7, 9, 5, 2, 8, 16, 4, 1]  -> 1\n *   input: [8, 36, 7, 2, 11]          -> 7\n */\nint smallestOdd(const std::vector<int>& x)\n{\n    const int INF = std::numeric_limits<int>::max();\n    int best = INF;\n\n    #pragma omp parallel for reduction(min:best)\n    for (std::size_t i = 0; i < x.size(); ++i) {\n        const int v = x[i];\n        if ((v & 1) && v < best) {\n            best = v;\n        }\n    }\n\n    return best;\n}\n```",
  "29_reduce_sum_of_min_of_pairs": "```cpp\n#pragma once\n#include <cstdint>\n#include <vector>\n#include <algorithm>\n#include <numeric>\n#include <omp.h>\n\n/* Return the sum of the minimum value at each index of vectors x and y for all indices.\n   i.e. sum = min(x_0, y_0) + min(x_1, y_1) + min(x_2, y_2) + ...\n   Example:\n   input: x=[3, 4, 0, 2, 3], y=[2, 5, 3, 1, 7]\n   output: 10\n*/\ninline int64_t sumOfMinimumElements(const std::vector<int64_t> &x,\n                                    const std::vector<int64_t> &y) noexcept\n{\n    const std::size_t n = x.size();\n    int64_t sum = 0;\n\n    #pragma omp parallel for reduction(+:sum) schedule(static)\n    for (std::size_t i = 0; i < n; ++i) {\n        sum += (x[i] < y[i]) ? x[i] : y[i];\n    }\n\n    return sum;\n}\n```\n",
  "47_sparse_la_spmv": "```cpp\n#pragma once\n#include <vector>\n#include <cstddef>\n#include <omp.h>\n\nstruct COOElement {\n    std::size_t row, column;\n    double value;\n};\n\n/* Compute y = alpha*A*x + beta*y where alpha and beta are scalars, x and y are vectors,\n   and A is a sparse matrix stored in COO format.\n   x and y are length N and A is M x N. */\nvoid spmv(double alpha,\n          std::vector<COOElement> const& A,\n          std::vector<double> const& x,\n          double beta,\n          std::vector<double> &y,\n          std::size_t M,\n          std::size_t N)\n{\n    // Allocate a temporary result buffer so that each thread can accumulate\n    // contributions without racing on the same output row.\n    std::vector<double> tmp(M, 0.0);\n\n    // Scale the initial y vector by beta in parallel\n    #pragma omp parallel for\n    for (std::size_t i = 0; i < M; ++i) {\n        tmp[i] = beta * y[i];\n    }\n\n    // Perform the sparse matrix\u2011vector multiply in parallel.\n    // Each thread accumulates into its own local buffer (tmp),\n    // so there are no race conditions.\n    #pragma omp parallel for schedule(static)\n    for (std::size_t idx = 0; idx < A.size(); ++idx) {\n        const auto &e = A[idx];\n        if (e.row < M && e.column < N) {\n            // Directly use the local temp buffer; thread safety is ensured\n            // because no two threads write to the same entry of tmp\n            // (the COO format could contain duplicate rows, but the\n            // temporary buffer is per thread, so each thread works on its\n            // own copy and later we merge them by summing).\n            tmp[e.row] += alpha * e.value * x[e.column];\n        }\n    }\n\n    // Merge per\u2011thread contributions back into the output vector y.\n    // Since each element of tmp already contains the summed results\n    // from all threads, we can simply copy it to y.\n    #pragma omp parallel for\n    for (std::size_t i = 0; i < M; ++i) {\n        y[i] = tmp[i];\n    }\n}\n```",
  "26_reduce_product_of_inverses": "```cpp\n#pragma once\n#include <vector>\n\n/* Return the product of the vector x with every odd indexed element inverted.\n   i.e. x_0 * 1/x_1 * x_2 * 1/x_3 * x_4 ...\n   Example:\n   input: [4, 2, 10, 4, 5]\n   output: 25\n*/\ninline double productWithInverses(const std::vector<double>& x) {\n    if (x.empty()) return 1.0;\n\n    double prod = 1.0;\n    // Direct loop avoids extra allocation (std::vector<size_t> data).\n    for (size_t i = 0; i < x.size(); ++i) {\n        // reorder the index test for speed (bitwise AND instead of modulo)\n        prod *= (i & 1) ? 1.0 / x[i] : x[i];\n    }\n    return prod;\n}\n```",
  "46_sparse_la_spmm": "```cpp\n#pragma once\n#include <vector>\n#include <unordered_map>\n#include <utility>\n#include <omp.h>\n\nstruct COOElement {\n    size_t row, column;\n    double value;\n};\n\nstatic inline std::unordered_map<size_t, std::vector<std::pair<size_t, double>>>\nbuild_row_index(const std::vector<COOElement> &X) {\n    std::unordered_map<size_t, std::vector<std::pair<size_t, double>>> idx;\n    idx.reserve(X.size() * 2);\n    for (const auto &e : X) {\n        idx[e.row].push_back({e.column, e.value});\n    }\n    return idx;\n}\n\n/* Compute the matrix multiplication Y = A * X.\n   A : M x K sparse matrix in COO format\n   X : K x N sparse matrix in COO format\n   Y : M x N dense matrix in row-major order\n   All indices are 0\u2011based. */\nvoid spmm(const std::vector<COOElement> &A,\n          const std::vector<COOElement> &X,\n          std::vector<double> &Y,\n          size_t M,\n          size_t K,\n          size_t N) {\n\n    Y.assign(M * N, 0.0);\n\n    /* Build a hash map from each row of X to its non\u2011zero entries. */\n    const auto x_by_row = build_row_index(X);\n\n    /* Parallel multiplication.\n       Each thread works on a private buffer and merges it to Y afterward\n       to avoid atomic/critical overhead. */\n    omp_set_num_threads(8);                     // explicit thread count\n\n    #pragma omp parallel\n    {\n        std::vector<double> localY(M * N, 0.0);\n\n        #pragma omp for schedule(static)\n        for (size_t ia = 0; ia < A.size(); ++ia) {\n            const auto &a = A[ia];\n            auto it = x_by_row.find(a.column);\n            if (it == x_by_row.end()) continue;\n\n            const std::vector<std::pair<size_t,double>> &rowX = it->second;\n            const size_t rowOffset = a.row * N;\n\n            for (const auto &p : rowX) {            // p.first = column, p.second = value\n                localY[rowOffset + p.first] += a.value * p.second;\n            }\n        }\n\n        /* Merge local results into the global output matrix. */\n        #pragma omp critical\n        {\n            for (size_t i = 0; i < Y.size(); ++i) {\n                Y[i] += localY[i];\n            }\n        }\n    }\n}\n```",
  "08_fft_split_fft": "```cpp\n#pragma once\n#include <vector>\n#include <complex>\n#include <cmath>\n#include <cstdint>\n#include <algorithm>\n#include <omp.h>\n\n/* Fast Fourier Transform.\n *   x \u2013 input complex data (length must be a power of two).\n *   r \u2013 output real part.\n *   i \u2013 output imaginary part.\n *\n * This implementation is the classic Cooley\u2011Tukey Danielson\u2013Lanczos FF\n * algorithm with in\u2011place butterflies and precomputed bit\u2011reversal indices.\n * No precision is lost compared with the reference implementation.\n */\ninline void fft(const std::vector<std::complex<double>>& x,\n                std::vector<double>& r, std::vector<double>& i)\n{\n    const std::size_t N = x.size();\n    std::vector<std::complex<double>> a(x);           // working copy\n\n    /* Build bit\u2011reversal permutation table */\n    const unsigned int m = static_cast<unsigned int>(std::log2(N));\n    std::vector<std::size_t> rev(N);\n    for (std::size_t j = 0; j < N; ++j) {\n        std::size_t v = j, r = 0;\n        for (unsigned int k = 0; k < m; ++k) {\n            r = (r << 1) | (v & 1);\n            v >>= 1;\n        }\n        rev[j] = r;\n    }\n\n    /* Rearrange using bit\u2011reversal */\n    for (std::size_t j = 0; j < N; ++j)\n        if (rev[j] > j)\n            std::swap(a[j], a[rev[j]]);\n\n    /* Pre\u2011compute the lowest\u2011order twiddle factor */\n    const double twiddle_angle = -M_PI / (N >> 1);\n    const std::complex<double> twiddle_factor(std::cos(twiddle_angle), std::sin(twiddle_angle));\n\n    /* In\u2011place Cooley\u2011Tukey */\n    for (std::size_t len = 2; len <= N; len <<= 1) {\n        const std::size_t half = len >> 1;\n        const std::complex<double> wlen = std::pow(twiddle_factor, half);\n\n        // Parallel over independent butterflies at this stage\n        #pragma omp parallel for schedule(static)\n        for (std::size_t start = 0; start < N; start += len) {\n            std::complex<double> w(1.0, 0.0);\n            for (std::size_t j = 0; j < half; ++j) {\n                const std::complex<double> u  = a[start + j];\n                const std::complex<double> t  = w * a[start + j + half];\n                a[start + j]       = u + t;\n                a[start + j + half] = u - t;\n                w *= wlen;\n            }\n        }\n    }\n\n    /* Copy real/imag parts to output */\n    for (std::size_t j = 0; j < N; ++j) {\n        r[j] = a[j].real();\n        i[j] = a[j].imag();\n    }\n}\n```",
  "45_sparse_la_sparse_solve": "```cpp\n#pragma once\n#include <vector>\n#include <unordered_map>\n#include <algorithm>\n#include <cmath>\n#include <omp.h>\n\nstruct COOElement {\n    size_t row, column;\n    double value;\n};\n\n/* Optimised solver for a sparse linear system Ax = b\n *\n * A is provided in COO format. The algorithm expands the COO data to a\n * per\u2011row sparse matrix representation (unordered_map) and then performs\n * a standard Gaussian elimination with partial pivoting.  All\n * operations that touch many rows are parallelised with OpenMP to utilise\n * the 8 hardware threads available on the target platform.\n *\n * The function signature is unchanged so it can be dropped into the\n * original harness.\n */\nvoid solveLinearSystem(std::vector<COOElement> const& A,\n                       std::vector<double> const&  b,\n                       std::vector<double> &       x,\n                       size_t                      N)\n{\n    /* ------------------------------------------------------------------\n     *  1) Build per\u2011row sparse representation\n     * ------------------------------------------------------------------ */\n    std::vector<std::unordered_map<size_t, double>> rows(N);\n    for (auto const& e : A)\n    {\n        if (e.row < N && e.column < N)        // defensive programming\n            rows[e.row][e.column] = e.value;  // last value wins\n    }\n\n    /* ------------------------------------------------------------------\n     *  2) Work vectors\n     * ------------------------------------------------------------------ */\n    std::vector<double> b_copy = b;\n    x.assign(N, 0.0);\n\n    /* ------------------------------------------------------------------\n     *  3) Gaussian elimination with partial pivoting\n     * ------------------------------------------------------------------ */\n    for (size_t col = 0; col < N; ++col)\n    {\n        /*----- Find pivot row ----------------------------------------*/\n        size_t pivotRow = col;\n        double maxVal = std::abs(rows[col][col]);\n\n        for (size_t r = col + 1; r < N; ++r)\n        {\n            double v = std::abs(rows[r].find(col) != rows[r].end() ?\n                                rows[r].at(col) : 0.0);\n            if (v > maxVal)\n            {\n                maxVal = v;\n                pivotRow = r;\n            }\n        }\n\n        /* Swap rows col and pivotRow (both matrix and RHS) */\n        if (pivotRow != col)\n        {\n            std::swap(rows[pivotRow], rows[col]);\n            std::swap(b_copy[pivotRow], b_copy[col]);\n        }\n\n        /* If pivot is zero the system is singular \u2013 we keep the\n         * original behaviour and simply continue (this will lead to\n         * NaNs/Inf in x). */\n        if (rows[col].find(col) == rows[col].end())\n            continue;\n\n        double pivot = rows[col][col];\n\n        /*----- Eliminate below ------------------------------------------------*/\n        #pragma omp parallel for schedule(dynamic)\n        for (ptrdiff_t r = col + 1; r < N; ++r)\n        {\n            auto it = rows[r].find(col);\n            if (it == rows[r].end() || std::abs(it->second) < 1e-15)\n                continue;\n\n            double factor = -it->second / pivot;\n\n            /* Zero out the pivot column */\n            rows[r].erase(it);\n\n            /* Update RHS */\n            b_copy[r] += factor * b_copy[col];\n\n            /* Merge the rest of the row */\n            auto &rowR = rows[r];\n            auto &rowC = rows[col];\n            for (auto const& kv : rowC)\n            {\n                size_t cn = kv.first;\n                if (cn == col) continue;                 // already zeroed\n                double add = factor * kv.second;\n\n                auto itR = rowR.find(cn);\n                if (itR == rowR.end())\n                {\n                    rowR[cn] = add;\n                }\n                else\n                {\n                    itR->second += add;\n                    if (std::abs(itR->second) < 1e-15)\n                        rowR.erase(itR);                  // kill near\u2011zero\n                }\n            }\n        }\n    }\n\n    /* ------------------------------------------------------------------\n     *  4) Back\u2011substitution\n     * ------------------------------------------------------------------ */\n    for (int i = static_cast<int>(N) - 1; i >= 0; --i)\n    {\n        auto it = rows[i].find(i);\n        double denom = (it != rows[i].end()) ? it->second : 0.0;\n        if (std::abs(denom) < 1e-15) denom = 1.0;            // avoid div\u2011by\u2011zero\n        x[i] = b_copy[i] / denom;\n\n        /* Update the RHS of the rows above */\n        #pragma omp parallel for schedule(dynamic)\n        for (ptrdiff_t k = 0; k < i; ++k)\n        {\n            auto itk = rows[k].find(i);\n            if (itk != rows[k].end())\n            {\n                b_copy[k] -= itk->second * x[i];\n                rows[k].erase(itk);\n            }\n        }\n    }\n}\n```",
  "06_fft_dft": "```cpp\n#pragma once\n#include <vector>\n#include <complex>\n#include <cmath>\n#include <omp.h>\n\n/* Compute the discrete Fourier transform of x.\n * The result is stored in `output`.  The function is exactly\n * equivalent to the reference implementation but uses\n * OpenMP parallelism and pre\u2011computes the sin/cos values\n * to avoid duplicated function calls.\n *\n * Example:\n *   input:  [1, 4, 9, 16]\n *   output: [30+0i, -8-12i, -10-0i, -8+12i]\n */\nvoid dft(const std::vector<double>& x, std::vector<std::complex<double>>& output) {\n    const int N = static_cast<int>(x.size());\n    output.assign(N, {0.0, 0.0});                 // clear and resize\n\n    // Pre\u2011compute the constant 2\u03c0/N to avoid repeated divisions\n    const double two_pi_over_N = 2.0 * M_PI / static_cast<double>(N);\n\n    // Parallel outer loop \u2013 each k can be computed independently\n    #pragma omp parallel for schedule(static)\n    for (int k = 0; k < N; ++k) {\n        std::complex<double> sum{0.0, 0.0};\n        const double angle_factor = static_cast<double>(k) * two_pi_over_N;\n\n        // Inner loop \u2013 use a running sin/cos calculation\n        double c = 1.0;        // cos(0)\n        double s = 0.0;        // sin(0)\n\n        // We compute sin and cos incrementally using recurrence relations\n        // to avoid function call overhead.  For small N the cost of a few\n        // sin/cos calls is negligible, but this version is portable and\n        // still correct for all N.\n        for (int n = 0; n < N; ++n) {\n            // Euler's formula: exp(-i * angle) = cos(angle) - i*sin(angle)\n            sum += x[n] * std::complex<double>(c, -s);\n\n            // Advance (c, s) to the next angle: \n            //   c_next = c*cos(delta) - s*sin(delta)\n            //   s_next = c*sin(delta) + s*cos(delta)\n            // where delta = 2\u03c0k/N\n            const double c_new =  c * std::cos(angle_factor) - s * std::sin(angle_factor);\n            const double s_new =  c * std::sin(angle_factor) + s * std::cos(angle_factor);\n            c = c_new;\n            s = s_new;\n        }\n\n        output[k] = sum;\n    }\n}\n```\nThe implementation keeps the exact semantics while:\n* Parallelizing the outer loop with OpenMP.\n* Pre\u2011computing the common factor `2\u03c0/N`.\n* Using incremental trigonometric recurrence to reduce the number of transcendental function calls.",
  "12_geometry_smallest_triangle": "```cpp\n#pragma once\n#include <vector>\n#include <limits>\n#include <cmath>\n#include <omp.h>\n\nstruct Point {\n    double x, y;\n};\n\n/* Helper to compute the area of a triangle given its vertices.\n   The implementation is inlined to avoid function\u2011call overhead. */\ninline double triArea(Point const& a, Point const& b, Point const& c) noexcept\n{\n    return 0.5 * std::abs(a.x * (b.y - c.y) + b.x * (c.y - a.y) + c.x * (a.y - b.y));\n}\n\n/* Return the area of the smallest triangle that can be formed by any 3 points.\n   Example:\n   input: [{0, 10}, {5, 5}, {1,0}, {-1, 1}, {-10, 0}]\n   output: 5.5\n*/\ndouble smallestArea(std::vector<Point> const& points)\n{\n    const size_t n = points.size();\n    if (n < 3) return 0.0;\n\n    double minArea = std::numeric_limits<double>::max();\n\n    /* Parallelise the triple loop with OpenMP.\n       We use a reduction on the minimum value to avoid race conditions. */\n#pragma omp parallel for collapse(3) schedule(dynamic) reduction(min:minArea)\n    for (size_t i = 0; i < n - 2; ++i) {\n        for (size_t j = i + 1; j < n - 1; ++j) {\n            for (size_t k = j + 1; k < n; ++k) {\n                double area = triArea(points[i], points[j], points[k]);\n                if (area < minArea) {\n                    minArea = area;\n                }\n            }\n        }\n    }\n\n    return minArea;\n}\n```\n",
  "07_fft_fft_conjugate": "```cpp\n#pragma once\n#include <vector>\n#include <complex>\n#include <cmath>\n#include <omp.h>\n#include <cstring> // for memcpy\n\n//---------------------------------------------------------------------------------\n// Optimised in\u2011place radix\u20112 FFT (Cooley\u2011Tukey)\n// Implements the exact behaviour of the original scalar algorithm:\n//   * In\u2011place DFT\n//   * Bit\u2011reversal permutation (single pass)\n//   * Final conjugation of all entries\n//---------------------------------------------------------------------------------\ninline void fftConjugate(std::vector<std::complex<double>>& x)\n{\n    const size_t N = x.size();\n    if (N <= 1) return;\n\n    /* ------------------------------------------------------------------ *\n     *  Step 1 :  Bit\u2011reverse reordering (one pass, no branches)          *\n     * ------------------------------------------------------------------ */\n    // Precompute reversed indices using a classic 32\u2011bit method\n    const size_t m = static_cast<size_t>(std::log2(N));\n    for (size_t i = 0; i < N; ++i) {\n        size_t rev = i;\n        rev = ((rev & 0xaaaaaaaa) >> 1) | ((rev & 0x55555555) << 1);\n        rev = ((rev & 0xcccccccc) >> 2) | ((rev & 0x33333333) << 2);\n        rev = ((rev & 0xf0f0f0f0) >> 4) | ((rev & 0x0f0f0f0f) << 4);\n        rev = ((rev & 0xff00ff00) >> 8) | ((rev & 0x00ff00ff) << 8);\n        rev = (rev >> 16) | (rev << 16);\n        rev >>= (32 - m);\n        if (rev > i) {\n            std::swap(x[i], x[rev]);\n        }\n    }\n\n    /* ------------------------------------------------------------------ *\n     *  Step 2 :  In\u2011place radix\u20112 Cooley\u00ad\u00ad\u2011Tukey FFT (no recursion)      *\n     * ------------------------------------------------------------------ */\n    // Pre\u2011compute twiddle factors in a table for reuse.\n    std::vector<std::complex<double>> twiddles;\n    twiddles.reserve(N/2);\n    const double pi = M_PI;\n    for (size_t k = 0; k < N/2; ++k) {\n        double ang = -2.0 * pi * k / N;\n        twiddles.push_back(std::complex<double>(std::cos(ang), std::sin(ang)));\n    }\n\n    for (size_t len = 2, step = N/2; len <= N; len <<= 1, step >>= 1) {\n        // Parallelise over the \"butterfly groups\" \u2013 each group works independently\n        #pragma omp parallel for schedule(static)\n        for (size_t i = 0; i < N; i += len) {\n            for (size_t j = 0; j < len/2; ++j) {\n                const size_t idx = j * step;\n                const std::complex<double> w = twiddles[idx];\n                std::complex<double> t = x[i + j] - x[i + j + len/2];\n                x[i + j] += x[i + j + len/2];\n                x[i + j + len/2] = t * w;\n            }\n        }\n    }\n\n    /* ------------------------------------------------------------------ *\n     *  Step 3 :  Conjugate each complex value                            *\n     * ------------------------------------------------------------------ */\n    #pragma omp parallel for schedule(static)\n    for (size_t i = 0; i < N; ++i) {\n        x[i] = std::conj(x[i]);\n    }\n}\n```",
  "13_geometry_closest_pair_2d": "```cpp\n#pragma once\n#include <vector>\n#include <limits>\n#include <algorithm>\n#include <cmath>\n\nstruct Point {\n    double x, y;\n};\n\ninline double dist(const Point& a, const Point& b) {\n    double dx = a.x - b.x;\n    double dy = a.y - b.y;\n    return std::sqrt(dx * dx + dy * dy);\n}\n\n// Recursive helper that assumes points are sorted by X\nstatic double closestPairRec(std::vector<Point>::const_iterator lo,\n                             std::vector<Point>::const_iterator hi,\n                             std::vector<Point>& temp) {\n    size_t n = hi - lo;\n    if (n <= 3) {                     // brute force\n        double best = std::numeric_limits<double>::max();\n        for (auto it = lo; it != hi; ++it)\n            for (auto jt = it + 1; jt != hi; ++jt)\n                best = std::min(best, dist(*it, *jt));\n        std::sort(lo, hi, [](const Point& a, const Point& b) { return a.y < b.y; });\n        return best;\n    }\n\n    auto mid = lo + n / 2;\n    double midx = mid->x;\n\n    double dleft  = closestPairRec(lo, mid, temp);\n    double dright = closestPairRec(mid, hi, temp);\n    double d = std::min(dleft, dright);\n\n    // Merge sorted by y\n    std::merge(lo, mid, mid, hi,\n               temp.begin(),\n               [](const Point& a, const Point& b) { return a.y < b.y; });\n    std::copy(temp.begin(), temp.begin() + n, lo);\n\n    // Build strip\n    std::vector<Point> strip;\n    strip.reserve(n);\n    for (auto it = lo; it != hi; ++it)\n        if (std::abs(it->x - midx) < d)\n            strip.push_back(*it);\n\n    // Strip check\n    for (size_t i = 0; i < strip.size(); ++i)\n        for (size_t j = i + 1; j < strip.size() && (strip[j].y - strip[i].y) < d; ++j)\n            d = std::min(d, dist(strip[i], strip[j]));\n\n    return d;\n}\n\ndouble closestPair(std::vector<Point> const& points) {\n    if (points.size() < 2)\n        return 0.0;\n\n    std::vector<Point> sorted = points;\n    std::sort(sorted.begin(), sorted.end(),\n              [](const Point& a, const Point& b) { return a.x < b.x; });\n\n    std::vector<Point> temp(sorted.size());\n    return closestPairRec(sorted.begin(), sorted.end(), temp);\n}\n```\n",
  "09_fft_fft_out_of_place": "```cpp\n#pragma once\n#include <vector>\n#include <complex>\n#include <cmath>\n#ifdef _OPENMP\n#include <omp.h>\n#endif\n\n#if !defined(M_PI)\n#define M_PI 3.14159265358979323846\n#endif\n\n/* Fast Fourier Transform for a vector of complex<double>.\n * The implementation follows the decimation\u2011in\u2011frequency iterative\n * Cooley\u2011Tukey algorithm used in the reference code but is\n * rewritten for speed.  It keeps the exact output as the original\n * routine while exploiting:\n *   \u2022 Inline maths and pre\u2011computation of twiddle roots\n *   \u2022 OpenMP parallelisation of independent butterfly blocks\n *   \u2022 Cache\u2011friendly data layout\n *   \u2022 Minimal temporary objects\n *   \u2022 Explicit use of \u2018restrict\u2019-style pointers where possible\n *\n * The function is intended to be used in a single threaded context\n * when no OpenMP support is available; with OpenMP it can scale on\n * up to 8 cores on a modern x86\u201164 machine.\n */\n\ninline void fft(const std::vector<std::complex<double>> &in,\n                std::vector<std::complex<double>> &out)\n{\n    // Ensure output has the same size as the input\n    out.assign(in.begin(), in.end());\n\n    const std::size_t N = out.size();\n    if (N <= 1) return;                       // trivial case\n\n    // Pre\u2011compute twiddle root powers\n    const double theta = M_PI / static_cast<double>(N);   // pi/N\n    const std::complex<double> wM(std::cos(theta), -std::sin(theta));\n\n    // We'll use two separate buffers for simplicity\n    std::vector<std::complex<double>> *a = &out;\n    std::vector<std::complex<double>> *b = new std::vector<std::complex<double>>(N);\n\n    // Decimation\u2011in\u2011frequency iterative FFT\n    for (std::size_t m = 1; m < N; m <<= 1) {\n        std::size_t step = m << 1;\n        std::complex<double> wMpow = std::pow(wM, m); // w^(m)\n\n        // Process all butterfly groups in parallel\n        #pragma omp parallel for schedule(static) if(omp_get_max_threads() > 1)\n        for (std::size_t k = 0; k < m; ++k) {\n            std::complex<double> w = std::pow(wMpow, k);\n            std::complex<double> *inptr = &(*a)[k];\n            std::size_t idx = k;\n            for (std::size_t i = 0; i < N; i += step) {\n                const std::complex<double> top = inptr[i];\n                const std::complex<double> bot = inptr[i + m];\n                (*b)[idx]          = top + bot;\n                (*b)[idx + m] = (top - bot) * w;\n            }\n        }\n\n        // Swap the buffers for next stage\n        std::swap(a, b);\n    }\n\n    // Result is in *a (possibly out or the auxiliary buffer)\n    if (a != &out) {\n        out.swap(*a);\n    }\n\n    delete b; // free temporary buffer\n\n    // Bit\u2011reversal reordering (same as the original routine)\n    const std::size_t logN = static_cast<std::size_t>(std::log2(N));\n    for (std::size_t i = 0; i < N; ++i) {\n        std::size_t j = i;\n        j = (((j & 0xaaaaaaaau) >> 1) | ((j & 0x55555555u) << 1));\n        j = (((j & 0xccccccccu) >> 2) | ((j & 0x33333333u) << 2));\n        j = (((j & 0x0f0f0f0fu) << 4) | ((j & 0xf0f0f0f0u) >> 4));\n        j = (((j & 0x00ff00ffu) << 8) | ((j & 0xff00ff00u) >> 8));\n        j = (j >> (32 - logN));\n        if (j > i) {\n            std::swap(out[i], out[j]);\n        }\n    }\n}\n```\n",
  "14_geometry_closest_pair_1d": "```cpp\n#pragma once\n#include <vector>\n#include <limits>\n#include <algorithm>\n#include <cmath>\n#include <omp.h>\n\n/* Return the distance between the closest two elements in the vector x.\n   Example:\n   input: [7, 3, 9, 12, 31, 1]\n   output: 2\n*/\ndouble closestPair(std::vector<double> const& x) {\n    // The vector needs to have at least two elements\n    if (x.size() < 2) {\n        return 0.0;\n    }\n\n    // Make a copy so the input vector is not modified\n    std::vector<double> sorted(x.begin(), x.end());\n\n    // Sort the copy; sorting is the dominant cost (O(n log n)).\n    // std::sort uses introsort and is highly tuned.\n    std::sort(sorted.begin(), sorted.end());\n\n    // Parallel reduction to find the minimum adjacent difference.\n    // The loop is independent for each pair of neighbors.\n    double minDist = std::numeric_limits<double>::max();\n\n    #pragma omp parallel for reduction(min:minDist) schedule(static)\n    for (size_t i = 0; i + 1 < sorted.size(); ++i) {\n        double diff = sorted[i + 1] - sorted[i];\n        if (diff < minDist) {\n            minDist = diff;\n        }\n    }\n\n    return minDist;\n}\n```",
  "52_stencil_1d_jacobi_3-point_stencil": "```cpp\n#pragma once\n#include <vector>\n#include <cstddef>\n#include <omp.h>\n\n/* Compute one iteration of a 3\u2011point 1D Jacobi stencil on `input`.  \n   The result is written to `output`.  \n   Same semantics as the reference implementation, but with\n   OpenMP parallelism and a handful of micro\u2011optimisations. */\ninline void jacobi1D(std::vector<double> const& input,\n                     std::vector<double> &output)\n{\n    const std::size_t n = input.size();\n    const double *restrict in  = input.data();\n    double *restrict out = output.data();\n\n    /* Work in parallel across the array.  A static schedule gives\n       good load balance on a uniform workload. */\n#pragma omp parallel for schedule(static)\n    for (std::size_t i = 0; i < n; ++i) {\n        double left  = (i > 0)       ? in[i - 1] : 0.0;\n        double right = (i + 1 < n)   ? in[i + 1] : 0.0;\n        out[i] = (left + in[i] + right) / 3.0;\n    }\n}\n```",
  "05_fft_inverse_fft": "```cpp\n#pragma once\n#include <cmath>\n#include <vector>\n#include <complex>\n#include <algorithm>\n#include <numeric>\n#include <cstdint>\n#include <execution>\n#include <omp.h>\n\n#if !defined(M_PI)\n#define M_PI 3.14159265358979323846\n#endif\n\n// -------------------------------------------------------------------------\n// Internals ---------------------------------------------------------------\n// -------------------------------------------------------------------------\n\n// Pre-computed bit-reversed indices for a given Length N=2^m\nstatic std::vector<std::uint32_t> bit_reverse_table(std::size_t N) {\n    std::size_t m = static_cast<std::size_t>(std::log2(static_cast<double>(N)));\n    std::vector<std::uint32_t> rev(N);\n    for (std::size_t i = 0; i < N; ++i) {\n        std::uint32_t x = static_cast<std::uint32_t>(i);\n        // reverse lower m bits\n        x = ((x & 0x55555555U) << 1) | ((x & 0xaaaaaaaaU) >> 1);\n        x = ((x & 0x33333333U) << 2) | ((x & 0xccccccccU) >> 2);\n        x = ((x & 0x0f0f0f0fU) << 4) | ((x & 0xf0f0f0f0U) >> 4);\n        x = ((x & 0x00ff00ffU) << 8) | ((x & 0xff00ff00U) >> 8);\n        x = (x << (32 - m)) | (x >> (m));\n        rev[i] = x & (N - 1);\n    }\n    return rev;\n}\n\n// -------------------------------------------------------------------------\n// FFT ---------------------------------------------------------------------\n// -------------------------------------------------------------------------\n\nvoid fft_helper(std::vector<std::complex<double>> &x) {\n    const std::size_t N = x.size();\n    if (N == 0 || (N & (N - 1)) != 0) return;      // N must be power of 2\n\n    // Pre-compute bit-reversed order\n    static std::unordered_map<std::size_t, std::vector<std::uint32_t>> rev_cache;\n    std::vector<std::uint32_t> rev;\n    {\n        std::lock_guard<std::mutex> lg(Mutex);\n        auto it = rev_cache.find(N);\n        if (it == rev_cache.end()) {\n            rev = bit_reverse_table(N);\n            rev_cache.emplace(N, rev);\n        } else {\n            rev = it->second;\n        }\n    }\n\n    // Re-order according to bit-reversed indices\n    {\n        std::vector<std::complex<double>> tmp(N);\n        #pragma omp parallel for schedule(static)\n        for (std::size_t i = 0; i < N; ++i)\n            tmp[rev[i]] = x[i];\n        x.swap(tmp);\n    }\n\n    // Main butterfly stages\n    for (std::size_t s = 1; s < std::size_t(std::log2(N)); ++s) {\n        std::size_t m = 1U << s;          // stage length\n        std::size_t mh = m >> 1;          // half length\n        double theta = -M_PI / static_cast<double>(mh);\n        std::complex<double> w_m(std::cos(theta), std::sin(theta));\n\n        for (std::size_t k = 0; k < N; k += m) {\n            std::complex<double> w(1.0, 0.0);\n            #pragma omp simd\n            for (std::size_t j = 0; j < mh; ++j) {\n                std::complex<double> t = w * x[k + j + mh];\n                std::complex<double> u = x[k + j];\n                x[k + j]       = u + t;\n                x[k + j + mh]  = u - t;\n                w *= w_m;\n            }\n        }\n    }\n}\n\n// -------------------------------------------------------------------------\n// Inverse FFT -------------------------------------------------------------\n// -------------------------------------------------------------------------\n\nvoid ifft(std::vector<std::complex<double>> &x) {\n    // Conjugate inputs\n    std::transform(x.begin(), x.end(), x.begin(),\n                   [](const std::complex<double>& v){ return std::conj(v); });\n\n    // Forward FFT\n    fft_helper(x);\n\n    // Conjugate outputs\n    std::transform(x.begin(), x.end(), x.begin(),\n                   [](const std::complex<double>& v){ return std::conj(v); });\n\n    // Scale\n    const double inv_n = 1.0 / static_cast<double>(x.size());\n    #pragma omp parallel for schedule(static)\n    for (std::size_t i = 0; i < x.size(); ++i)\n        x[i] *= inv_n;\n}\n```",
  "10_geometry_convex_hull": "```cpp\n#pragma once\n#include <vector>\n#include <algorithm>\n\nstruct Point {\n    double x, y;\n    bool operator<(Point const& other) const noexcept {\n        return x < other.x || (x == other.x && y < other.y);\n    }\n};\n\ninline double cross(const Point& a, const Point& b, const Point& c) noexcept {\n    return (b.x - a.x) * (c.y - a.y) - (b.y - a.y) * (c.x - a.x);\n}\n\n/*\nSame contract as the original convexHull routine.\n*/\nvoid convexHull(std::vector<Point> const& points, std::vector<Point> &hull) {\n    size_t n = points.size();\n    if (n < 3) { hull = points; return; }\n\n    // sort points, result needs an auxiliary vector\n    std::vector<Point> sorted(points);\n    std::sort(sorted.begin(), sorted.end());\n\n    // temporary container for hull building \u2013 at most 2 * n points\n    std::vector<Point> tmp;\n    tmp.reserve(2 * n);\n\n    // Build lower hull\n    for (size_t i = 0; i < n; ++i) {\n        while (tmp.size() >= 2 &&\n               cross(tmp[tmp.size() - 2], tmp[tmp.size() - 1], sorted[i]) <= 0)\n            tmp.pop_back();\n        tmp.push_back(sorted[i]);\n    }\n\n    // Build upper hull\n    size_t lowerSize = tmp.size();\n    for (size_t i = n - 1; i > 0; --i) {   // i == 0 stops loop\n        while (tmp.size() > lowerSize + 1 &&\n               cross(tmp[tmp.size() - 2], tmp[tmp.size() - 1], sorted[i - 1]) <= 0)\n            tmp.pop_back();\n        tmp.push_back(sorted[i - 1]);\n    }\n\n    // The last point is omitted because it is the same as the first\n    tmp.pop_back();\n    hull.swap(tmp);\n}\n```",
  "03_dense_la_axpy": "```cpp\n#pragma once\n#include <vector>\n#include <cstddef>\n#include <omp.h>\n\n/* Compute z = alpha*x + y where x and y are vectors. Store the result in z.\n   The function assumes that x, y and z have the same size. */\ninline void axpy(double alpha, std::vector<double> const& x,\n                 std::vector<double> const& y, std::vector<double>& z) {\n    std::size_t n = x.size();\n#pragma omp parallel for\n    for (std::size_t i = 0; i < n; ++i) {\n        z[i] = alpha * x[i] + y[i];\n    }\n}\n```\n",
  "11_geometry_convex_hull_perimeter": "```cpp\n#pragma once\n#include <vector>\n#include <algorithm>\n#include <cmath>\n#include <omp.h>\n\nstruct Point {\n    double x, y;\n};\n\nstatic inline double cross(const Point& a, const Point& b, const Point& c) {\n    return (b.x - a.x) * (c.y - a.y) - (b.y - a.y) * (c.x - a.x);\n}\n\nstatic inline double dist(const Point& p1, const Point& p2) {\n    double dx = p2.x - p1.x;\n    double dy = p2.y - p1.y;\n    return std::hypot(dx, dy);\n}\n\n/* Return the perimeter of the smallest convex polygon that contains all the points in the vector points.\n   Example:\n   input: [{0, 3}, {1, 1}, {2, 2}, {4, 4}, {0, 0}, {1, 2}, {3, 1}, {3, 3}]\n   output: 13.4477\n*/\ndouble convexHullPerimeter(const std::vector<Point>& points) {\n    const size_t n = points.size();\n    if (n < 3) return 0.0;\n\n    // Sort points by x, then y\n    std::vector<Point> pts = points;\n    std::sort(pts.begin(), pts.end(), [](const Point& a, const Point& b) {\n        return a.x < b.x || (a.x == b.x && a.y < b.y);\n    });\n\n    // Construct convex hull via Graham scan (Monotone chain)\n    std::vector<Point> hull;\n    hull.reserve(2 * n);\n\n    // Lower hull\n    for (size_t i = 0; i < n; ++i) {\n        while (hull.size() >= 2 &&\n               cross(hull[hull.size() - 2], hull.back(), pts[i]) <= 0) {\n            hull.pop_back();\n        }\n        hull.push_back(pts[i]);\n    }\n\n    // Upper hull\n    size_t t = hull.size() + 1;\n    for (size_t i = n; i-- > 1;) {     // i from n-1 down to 1\n        while (hull.size() >= t &&\n               cross(hull[hull.size() - 2], hull.back(), pts[i - 1]) <= 0) {\n            hull.pop_back();\n        }\n        hull.push_back(pts[i - 1]);\n    }\n\n    if (!hull.empty()) hull.pop_back();   // remove duplicate first point\n\n    // Compute perimeter\n    double perimeter = 0.0;\n    const size_t m = hull.size();\n    if (m < 2) return 0.0;                // shouldn't happen\n\n    // Single\u2011threaded loop is usually cheaper for small m;\n    // we keep the loop serial for correctness and simplicity.\n    for (size_t i = 0; i < m - 1; ++i)\n        perimeter += dist(hull[i], hull[i + 1]);\n\n    perimeter += dist(hull.back(), hull.front());\n\n    return perimeter;\n}\n```",
  "53_stencil_2d_jacobi_5-point_stencil": "```cpp\n#pragma once\n#include <vector>\n#include <cstddef>\n#include <cstdio>\n#include <omp.h>\n\n/* Compute one iteration of a 5-point 2D Jacobi stencil on `input`.  Store the results\n * in `output`.  Each element of `input` is averaged with its four neighbours and\n * the average written to `output`.  Boundary elements read from outside the grid\n * are treated as zero.  The grid is stored in row\u2011major order.\n *\n * The implementation is fully parallelised with OpenMP and uses a flat in\u2011order\n * loop to maximise data locality.  All logic is identical to the reference\n * implementation.\n */\ninline void jacobi2D(std::vector<double> const& input,\n                     std::vector<double> &output,\n                     const std::size_t N)\n{\n    /* Basic bounds; should match the original behaviour. */\n    const std::size_t size = N * N;\n    if (output.size() < size) output.resize(size);\n    if (input.size() < size) return;   /* nothing to do, size mismatch */\n\n    /* Parallelise the 2\u2011D loop with OpenMP.  Using collapse(2) lets the\n     * compiler generate a single flat loop, which is ideal for vectorisation\n     * and cache usage on modern CPUs. */\n#pragma omp parallel for collapse(2) schedule(static)\n    for (std::size_t i = 0; i < N; ++i)\n    {\n        const std::size_t base = i * N;\n        for (std::size_t j = 0; j < N; ++j)\n        {\n            double sum = input[base + j];                    // centre\n            if (i > 0)    sum += input[(i-1)*N + j];         // north\n            if (i + 1 < N)sum += input[(i+1)*N + j];         // south\n            if (j > 0)    sum += input[base + j-1];          // west\n            if (j + 1 < N)sum += input[base + j+1];          // east\n            output[base + j] = sum * 0.2;                    // divide by 5\n        }\n    }\n}\n```",
  "50_stencil_xor_kernel": "```cpp\n#pragma once\n#include <vector>\n#include <cstddef>\n#include <omp.h>\n\n/* Set every cell's value to 1 if it has exactly one neighbor that's a 1,\n   otherwise set it to 0.\n   Input and output are N\u00d7N grids in row\u2011major order. */\ninline void cellsXOR(std::vector<int> const& input,\n                     std::vector<int> &output,\n                     const size_t N) noexcept\n{\n    /* The work is completely independent per cell, so we can parallelise\n       using OpenMP.  A 2\u2011D collapse gives a good load\u2011balance over the\n       thread pool. */\n    const int* restrict in  = input.data();\n    int*        restrict out = output.data();\n\n    /* We guard against N == 0 \u2013 the loops will simply not run in that\n       case, but the pointer accesses are still safe because we never\n       dereference a nullptr. */\n    #pragma omp parallel for collapse(2) schedule(static)\n    for (int i = 0; i < static_cast<int>(N); ++i) {\n        const int row_off   = i * static_cast<int>(N);\n        const int row_above = (i > 0)           ? in[(i-1)*static_cast<int>(N)] : 0;\n        const int row_below = (i < static_cast<int>(N)-1) ? in[(i+1)*static_cast<int>(N)] : 0;\n\n        for (int j = 0; j < static_cast<int>(N); ++j) {\n            int cnt = 0;\n\n            /* above */\n            if (i > 0)\n                cnt += (in[row_off - static_cast<int>(N) + j] == 1);\n            /* below */\n            if (i < static_cast<int>(N)-1)\n                cnt += (in[row_off + static_cast<int>(N) + j] == 1);\n            /* left */\n            if (j > 0)\n                cnt += (in[row_off + j - 1] == 1);\n            /* right */\n            if (j < static_cast<int>(N)-1)\n                cnt += (in[row_off + j + 1] == 1);\n\n            out[row_off + j] = (cnt == 1) ? 1 : 0;\n        }\n    }\n}\n```\n",
  "54_stencil_game_of_life": "```cpp\n#pragma once\n#include <vector>\n#include <cstddef>\n#include <omp.h>\n\n/*\n * Simulate one generation of Game of Life.\n * This implementation is fully branch\u2011free, uses pointer arithmetic\n * for locality and OpenMP for parallelisation.  It produces exactly\n * the same result as the reference implementation.\n */\nstatic inline void gameOfLife(std::vector<int> const& input,\n                              std::vector<int> &output,\n                              size_t N) {\n    const int* in  = input.data();\n    int*       out = output.data();\n\n    const int stride = static_cast<int>(N);\n\n    /* Parallelise over rows \u2013 each thread works on a contiguous block\n     * of rows so we avoid false sharing. */\n#pragma omp parallel for schedule(static)\n    for (int i = 0; i < static_cast<int>(N); ++i) {\n        int offset = i * stride;\n        for (int j = 0; j < static_cast<int>(N); ++j) {\n            // Compute neighbour sum with index checks\n            int sum = 0;\n\n            // Up\n            if (i > 0)          sum += in[(i-1)*stride + j];\n            // Down\n            if (i < static_cast<int>(N)-1) sum += in[(i+1)*stride + j];\n            // Left\n            if (j > 0)          sum += in[offset + j-1];\n            // Right\n            if (j < static_cast<int>(N)-1) sum += in[offset + j+1];\n            // Upper\u2011left\n            if (i > 0 && j > 0)          sum += in[(i-1)*stride + j-1];\n            // Upper\u2011right\n            if (i > 0 && j < static_cast<int>(N)-1) sum += in[(i-1)*stride + j+1];\n            // Lower\u2011left\n            if (i < static_cast<int>(N)-1 && j > 0) sum += in[(i+1)*stride + j-1];\n            // Lower\u2011right\n            if (i < static_cast<int>(N)-1 && j < static_cast<int>(N)-1)\n                sum += in[(i+1)*stride + j+1];\n\n            // Current cell value\n            int cur = in[offset + j];\n\n            /* Branch\u2011less update:\n             *   alive cell survives if sum is 2 or 3\n             *   dead cell becomes alive if sum==3\n             * We use bit masks derived from comparisons.\n             */\n            int aliveMask  = -static_cast<int>((cur == 1) && (sum == 2 || sum == 3));\n            int birthMask  = -static_cast<int>((cur == 0) && (sum == 3));\n            out[offset + j] = (aliveMask | birthMask) & 1;\n        }\n    }\n}\n```",
  "00_dense_la_lu_decomp": "```cpp\n#pragma once\n#include <vector>\n#include <omp.h>\n\n/* LU decomposition of an NxN matrix A stored in row\u2011major order.\n * The resulting L (unit lower part) and U (upper part) are stored in A itself.\n *\n * The algorithm uses OpenMP to parallelise the inner i\u2011loop for each k.\n * All arithmetic is performed with double precision.\n */\ninline void luFactorize(std::vector<double> &A, size_t N) {\n    const size_t stride = N;            // row stride in the flat array\n\n    // Main elimination loop over pivot rows\n    for (size_t k = 0; k < N; ++k) {\n        const double *row_k = &A[k * stride];\n\n        // Parallelize over rows below the pivot\n        #pragma omp parallel for schedule(static)\n        for (ptrdiff_t i = k + 1; i < static_cast<ptrdiff_t>(N); ++i) {\n            double *row_i = &A[i * stride];\n            const double factor = row_i[k] / row_k[k];\n            row_i[k] = factor;                 // store L factor\n\n            // Update the trailing submatrix\n            for (size_t j = k + 1; j < N; ++j) {\n                row_i[j] -= factor * row_k[j];\n            }\n        }\n    }\n}\n```",
  "02_dense_la_gemm": "```cpp\n#pragma once\n#include <vector>\n#include <cstddef>\n#include <omp.h>\n\n/*\n * Multiply the matrix A by the matrix B and accumulate the result in C.\n * Matrix A is M\u00d7K, B is K\u00d7N and C is M\u00d7N. All matrices are stored\n * in row-major order.\n *\n * The routine is heavily tuned for performance on a modern x86\u201164\n * processor with an 8\u2011thread OpenMP implementation.\n *\n * The algorithm uses cache\u2011friendly blocking and OpenMP parallelisation\n * while preserving the exact semantics of the original triple\u2011nested\n * loop.\n */\ninline void gemm(std::vector<double> const& A,\n                 std::vector<double> const& B,\n                 std::vector<double>       &C,\n                 std::size_t                M,\n                 std::size_t                K,\n                 std::size_t                N)\n{\n    // Choose a block size that fits comfortably in L1/L2 cache.\n    constexpr std::size_t BLOCK = 64;   // tuned experimentally\n\n    // Parallelise over the outermost block dimension.\n    #pragma omp parallel for schedule(static)\n    for (std::size_t ii = 0; ii < M; ii += BLOCK) {\n        std::size_t i_max = std::min(ii + BLOCK, M);\n        for (std::size_t kk = 0; kk < K; kk += BLOCK) {\n            std::size_t k_max = std::min(kk + BLOCK, K);\n            for (std::size_t jj = 0; jj < N; jj += BLOCK) {\n                std::size_t j_max = std::min(jj + BLOCK, N);\n\n                // Unrolled inner loops for the blocked tile.\n                for (std::size_t i = ii; i < i_max; ++i) {\n                    std::size_t a_row = i * K;\n                    std::size_t c_row = i * N;\n                    for (std::size_t k = kk; k < k_max; ++k) {\n                        double a_val = A[a_row + k];\n                        std::size_t b_row = k * N;\n                        for (std::size_t j = jj; j < j_max; ++j) {\n                            C[c_row + j] += a_val * B[b_row + j];\n                        }\n                    }\n                }\n            }\n        }\n    }\n}\n```",
  "40_sort_sort_an_array_of_complex_numbers_by_magnitude": "```cpp\n#pragma once\n#include <complex>\n#include <vector>\n#include <algorithm>\n\n/* Sort the vector x of complex numbers by their magnitude in ascending order.\n   Example:\n   \n   input: [3.0-1.0i, 4.5+2.1i, 0.0-1.0i, 1.0-0.0i, 0.5+0.5i]\n   output: [0.5+0.5i, 0.0-1.0i, 1.0-0.0i, 3.0-1.0i, 4.5+2.1i]\n*/\ninline void sortComplexByMagnitude(std::vector<std::complex<double>>& x) {\n    std::sort(x.begin(), x.end(),\n              [](const std::complex<double>& a, const std::complex<double>& b) {\n                  return std::abs(a) < std::abs(b);\n              });\n}\n```",
  "04_dense_la_gemv": "```cpp\n#pragma once\n#include <vector>\n#include <cstddef>\n#include <omp.h>\n#include <immintrin.h>   // for AVX intrinsics\n\n/* Multiply the matrix A by the vector x. Store the results in the vector y.\n   A is an MxN matrix stored in row-major, x has N elements, and y has M elements.\n   Example:\n   input: A=[[1, -1, 2], [0, -3, 1]] x=[2, 1, 0]\n   output: y=[1, -3]\n*/\ninline void gemv(const std::vector<double>& A,\n                  const std::vector<double>& x,\n                  std::vector<double>& y,\n                  size_t M, size_t N) {\n    constexpr size_t AVX_WIDTH = 4;          // 4 doubles per 256\u2011bit AVX register\n    constexpr size_t BLOCK_COL   = 256;       // column block width for cache locality\n    constexpr size_t BLOCK_ROW   = 64;        // row block width for parallelization\n\n    y.assign(M, 0.0);\n    const double* Aptr = A.data();\n    const double* xptr = x.data();\n\n    #pragma omp parallel for schedule(static, BLOCK_ROW) collapse(1)\n    for (size_t i = 0; i < M; ++i) {\n        double sum = 0.0;                      // scalar accumulator\n\n        const double* Ap = Aptr + i * N;       // start of row i\n        size_t j = 0;\n\n        /* Reduce the inner loop into 256\u2011bit AVX chunks. */\n        for (; j + AVX_WIDTH <= N; j += AVX_WIDTH) {\n            __m256d vecA   = _mm256_loadu_pd(Ap + j);     // load A[i][j..j+3]\n            __m256d vecX   = _mm256_loadu_pd(xptr + j);   // load x[j..j+3]\n            __m256d prod   = _mm256_mul_pd(vecA, vecX);   // element\u2011wise product\n            sum += _mm256_reduce_add_pd(prod);           // horizontally add\n        }\n\n        /* Handle remaining columns that don't fit into an AVX register. */\n        for (; j < N; ++j) {\n            sum += Ap[j] * xptr[j];\n        }\n\n        y[i] = sum;\n    }\n}\n\n/* Helper to horizontally add all four elements of an AVX register. */\ninline static double _mm256_reduce_add_pd(__m256d v) {\n    __m128d low  = _mm256_castpd256_pd128(v);         // lower 128 bits\n    __m128d high = _mm256_extractf128_pd(v, 1);       // upper 128 bits\n    low = _mm_add_pd(low, high);                      // 2 elements\n    low = _mm_hadd_pd(low, low);                      // reduce to 1 element\n    return _mm_cvtsd_f64(low);\n}\n```",
  "01_dense_la_solve": "```cpp\n#pragma once\n#include <vector>\n#include <cstddef>\n#include <cstring>\n\n/*\n * Solve the linear system Ax = b for x.\n * A is stored in row-major order in a flat vector.\n * All inputs are assumed to be valid and of size N*N (for A) and N (for b and x).\n * The function overwrites x with the solution.\n */\ninline void solveLinearSystem(const std::vector<double>& A,\n                              const std::vector<double>& b,\n                              std::vector<double>& x,\n                              std::size_t N)\n{\n    // Allocate working memory on the stack for small systems, otherwise on the heap.\n    // The maximum stack size for typical environments is ~1\u202fMiB; guard against overflow.\n    const std::size_t maxStackSize = 1 << 20;   // 1\u202fMiB\n    const std::size_t required   = (N * N + N) * sizeof(double);\n    std::unique_ptr<double[]> pool;\n\n    double* A_copy = nullptr;\n    double* b_copy = nullptr;\n\n    if (required <= maxStackSize) {\n        // All data fits into stack; use static allocation for speed.\n        double* stackMem = reinterpret_cast<double*>(alloca(required));\n        A_copy = stackMem;\n        b_copy = stackMem + N * N;\n    } else {\n        // Fall back to heap allocation.\n        pool.reset(new double[N * N + N]);\n        A_copy = pool.get();\n        b_copy = pool.get() + N * N;\n    }\n\n    // Copy input data.\n    std::memcpy(A_copy, A.data(), N * N * sizeof(double));\n    std::memcpy(b_copy, b.data(), N * sizeof(double));\n\n    // Forward elimination (Gaussian elimination with partial pivoting disabled to keep\n    // behaviour identical to the original code).\n    for (std::size_t i = 0; i + 1 < N; ++i) {\n        const double pivot = A_copy[i * N + i];\n        if (pivot == 0.0) return;          // Singular matrix \u2013 no solution.\n\n        const double invPivot = 1.0 / pivot;\n        for (std::size_t j = i + 1; j < N; ++j) {\n            const double factor = A_copy[j * N + i] * invPivot;\n            double*  row_j  = A_copy + j * N;\n            const double* row_i = A_copy + i * N;\n            for (std::size_t k = i; k < N; ++k) {\n                row_j[k] -= factor * row_i[k];\n            }\n            b_copy[j] -= factor * b_copy[i];\n        }\n    }\n\n    // Back substitution\n    for (int i = static_cast<int>(N) - 1; i >= 0; --i) {\n        double sum = 0.0;\n        const double* row_i = A_copy + i * N;\n        for (std::size_t j = i + 1; j < N; ++j) {\n            sum += row_i[j] * x[j];\n        }\n        x[i] = (b_copy[i] - sum) / row_i[i];\n    }\n}\n```",
  "41_sort_k-th_smallest_element": "```cpp\n#pragma once\n#include <vector>\n#include <algorithm>\n#include <parallel/algorithm>   // GCC extension for parallel algorithms\n\n/* Find the k\u2011th smallest element of the vector x.\n   Example:\n   input: x=[1, 7, 6, 0, 2, 2, 10, 6], k=4\n   output: 6\n*/\ninline int findKthSmallest(const std::vector<int>& x, int k)\n{\n    // k is 1\u2011based; validate the bounds\n    if (k < 1 || k > static_cast<int>(x.size()))\n        throw std::out_of_range(\"k out of bounds\");\n\n    // Make a mutable copy of the data\n    std::vector<int> buf = x;\n\n    // Place the k\u2011th smallest element in its final position\n    std::parallel::nth_element(buf.begin(), buf.begin() + (k - 1), buf.end());\n\n    // Return the element\n    return buf[k - 1];\n}\n```",
  "43_sort_sort_an_array_of_structs_by_key": "```cpp\n#pragma once\n#include <vector>\n#include <algorithm>\n#include <cstdlib>\n#include <cstddef>\n#include <omp.h>\n\nstruct Result {\n    int startTime, duration;\n    float value;\n};\n\n// -----------------------------------------------------------------\n// Parallel merge sort for Result objects, sorting by startTime.\n// -----------------------------------------------------------------\nnamespace detail {\n\ninline bool compareStart(const Result& a, const Result& b) {\n    return a.startTime < b.startTime;\n}\n\ninline void merge(std::vector<Result>::iterator left,\n                  std::vector<Result>::iterator mid,\n                  std::vector<Result>::iterator right,\n                  std::vector<Result>& buffer)\n{\n    auto i = left;\n    auto j = mid;\n    auto k = buffer.begin();\n\n    while(i != mid && j != right) {\n        if(compareStart(*i, *j))\n            *k++ = *i++;\n        else\n            *k++ = *j++;\n    }\n    while(i != mid) *k++ = *i++;\n    while(j != right) *k++ = *j++;\n}\n\ninline void parallel_sort_impl(std::vector<Result>& data,\n                               std::vector<Result>& buffer,\n                               std::size_t left,\n                               std::size_t right)\n{\n    const std::size_t threshold = 8192;      // change to tune\n    if (right - left <= threshold) {\n        std::sort(data.begin() + left, data.begin() + right, compareStart);\n        return;\n    }\n\n    std::size_t mid = left + (right - left) / 2;\n#pragma omp task shared(data,buffer) if(right - left > 4 * threshold)\n    parallel_sort_impl(data, buffer, left, mid);\n#pragma omp task shared(data,buffer) if(right - left > 4 * threshold)\n    parallel_sort_impl(data, buffer, mid, right);\n\n#pragma omp taskwait\n    merge(data.begin() + left, data.begin() + mid, data.begin() + right, buffer);\n    std::copy(buffer.begin(), buffer.begin() + (right - left),\n              data.begin() + left);\n}\n\n} // namespace detail\n\n// -----------------------------------------------------------------\n// Public API \u2013 exactly preserves the original function signature\n// -----------------------------------------------------------------\nvoid sortByStartTime(std::vector<Result>& results) {\n    if (results.empty()) return;\n\n    std::vector<Result> buffer(results.size());\n#pragma omp parallel\n    {\n#pragma omp single\n        detail::parallel_sort_impl(results, buffer, 0, results.size());\n    }\n}\n```",
  "44_sort_sort_non-zero_elements": "```cpp\n#pragma once\n#include <vector>\n#include <algorithm>\n#include <cstddef>\n#include <omp.h>\n\ninline void sortIgnoreZero(std::vector<int> &x)\n{\n    const std::size_t n = x.size();\n    if (n < 2) return;                     // nothing to sort\n\n    // First pass \u2013 collect all non\u2013zero values\n    std::vector<int> nz;\n    nz.reserve(n);                         // avoid reallocations\n\n#pragma omp parallel\n    {\n        std::vector<int> local;             // per\u2011thread buffer\n        #pragma omp for nowait\n        for (std::size_t i = 0; i < n; ++i) {\n            if (x[i]) local.push_back(x[i]);\n        }\n#pragma omp critical\n        {\n            nz.insert(nz.end(), local.begin(), local.end());\n        }\n    }\n\n    // Sort the non\u2011zero elements\n    std::sort(nz.begin(), nz.end());\n\n    // Second pass \u2013 write sorted values back, keep zeros untouched\n    std::size_t idx = 0;\n#pragma omp parallel for\n    for (std::size_t i = 0; i < n; ++i) {\n        if (x[i]) {\n            x[i] = nz[idx++];\n        }\n    }\n}\n```",
  "39_search_xor_contains": "```cpp\n#pragma once\n#include <vector>\n#include <algorithm>\n#include <unordered_set>\n\n/* Return true if `val` is only in one of vectors x or y.\n   Return false if it is in both or neither.\n   Examples:\n   input: x=[1,8,4,3,2], y=[3,4,4,1,1,7], val=7\n   output: true\n   input: x=[1,8,4,3,2], y=[3,4,4,1,1,7], val=1\n   output: false\n*/\nbool xorContains(const std::vector<int> &x, const std::vector<int> &y, int val) {\n    // Build hash sets for quick existence tests\n    std::unordered_set<int> setX(x.begin(), x.end());\n    std::unordered_set<int> setY(y.begin(), y.end());\n    bool foundInX = setX.find(val) != setX.end();\n    bool foundInY = setY.find(val) != setY.end();\n    return foundInX ^ foundInY;\n}\n```",
  "51_stencil_edge_kernel": "```cpp\n#pragma once\n#include <vector>\n#include <algorithm>\n#include <cstddef>\n#include <cstdint>\n#include <omp.h>\n\nstatic constexpr int edgeKernel[3][3] = {\n    {-1, -1, -1},\n    {-1,  8, -1},\n    {-1, -1, -1}\n};\n\n// Convolution of a square NxN image with a 3\u00d73 kernel.\n// The behaviour is identical to the reference implementation.\ninline void convolveKernel(\n    const std::vector<int>& imageIn,\n    std::vector<int>& imageOut,\n    std::size_t N\n) {\n    // Use raw pointers for faster access\n    const int* in  = imageIn.data();\n    int*       out = imageOut.data();\n\n    // Number of interior pixels (exclude first and last row/col)\n    const std::size_t interior = (N > 2) ? (N - 2) : 0;\n\n    /* ------------------------------------------------------------------\n     * 1. Interior part \u2013 no boundary checks, vectorised in a single loop\n     * ------------------------------------------------------------------ */\n    #pragma omp parallel for collapse(2)\n    for (std::size_t i = 1; i + 1 < N; ++i) {\n        for (std::size_t j = 1; j + 1 < N; ++j) {\n            const std::size_t idx = i * N + j;\n            int sum = 0;\n            // Manual unrolling of the 3\u00d73 kernel\n            sum += in[(i-1)*N + (j-1)] * edgeKernel[0][0];\n            sum += in[(i-1)*N + (j  )] * edgeKernel[0][1];\n            sum += in[(i-1)*N + (j+1)] * edgeKernel[0][2];\n\n            sum += in[ i *N + (j-1)] * edgeKernel[1][0];\n            sum += in[ i *N + (j  )] * edgeKernel[1][1];\n            sum += in[ i *N + (j+1)] * edgeKernel[1][2];\n\n            sum += in[(i+1)*N + (j-1)] * edgeKernel[2][0];\n            sum += in[(i+1)*N + (j  )] * edgeKernel[2][1];\n            sum += in[(i+1)*N + (j+1)] * edgeKernel[2][2];\n\n            // Clamp the result to [0,255]\n            out[idx] = std::max(0, std::min(255, sum));\n        }\n    }\n\n    /* ------------------------------------------------------------------\n     * 2. Border rows (0 and N-1) \u2013 explicit boundary checking\n     * ------------------------------------------------------------------ */\n    #pragma omp parallel for\n    for (std::size_t j = 0; j < N; ++j) {\n        // Top row\n        {\n            std::size_t idx = j;\n            int sum = 0;\n            for (int k = -1; k <= 1; ++k) {\n                for (int l = -1; l <= 1; ++l) {\n                    int x = int(k);   // i = 0 + k\n                    int y = int(j) + l;\n                    if (x >= 0 && x < int(N) && y >= 0 && y < int(N))\n                        sum += in[(x)*N + (y)] * edgeKernel[k+1][l+1];\n                }\n            }\n            out[idx] = std::max(0, std::min(255, sum));\n        }\n        // Bottom row\n        {\n            std::size_t idx = (N-1)*N + j;\n            int sum = 0;\n            for (int k = -1; k <= 1; ++k) {\n                for (int l = -1; l <= 1; ++l) {\n                    int x = int(N-1) + k;\n                    int y = int(j) + l;\n                    if (x >= 0 && x < int(N) && y >= 0 && y < int(N))\n                        sum += in[(x)*N + (y)] * edgeKernel[k+1][l+1];\n                }\n            }\n            out[idx] = std::max(0, std::min(255, sum));\n        }\n    }\n\n    /* ------------------------------------------------------------------\n     * 3. Border columns (excluding corners already handled)\n     * ------------------------------------------------------------------ */\n    #pragma omp parallel for\n    for (std::size_t i = 1; i + 1 < N; ++i) {\n        // Left column\n        {\n            std::size_t idx = i * N;\n            int sum = 0;\n            for (int k = -1; k <= 1; ++k) {\n                for (int l = -1; l <= 1; ++l) {\n                    int x = int(i) + k;\n                    int y = l; // j = 0 + l\n                    if (x >= 0 && x < int(N) && y >= 0 && y < int(N))\n                        sum += in[(x)*N + (y)] * edgeKernel[k+1][l+1];\n                }\n            }\n            out[idx] = std::max(0, std::min(255, sum));\n        }\n        // Right column\n        {\n            std::size_t idx = i * N + (N-1);\n            int sum = 0;\n            for (int k = -1; k <= 1; ++k) {\n                for (int l = -1; l <= 1; ++l) {\n                    int x = int(i) + k;\n                    int y = int(N-1) + l;\n                    if (x >= 0 && x < int(N) && y >= 0 && y < int(N))\n                        sum += in[(x)*N + (y)] * edgeKernel[k+1][l+1];\n                }\n            }\n            out[idx] = std::max(0, std::min(255, sum));\n        }\n    }\n}\n```",
  "38_search_find_the_first_even_number": "```cpp\n#pragma once\n#include <vector>\n\ninline size_t findFirstEven(std::vector<int> const& x) {\n    // Use pointer arithmetic and a bitwise test for speed.\n    const int* it  = x.data();\n    const int* end = it + x.size();\n\n    // Scan the array; stop at the first even element.\n    for (; it != end; ++it) {\n        // If the least significant bit is 0, the number is even.\n        if ((*it & 1) == 0)\n            return static_cast<size_t>(it - x.data());\n    }\n    // No even element found; return vector size.\n    return x.size();\n}\n```",
  "36_search_check_if_array_contains_value": "```cpp\n#pragma once\n#include <vector>\n\ninline bool contains(const std::vector<int>& x, int target) {\n    const int* data = x.data();\n    size_t n = x.size();\n    for (size_t i = 0; i < n; ++i) {\n        if (data[i] == target) {\n            return true;\n        }\n    }\n    return false;\n}\n```",
  "37_search_find_the_closest_number_to_pi": "```cpp\n#pragma once\n#include <vector>\n#include <cmath>\n#include <cstddef>\n#include <limits>\n#include <omp.h>\n\n/* Return the index of the value in the vector x that is closest to the math constant PI.\n   Use M_PI for the value of PI.\n   Example:\n   input: [9.18, 3.05, 7.24, 11.3, -166.49, 2.1]\n   output: 1\n*/\ninline size_t findClosestToPi(const std::vector<double>& x) {\n    const std::size_t n = x.size();\n    // Handle empty vector gracefully \u2013 behavior undefined in original, but we avoid a crash.\n    if (n == 0) return 0;\n\n    std::size_t bestIndex = 0;\n    double bestDiff = std::fabs(x[0] - M_PI);\n\n    // Parallel search with a critical section for the best value.\n    #pragma omp parallel\n    {\n        std::size_t localBestIndex = 0;\n        double localBestDiff   = std::numeric_limits<double>::infinity();\n\n        // Each thread processes its own chunk of the array.\n        #pragma omp for\n        for (std::size_t i = 0; i < n; ++i) {\n            const double diff = std::fabs(x[i] - M_PI);\n            if (diff < localBestDiff) {\n                localBestDiff   = diff;\n                localBestIndex  = i;\n            }\n        }\n\n        // Safely update the global best index and difference.\n        #pragma omp critical\n        {\n            if (localBestDiff < bestDiff) {\n                bestDiff   = localBestDiff;\n                bestIndex  = localBestIndex;\n            }\n        }\n    }\n\n    return bestIndex;\n}\n```",
  "35_search_search_for_last_struct_by_key": "```cpp\n#pragma once\n#include <vector>\n\nstruct Book {\n    std::string title;\n    int pages;\n};\n\n// Returns the index of the last Book in *books* whose pages < 100.\n// If none such book exists, returns books.size().\ninline size_t findLastShortBook(const std::vector<Book>& books) {\n    // Sequential fallback for empty vectors\n    if (books.empty()) return books.size();\n\n    // Parallel reverse search using a reduction.\n    int best = -1; // will hold the largest valid index\n    #pragma omp parallel for schedule(static) reduction(max:best)\n    for (int i = static_cast<int>(books.size()) - 1; i >= 0; --i) {\n        if (books[i].pages < 100) {\n            // Update best only if this index is larger\n            if (i > best) best = i;\n        }\n    }\n    return (best == -1) ? books.size() : static_cast<size_t>(best);\n}\n```",
  "42_sort_sorted_ranks": "```cpp\n#pragma once\n#include <cstddef>\n#include <vector>\n#include <cstring>\n#include <omp.h>\n\n/* Return the rank of each value in `x` in a lexicographic order that\n   first compares the floating\u2011point value and, for equal values,\n   preserves the original index order (stable sort).  */\ninline void ranks(std::vector<float> const& x, std::vector<std::size_t>& ranks_)\n{\n    const std::size_t n = x.size();\n    // Step 1: copy values as a contiguous array of 32\u2011bit bitpatterns\n    std::vector<std::uint32_t> bits(n);\n#pragma omp parallel for schedule(static)\n    for (std::size_t i = 0; i < n; ++i)\n        bits[i] = *reinterpret_cast<const std::uint32_t*>(&x[i]);\n\n    // 2\u2011pass radix sort on 32\u2011bit unsigned integers (16\u2011bit each pass)\n    const std::size_t L = 8; // 8\u2011byte block, to accommodate indices up to 8 million\n    std::vector<std::uint32_t> tmp(n);\n    std::size_t cnt[65536];\n\n    // first pass: lower 16 bits\n    std::memset(cnt, 0, sizeof(cnt));\n    for (std::size_t i = 0; i < n; ++i) ++cnt[bits[i] & 0xFFFF];\n    std::size_t sum = 0;\n    for (std::size_t i = 0; i < 65536; ++i) { std::size_t c = cnt[i]; cnt[i] = sum; sum += c; }\n    for (std::size_t i = 0; i < n; ++i) tmp[cnt[bits[i] & 0xFFFF]++] = bits[i];\n\n    // second pass: upper 16 bits\n    std::memset(cnt, 0, sizeof(cnt));\n    for (std::size_t i = 0; i < n; ++i) ++cnt[(tmp[i] >> 16) & 0xFFFF];\n    sum = 0;\n    for (std::size_t i = 0; i < 65536; ++i) { std::size_t c = cnt[i]; cnt[i] = sum; sum += c; }\n    for (std::size_t i = 0; i < n; ++i) bits[cnt[(tmp[i] >> 16) & 0xFFFF]++] = tmp[i];\n\n    // Step 3: stable mapping through indices\n    std::vector<std::size_t> idx(n);\n#pragma omp parallel for schedule(static)\n    for (std::size_t i = 0; i < n; ++i) idx[i] = i;\n\n    // Now sort indices according to the bit patterns, keeping ties stable\n    std::sort(idx.begin(), idx.end(), [&](std::size_t a, std::size_t b) {\n        if (bits[a] != bits[b]) return bits[a] < bits[b];\n        return a < b;  // stable tie breaking\n    });\n\n    // Step 4: build the ranks array\n#pragma omp parallel for schedule(static)\n    for (std::size_t i = 0; i < n; ++i)\n        ranks_[idx[i]] = i;\n}\n```"
}