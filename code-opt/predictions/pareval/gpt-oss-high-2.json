{
  "15_graph_edge_count": "```c++\n// Optimized edge count for a directed graph adjacency matrix.\n// Uses OpenMP for parallel summation, preserving exact original semantics.\n#pragma once\n#include <vector>\n#include <omp.h>\n\ninline int edgeCount(std::vector<int> const& A, size_t N) {\n    const size_t total = N * N;               // number of matrix entries\n    const int* const data = A.data();         // pointer to the underlying array\n    int count = 0;                            // result counter\n\n    // Parallel loop with reduction on 'count'.\n    #pragma omp parallel for reduction(+:count) schedule(static)\n    for (size_t i = 0; i < total; ++i) {\n        if (data[i] == 1) {\n            ++count;\n        }\n    }\n\n    return count;\n}\n```",
  "17_graph_highest_degree": "#pragma once\n#include <vector>\n#include <cstddef>\n#include <algorithm>\n#include <omp.h>\n\ninline int maxDegree(const std::vector<int>& A, std::size_t N) {\n    const int* data = A.data();\n    int highest = 0;\n    int n = static_cast<int>(N);\n\n    #pragma omp parallel for reduction(max:highest) schedule(static)\n    for (int i = 0; i < n; ++i) {\n        const int* row = data + i * n;\n        int sum = 0;\n        #pragma omp simd\n        for (int j = 0; j < n; ++j) {\n            sum += row[j];\n        }\n        if (sum > highest) highest = sum;\n    }\n\n    return highest;\n}\n",
  "59_transform_map_function": "```cpp\n#pragma once\n#include <vector>\n\ninline bool isPowerOfTwo(int x) {\n    return x > 0 && !(x & (x - 1));\n}\n\nvoid mapPowersOfTwo(const std::vector<int>& x, std::vector<bool>& mask) {\n    const std::size_t n = x.size();\n    const int N = static_cast<int>(n);\n    const int* data = x.data();\n\n    #pragma omp parallel for schedule(static)\n    for (int i = 0; i < N; ++i) {\n        mask[i] = isPowerOfTwo(data[i]);\n    }\n}\n```",
  "34_scan_largest_contiguous_subarray_sum": "```cpp\n#pragma once\n#include <vector>\n#include <limits>\n\nint maximumSubarray(const std::vector<int> &x) {\n    if (x.empty())\n        return std::numeric_limits<int>::lowest();\n\n    int maxSum = std::numeric_limits<int>::lowest();\n    int curSum = 0;\n    for (const int *ptr = x.data(), *end = ptr + x.size(); ptr != end; ++ptr) {\n        curSum += *ptr;\n        if (curSum > maxSum)\n            maxSum = curSum;\n        if (curSum < 0)\n            curSum = 0;\n    }\n    return maxSum;\n}\n```",
  "56_transform_negate_odds": "```cpp\n#pragma once\n\n#include <vector>\n#include <cstddef>\n\n/// In the vector `x` negate all odd values and halve all even values.\n/// For example:\n///   input  : [16, 11, 12, 14, 1, 0, 5]\n///   output : [8, -11, 6, 7, -1, 0, -5]\n///\nvoid negateOddsAndHalveEvens(std::vector<int> &x) {\n    const std::size_t n = x.size();\n    if (n == 0) return;            // nothing to do\n\n    int *data = x.data();          // raw pointer for fast access\n\n    // Parallelize the loop over the available threads.\n    // Each thread works on a disjoint chunk, so no data races.\n    #pragma omp parallel for schedule(static)\n    for (std::size_t i = 0; i < n; ++i) {\n        const int val = data[i];\n        // `val & 1` is 0 for even, 1 for odd.\n        // For even values, shift-right computes the integer division by 2.\n        data[i] = (val & 1) ? -val : (val >> 1);\n    }\n}\n```",
  "57_transform_inverse_offset": "```cpp\n#pragma once\n#include <vector>\n\ninline void oneMinusInverse(std::vector<double> &x) {\n    const std::size_t n = x.size();\n    if (n == 0) return;\n\n    #pragma omp parallel for schedule(static)\n    for (std::size_t i = 0; i < n; ++i) {\n        const double val = x[i];\n        x[i] = 1.0 - 1.0 / val;\n    }\n}\n```\n",
  "21_histogram_bin_0-100": "```cpp\n#pragma once\n#include <array>\n#include <vector>\n#include <cstddef>\n\ninline void binsBy10Count(std::vector<double> const &x,\n                          std::array<size_t, 10> &bins) {\n    const std::size_t n = x.size();\n\n    #pragma omp parallel\n    {\n        std::array<size_t, 10> local = {};\n\n        #pragma omp for\n        for (std::size_t i = 0; i < n; ++i) {\n            const std::size_t bin = static_cast<std::size_t>(x[i] / 10.0);\n            local[bin] += 1;               // keeps original UB for x[i] == 100\n        }\n\n        #pragma omp critical\n        {\n            for (std::size_t j = 0; j < 10; ++j)\n                bins[j] += local[j];\n        }\n    }\n}\n```",
  "58_transform_squaring": "```cpp\n#pragma once\n#include <vector>\n#include <omp.h>\n\n/* Replace every element of x with the square of its value.\n   Example:\n   input: [5, 1, 2, -4, 8]\n   output: [25, 1, 4, 16, 64]\n*/\nvoid squareEach(std::vector<int> &x) {\n    const size_t n = x.size();\n    if (n == 0) {\n        return;\n    }\n    #pragma omp parallel for schedule(static)\n    for (size_t i = 0; i < n; ++i) {\n        int v = x[i];\n        x[i] = v * v;\n    }\n}\n```",
  "20_histogram_pixel_histogram": "```cpp\n#pragma once\n#include <vector>\n#include <array>\n#ifdef _OPENMP\n#include <omp.h>\n#endif\n\nvoid pixelCounts(std::vector<int> const& image, std::array<size_t, 256> &bins) {\n#ifdef _OPENMP\n    const size_t n = image.size();\n#pragma omp parallel\n    {\n        std::array<size_t, 256> local = {};\n\n#pragma omp for schedule(static)\n        for (size_t i = 0; i < n; ++i) {\n            // Behaves like original: no bounds\u2011check, identical UB for out\u2011of\u2011range values\n            local[static_cast<size_t>(image[i])]++; \n        }\n\n#pragma omp critical\n        {\n            for (size_t j = 0; j < 256; ++j) {\n                bins[j] += local[j];\n            }\n        }\n    }\n#else\n    // Fallback when OpenMP is not available\n    for (const int v : image) {\n        bins[v] += 1;\n    }\n#endif\n}\n```",
  "23_histogram_first_letter_counts": "```cpp\n#pragma once\n#include <array>\n#include <string>\n#include <vector>\n#include <omp.h>\n#include <cstddef>\n\nvoid firstLetterCounts(std::vector<std::string> const& s, std::array<size_t, 26> &bins) {\n    const size_t n = s.size();\n    if (n == 0) return;\n\n    int max_threads = omp_get_max_threads();\n    std::vector<std::array<size_t, 26>> local(max_threads, std::array<size_t, 26>{});\n\n    #pragma omp parallel\n    {\n        int tid = omp_get_thread_num();\n        auto &local_counts = local[tid];\n        #pragma omp for schedule(static)\n        for (size_t i = 0; i < n; ++i) {\n            char c = s[i][0];\n            unsigned idx = static_cast<unsigned>(c - 'a');\n            local_counts[idx] += 1;\n        }\n    }\n\n    for (int t = 0; t < max_threads; ++t) {\n        const auto &counts = local[t];\n        for (size_t j = 0; j < 26; ++j) {\n            bins[j] += counts[j];\n        }\n    }\n}\n```\n",
  "30_scan_prefix_sum": "```cpp\n#pragma once\n#include <vector>\n#include <cstddef>\n#include <cstdint>\n#include <algorithm>\n#include <omp.h>\n\n/* Compute the inclusive prefix sum of `x` into `output`.\n   The result is identical to:\n   std::inclusive_scan(x.begin(), x.end(), output.begin());\n   This implementation uses OpenMP to parallelise the computation\n   for maximum speed on an 8\u2011thread x86\u201164 system. */\ninline void prefixSum(const std::vector<int64_t> &x, std::vector<int64_t> &output)\n{\n    std::size_t n = x.size();\n    if (n == 0) return;                      // nothing to do\n\n    output.resize(n);\n\n    const int64_t* in  = x.data();\n    int64_t*       out = output.data();\n\n    int num_threads = omp_get_max_threads();\n    std::size_t block_size = (n + num_threads - 1) / num_threads;   // ceil division\n\n    // 1. Compute local inclusive sums and record block totals\n    std::vector<int64_t> block_sums(num_threads, 0);\n\n    #pragma omp parallel\n    {\n        int id = omp_get_thread_num();\n        std::size_t start = id * block_size;\n        std::size_t end   = std::min(start + block_size, n);\n\n        if (start < end) {\n            out[start] = in[start];\n            for (std::size_t i = start + 1; i < end; ++i)\n                out[i] = out[i - 1] + in[i];\n            block_sums[id] = out[end - 1];\n        }\n    }\n\n    // 2. Compute offsets for each block (prefix sums of block_sums)\n    std::vector<int64_t> offsets(num_threads);\n    int64_t running = 0;\n    for (int i = 0; i < num_threads; ++i) {\n        offsets[i] = running;\n        running += block_sums[i];\n    }\n\n    // 3. Add block offsets to each element of its block\n    #pragma omp parallel\n    {\n        int id = omp_get_thread_num();\n        std::size_t start = id * block_size;\n        std::size_t end   = std::min(start + block_size, n);\n        int64_t add = offsets[id];\n        if (add != 0 && start < end) {\n            for (std::size_t i = start; i < end; ++i)\n                out[i] += add;\n        }\n    }\n}\n```",
  "22_histogram_count_quadrants": "```cpp\n#pragma once\n#include <array>\n#include <vector>\n\nstruct Point {\n    double x, y;\n};\n\ninline void countQuadrants(const std::vector<Point>& points, std::array<size_t,4>& bins)\n{\n    size_t q0 = 0, q1 = 0, q2 = 0, q3 = 0;\n    const size_t n = points.size();\n    const Point* const data = points.data();\n\n    if (n > 0) {\n#pragma omp parallel for reduction(+:q0,q1,q2,q3) schedule(static)\n        for (size_t i = 0; i < n; ++i) {\n            const double x = data[i].x;\n            const double y = data[i].y;\n            if (x >= 0.0) {\n                if (y >= 0.0) ++q0;\n                else          ++q3;\n            } else {\n                if (y >= 0.0) ++q1;\n                else          ++q2;\n            }\n        }\n    }\n\n    bins[0] += q0;\n    bins[1] += q1;\n    bins[2] += q2;\n    bins[3] += q3;\n}\n```",
  "24_histogram_count_quartile": "```cpp\n#pragma once\n#include <array>\n#include <vector>\n#ifdef _OPENMP\n#include <omp.h>\n#endif\n\ninline void countQuartiles(const std::vector<double>& x, std::array<size_t, 4>& bins)\n{\n    const size_t n = x.size();\n    #pragma omp parallel\n    {\n        size_t local[4] = {0, 0, 0, 0};\n\n        #pragma omp for schedule(static) nowait\n        for (size_t i = 0; i < n; ++i)\n        {\n            double val  = x[i];\n            double frac = val - static_cast<int>(val);\n\n            if (frac < 0.25)\n                ++local[0];\n            else if (frac < 0.5)\n                ++local[1];\n            else if (frac < 0.75)\n                ++local[2];\n            else\n                ++local[3];\n        }\n\n        #pragma omp critical\n        {\n            bins[0] += local[0];\n            bins[1] += local[1];\n            bins[2] += local[2];\n            bins[3] += local[3];\n        }\n    }\n}\n```",
  "32_scan_sum_of_prefix_sum_array": "```cpp\n#pragma once\n#include <cstdint>\n#include <vector>\n\ninline int64_t sumOfPrefixSum(const std::vector<int64_t>& x) {\n    const std::size_t n = x.size();\n    const int64_t* data = x.data();\n    unsigned long long sum = 0;\n\n#pragma omp parallel for reduction(+:sum) schedule(static)\n    for (std::size_t i = 0; i < n; ++i) {\n        unsigned long long prod =\n            static_cast<unsigned long long>(data[i]) *\n            static_cast<unsigned long long>(n - i);\n        sum += prod;\n    }\n    return static_cast<int64_t>(sum);\n}\n```",
  "33_scan_reverse_prefix_sum": "```cpp\n#pragma once\n#include <vector>\n#include <cstddef>\n#include <algorithm>\n#include <omp.h>\n\ninline void reversePrefixSum(const std::vector<int>& x, std::vector<int>& output)\n{\n    const std::size_t n = x.size();\n    output.resize(n);\n    if (n == 0) return;\n\n    const int maxThreads = omp_get_max_threads();\n    std::vector<int> blockSums(maxThreads, 0);\n    const std::size_t chunk = (n + maxThreads - 1) / maxThreads;\n\n    #pragma omp parallel\n    {\n        const int tid = omp_get_thread_num();\n        const std::size_t start = static_cast<std::size_t>(tid) * chunk;\n        std::size_t end = start + chunk;\n        if (end > n) end = n;\n\n        int localSum = 0;\n        for (std::size_t i = start; i < end; ++i) {\n            localSum += x[n - 1 - i];\n            output[i] = localSum;\n        }\n        blockSums[tid] = localSum;\n    }\n\n    // Compute offsets for each chunk\n    int offset = 0;\n    for (int t = 0; t < maxThreads; ++t) {\n        const int val = blockSums[t];\n        blockSums[t] = offset;\n        offset += val;\n    }\n\n    #pragma omp parallel\n    {\n        const int tid    = omp_get_thread_num();\n        const std::size_t start = static_cast<std::size_t>(tid) * chunk;\n        std::size_t end = start + chunk;\n        if (end > n) end = n;\n        const int add = blockSums[tid];\n        for (std::size_t i = start; i < end; ++i)\n            output[i] += add;\n    }\n}\n```\n",
  "29_reduce_sum_of_min_of_pairs": "```cpp\n#pragma once\n#include <vector>\n#include <cstdint>\n#include <cstddef>\n#include <algorithm>\n#ifdef _OPENMP\n#include <omp.h>\n#endif\n\n// Return the sum of the minimum value at each index of vectors x and y for all indices.\n// Equivalent to: sum = min(x[0], y[0]) + min(x[1], y[1]) + ... .\ninline int64_t sumOfMinimumElements(const std::vector<int64_t>& x,\n                                    const std::vector<int64_t>& y) {\n    const size_t n = x.size();\n    int64_t sum = 0;\n\n    #pragma omp parallel for reduction(+:sum) if(n > 0)\n    for (size_t i = 0; i < n; ++i) {\n        sum += (x[i] < y[i] ? x[i] : y[i]);   // inline min\n    }\n\n    return sum;\n}\n```\n",
  "25_reduce_xor": "```cpp\n#pragma once\n#include <vector>\n#include <cstddef>\n\nbool reduceLogicalXOR(const std::vector<bool>& x) {\n    const std::size_t n = x.size();\n\n    // Small vectors: avoid threading overhead\n    if (n < 256) {\n        bool r = false;\n        for (std::size_t i = 0; i < n; ++i) r ^= x[i];\n        return r;\n    }\n\n    unsigned char parity = 0;\n    #pragma omp parallel for schedule(static) reduction(^:parity)\n    for (std::size_t i = 0; i < n; ++i) {\n        parity ^= static_cast<unsigned char>(x[i]);\n    }\n    return parity != 0;\n}\n```\n",
  "26_reduce_product_of_inverses": "```cpp\n#pragma once\n#include <vector>\n\ninline double productWithInverses(const std::vector<double>& x) {\n    if (x.empty()) return 1.0;            // matches std::reduce's identity\n\n    double prod = 1.0;\n#pragma omp parallel for reduction(*: prod) schedule(static)\n    for (std::size_t i = 0; i < x.size(); ++i) {\n        prod *= (i & 1 ? 1.0 / x[i] : x[i]);   // odd indices inverted\n    }\n    return prod;\n}\n```",
  "27_reduce_average": "```cpp\n#pragma once\n#include <vector>\n\ninline double average(std::vector<double> const& x) {\n    const std::size_t n = x.size();\n    double sum = 0.0;\n    const double* ptr = x.data();\n    for (std::size_t i = 0; i < n; ++i) {\n        sum += ptr[i];\n    }\n    return sum / static_cast<double>(n);\n}\n```",
  "28_reduce_smallest_odd_number": "```cpp\n#pragma once\n#include <vector>\n#include <limits>\n\nint smallestOdd(std::vector<int> const& x) {\n    constexpr int INF = std::numeric_limits<int>::max();\n    int minOdd = INF;\n    const int* data = x.data();\n    const size_t n = x.size();\n\n    #pragma omp parallel for reduction(min:minOdd)\n    for (size_t i = 0; i < n; ++i) {\n        const int v = data[i];\n        if ((v % 2) == 1 && v < minOdd) {\n            minOdd = v;\n        }\n    }\n    return minOdd;\n}\n```",
  "45_sparse_la_sparse_solve": "```cpp\n#pragma once\n#include <vector>\n#include <cmath>\n#include <algorithm>\n\nstruct COOElement {\n    size_t row, column;\n    double value;\n};\n\n/* Solve the sparse linear system Ax=b for x.\n   A is a sparse NxN matrix in COO format. x and b are dense vectors with N elements.\n   Example:\n   input: A=[{0,0,1}, {0,1,1}, {1,1,-2}] b=[1,4]\n   output: x=[3,-2]\n*/\nvoid solveLinearSystem(std::vector<COOElement> const& A,\n                       std::vector<double> const& b,\n                       std::vector<double> &x,\n                       size_t N)\n{\n    /* Allocate a dense matrix and initialise to zero */\n    std::vector<double> mat(N * N, 0.0);\n\n    /* Build the matrix from the COO representation */\n    for (const auto &e : A) {\n        mat[e.row * N + e.column] = e.value;\n    }\n\n    /* Working copy of the RHS vector */\n    std::vector<double> b_copy = b;\n\n    /* Gaussian elimination with partial pivoting */\n    for (size_t i = 0; i < N; ++i) {\n        /* Pivot selection */\n        size_t maxRow = i;\n        double maxVal = std::fabs(mat[i * N + i]);\n        for (size_t k = i + 1; k < N; ++k) {\n            double val = std::fabs(mat[k * N + i]);\n            if (val > maxVal) {\n                maxVal = val;\n                maxRow = k;\n            }\n        }\n\n        /* Swap rows if necessary (only columns i..N-1 are swapped) */\n        if (maxRow != i) {\n            for (size_t j = i; j < N; ++j) {\n                std::swap(mat[maxRow * N + j], mat[i * N + j]);\n            }\n            std::swap(b_copy[maxRow], b_copy[i]);\n        }\n\n        double *pivotRow = mat.data() + i * N;\n        double pivot = pivotRow[i];\n\n        /* Eliminate rows below the pivot */\n        #pragma omp parallel for schedule(static)\n        for (size_t k = i + 1; k < N; ++k) {\n            double *row_k = mat.data() + k * N;\n            double factor = -row_k[i] / pivot;\n            row_k[i] = 0.0;                         // explicitly set to zero\n            for (size_t j = i + 1; j < N; ++j) {\n                row_k[j] += factor * pivotRow[j];\n            }\n            b_copy[k] += factor * b_copy[i];\n        }\n    }\n\n    /* Back\u2011substitution */\n    x.assign(N, 0.0);\n    for (int i = static_cast<int>(N) - 1; i >= 0; --i) {\n        double sum = b_copy[i];\n        for (int j = i + 1; j < static_cast<int>(N); ++j) {\n            sum -= mat[i * N + j] * x[j];\n        }\n        x[i] = sum / mat[i * N + i];\n    }\n}\n```",
  "13_geometry_closest_pair_2d": "```cpp\n#pragma once\n#include <vector>\n#include <cmath>\n#include <limits>\n#include <cstddef>\n\nstruct Point {\n    double x, y;\n};\n\n/* Auxiliary function (kept for API compatibility). */\ninline double distance(Point const& p1, Point const& p2)\n{\n    const double dx = p2.x - p1.x;\n    const double dy = p2.y - p1.y;\n    return std::sqrt(dx * dx + dy * dy);\n}\n\n/* Optimised closest\u2011pair routine.\n *\n * The algorithm is an O(n\u00b2) brute\u2011force search, but all distance\n * computations are performed using squared values (to avoid the\n * expensive `sqrt()` in the inner loop) and the work is split\n * among the available OpenMP threads using a reduction on the\n * minimum squared distance.  The final answer is the square\u2011root\n * of the minimum found.\n *\n * This preserves the exact semantics of the original implementation\n * while exploiting multi\u2011core hardware and reducing the number of\n * transcendental function calls.\n */\ninline double closestPair(std::vector<Point> const& points)\n{\n    const std::size_t n = points.size();\n    if (n < 2) {\n        return 0.0;\n    }\n\n    /* Working with raw pointers gives the compiler more freedom to\n     * optimise and reduces the overhead of vector bounds checking.\n     */\n    const Point* const pts = points.data();\n\n    double minDistSq = std::numeric_limits<double>::max();\n\n    /* Parallel outer loop: each thread updates its own local copy\n     * of `minDistSq`, with the OpenMP reduction combining the\n     * results at the end.\n     */\n#pragma omp parallel for reduction(min:minDistSq) schedule(static)\n    for (std::size_t i = 0; i + 1 < n; ++i) {\n        const double xi = pts[i].x;\n        const double yi = pts[i].y;\n        for (std::size_t j = i + 1; j < n; ++j) {\n            const double dx = pts[j].x - xi;\n            const double dy = pts[j].y - yi;\n            const double dSq = dx * dx + dy * dy;\n            if (dSq < minDistSq) {\n                minDistSq = dSq;\n            }\n        }\n    }\n\n    return std::sqrt(minDistSq);\n}\n```",
  "46_sparse_la_spmm": "#pragma once\n#include <vector>\n#include <cstddef>\n#include <omp.h>\n\nstruct COOElement {\n    std::size_t row, column;\n    double value;\n};\n\nvoid spmm(const std::vector<COOElement> &A,\n          const std::vector<COOElement> &X,\n          std::vector<double> &Y,\n          std::size_t M,\n          std::size_t K,\n          std::size_t N) {\n    Y.assign(M * N, 0.0);\n\n    // Group A and X elements by their row indices to avoid scanning all X for each A.\n    std::vector<std::vector<std::size_t>> a_rows(M);\n    std::vector<std::vector<std::size_t>> x_rows(K);\n\n    // Pre\u2011allocate capacities to minimise reallocations.\n    std::vector<std::size_t> a_cnt(M, 0);\n    std::vector<std::size_t> x_cnt(K, 0);\n    for (const auto &a : A) {\n        if (a.row < M) ++a_cnt[a.row];\n    }\n    for (const auto &x : X) {\n        if (x.row < K) ++x_cnt[x.row];\n    }\n    for (std::size_t r = 0; r < M; ++r) a_rows[r].reserve(a_cnt[r]);\n    for (std::size_t r = 0; r < K; ++r) x_rows[r].reserve(x_cnt[r]);\n\n    for (std::size_t i = 0; i < A.size(); ++i) {\n        const auto &a = A[i];\n        if (a.row < M) a_rows[a.row].push_back(i);\n    }\n    for (std::size_t i = 0; i < X.size(); ++i) {\n        const auto &x = X[i];\n        if (x.row < K) x_rows[x.row].push_back(i);\n    }\n\n    // Parallel row\u2011by\u2011row multiplication.\n    #pragma omp parallel for schedule(static)\n    for (std::size_t r = 0; r < M; ++r) {\n        double *row_ptr = Y.data() + r * N;  // pointer to the start of row r\n        for (const std::size_t ai : a_rows[r]) {\n            const auto &a = A[ai];\n            std::size_t col = a.column;\n            if (col >= K) continue;  // safety: no matching X rows\n            const std::vector<std::size_t> &x_indices = x_rows[col];\n            double a_val = a.value;\n            for (const std::size_t xi : x_indices) {\n                const auto &x = X[xi];\n                row_ptr[x.column] += a_val * x.value;\n            }\n        }\n    }\n}\n",
  "55_transform_relu": "```cpp\n#pragma once\n#include <vector>\n#include <cstddef>\n#include <omp.h>\n\n#if defined(__AVX__) || defined(__AVX2__)\n#include <immintrin.h>\n\ninline void relu(std::vector<double> &x) {\n    const size_t n = x.size();\n    if (n == 0) return;\n\n    constexpr size_t stride = 4;                     // 4 doubles per AVX register\n    const __m256d zero_vec = _mm256_setzero_pd();   // constant 0.0\n    const size_t full = n - (n % stride);           // process in chunks of 4\n    double *data = x.data();\n\n#pragma omp parallel for schedule(static)\n    for (size_t i = 0; i < full; i += stride) {\n        __m256d vals  = _mm256_loadu_pd(&data[i]);                    // load 4 d\n        __m256d mask  = _mm256_cmp_pd(vals, zero_vec, _CMP_GT_OQ);    // v > 0?\n        __m256d res   = _mm256_blendv_pd(zero_vec, vals, mask);       // select\n        _mm256_storeu_pd(&data[i], res);\n    }\n\n    // Handle trailing elements that don't fit in a 256\u2011bit block\n    for (size_t i = full; i < n; ++i) {\n        double v = data[i];\n        data[i] = (v > 0.0) ? v : 0.0;\n    }\n}\n\n#else   // fallback scalar implementation\n\ninline void relu(std::vector<double> &x) {\n    const size_t n = x.size();\n#pragma omp parallel for schedule(static)\n    for (size_t i = 0; i < n; ++i) {\n        double v = x[i];\n        x[i] = (v > 0.0) ? v : 0.0;\n    }\n}\n#endif\n```",
  "18_graph_count_components": "```cpp\n#include <vector>\n#include <atomic>\n#include <omp.h>\n\nint componentCount(std::vector<int> const& A, size_t N) {\n    // Flag array \u2013 one atomic flag per node\n    std::vector<std::atomic<bool>> visited(N);\n    for (size_t i = 0; i < N; ++i) {\n        visited[i].store(false, std::memory_order_relaxed);\n    }\n\n    int total = 0;\n    constexpr size_t n = N;\n\n    #pragma omp parallel\n    {\n        // Per\u2011thread stack used for an iterative DFS\n        std::vector<size_t> stack;\n        stack.reserve(n);\n\n        #pragma omp for schedule(static) reduction(+:total)\n        for (size_t i = 0; i < n; ++i) {\n            // Claim this node if it hasn't been visited yet\n            if (!visited[i].exchange(true, std::memory_order_relaxed)) {\n                ++total;                 // A new component is found\n\n                stack.clear();\n                stack.push_back(i);\n\n                // Iterative DFS\n                while (!stack.empty()) {\n                    size_t u = stack.back();\n                    stack.pop_back();\n                    size_t row_start = u * n;\n\n                    for (size_t j = 0; j < n; ++j) {\n                        if (A[row_start + j]) {                 // Edge exists\n                            if (!visited[j].exchange(true, std::memory_order_relaxed)) {\n                                stack.push_back(j);\n                            }\n                        }\n                    }\n                }\n            }\n        }\n    }\n\n    return total;\n}\n```",
  "14_geometry_closest_pair_1d": "```cpp\n#pragma once\n#include <vector>\n#include <limits>\n#include <algorithm>\n#include <cmath>\n#ifdef _OPENMP\n#include <omp.h>\n#endif\n\n// Helper function (unchanged from the original)\ndouble distance(double x1, double x2) {\n    return std::abs(x1 - x2);\n}\n\n// Optimised closest pair \u2013 O(n log n) with optional parallel reduction\ndouble closestPair(std::vector<double> const& x) {\n    const size_t n = x.size();\n    if (n < 2) return 0.0;          // As in the original code\n\n    // Copy and sort \u2013 sorting is the dominant cost, so we pay only O(n log n)\n    std::vector<double> sorted(x.begin(), x.end());\n    std::sort(sorted.begin(), sorted.end());\n\n    double minDist = std::numeric_limits<double>::max();\n\n    // Parallel reduction over consecutive sorted elements\n#ifdef _OPENMP\n    #pragma omp parallel for reduction(min:minDist)\n#endif\n    for (size_t i = 1; i < n; ++i) {\n        const double diff = sorted[i] - sorted[i - 1];\n        if (diff < minDist) minDist = diff;\n    }\n\n    return minDist;\n}\n```",
  "19_graph_shortest_path": "```cpp\n#include <vector>\n#include <limits>\n#include <atomic>\n\n/* Return the length of the shortest path from source to dest in the undirected graph defined by\n   the adjacency matrix A (row\u2011major).  A is guaranteed to describe a connected, undirected\n   graph.  If source == dest, 0 is returned.  If the graph is disconnected the function\n   returns std::numeric_limits<int>::max().\n*/\nint shortestPathLength(std::vector<int> const& A, size_t N, int source, int dest) {\n    if (source == dest) return 0;\n    if (N == 0) return std::numeric_limits<int>::max();\n\n    const int* matrix = A.data();\n\n    /* distance array: -1 means unvisited, otherwise the shortest distance from source */\n    std::atomic<int>* dist = new std::atomic<int>[N];\n    for (size_t i = 0; i < N; ++i) {\n        dist[i].store(-1, std::memory_order_relaxed);\n    }\n    dist[source].store(0, std::memory_order_relaxed);\n\n    /* frontier queues for the current BFS level and the next level */\n    int* frontier      = new int[N];\n    int* nextFrontier  = new int[N];\n    frontier[0] = source;\n    int frontierSize = 1;\n\n    std::atomic<int> nextSize(0);   // size of nextFrontier\n    bool found = false;             // true when the destination has been reached\n    int result = std::numeric_limits<int>::max();\n\n    #pragma omp parallel\n    {\n        while (true) {\n            if (found) break;  // exit if destination already found\n\n            int localSize = frontierSize;  // snapshot of current frontier\n\n            /* Process all vertices of the current frontier in parallel */\n            #pragma omp for schedule(static)\n            for (int fi = 0; fi < localSize; ++fi) {\n                int v = frontier[fi];\n                int dist_v = dist[v].load(std::memory_order_relaxed);\n\n                const int* row = matrix + static_cast<size_t>(v) * N;\n\n                for (int i = 0; i < static_cast<int>(N); ++i) {\n                    if (row[i]) {\n                        int expected = -1;\n                        if (dist[i].compare_exchange_strong(expected,\n                                                            dist_v + 1,\n                                                            std::memory_order_relaxed)) {\n                            int idx = nextSize.fetch_add(1,\n                                                          std::memory_order_relaxed);\n                            nextFrontier[idx] = i;\n                        }\n                    }\n                }\n            }\n\n            #pragma omp barrier  // ensure all threads finished the level\n\n            /* One thread decides whether we should continue */\n            #pragma omp single\n            {\n                int distDest = dist[dest].load(std::memory_order_relaxed);\n                if (distDest != -1) {\n                    result = distDest;\n                    found = true;          // destination reached\n                } else if (nextSize.load(std::memory_order_relaxed) == 0) {\n                    found = true;          // frontier exhausted, no path\n                } else {\n                    /* Prepare next level */\n                    int* tmp = frontier;\n                    frontier = nextFrontier;\n                    nextFrontier = tmp;\n                    frontierSize = nextSize.load(std::memory_order_relaxed);\n                    nextSize.store(0, std::memory_order_relaxed);\n                }\n            }\n\n            #pragma omp barrier  // make sure all threads see the updated state\n            if (found) break;    // exit if destination reached or no more vertices\n        }\n    }\n\n    delete[] frontier;\n    delete[] nextFrontier;\n    delete[] dist;\n    return result;\n}\n```",
  "52_stencil_1d_jacobi_3-point_stencil": "```cpp\n#pragma once\n#include <vector>\n#include <omp.h>\n\n/* Compute one iteration of a 3-point 1D Jacobi stencil on `input`. Store the results in `output`.\n   Each element of `input` will be averaged with its two neighbors and stored in the corresponding element of `output`.\n   i.e. output[i] = (input[i-1]+input[i]+input[i+1])/3\n   Replace 0 when reading past the boundaries of `input`.\n   Example:\n   input: [9, -6, -1, 2, 3]\n   output: [1, 2/3, -5/3, 4/3, 5/3]\n*/\nvoid jacobi1D(std::vector<double> const& input, std::vector<double> &output) {\n    const std::size_t N = input.size();\n    if (N == 0) return;\n\n    const double inv3 = 1.0 / 3.0;\n    const double *in = input.data();\n    double       *out = output.data();\n\n    // Handle first element (boundary)\n    out[0] = (in[0] + (N > 1 ? in[1] : 0.0)) * inv3;\n\n    // Process interior elements in parallel\n    if (N > 2) {\n        #pragma omp parallel for\n        for (std::size_t i = 1; i + 1 < N; ++i) {\n            out[i] = (in[i - 1] + in[i] + in[i + 1]) * inv3;\n        }\n    }\n\n    // Handle last element (boundary)\n    if (N > 1) {\n        out[N - 1] = (in[N - 1] + in[N - 2]) * inv3;\n    }\n}\n```",
  "47_sparse_la_spmv": "```cpp\n#pragma once\n#include <vector>\n#include <cstddef>\n#ifdef _OPENMP\n#include <omp.h>\n#endif\n\nstruct COOElement {\n   std::size_t row, column;\n   double value;\n};\n\n/*\n * Compute y = alpha * A * x + beta * y\n *   A is given in COO format\n *   y is the result vector (size M)\n *   x is the input vector (size N)\n *\n * The routine preserves the exact numerical behaviour of the original\n * implementation while exploiting parallelism and improving memory\n * locality by internally converting the matrix to CSR format.\n */\ninline void spmv(double alpha,\n                 const std::vector<COOElement>& A,\n                 const std::vector<double>& x,\n                 double beta,\n                 std::vector<double>& y,\n                 std::size_t M, std::size_t N)\n{\n   /* 1. Scale the existing y vector by beta \u2013 each element is independent. */\n   #pragma omp parallel for schedule(static)\n   for (std::size_t i = 0; i < y.size(); ++i) {\n      y[i] *= beta;\n   }\n\n   /* 2. Build CSR representation of the sparse matrix A. */\n   std::vector<std::size_t> row_ptr(M + 1, 0);   // count + CSR pointers\n\n   /* First pass: count elements per row (ignore out\u2011of\u2011bounds). */\n   for (const auto& e : A) {\n      std::size_t r = e.row;\n      std::size_t c = e.column;\n      if (r < M && c < N) {\n         ++row_ptr[r + 1];\n      }\n   }\n\n   /* Prefix sum \u2192 starting index of each row. */\n   for (std::size_t i = 0; i < M; ++i) {\n      row_ptr[i + 1] += row_ptr[i];\n   }\n   const std::size_t nnz = row_ptr[M];\n\n   /* Allocate storage for column indices and values. */\n   std::vector<std::size_t> col_idx(nnz);\n   std::vector<double>      vals(nnz);\n   std::vector<std::size_t> row_off(M, 0);     // next free position per row\n\n   /* Second pass: fill CSR arrays. */\n   for (const auto& e : A) {\n      std::size_t r = e.row;\n      std::size_t c = e.column;\n      if (r < M && c < N) {\n         std::size_t idx = row_ptr[r] + row_off[r];\n         col_idx[idx] = c;\n         vals[idx]    = e.value;\n         ++row_off[r];\n      }\n   }\n\n   /* 3. Perform the sparse matrix\u2011vector multiplication. */\n   const double*   x_data  = x.data();\n   const std::size_t* col_data = col_idx.data();\n   const double*   val_data = vals.data();\n   double*         y_data  = y.data();\n\n   #pragma omp parallel for schedule(static)\n   for (std::size_t r = 0; r < M; ++r) {\n      double sum = y_data[r];               // y already contains beta * original value\n      std::size_t start = row_ptr[r];\n      std::size_t end   = row_ptr[r + 1];\n      for (std::size_t idx = start; idx < end; ++idx) {\n         sum += alpha * val_data[idx] * x_data[col_data[idx]];\n      }\n      y_data[r] = sum;\n   }\n}\n```",
  "16_graph_largest_component": "```cpp\n#pragma once\n#include <vector>\n#include <atomic>\n#include <cstddef>\n\nint largestComponent(const std::vector<int>& A, size_t N)\n{\n    /* Mark all vertices as unvisited using atomic_flag for thread safety */\n    std::vector<std::atomic_flag> visited(N);\n    for (auto& f : visited) f.clear();          // set all flags to false\n\n    int maxCount = 0;                            // global maximum component size\n\n    #pragma omp parallel\n    {\n        /* Each thread keeps its own DFS stack */\n        std::vector<int> stack;\n        stack.reserve(N);\n\n        #pragma omp for schedule(static) reduction(max:maxCount)\n        for (int i = 0; i < static_cast<int>(N); ++i) {\n            /* Claim the vertex as the root of a new component */\n            if (!visited[i].test_and_set()) {\n                int componentSize = 0;\n                stack.clear();\n                stack.push_back(i);\n\n                /* Iterative DFS using the stack */\n                while (!stack.empty()) {\n                    size_t cur = stack.back();\n                    stack.pop_back();\n                    ++componentSize;\n\n                    const int* row = A.data() + cur * N;\n                    for (size_t j = 0; j < N; ++j) {\n                        /* Edge exists only when the entry equals 1 */\n                        if (row[j] == 1 && !visited[j].test_and_set()) {\n                            stack.push_back(static_cast<int>(j));\n                        }\n                    }\n                }\n                if (componentSize > maxCount) {\n                    maxCount = componentSize;\n                }\n            }\n        }\n    }\n\n    return maxCount;\n}\n```\n",
  "12_geometry_smallest_triangle": "```cpp\n#pragma once\n#include <vector>\n#include <limits>\n#include <cmath>\n\nstruct Point {\n    double x, y;\n};\n\n/* Return the area of the smallest triangle that can be formed by any 3 points. */\ninline double smallestArea(const std::vector<Point>& points) {\n    const size_t n = points.size();\n    if (n < 3) {\n        return 0.0;\n    }\n\n    double minArea = std::numeric_limits<double>::max();\n    const int n_int = static_cast<int>(n);\n\n#pragma omp parallel for schedule(static) reduction(min:minArea)\n    for (int i = 0; i < n_int - 2; ++i) {\n        const Point& A = points[i];\n        for (int j = i + 1; j < n_int - 1; ++j) {\n            const Point& B = points[j];\n            for (int k = j + 1; k < n_int; ++k) {\n                const Point& C = points[k];\n                double area = 0.5 * std::abs(\n                    A.x * (B.y - C.y) +\n                    B.x * (C.y - A.y) +\n                    C.x * (A.y - B.y));\n                if (area < minArea) {\n                    minArea = area;\n                }\n            }\n        }\n    }\n    return minArea;\n}\n```\n",
  "11_geometry_convex_hull_perimeter": "```cpp\n#pragma once\n#include <vector>\n#include <algorithm>\n#include <cmath>\n#include <omp.h>\n\nstruct Point {\n    double x, y;\n};\n\ninline double cross(const Point& a, const Point& b, const Point& c) noexcept {\n    return (b.x - a.x) * (c.y - a.y) - (b.y - a.y) * (c.x - a.x);\n}\n\ninline double dist(const Point& a, const Point& b) noexcept {\n    const double dx = a.x - b.x;\n    const double dy = a.y - b.y;\n    return std::sqrt(dx * dx + dy * dy);\n}\n\ndouble convexHullPerimeter(std::vector<Point> const& points) {\n    const size_t n = points.size();\n    if (n < 3) return 0.0;\n\n    // Copy and sort the points\n    std::vector<Point> p = points;\n    std::sort(p.begin(), p.end(),\n              [](const Point& a, const Point& b) {\n                  return (a.x < b.x) || (a.x == b.x && a.y < b.y);\n              });\n\n    // Monotone chain algorithm\n    std::vector<Point> hull(2 * n);\n    size_t k = 0;\n\n    // Lower hull\n    for (size_t i = 0; i < n; ++i) {\n        while (k >= 2 && cross(hull[k - 2], hull[k - 1], p[i]) <= 0) --k;\n        hull[k++] = p[i];\n    }\n\n    // Upper hull\n    size_t t = k + 1;\n    for (size_t i = n - 1; i > 0; --i) {\n        while (k >= t && cross(hull[k - 2], hull[k - 1], p[i - 1]) <= 0) --k;\n        hull[k++] = p[i - 1];\n    }\n\n    hull.resize(k - 1);                // Remove duplicated first point\n\n    const size_t m = hull.size();\n    double perimeter = 0.0;\n\n    // Compute perimeter in parallel\n    #pragma omp parallel for reduction(+:perimeter) schedule(static)\n    for (size_t i = 0; i < m; ++i) {\n        size_t j = (i + 1) % m;\n        perimeter += dist(hull[i], hull[j]);\n    }\n\n    return perimeter;\n}\n```",
  "48_sparse_la_sparse_axpy": "```cpp\n#pragma once\n#include <vector>\n#include <algorithm>\n#ifdef _OPENMP\n#include <omp.h>\n#endif\n\nstruct Element {\n    size_t index;\n    double value;\n};\n\n/* Serial implementation \u2013 identical to the original logic, but\n   written in a slightly optimized form. */\nstatic inline void sparseAxpySerial(double alpha,\n                                    std::vector<Element> const& x,\n                                    std::vector<Element> const& y,\n                                    std::vector<double> &z) {\n    const size_t xn = x.size(), yn = y.size();\n    size_t xi = 0, yi = 0;\n    const Element* xptr = x.data();\n    const Element* yptr = y.data();\n    double* zptr = z.data();\n\n    while (xi < xn && yi < yn) {\n        const Element& xe = xptr[xi];\n        const Element& ye = yptr[yi];\n        if (xe.index < ye.index) {\n            zptr[xe.index] += alpha * xe.value;\n            ++xi;\n        } else if (xe.index > ye.index) {\n            zptr[ye.index] += ye.value;\n            ++yi;\n        } else {                // same index\n            zptr[xe.index] += alpha * xe.value + ye.value;\n            ++xi;\n            ++yi;\n        }\n    }\n\n    while (xi < xn) {\n        const Element& xe = xptr[xi];\n        zptr[xe.index] += alpha * xe.value;\n        ++xi;\n    }\n\n    while (yi < yn) {\n        const Element& ye = yptr[yi];\n        zptr[ye.index] += ye.value;\n        ++yi;\n    }\n}\n\n#ifdef _OPENMP\n/* Parallel implementation \u2013 the index range 0\u2026z.size() is split\n   among the OpenMP threads.  Each thread works on a disjoint\n   slice, so no atomic operations are required. */\nstatic inline void sparseAxpyParallel(double alpha,\n                                      std::vector<Element> const& x,\n                                      std::vector<Element> const& y,\n                                      std::vector<double> &z) {\n    const size_t zsize = z.size();\n    if (zsize == 0) return;\n\n    #pragma omp parallel\n    {\n        const int  tid       = omp_get_thread_num();\n        const int  nthreads  = omp_get_num_threads();\n        const size_t chunk   = (zsize + nthreads - 1) / nthreads;\n        const size_t start   = static_cast<size_t>(tid) * chunk;\n        const size_t end     = std::min(start + chunk, zsize);\n        if (start >= end) return;\n\n        const Element *xptr = x.data();\n        const Element *yptr = y.data();\n        double       *zptr = z.data();\n\n        /* Find sub\u2011ranges of x and y that fall inside [start, end). */\n        const size_t xi_start = std::lower_bound(xptr, xptr + x.size(),\n                                    start,\n                                    [](const Element &e, size_t v){ return e.index < v; })\n                                - xptr;\n        const size_t xi_end   = std::lower_bound(xptr, xptr + x.size(),\n                                    end,\n                                    [](const Element &e, size_t v){ return e.index < v; })\n                                - xptr;\n\n        const size_t yi_start = std::lower_bound(yptr, yptr + y.size(),\n                                    start,\n                                    [](const Element &e, size_t v){ return e.index < v; })\n                                - yptr;\n        const size_t yi_end   = std::lower_bound(yptr, yptr + y.size(),\n                                    end,\n                                    [](const Element &e, size_t v){ return e.index < v; })\n                                - yptr;\n\n        size_t xi = xi_start, yi = yi_start;\n        while (xi < xi_end && yi < yi_end) {\n            const Element& xe = xptr[xi];\n            const Element& ye = yptr[yi];\n            if (xe.index < ye.index) {\n                zptr[xe.index] += alpha * xe.value;\n                ++xi;\n            } else if (xe.index > ye.index) {\n                zptr[ye.index] += ye.value;\n                ++yi;\n            } else {                // same index\n                zptr[xe.index] += alpha * xe.value + ye.value;\n                ++xi;\n                ++yi;\n            }\n        }\n\n        while (xi < xi_end) {\n            const Element& xe = xptr[xi];\n            zptr[xe.index] += alpha * xe.value;\n            ++xi;\n        }\n\n        while (yi < yi_end) {\n            const Element& ye = yptr[yi];\n            zptr[ye.index] += ye.value;\n            ++yi;\n        }\n    }\n}\n#endif\n\n/* Public entry point \u2013 chooses serial or parallel implementation. */\nvoid sparseAxpy(double alpha,\n                std::vector<Element> const& x,\n                std::vector<Element> const& y,\n                std::vector<double> &z) {\n    const size_t total = x.size() + y.size();\n    /* Avoid the cost of creating a team for tiny inputs. */\n    const size_t PARALLEL_THRESHOLD = 1 << 14;   // 16384 elements\n\n#ifdef _OPENMP\n    if (total > PARALLEL_THRESHOLD) {\n        sparseAxpyParallel(alpha, x, y, z);\n        return;\n    }\n#endif\n    sparseAxpySerial(alpha, x, y, z);\n}\n```",
  "03_dense_la_axpy": "```cpp\n#pragma once\n#include <vector>\n#include <cstddef>\n#include <omp.h>\n\n/* Compute z = alpha*x + y where x and y are vectors.\n   The result is stored in z.  The function is thread\u2011safe\n   and uses OpenMP parallelisation for speed.\n   Example:\n   input:  x=[1, -5, 2, 9]  y=[0, 4, 1, -1]  alpha=2\n   output: z=[2, -6, 5, 17]\n*/\ninline void axpy(double alpha,\n                 const std::vector<double>& x,\n                 const std::vector<double>& y,\n                 std::vector<double>& z)\n{\n    const std::size_t N = x.size();\n    if (N == 0) return;\n\n    const double* const px = x.data();\n    const double* const py = y.data();\n    double* const pz = z.data();\n\n    /* Parallelise the loop across the available threads.\n       The iterations are independent, so ordering is irrelevant. */\n    #pragma omp parallel for schedule(static)\n    for (std::size_t i = 0; i < N; ++i) {\n        pz[i] = alpha * px[i] + py[i];\n    }\n}\n```",
  "06_fft_dft": "```cpp\n#pragma once\n#include <vector>\n#include <complex>\n#include <cmath>\n#include <cstddef>\n#include <omp.h>\n\n#ifndef M_PI\n#define M_PI 3.14159265358979323846\n#endif\n\ninline void dft(const std::vector<double>& x, std::vector<std::complex<double>>& output) {\n    const std::size_t N = x.size();\n    output.resize(N);\n    if (N == 0) {\n        return;\n    }\n\n    const double two_pi = 2.0 * M_PI;\n    const double invN   = 1.0 / static_cast<double>(N);\n\n    const double* x_ptr = x.data();\n    std::complex<double>* out_ptr = output.data();\n\n    #pragma omp parallel for schedule(static)\n    for (std::size_t k = 0; k < N; ++k) {\n        double sum_real = 0.0;\n        double sum_imag = 0.0;\n        const double k_d = static_cast<double>(k);\n\n        for (std::size_t n = 0; n < N; ++n) {\n            double angle = two_pi * static_cast<double>(n) * k_d * invN;\n            double c = std::cos(angle);\n            double s = std::sin(angle);\n            sum_real += x_ptr[n] * c;\n            sum_imag -= x_ptr[n] * s;\n        }\n\n        out_ptr[k] = std::complex<double>(sum_real, sum_imag);\n    }\n}\n```",
  "49_sparse_la_sparse_lu_decomp": "```cpp\n#pragma once\n#include <vector>\n\nstruct COOElement {\n    size_t row;\n    size_t column;\n    double value;\n};\n\n/*\n * In-place LU factorisation of a sparse matrix given in COO format.\n * L and U are returned as N\u00d7N dense matrices in row\u2011major order.\n * This implementation retains the exact behaviour of the original\n * algorithm while exploiting better cache utilisation and\n * optional OpenMP parallelisation for the independent column updates.\n */\ninline void luFactorize(std::vector<COOElement> const& A,\n                        std::vector<double>& L,\n                        std::vector<double>& U,\n                        size_t N)\n{\n    /* ------------------------------------------------------------------\n     *  1. Build a dense copy of the input matrix.\n     * ------------------------------------------------------------------*/\n    std::vector<double> a(N * N, 0.0);\n    for (const auto& e : A) {\n        a[e.row * N + e.column] = e.value;\n    }\n\n    /* ------------------------------------------------------------------\n     *  2. Prepare output containers.\n     * ------------------------------------------------------------------*/\n    L.resize(N * N, 0.0);\n    U.resize(N * N, 0.0);\n\n    std::vector<double> tmp(N);          // temporary numerator array\n    double* Lptr  = L.data();\n    double* Uptr  = U.data();\n    const double* Aptr = a.data();\n\n    /* ------------------------------------------------------------------\n     *  3. Main Doolittle factorisation loop.\n     * ------------------------------------------------------------------*/\n    for (size_t i = 0; i < N; ++i) {\n        /* ---- compute the sub\u2011diagonal part of L (L[i][0..i-1]) ---- */\n        if (i > 0) {\n            /* a) Copy the relevant entries of the original row */\n            for (size_t j = 0; j < i; ++j) {\n                tmp[j] = Aptr[i * N + j];\n            }\n\n            /* b) Eliminate using previously computed elements of L[i] */\n            for (size_t k = 0; k < i; ++k) {\n                double lk = tmp[k] / Uptr[k * N + k];   // final L[i][k]\n                tmp[k] = lk;\n                double* Urowk = Uptr + k * N;\n                for (size_t j = k + 1; j < i; ++j) {\n                    tmp[j] -= lk * Urowk[j];\n                }\n            }\n\n            /* c) Store the completed sub\u2011diagonal entries of the row */\n            double* Lrowi = Lptr + i * N;\n            for (size_t j = 0; j < i; ++j) {\n                Lrowi[j] = tmp[j];\n            }\n        }\n\n        /* ---- compute the upper part of U (U[i][i..N-1]) ---- */\n#pragma omp parallel for schedule(static)\n        for (size_t j = i; j < N; ++j) {\n            double sum = Aptr[i * N + j];\n            for (size_t k = 0; k < i; ++k) {\n                sum -= (Lptr[i * N + k]) * (Uptr + k * N)[j];\n            }\n            Uptr[i * N + j] = sum;\n        }\n\n        /* ---- set the unit diagonal of L ---- */\n        Lptr[i * N + i] = 1.0;\n    }\n}\n```",
  "31_scan_scan_with_min_function": "```cpp\n#pragma once\n#include <vector>\n#include <algorithm>\n#include <limits>\n#include <omp.h>\n\nvoid partialMinimums(std::vector<float> &x)\n{\n    const size_t N = x.size();\n    if (N == 0)\n        return;\n\n    const int num_threads = omp_get_max_threads();\n    const size_t chunk  = (N + static_cast<size_t>(num_threads) - 1) / num_threads;\n    const size_t nblocks = (N + chunk - 1) / chunk;          // number of non\u2011empty blocks\n\n    std::vector<float> block_min(nblocks);\n\n    // Local prefix minima per block\n    #pragma omp parallel for schedule(static)\n    for (size_t block = 0; block < nblocks; ++block)\n    {\n        const size_t start = block * chunk;\n        const size_t end   = std::min(start + chunk, N);\n\n        float cur_min = x[start];\n        x[start] = cur_min;\n        for (size_t i = start + 1; i < end; ++i)\n        {\n            const float val = x[i];\n            if (val < cur_min)\n                cur_min = val;\n            x[i] = cur_min;\n        }\n        block_min[block] = cur_min;\n    }\n\n    // Prefix of block minima\n    const float INF = std::numeric_limits<float>::max();\n    std::vector<float> block_prefix(nblocks);\n    float prefix = INF;\n    for (size_t block = 0; block < nblocks; ++block)\n    {\n        block_prefix[block] = prefix;\n        prefix = std::min(prefix, block_min[block]);\n    }\n\n    // Apply block prefixes to obtain global prefix minima\n    #pragma omp parallel for schedule(static)\n    for (size_t block = 0; block < nblocks; ++block)\n    {\n        const float inc  = block_prefix[block];\n        const size_t start = block * chunk;\n        const size_t end   = std::min(start + chunk, N);\n        for (size_t i = start; i < end; ++i)\n            x[i] = std::min(inc, x[i]);\n    }\n}\n```",
  "43_sort_sort_an_array_of_structs_by_key": "```cpp\n#pragma once\n#include <vector>\n#include <algorithm>\n#include <parallel/algorithm>\n\n// Result structure\nstruct Result\n{\n    int startTime, duration;\n    float value;\n};\n\n// Comparator used by every sort\ninline bool compareStartTime(const Result& a, const Result& b)\n{\n    return a.startTime < b.startTime;\n}\n\n// Sort the vector by startTime.  Small vectors use the standard\n// sequential std::sort, larger ones use the OpenMP\u2011parallel version\n// provided by libstdc++.\ninline void sortByStartTime(std::vector<Result>& results)\n{\n    const size_t n = results.size();\n    if (n < 2048)\n    {\n        std::sort(results.begin(), results.end(), compareStartTime);\n    }\n    else\n    {\n        __gnu_parallel::sort(results.begin(), results.end(), compareStartTime);\n    }\n}\n```",
  "00_dense_la_lu_decomp": "```cpp\n#pragma once\n#include <vector>\n#include <cstddef>\n#ifdef _OPENMP\n#include <omp.h>\n#endif\n\n// In\u2011place LU factorization without pivoting.\n// A is a row\u2011major N\u00d7N matrix stored in a 1\u2011D vector.\n// After the call:  U is stored in the upper triangle (including diagonal),\n//                 L is stored in the lower triangle with unit diagonal.\nvoid luFactorize(std::vector<double> &A, size_t N) {\n    double* data = A.data();\n    for (size_t k = 0; k < N; ++k) {\n        double* pk = data + k * N;            // pointer to the k\u2011th row\n        double pivot = pk[k];                  // pivot element\n\n        // Parallelise work on rows below the pivot\n#pragma omp parallel for schedule(static)\n        for (size_t i = k + 1; i < N; ++i) {\n            double* pi = data + i * N;          // pointer to the i\u2011th row\n            double factor = pi[k] / pivot;      // L(i,k)\n            pi[k] = factor;                     // store L(i,k)\n\n            double* pj = pi + k + 1;             // pointer to A[i][k+1]\n            double* pkj = pk + k + 1;           // pointer to A[k][k+1]\n            for (size_t j = k + 1; j < N; ++j, ++pj, ++pkj) {\n                *pj -= factor * (*pkj);          // U(k,j) is subtracted\n            }\n        }\n    }\n}\n```",
  "02_dense_la_gemm": "```cpp\n#pragma once\n#include <vector>\n#include <cstddef>\n\n/* Multiply the matrix A by the matrix B. Store the results in the matrix C.\n   A is an MxK matrix, B is a KxN matrix, and C is a MxN matrix. The matrices are stored in row\u2011major.\n   Example:\n   input: A=[[1, -1, 2], [0, -2, 1]] B=[[4, 1], [-1, 0], [2, 2]]\n   output: C=[[9, 5], [4, 2]]\n*/\nvoid gemm(std::vector<double> const& A,\n          std::vector<double> const& B,\n          std::vector<double> &C,\n          size_t M,\n          size_t K,\n          size_t N)\n{\n    const double* a = A.data();\n    const double* b = B.data();\n    double* c = C.data();\n\n    #pragma omp parallel for schedule(static)\n    for (size_t i = 0; i < M; ++i) {\n        const double* a_row = a + i * K;\n        double* c_row = c + i * N;\n        for (size_t k = 0; k < K; ++k) {\n            double aik = a_row[k];\n            const double* b_row = b + k * N;\n            #pragma omp simd\n            for (size_t j = 0; j < N; ++j) {\n                c_row[j] += aik * b_row[j];\n            }\n        }\n    }\n}\n```\n",
  "40_sort_sort_an_array_of_complex_numbers_by_magnitude": "```cpp\n#pragma once\n#include <complex>\n#include <vector>\n#include <parallel/algorithm>   // GNU parallel algorithm header\n\nvoid sortComplexByMagnitude(std::vector<std::complex<double>>& x) {\n    // Use a parallel sort (introsort) and compare using squared magnitude to avoid the expensive sqrt.\n    __gnu_parallel::sort(x.begin(), x.end(),\n        [](const std::complex<double>& a, const std::complex<double>& b) {\n            return std::norm(a) < std::norm(b);   // std::norm = |a|^2\n        });\n}\n```\n",
  "09_fft_fft_out_of_place": "#pragma once\n#include <vector>\n#include <complex>\n#include <cmath>\n#ifndef M_PI\n#define M_PI 3.1415926535897932384626433832795028841971693993751\n#endif\n\n// Bit reverse for 32\u2011bit indices (m \u2264 32)\nstatic inline unsigned int bit_reverse32(unsigned int x, unsigned int m)\n{\n    x = ((x & 0x55555555u) << 1) | ((x & 0xAAAAAAAAu) >> 1);\n    x = ((x & 0x33333333u) << 2) | ((x & 0xCCCCCCCCu) >> 2);\n    x = ((x & 0x0F0F0F0Fu) << 4) | ((x & 0xF0F0F0F0u) >> 4);\n    x = ((x & 0x00FF00FFu) << 8) | ((x & 0xFF00FF00u) >> 8);\n    x = ((x & 0x0000FFFFu) << 16) | ((x & 0xFFFF0000u) >> 16);\n    return x >> (32 - m);\n}\n\ninline void fft(std::vector<std::complex<double>> const& x,\n                std::vector<std::complex<double>> &output)\n{\n    size_t N = x.size();\n    output = x;              // copy input\n    if (N <= 1) return;      // nothing to do\n\n    // ---- bit\u2011reversal permutation ----\n    unsigned int logN = 0;\n    while ((1ULL << logN) < N) ++logN;\n    for (size_t i = 0; i < N; ++i)\n    {\n        size_t j = bit_reverse32((unsigned int)i, logN);\n        if (j > i) std::swap(output[i], output[j]);\n    }\n\n    // ---- pre\u2011compute twiddle factors W_N^k ----\n    std::vector<std::complex<double>> w(N / 2);\n    const std::complex<double> phi = std::exp(std::complex<double>(0.0, -2.0 * M_PI / N));\n    w[0] = std::complex<double>(1.0, 0.0);\n    for (size_t i = 1; i < w.size(); ++i)\n        w[i] = w[i - 1] * phi;\n\n    // ---- radix\u20112 decimation\u2011in\u2011time FFT ----\n    #pragma omp parallel\n    {\n        for (size_t len = 2; len <= N; len <<= 1)\n        {\n            const size_t half = len >> 1;\n            const size_t step = N / len;\n            #pragma omp for schedule(static)\n            for (size_t i = 0; i < N; i += len)\n            {\n                std::complex<double>* a = output.data() + i;\n                std::complex<double>* b = a + half;\n#pragma omp simd\n                for (size_t j = 0; j < half; ++j, ++a, ++b)\n                {\n                    std::complex<double> u = *a;\n                    std::complex<double> v = (*b) * w[step * j];\n                    *a = u + v;\n                    *b = u - v;\n                }\n            }\n        }\n    }\n}\n",
  "39_search_xor_contains": "```cpp\n#pragma once\n#include <vector>\n\ninline bool xorContains(const std::vector<int> &x,\n                        const std::vector<int> &y,\n                        int val)\n{\n    // Search `x` for `val`. The loop is intentionally written in\n    // a form that the compiler can auto\u2011vectorise and that has\n    // very low overhead (no function call, no iterator objects).\n    bool foundX = false;\n    const int *px = x.data();\n    const int *px_end = px + x.size();\n    for (; px != px_end; ++px) {\n        if (*px == val) {\n            foundX = true;\n            break;\n        }\n    }\n\n    // Search `y` in the same efficient manner.\n    bool foundY = false;\n    const int *py = y.data();\n    const int *py_end = py + y.size();\n    for (; py != py_end; ++py) {\n        if (*py == val) {\n            foundY = true;\n            break;\n        }\n    }\n\n    // Return true iff `val` is in exactly one of the vectors.\n    return foundX ^ foundY;\n}\n```",
  "53_stencil_2d_jacobi_5-point_stencil": "```cpp\n#pragma once\n#include <vector>\n#include <cstddef>\n#ifdef _OPENMP\n#include <omp.h>\n#endif\n\ninline void jacobi2D(const std::vector<double> &input,\n                     std::vector<double>       &output,\n                     size_t                     N)\n{\n    if (N == 0) return;                 // nothing to do\n\n    const double *in  = input.data();\n    double       *out = output.data();\n    const double inv5 = 1.0 / 5.0;        // 1/5\n\n    /* --- single-cell corner ------------------------------------------------ */\n    if (N == 1) {                       // all neighbours are zero\n        out[0] = in[0] * inv5;\n        return;\n    }\n\n    const size_t last = N - 1;\n\n    /* --- top row (i = 0) --------------------------------------------------- */\n    const double *row0      = in;          // i = 0\n    const double *row1      = in + N;      // i = 1\n    double       *out_row0  = out;         // output row 0\n    for (size_t j = 0; j < N; ++j) {\n        double sum = row1[j];                 // down neighbour\n        if (j > 0)    sum += row0[j - 1];    // left neighbour\n        if (j + 1 < N) sum += row0[j + 1];   // right neighbour\n        sum += row0[j];                      // centre\n        out_row0[j] = sum * inv5;\n    }\n\n    /* --- interior rows (1 <= i <= last-1) --------------------------------- */\n    if (N > 2) {\n#pragma omp parallel for schedule(static)\n        for (size_t i = 1; i + 1 < N; ++i) {\n            const double *row_up = in + (i - 1) * N;\n            const double *row_i  = in +   i     * N;\n            const double *row_dn = in + (i + 1) * N;\n            double       *out_row = out + i * N;\n\n            /* column 0 (j = 0) */\n            double sum0 = row_up[0];\n            sum0 += row_dn[0];\n            sum0 += row_i[1];    // right neighbour\n            sum0 += row_i[0];    // centre\n            out_row[0] = sum0 * inv5;\n\n            /* columns 1 .. last-1 (interior columns) */\n            for (size_t j = 1; j < last; ++j) {\n                double s = row_up[j];\n                s += row_dn[j];\n                s += row_i[j - 1];  // left\n                s += row_i[j + 1];  // right\n                s += row_i[j];      // centre\n                out_row[j] = s * inv5;\n            }\n\n            /* column last (j = last) */\n            double sum_last = row_up[last];\n            sum_last += row_dn[last];\n            sum_last += row_i[last - 1];   // left\n            sum_last += row_i[last];       // centre\n            out_row[last] = sum_last * inv5;\n        }\n    }\n\n    /* --- bottom row (i = last) --------------------------------------------- */\n    const double *rowN   = in  + last * N;      // i = last\n    const double *rowN1  = in  + (last - 1) * N; // i = last-1\n    double       *out_rowN = out + last * N;\n    for (size_t j = 0; j < N; ++j) {\n        double sum = rowN1[j];                   // up neighbour\n        if (j > 0)    sum += rowN[j - 1];        // left neighbour\n        if (j + 1 < N) sum += rowN[j + 1];       // right neighbour\n        sum += rowN[j];                          // centre\n        out_rowN[j] = sum * inv5;\n    }\n}\n```",
  "42_sort_sorted_ranks": "```cpp\n#pragma once\n#include <algorithm>\n#include <numeric>\n#include <vector>\n\n#if __cplusplus >= 201703L\n#include <execution>\n#endif\n\nvoid ranks(const std::vector<float>& x, std::vector<size_t>& ranks_) {\n    const size_t n = x.size();\n    std::vector<size_t> indices(n);\n\n    /* Fill the indices array in parallel */\n    #pragma omp parallel for schedule(static)\n    for (size_t i = 0; i < n; ++i) {\n        indices[i] = i;\n    }\n\n    /* Sort the indices according to the corresponding values of x.\n       Use the parallel execution policy when available. */\n#if __cplusplus >= 201703L\n    std::sort(std::execution::par, indices.begin(), indices.end(),\n              [&](size_t i1, size_t i2) { return x[i1] < x[i2]; });\n#else\n    std::sort(indices.begin(), indices.end(),\n              [&](size_t i1, size_t i2) { return x[i1] < x[i2]; });\n#endif\n\n    /* Write the ranks back to the output vector in parallel */\n    #pragma omp parallel for schedule(static)\n    for (size_t i = 0; i < n; ++i) {\n        ranks_[indices[i]] = i;\n    }\n}\n```",
  "44_sort_sort_non-zero_elements": "```cpp\n#pragma once\n#include <algorithm>\n#include <vector>\n#include <omp.h>\n#include <parallel/algorithm>   // provides __gnu_parallel::sort for OpenMP parallelism\n\nvoid sortIgnoreZero(std::vector<int> &x) {\n    const size_t n = x.size();\n    if (n == 0)\n        return;\n\n    const int num_threads = omp_get_max_threads();\n    std::vector<std::vector<int>> local_vecs(num_threads);\n\n    /* 1. Collect non\u2011zero elements in per\u2011thread buffers (parallel) */\n    #pragma omp parallel for schedule(static)\n    for (size_t i = 0; i < n; ++i) {\n        if (x[i] != 0) {\n            int tid = omp_get_thread_num();\n            local_vecs[tid].push_back(x[i]);\n        }\n    }\n\n    /* 2. Compute total number of non\u2011zero elements */\n    size_t total_nonzeros = 0;\n    for (int t = 0; t < num_threads; ++t)\n        total_nonzeros += local_vecs[t].size();\n\n    /* 3. Flatten per\u2011thread buffers into a single vector */\n    std::vector<int> nonZeroElements;\n    nonZeroElements.resize(total_nonzeros);\n    size_t offset = 0;\n    for (int t = 0; t < num_threads; ++t) {\n        const std::vector<int> &buf = local_vecs[t];\n        std::copy(buf.begin(), buf.end(),\n                  nonZeroElements.begin() + offset);\n        offset += buf.size();\n    }\n\n    /* 4. Sort the non\u2011zero values (parallel sort) */\n    __gnu_parallel::sort(nonZeroElements.begin(), nonZeroElements.end());\n\n    /* 5. Write sorted non\u2011zero values back to the original vector */\n    size_t sortedIndex = 0;\n    for (size_t i = 0; i < n; ++i) {\n        if (x[i] != 0) {\n            x[i] = nonZeroElements[sortedIndex++];\n        }\n    }\n}\n```",
  "01_dense_la_solve": "#pragma once\n#include <vector>\n#include <cstddef>\n#if defined(_OPENMP)\n#include <omp.h>\n#endif\n\nvoid solveLinearSystem(const std::vector<double>& A, const std::vector<double>& b,\n                       std::vector<double>& x, size_t N)\n{\n    // Create local copies to avoid modifying the originals\n    std::vector<double> A_copy = A;\n    std::vector<double> b_copy = b;\n    double* a      = A_copy.data();\n    double* bvec   = b_copy.data();\n    double* xvec   = x.data();\n\n    // Gaussian elimination (no pivoting)\n    for (size_t i = 0; i < N - 1; ++i) {\n        double* row_i = a + i * N;\n        double  pivot = row_i[i];\n        if (pivot == 0.0) {\n            return;  // As per original behavior\n        }\n\n#pragma omp parallel for schedule(static)\n        for (size_t j = i + 1; j < N; ++j) {\n            double* row_j = a + j * N;\n            double factor = row_j[i] / pivot;  // Preserve exact division behavior\n#pragma omp simd\n            for (size_t k = i; k < N; ++k) {\n                row_j[k] -= factor * row_i[k];\n            }\n            bvec[j] -= factor * bvec[i];\n        }\n    }\n\n    // Back substitution\n    for (int i = static_cast<int>(N) - 1; i >= 0; --i) {\n        double sum = 0.0;\n        double* row_i = a + i * N;\n        for (size_t j = i + 1; j < N; ++j) {\n            sum += row_i[j] * xvec[j];\n        }\n        xvec[i] = (bvec[i] - sum) / row_i[i];\n    }\n}\n",
  "08_fft_split_fft": "```cpp\n#pragma once\n#include <vector>\n#include <complex>\n#include <cmath>\n#include <omp.h>\n\nvoid fft(std::vector<std::complex<double>> const& x,\n         std::vector<double>& r,\n         std::vector<double>& i)\n{\n    std::vector<std::complex<double>> x_copy = x;\n    const unsigned int N = static_cast<unsigned int>(x_copy.size());\n    unsigned int k = N;\n    unsigned int n;\n\n    // Initial twiddle factor\n    double thetaT = 3.14159265358979323846264338328L / N;\n    std::complex<double> phiT = std::complex<double>(std::cos(thetaT), -std::sin(thetaT));\n\n    // Buffer for pre\u2011computed twiddle factors\n    std::vector<std::complex<double>> twiddle;\n    twiddle.reserve(N / 2 + 1);\n\n    while (k > 1) {\n        n = k;\n        k >>= 1;\n        phiT *= phiT;                     // phiT = phiT^2\n        const unsigned int stage_k = k;    // number of sub\u2011buckets\n        twiddle.resize(stage_k);\n\n        // Compute T = phiT^l for each l\n        std::complex<double> T = 1.0L;\n        for (unsigned int l = 0; l < stage_k; ++l) {\n            twiddle[l] = T;\n            T *= phiT;\n        }\n\n        if (stage_k > 1) {\n            #pragma omp parallel for schedule(static)\n            for (unsigned int l = 0; l < stage_k; ++l) {\n                const std::complex<double> T_local = twiddle[l];\n                for (unsigned int a = l; a < N; a += n) {\n                    unsigned int b = a + k;\n                    std::complex<double> tmp = x_copy[a] - x_copy[b];\n                    x_copy[a] += x_copy[b];\n                    x_copy[b] = tmp * T_local;\n                }\n            }\n        } else { // stage_k == 1\n            #pragma omp parallel for schedule(static)\n            for (unsigned int a = 0; a < N; a += n) {\n                unsigned int b = a + k;\n                std::complex<double> tmp = x_copy[a] - x_copy[b];\n                x_copy[a] += x_copy[b];\n                x_copy[b] = tmp * twiddle[0];\n            }\n        }\n    }\n\n    // Bit\u2011reversal permutation\n    unsigned int m = static_cast<unsigned int>(std::log2(static_cast<double>(N)));\n    #pragma omp parallel for schedule(static)\n    for (unsigned int a = 0; a < N; ++a) {\n        unsigned int b = a;\n        b = ((b & 0xaaaaaaaau) >> 1) | ((b & 0x55555555u) << 1);\n        b = ((b & 0xccccccccu) >> 2) | ((b & 0x33333333u) << 2);\n        b = ((b & 0xf0f0f0f0u) >> 4) | ((b & 0x0f0f0f0fu) << 4);\n        b = ((b & 0xff00ff00u) >> 8) | ((b & 0x00ff00ffu) << 8);\n        b = ((b >> 16) | (b << 16)) >> (32 - m);\n        if (b > a) {\n            std::complex<double> tmp = x_copy[a];\n            x_copy[a] = x_copy[b];\n            x_copy[b] = tmp;\n        }\n    }\n\n    // Split into real and imaginary parts\n    for (size_t j = 0; j < N; ++j) {\n        r[j] = x_copy[j].real();\n        i[j] = x_copy[j].imag();\n    }\n}\n```",
  "10_geometry_convex_hull": "```cpp\n#pragma once\n#include <vector>\n#include <algorithm>\n#if __cplusplus >= 201703L\n#include <execution>\n#endif\n\nstruct Point {\n    double x, y;\n};\n\ninline double cross(const Point& a, const Point& b, const Point& c) noexcept {\n    return (b.x - a.x) * (c.y - a.y) - (b.y - a.y) * (c.x - a.x);\n}\n\nstruct PointComparator {\n    constexpr bool operator()(const Point& a, const Point& b) const noexcept {\n        return a.x < b.x || (a.x == b.x && a.y < b.y);\n    }\n};\n\ninline void convexHull(const std::vector<Point>& points, std::vector<Point>& hull) noexcept {\n    const std::size_t n = points.size();\n    if (n < 3) {\n        hull = points;\n        return;\n    }\n\n    std::vector<Point> sorted(points);\n#if __cplusplus >= 201703L\n    std::sort(std::execution::par, sorted.begin(), sorted.end(), PointComparator());\n#else\n    std::sort(sorted.begin(), sorted.end(), PointComparator());\n#endif\n\n    std::vector<Point> ans;\n    ans.reserve(2 * n);\n\n    // Build lower hull\n    for (const auto& p : sorted) {\n        while (ans.size() >= 2 && cross(ans[ans.size() - 2], ans[ans.size() - 1], p) <= 0) {\n            ans.pop_back();\n        }\n        ans.push_back(p);\n    }\n\n    // Build upper hull\n    const std::size_t t = ans.size() + 1;\n    for (std::size_t i = n - 1; i > 0; --i) {\n        const auto& p = sorted[i - 1];\n        while (ans.size() >= t && cross(ans[ans.size() - 2], ans[ans.size() - 1], p) <= 0) {\n            ans.pop_back();\n        }\n        ans.push_back(p);\n    }\n\n    // Remove duplicate last point\n    if (!ans.empty()) {\n        ans.pop_back();\n    }\n\n    hull = std::move(ans);\n}\n```",
  "05_fft_inverse_fft": "```cpp\n#pragma once\n#include <vector>\n#include <complex>\n#include <cmath>\n#include <cstddef>\n\n#ifndef M_PI\n#define M_PI 3.14159265358979323846\n#endif\n\n// In\u2011place FFT (Cooley\u2013Tukey, iterative, decimation\u2011in\u2011time)\nstatic inline void fft_helper(std::vector<std::complex<double>>& x) {\n    const std::size_t N = x.size();\n    if (N <= 1) return;\n\n    std::size_t k = N, n;\n    long double thetaT = 3.14159265358979323846264338328L /\n                         static_cast<long double>(N);\n    std::complex<double> phiT(std::cos(thetaT), -std::sin(thetaT));\n\n    // Allocate once to avoid repeated reallocations\n    std::vector<std::complex<double>> twiddles;\n    twiddles.reserve(N / 2 + 1);\n\n    while (k > 1) {\n        n = k;          // stage stride\n        k >>= 1;        // half the current k\n        phiT *= phiT;   // advance twiddle factor for this stage\n\n        // Pre\u2011compute twiddle factors for all butterflies in this stage\n        twiddles.resize(k);\n        twiddles[0] = std::complex<double>(1.0, 0.0);\n        for (std::size_t l = 1; l < k; ++l) {\n            twiddles[l] = twiddles[l - 1] * phiT;\n        }\n\n        // Parallel butterfly operations for each l\n        #pragma omp parallel for schedule(static)\n        for (std::size_t l = 0; l < k; ++l) {\n            const std::complex<double> T_local = twiddles[l];\n            for (std::size_t a = l; a < N; a += n) {\n                const std::size_t b = a + k;\n                const std::complex<double> t = x[a] - x[b];\n                x[a] += x[b];\n                x[b] = t * T_local;\n            }\n        }\n    }\n\n    // Decimate\u2011in\u2011frequency (bit\u2011reversal permutation)\n    const unsigned m = static_cast<unsigned>(std::log2(static_cast<double>(N)));\n    #pragma omp parallel for schedule(static)\n    for (std::size_t a = 0; a < N; ++a) {\n        unsigned b = static_cast<unsigned>(a);\n        b = ((b & 0xaaaaaaaau) >> 1) | ((b & 0x55555555u) << 1);\n        b = ((b & 0xccccccccu) >> 2) | ((b & 0x33333333u) << 2);\n        b = ((b & 0xf0f0f0f0u) >> 4) | ((b & 0x0f0f0f0fu) << 4);\n        b = ((b & 0xff00ff00u) >> 8) | ((b & 0x00ff00ffu) << 8);\n        b = ((b >> 16) | (b << 16)) >> (32 - m);\n        if (b > a) {\n            std::complex<double> tmp = x[a];\n            x[a] = x[b];\n            x[b] = tmp;\n        }\n    }\n}\n\n// Inverse FFT (IFFT) \u2013 conjugate, forward FFT, conjugate, scale\nstatic inline void ifft(std::vector<std::complex<double>>& x) {\n    const std::size_t N = x.size();\n    if (N == 0) return;\n\n    #pragma omp parallel for schedule(static)\n    for (std::size_t i = 0; i < N; ++i) {\n        x[i] = std::conj(x[i]);\n    }\n\n    fft_helper(x);\n\n    #pragma omp parallel for schedule(static)\n    for (std::size_t i = 0; i < N; ++i) {\n        x[i] = std::conj(x[i]);\n    }\n\n    const double inv = 1.0 / static_cast<double>(N);\n    #pragma omp parallel for schedule(static)\n    for (std::size_t i = 0; i < N; ++i) {\n        x[i] *= inv;\n    }\n}\n```\n",
  "38_search_find_the_first_even_number": "```cpp\n#pragma once\n#include <vector>\n\n#if defined(_OPENMP)\n#include <omp.h>\n#endif\n\nsize_t findFirstEven(std::vector<int> const& x) {\n    const size_t n = x.size();\n    if (n == 0) return 0;\n\n    const int* arr = x.data();\n    size_t min_idx = n;\n\n#pragma omp parallel for reduction(min:min_idx) if(n >= 16384)\n    for (size_t i = 0; i < n; ++i) {\n        if ((arr[i] & 1) == 0) {\n            if (i < min_idx) min_idx = i;\n        }\n    }\n    return min_idx;\n}\n```\n",
  "50_stencil_xor_kernel": "```cpp\n#pragma once\n#include <vector>\n#include <cstddef>\n\nvoid cellsXOR(std::vector<int> const &input, std::vector<int> &output, size_t N) {\n    if (N == 0) return;\n    const int *in = input.data();\n    int *out = output.data();\n\n    if (N == 1) {\n        out[0] = 0;\n        return;\n    }\n\n    const size_t lastIdx = N - 1;\n\n#pragma omp parallel for schedule(static)\n    for (size_t i = 0; i < N; ++i) {\n        const int *row   = in + i * N;\n        int *outRow      = out + i * N;\n\n        if (i == 0) { // first row \u2013 no 'above' neighbours\n            int cnt0 = (in[(i + 1) * N + 0] == 1) + (row[1] == 1);\n            outRow[0] = (cnt0 == 1);\n\n            for (size_t j = 1; j + 1 < N; ++j) {\n                int cnt = (in[(i + 1) * N + j] == 1) + (row[j - 1] == 1) + (row[j + 1] == 1);\n                outRow[j] = (cnt == 1);\n            }\n\n            int cntLast = (in[(i + 1) * N + lastIdx] == 1) + (row[lastIdx - 1] == 1);\n            outRow[lastIdx] = (cntLast == 1);\n        } else if (i == lastIdx) { // last row \u2013 no 'below' neighbours\n            int cnt0 = (in[(i - 1) * N + 0] == 1) + (row[1] == 1);\n            outRow[0] = (cnt0 == 1);\n\n            for (size_t j = 1; j + 1 < N; ++j) {\n                int cnt = (in[(i - 1) * N + j] == 1) + (row[j - 1] == 1) + (row[j + 1] == 1);\n                outRow[j] = (cnt == 1);\n            }\n\n            int cntLast = (in[(i - 1) * N + lastIdx] == 1) + (row[lastIdx - 1] == 1);\n            outRow[lastIdx] = (cntLast == 1);\n        } else { // interior rows \u2013 all neighbour directions are present\n            const int *above = in + (i - 1) * N;\n            const int *below = in + (i + 1) * N;\n\n            int cnt0 = (above[0] == 1) + (below[0] == 1) + (row[1] == 1);\n            outRow[0] = (cnt0 == 1);\n\n            for (size_t j = 1; j + 1 < N; ++j) {\n                int cnt = (above[j] == 1) + (below[j] == 1) + (row[j - 1] == 1) + (row[j + 1] == 1);\n                outRow[j] = (cnt == 1);\n            }\n\n            int cntLast = (above[lastIdx] == 1) + (below[lastIdx] == 1) + (row[lastIdx - 1] == 1);\n            outRow[lastIdx] = (cntLast == 1);\n        }\n    }\n}\n```",
  "35_search_search_for_last_struct_by_key": "```cpp\n#pragma once\n#include <vector>\n#include <string>\n#include <omp.h>\n\nstruct Book {\n    std::string title;\n    int pages;\n};\n\n/* Return the index of the last Book item in the vector books where Book.pages is less than 100.\n   Example:\n   \n   input: [{title=\"Green Eggs and Ham\", pages=72}, {title=\"gulliver's travels\", pages=362},\n           {title=\"Stories of Your Life\", pages=54}, {title=\"Hamilton\", pages=818}]\n   output: 2\n*/\nsize_t findLastShortBook(std::vector<Book> const& books) {\n    const std::size_t n = books.size();\n    long long maxIdx = -1;          // sentinel for \"not found\"\n\n    #pragma omp parallel for reduction(max:maxIdx)\n    for (std::size_t i = 0; i < n; ++i) {\n        if (books[i].pages < 100) {\n            maxIdx = static_cast<long long>(i);\n        }\n    }\n\n    return (maxIdx < 0) ? n : static_cast<std::size_t>(maxIdx);\n}\n```",
  "41_sort_k-th_smallest_element": "```cpp\n#pragma once\n#include <vector>\n#include <algorithm>\n\ninline int findKthSmallest(const std::vector<int>& x, int k) {\n    // Make a copy of the input data\n    std::vector<int> x_copy(x);\n\n    // Find the k\u2011th smallest element (1\u2011based indexing)\n    auto it = x_copy.begin() + (k - 1);\n    std::nth_element(x_copy.begin(), it, x_copy.end());\n\n    return *it;\n}\n```",
  "04_dense_la_gemv": "```cpp\n#pragma once\n#include <vector>\n#include <cstddef>\n#ifdef _OPENMP\n#include <omp.h>\n#endif\n\nvoid gemv(std::vector<double> const& A, std::vector<double> const& x,\n          std::vector<double> &y, size_t M, size_t N) {\n    const double *Ad = A.data();\n    const double *Xd = x.data();\n    double *Yd = y.data();\n\n    #pragma GCC push_options\n    #pragma GCC optimize (\"-fno-tree-vectorize\")\n    #pragma omp parallel for schedule(static)\n    for (size_t i = 0; i < M; ++i) {\n        const double *row = Ad + i * N;\n        double sum = 0.0;\n        for (size_t j = 0; j < N; ++j) {\n            sum += row[j] * Xd[j];\n        }\n        Yd[i] = sum;\n    }\n    #pragma GCC pop_options\n}\n```",
  "37_search_find_the_closest_number_to_pi": "```cpp\n#pragma once\n#include <vector>\n#include <cmath>\n#include <cfloat>\n#include <limits>\n\nstruct MinInfo {\n    double diff;\n    size_t idx;\n};\n\n#pragma omp declare reduction(minimum : MinInfo : \\\n    omp_out = (omp_in.diff < omp_out.diff) ? omp_in : \\\n              ((omp_in.diff == omp_out.diff && omp_in.idx < omp_out.idx) ? omp_in : omp_out)) \\\n    initializer(omp_priv = {DBL_MAX, std::numeric_limits<size_t>::max()})\n\ninline size_t findClosestToPi(std::vector<double> const& x) {\n    const double pi = M_PI;\n    MinInfo local{DBL_MAX, std::numeric_limits<size_t>::max()};\n    #pragma omp parallel for reduction(minimum:local)\n    for (size_t i = 0; i < x.size(); ++i) {\n        double diff = std::abs(x[i] - pi);\n        if (diff < local.diff || (diff == local.diff && i < local.idx)) {\n            local.diff = diff;\n            local.idx = i;\n        }\n    }\n    return local.idx;\n}\n```\n",
  "07_fft_fft_conjugate": "```cpp\n#pragma once\n#include <vector>\n#include <complex>\n#include <cmath>\n#include <cstddef>\n#include <algorithm>\n#include <omp.h>\n\nvoid fftConjugate(std::vector<std::complex<double>>& x) {\n    const std::size_t N = x.size();\n    if (N <= 1) return;\n\n    // ---- compute the fundamental twiddle factor (exp(-i*\u03c0/N))\n    constexpr long double PI = 3.14159265358979323846264338328L;\n    long double thetaT = PI / static_cast<long double>(N);\n    double real_phi = std::cos(thetaT);\n    double imag_phi = -std::sin(thetaT);\n    std::complex<double> phiT(real_phi, imag_phi);\n\n    // ---- iterative Cooley\u2011Tukey FFT (decimation\u2011in\u2011frequency)\n    std::vector<std::complex<double>> Tvals;\n    std::size_t k = N;\n\n    while (k > 1) {\n        const std::size_t n = k;      // stride of the butterflies\n        k >>= 1;                      // new group size\n        phiT *= phiT;                 // \u03c6T = \u03c6T\u00b2 for this stage\n\n        // pre\u2011compute all powers of \u03c6T needed for this stage\n        Tvals.resize(k);\n        Tvals[0] = std::complex<double>(1.0, 0.0);\n        for (std::size_t l = 1; l < k; ++l) {\n            Tvals[l] = Tvals[l - 1] * phiT;\n        }\n\n#pragma omp parallel for schedule(static)\n        for (std::ptrdiff_t l = 0; l < static_cast<std::ptrdiff_t>(k); ++l) {\n            const std::complex<double> T = Tvals[static_cast<std::size_t>(l)];\n            for (std::size_t a = static_cast<std::size_t>(l); a < N; a += n) {\n                const std::size_t b = a + k;\n                const std::complex<double> t = x[a] - x[b];\n                x[a] += x[b];\n                x[b] = t * T;\n            }\n        }\n    }\n\n    // ---- bit\u2011reversal permutation\n    unsigned int m = 0;\n    for (std::size_t tmp = N; tmp > 1; tmp >>= 1) ++m;\n\n#pragma omp parallel for schedule(static)\n    for (std::ptrdiff_t i = 0; i < static_cast<std::ptrdiff_t>(N); ++i) {\n        std::size_t a = static_cast<std::size_t>(i);\n        unsigned int ai = static_cast<unsigned int>(a);\n        unsigned int b = ai;\n        b = ((b & 0xaaaaaaaa) >> 1) | ((b & 0x55555555) << 1);\n        b = ((b & 0xcccccccc) >> 2) | ((b & 0x33333333) << 2);\n        b = ((b & 0xf0f0f0f0) >> 4) | ((b & 0x0f0f0f0f) << 4);\n        b = ((b & 0xff00ff00) >> 8) | ((b & 0x00ff00ff) << 8);\n        b = ((b >> 16) | (b << 16)) >> (32 - m);\n        if (b > ai) {\n            std::swap(x[a], x[b]);   // swap only once per pair\n        }\n    }\n\n    // ---- conjugate the result\n#pragma omp parallel for schedule(static)\n    for (std::ptrdiff_t i = 0; i < static_cast<std::ptrdiff_t>(N); ++i) {\n        x[static_cast<std::size_t>(i)] = std::conj(x[static_cast<std::size_t>(i)]);\n    }\n}\n```\n",
  "36_search_check_if_array_contains_value": "```cpp\n#pragma once\n#include <vector>\n#include <atomic>\n\n/* Return true if the vector x contains the value `target`. Return false otherwise.\n   Examples:\n   \n   input: x=[1, 8, 2, 6, 4, 6], target=3\n   output: false\n   input: x=[1, 8, 2, 6, 4, 6], target=8\n   output: true\n*/\ninline bool contains(std::vector<int> const& x, int target) {\n    const size_t n = x.size();\n    if (n == 0) return false;\n\n    constexpr size_t PARALLEL_THRESHOLD = 1'048'576; // 1M elements\n    const int* data = x.data();\n\n    if (n < PARALLEL_THRESHOLD) {\n        for (size_t i = 0; i < n; ++i) {\n            if (data[i] == target) return true;\n        }\n        return false;\n    } else {\n        std::atomic<bool> found(false);\n\n#pragma omp parallel\n        {\n#pragma omp for schedule(static)\n            for (size_t i = 0; i < n; ++i) {\n                if (data[i] == target) {\n                    found.store(true, std::memory_order_relaxed);\n#pragma omp cancel for\n                }\n            }\n        }\n\n        return found.load(std::memory_order_relaxed);\n    }\n}\n```",
  "54_stencil_game_of_life": "```cpp\n#pragma once\n#include <vector>\n#include <cstddef>\n\n/* Simulate one generation of Game of Life on `input`. Store the results in `output`.\n   A cell is 1 if it is alive and 0 if it is dead.\n   If a live cell has fewer than 2 live neighbors then it dies.\n   If a live cell has 2 or 3 live neighbors then it lives on.\n   If a live cell has more than 3 live neighbors then it dies.\n   If a cell is dead and has exactly 3 live neighbors then it becomes alive.\n   `input` and `output` are NxN grids stored in row\u2011major order. */\nvoid gameOfLife(std::vector<int> const& input, std::vector<int> &output, size_t N) {\n    const int *in  = input.data();\n    int       *out = output.data();\n\n    if (N <= 2) {\n        /* Handle 0,1,2 size grids with the generic implementation. */\n        #pragma omp parallel for collapse(2) schedule(static)\n        for (size_t i = 0; i < N; ++i) {\n            for (size_t j = 0; j < N; ++j) {\n                int sum = 0;\n                if (i > 0)          sum += in[(i - 1) * N + j];\n                if (i + 1 < N)      sum += in[(i + 1) * N + j];\n                if (j > 0)          sum += in[i * N + (j - 1)];\n                if (j + 1 < N)      sum += in[i * N + (j + 1)];\n                if (i > 0 && j > 0) sum += in[(i - 1) * N + (j - 1)];\n                if (i > 0 && j + 1 < N) sum += in[(i - 1) * N + (j + 1)];\n                if (i + 1 < N && j > 0) sum += in[(i + 1) * N + (j - 1)];\n                if (i + 1 < N && j + 1 < N) sum += in[(i + 1) * N + (j + 1)];\n                int cur = in[i * N + j];\n                out[i * N + j] = (sum == 3 || (cur && sum == 2)) ? 1 : 0;\n            }\n        }\n        return;\n    }\n\n    /* Main implementation, split work into disjoint regions. */\n    #pragma omp parallel\n    {\n        /* Interior cells */\n        #pragma omp for schedule(static)\n        for (size_t i = 1; i + 1 < N; ++i) {\n            const size_t row  = i * N;\n            const size_t prev = (i - 1) * N;\n            const size_t next = (i + 1) * N;\n            for (size_t j = 1; j + 1 < N; ++j) {\n                const size_t idx = row + j;\n                int sum = in[prev + j - 1] + in[prev + j] + in[prev + j + 1]\n                        + in[next + j - 1] + in[next + j] + in[next + j + 1]\n                        + in[row + j - 1] + in[row + j + 1];\n                int cur = in[idx];\n                out[idx] = (sum == 3 || (cur && sum == 2)) ? 1 : 0;\n            }\n        }\n\n        /* Top row */\n        #pragma omp for schedule(static)\n        for (size_t j = 0; j < N; ++j) {\n            const size_t idx  = j;                 // row 0\n            int sum = in[N + j];                   // below\n            if (j > 0)     sum += in[j - 1];       // left\n            if (j + 1 < N) sum += in[j + 1];       // right\n            if (j > 0)     sum += in[N + j - 1];   // down\u2011left\n            if (j + 1 < N) sum += in[N + j + 1];   // down\u2011right\n            int cur = in[idx];\n            out[idx] = (sum == 3 || (cur && sum == 2)) ? 1 : 0;\n        }\n\n        /* Bottom row */\n        #pragma omp for schedule(static)\n        for (size_t j = 0; j < N; ++j) {\n            const size_t idx  = (N - 1) * N + j;    // row N-1\n            int sum = in[(N - 2) * N + j];          // above\n            if (j > 0)     sum += in[(N - 1) * N + (j - 1)]; // left\n            if (j + 1 < N) sum += in[(N - 1) * N + (j + 1)]; // right\n            if (j > 0)     sum += in[(N - 2) * N + (j - 1)]; // up\u2011left\n            if (j + 1 < N) sum += in[(N - 2) * N + (j + 1)]; // up\u2011right\n            int cur = in[idx];\n            out[idx] = (sum == 3 || (cur && sum == 2)) ? 1 : 0;\n        }\n\n        /* Left column (excluding corners) */\n        #pragma omp for schedule(static)\n        for (size_t i = 1; i + 1 < N; ++i) {\n            const size_t idx = i * N;          // column 0\n            int sum = in[i * N + 1];            // right\n            sum += in[(i + 1) * N + 1];        // down\u2011right\n            sum += in[(i + 1) * N];            // down\n            sum += in[(i - 1) * N];            // up\n            sum += in[(i - 1) * N + 1];        // up\u2011right\n            int cur = in[idx];\n            out[idx] = (sum == 3 || (cur && sum == 2)) ? 1 : 0;\n        }\n\n        /* Right column (excluding corners) */\n        #pragma omp for schedule(static)\n        for (size_t i = 1; i + 1 < N; ++i) {\n            const size_t idx = i * N + (N - 1); // column N-1\n            int sum = in[i * N + (N - 2)];      // left\n            sum += in[(i + 1) * N + (N - 2)];   // down\u2011left\n            sum += in[(i + 1) * N + (N - 1)];   // down\n            sum += in[(i - 1) * N + (N - 1)];   // up\n            sum += in[(i - 1) * N + (N - 2)];   // up\u2011left\n            int cur = in[idx];\n            out[idx] = (sum == 3 || (cur && sum == 2)) ? 1 : 0;\n        }\n    }\n}\n```",
  "51_stencil_edge_kernel": "```cpp\n#pragma once\n#include <vector>\n\nconst int edgeKernel[3][3] = {{-1, -1, -1}, {-1, 8, -1}, {-1, -1, -1}};\n\nvoid convolveKernel(std::vector<int> const& imageIn, std::vector<int> &imageOut, size_t N) {\n    if (N == 0) return;\n    const int* in  = imageIn.data();\n    int* out       = imageOut.data();\n    const size_t n = N;\n\n    #pragma omp parallel\n    {\n        const int* row0 = in;\n        const int* row1 = in + n;\n\n        // Interior region\n        #pragma omp for schedule(static) nowait\n        for (size_t i = 1; i < n - 1; ++i) {\n            const int* rowPrev = in + (i - 1) * n;\n            const int* rowCurr = in + i * n;\n            const int* rowNext = in + (i + 1) * n;\n            int* outRow = out + i * n;\n            for (size_t j = 1; j < n - 1; ++j) {\n                int sum = 8 * rowCurr[j];\n                sum -= rowPrev[j - 1] + rowPrev[j] + rowPrev[j + 1];\n                sum -= rowCurr[j - 1] + rowCurr[j + 1];\n                sum -= rowNext[j - 1] + rowNext[j] + rowNext[j + 1];\n                if (sum < 0) sum = 0;\n                else if (sum > 255) sum = 255;\n                outRow[j] = sum;\n            }\n        }\n\n        // Left and right columns of interior rows\n        #pragma omp for schedule(static) nowait\n        for (size_t i = 1; i < n - 1; ++i) {\n            const int* rowPrev = in + (i - 1) * n;\n            const int* rowCurr = in + i * n;\n            const int* rowNext = in + (i + 1) * n;\n            int* outRow = out + i * n;\n            const size_t last = n - 1;\n\n            int sumL = 8 * rowCurr[0];\n            sumL -= rowPrev[0] + rowPrev[1];\n            sumL -= rowCurr[1];\n            sumL -= rowNext[0] + rowNext[1];\n            if (sumL < 0) sumL = 0;\n            else if (sumL > 255) sumL = 255;\n            outRow[0] = sumL;\n\n            int sumR = 8 * rowCurr[last];\n            sumR -= rowPrev[last - 1] + rowPrev[last];\n            sumR -= rowCurr[last - 1];\n            sumR -= rowNext[last - 1] + rowNext[last];\n            if (sumR < 0) sumR = 0;\n            else if (sumR > 255) sumR = 255;\n            outRow[last] = sumR;\n        }\n\n        // Top row\n        #pragma omp for schedule(static) nowait\n        for (size_t j = 0; j < n; ++j) {\n            int sum = 8 * row0[j];\n            if (j > 0)     sum -= row0[j - 1];\n            if (j + 1 < n) sum -= row0[j + 1];\n            if (j > 0)     sum -= row1[j - 1];\n            if (n > 1)     sum -= row1[j];\n            if (j + 1 < n) sum -= row1[j + 1];\n            if (sum < 0) sum = 0;\n            else if (sum > 255) sum = 255;\n            out[j] = sum;\n        }\n\n        // Bottom row\n        if (n > 1) {\n            const int* rowN_1 = in + (n - 1) * n;\n            const int* rowN_2 = in + (n - 2) * n;\n            #pragma omp for schedule(static)\n            for (size_t j = 0; j < n; ++j) {\n                int sum = 8 * rowN_1[j];\n                if (j > 0)     sum -= rowN_2[j - 1];\n                sum -= rowN_2[j];\n                if (j + 1 < n) sum -= rowN_2[j + 1];\n                if (j > 0)     sum -= rowN_1[j - 1];\n                if (j + 1 < n) sum -= rowN_1[j + 1];\n                if (sum < 0) sum = 0;\n                else if (sum > 255) sum = 255;\n                out[(n - 1) * n + j] = sum;\n            }\n        }\n    }\n}\n```\n"
}