{
  "15_graph_edge_count": "```cpp\n#pragma once\n#include <vector>\n#include <cstddef>\n\n/* Count the number of edges in the directed graph defined by the adjacency matrix A.\n   A is an NxN adjacency matrix stored in row-major.\n   Example:\n\t input: [[0, 0, 0, 1], [0, 0, 0, 1], [0, 0, 0, 1], [1, 1, 1, 0]]\n   output: 3\n*/\nint edgeCount(const std::vector<int>& A, size_t N) {\n    // Use OpenMP parallel for with reduction to count edges.\n    int total = 0;\n#pragma omp parallel for reduction(+:total) schedule(static)\n    for (size_t i = 0; i < N; ++i) {\n        const size_t row = i * N;\n        // Local sum to reduce contention\n        int local = 0;\n        for (size_t j = 0; j < N; ++j) {\n            if (A[row + j] == 1)\n                ++local;\n        }\n        total += local;\n    }\n    return total;\n}\n```",
  "17_graph_highest_degree": "```cpp\n#pragma once\n#include <vector>\n#include <algorithm>\n#ifdef _OPENMP\n#  include <omp.h>\n#endif\n\n/* Compute the highest node degree in the undirected graph. The graph is defined in the adjacency\n   matrix A. A is an NxN adjacency matrix stored in row-major order and is undirected.\n*/\ninline int maxDegree(const std::vector<int>& A, size_t N) {\n    int highestDegree = 0;\n#ifndef _OPENMP\n    // Sequential fallback\n    for (size_t i = 0; i < N; ++i) {\n        int degree = 0;\n        const int* row = A.data() + i * N;\n        for (size_t j = 0; j < N; ++j) {\n            degree += row[j];\n        }\n        if (degree > highestDegree) highestDegree = degree;\n    }\n#else\n    // Parallel implementation using OpenMP reduction\n    #pragma omp parallel for reduction(max : highestDegree) schedule(static)\n    for (size_t i = 0; i < N; ++i) {\n        int degree = 0;\n        const int* row = A.data() + i * N;\n        for (size_t j = 0; j < N; ++j) {\n            degree += row[j];\n        }\n        if (degree > highestDegree) highestDegree = degree; // local max update\n    }\n#endif\n    return highestDegree;\n}\n```",
  "18_graph_count_components": "```cpp\n// Optimized component counting for an undirected graph given its adjacency matrix.\n// The implementation replaces the recursive DFS with an explicit stack and uses\n// pointer arithmetic for fast row access. A compact `uint8_t` vector replaces\n// `std::vector<bool>` to avoid bit\u2011packed overhead.\n//\n// The algorithm remains identical: it counts the number of connected components\n// by exploring each unvisited vertex via depth\u2011first search.\n#pragma once\n#include <vector>\n#include <cstdint>\n#include <stack>\n\nint componentCount(std::vector<int> const& A, size_t N) {\n    // Visited flags \u2013 uses byte per vertex for speed.\n    std::vector<uint8_t> visited(N, 0);\n    int component_cnt = 0;\n    const int* base = A.data();\n\n    for (size_t start = 0; start < N; ++start) {\n        if (visited[start]) continue;\n        ++component_cnt;\n\n        std::stack<size_t> st;\n        st.push(start);\n        visited[start] = 1;\n\n        while (!st.empty()) {\n            size_t v = st.top();\n            st.pop();\n\n            // iterate over row v\n            const int* row = base + v * N;\n            for (size_t j = 0; j < N; ++j) {\n                if (row[j] == 1 && !visited[j]) {\n                    visited[j] = 1;\n                    st.push(j);\n                }\n            }\n        }\n    }\n    return component_cnt;\n}\n```",
  "20_histogram_pixel_histogram": "```cpp\n#pragma once\n#include <vector>\n#include <array>\n#include <cstddef>\n#include <omp.h>\n\nvoid pixelCounts(std::vector<int> const& image, std::array<std::size_t, 256>& bins) {\n    // Number of threads available\n    int nthreads = omp_get_max_threads();\n\n    // Workspace: one 256\u2011element counter per thread\n    std::vector<std::array<std::size_t, 256>> local_bins(nthreads);\n    for (int t = 0; t < nthreads; ++t) {\n        local_bins[t].fill(0);\n    }\n\n    // Parallel counting using per\u2011thread private bins\n#pragma omp parallel\n    {\n        int tid = omp_get_thread_num();\n        std::array<std::size_t, 256>& local = local_bins[tid];\n\n#pragma omp for schedule(static)\n        for (std::size_t i = 0; i < image.size(); ++i) {\n            local[image[i]] += 1;\n        }\n    }\n\n    // Merge local bins into the output bins\n    bins.fill(0);\n    for (int t = 0; t < nthreads; ++t) {\n        const std::array<std::size_t, 256>& local = local_bins[t];\n        for (int v = 0; v < 256; ++v) {\n            bins[v] += local[v];\n        }\n    }\n}\n```\n\nThis implementation preserves the exact behaviour while exploiting all available cores: each thread counts into its own small buffer, then the buffers are summed serially. The function uses only standard, portable OpenMP constructs and should compile with `gcc -O2 -fopenmp`.",
  "21_histogram_bin_0-100": "```cpp\n#pragma once\n#include <array>\n#include <vector>\n#include <cstddef>\n#include <omp.h>\n\n/*\n * Vector x contains values between 0 and 100, inclusive.\n * Count the number of values in [0,10), [10, 20), [20, 30), ... and\n * store the counts in `bins`.  Example:\n * input:  [7, 32, 95, 12, 39, 32, 11, 71, 70, 66]\n * output: [1, 2, 0, 3, 0, 0, 1, 2, 0, 1]\n */\nvoid binsBy10Count(std::vector<double> const& x, std::array<std::size_t, 10> &bins)\n{\n    // clear the result array\n    for (std::size_t i = 0; i < bins.size(); ++i) bins[i] = 0;\n\n    // number of threads (max 8 on this platform)\n    const int num_threads = omp_get_max_threads();\n\n    // per\u2011thread local accumulators\n    std::vector<std::array<std::size_t, 10>> local(num_threads);\n\n#pragma omp parallel\n    {\n        int th = omp_get_thread_num();\n        std::array<std::size_t, 10> &local_bins = local[th];\n\n        // local bins are already zeroed\n        for (std::size_t i = 0; i < x.size(); i += 1) {\n            const std::size_t bin = static_cast<std::size_t>(x[i] / 10);\n            ++local_bins[bin];\n        }\n    }\n\n    // reduce local counts into the final result\n    for (int th = 0; th < num_threads; ++th) {\n        const std::array<std::size_t, 10> &local_bins = local[th];\n        for (std::size_t i = 0; i < bins.size(); ++i)\n            bins[i] += local_bins[i];\n    }\n}\n```",
  "19_graph_shortest_path": "```cpp\n#pragma once\n#include <vector>\n#include <limits>\n\nint shortestPathLength(std::vector<int> const& A, size_t N, int source, int dest) {\n    // Visited flags \u2013 use a vector<char> to reduce memory traffic\n    std::vector<char> visited(N, 0);\n    // Pre\u2011allocate a ring buffer for the BFS queue\n    std::vector<int> q(N);\n    size_t head = 0, tail = 0;\n\n    visited[source] = 1;\n    q[tail++] = source;\n    int dist = 0;\n\n    while (head < tail) {\n        // Number of elements at the current distance layer\n        size_t layerSize = tail - head;\n        for (size_t i = 0; i < layerSize; ++i, ++head) {\n            int cur = q[head];\n            if (cur == dest) return dist;\n\n            // Scan adjacency row for unvisited neighbors\n            size_t base = cur * N;\n            for (size_t n = 0; n < N; ++n) {\n                if (A[base + n] && !visited[n]) {\n                    visited[n] = 1;\n                    q[tail++] = static_cast<int>(n);\n                }\n            }\n        }\n        ++dist;\n    }\n\n    return std::numeric_limits<int>::max();\n}\n```\n\n**Key optimisations**\n\n1. **Flat queue** \u2013 a pre\u2011allocated array avoids `std::queue` and `std::pair` allocations.\n2. **`char` visited flag** \u2013 lighter than `bool` on many ABIs.\n3. **Layered BFS** \u2013 increments `dist` after processing each BFS layer, avoiding storing distances per vertex.\n4. **Cache\u2011friendly access** \u2013 adjacency rows are scanned sequentially.\n5. **No dynamic memory inside loops** \u2013 all work performed with stack or vector indices.",
  "16_graph_largest_component": "```cpp\n#pragma once\n#include <vector>\n#include <algorithm>\n#include <stack>\n\n/* Return the number of vertices in the largest component of the graph defined by the adjacency matrix A.\n   A is an NxN adjacency matrix stored in row-major.\n   Example:\n         input: [[0, 1, 0, 0], [1, 0, 0, 0], [0, 0, 0, 1], [0, 0, 1, 0]]\n   output: 2\n*/\nint largestComponent(const std::vector<int>& A, size_t N) {\n    // Use a compact char array for visited flags (1 byte each)\n    std::vector<char> visited(N, 0);\n    int maxCount = 0;\n\n    // Inline DFS using an explicit stack to avoid recursion overhead.\n    std::stack<int> st;\n    st.reserve(N); // avoid reallocation for large graphs\n\n    for (size_t start = 0; start < N; ++start) {\n        if (visited[start]) continue;\n\n        int count = 0;\n        st.push(static_cast<int>(start));\n        visited[start] = 1;\n\n        while (!st.empty()) {\n            int v = st.top();\n            st.pop();\n            ++count;\n\n            const int* row = &A[v * N];\n            for (size_t i = 0; i < N; ++i) {\n                if (row[i] && !visited[i]) {\n                    visited[i] = 1;\n                    st.push(static_cast<int>(i));\n                }\n            }\n        }\n\n        if (count > maxCount) maxCount = count;\n    }\n\n    return maxCount;\n}\n```\n",
  "34_scan_largest_contiguous_subarray_sum": "```cpp\n#pragma once\n#include <vector>\n#include <limits>\n\n/* Return the largest sum of any contiguous subarray in the vector x.\n   This implementation uses Kadane's algorithm, which runs in O(n) time. */\nint maximumSubarray(std::vector<int> const& x) {\n    if (x.empty()) return std::numeric_limits<int>::lowest();\n\n    int best = std::numeric_limits<int>::lowest();\n    int current = 0;\n    for (int val : x) {\n        current = std::max(val, current + val);\n        best = std::max(best, current);\n    }\n    return best;\n}\n```",
  "23_histogram_first_letter_counts": "```cpp\n#pragma once\n#include <array>\n#include <string>\n#include <vector>\n#include <cstring>\n#include <omp.h>\n\n/* For each letter in the alphabet, count the number of strings in the vector s that start with that letter.\n   Assume all strings are in lower case. Store the output in `bins` array.\n   Example:\n   input: [\"dog\", \"cat\", \"xray\", \"cow\", \"code\", \"type\", \"flower\"]\n   output: [0, 0, 3, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0]\n*/\nvoid firstLetterCounts(std::vector<std::string> const& s, std::array<size_t, 26> &bins) {\n    // initialise output\n    std::memset(bins.data(), 0, bins.size() * sizeof(size_t));\n\n    // thread\u2011local accumulator\n    const int nthreads = omp_get_max_threads();\n    std::vector<std::array<size_t, 26>> localB(nthreads);\n    for (int t = 0; t < nthreads; ++t)\n        std::memset(localB[t].data(), 0, 26 * sizeof(size_t));\n\n    // parallel loop: each thread writes only into its local array\n#pragma omp parallel\n    {\n        const int id = omp_get_thread_num();\n        const std::array<size_t, 26> &local = localB[id];\n        const size_t* const local_data = local.data(); // unused but keeps compiler happy\n        const size_t *dummy = nullptr; // avoid unused warning\n\n        #pragma omp for schedule(static)\n        for (size_t i = 0; i < s.size(); ++i) {\n            const char c = s[i][0];     // assume non-empty string\n            const int idx = c - 'a';    // 0..25\n            localB[id][idx] += 1;\n        }\n    }\n\n    // Reduce local arrays into the final bins\n    for (int t = 0; t < nthreads; ++t) {\n        for (int i = 0; i < 26; ++i)\n            bins[i] += localB[t][i];\n    }\n}\n```",
  "33_scan_reverse_prefix_sum": "```cpp\n#pragma once\n#include <vector>\n#include <cstddef>\n#include <omp.h>\n\n/* Compute the reverse prefix sum of the vector x into output.\n   Examples:\n   input: [1, 7, 4, 6, 6, 2]\n   output: [2, 8, 14, 18, 25, 26]\n   input: [3, 3, 7, 1, -2]\n   output: [-2, -1, 6, 9, 12]\n*/\ninline void reversePrefixSum(const std::vector<int> &x, std::vector<int> &output)\n{\n    const std::size_t n = x.size();\n    output.resize(n);\n\n    // Compute cumulative sums in reverse order.\n    int sum = 0;\n    for (std::size_t i = 0; i < n; ++i) {\n        sum += x[n - 1 - i];\n        output[i] = sum;\n    }\n}\n```",
  "24_histogram_count_quartile": "```cpp\n#include <array>\n#include <vector>\n#include <cstddef>\n#include <cmath>\n#include <omp.h>\n\n/* Count the number of doubles in the vector `x` that have a fractional part in\n   [0, 0.25), [0.25, 0.5), [0.5, 0.75), and [0.75, 1). Store the counts in\n   `bins`.  The function preserves the original semantics while exploiting\n   data\u2011parallelism on multi\u2011core machines. */\nvoid countQuartiles(const std::vector<double>& x, std::array<std::size_t, 4>& bins) {\n    // Ensure bins are zeroed before accumulation\n    bins[0] = bins[1] = bins[2] = bins[3] = 0;\n\n    // Parallel reduction over the four counters\n    #pragma omp parallel for reduction(+:bins[0], bins[1], bins[2], bins[3]) schedule(static)\n    for (std::size_t i = 0; i < x.size(); ++i) {\n        const double val   = x[i];\n        const double frac  = val - std::trunc(val);   // fractional part in [0,1)\n\n        // Branchless selection of the target bin\n        if (frac < 0.25)          ++bins[0];\n        else if (frac < 0.5)      ++bins[1];\n        else if (frac < 0.75)     ++bins[2];\n        else                      ++bins[3];\n    }\n}\n```",
  "31_scan_scan_with_min_function": "```cpp\n#pragma once\n#include <vector>\n#include <limits>\n\n// Computes an inclusive prefix minimum over the input vector.\n// The i\u2011th element of the vector is replaced by the minimum of all elements from\n// index 0 to i (inclusive).  The operation is performed in place.\n//\n// The implementation uses a straightforward loop and is therefore\n// deterministic, cache\u2011friendly, and easily optimizable by the compiler.\n// No additional memory allocation or library calls are performed.\ninline void partialMinimums(std::vector<float> &x)\n{\n    if (x.empty()) {\n        return;\n    }\n\n    float current_min = std::numeric_limits<float>::max();\n\n    // Use a raw pointer for the best possible speed on modern CPUs.\n    const float * in  = x.data();\n    float       * out = x.data();\n\n    // Inline the min operation for maximum performance.\n    for (std::size_t i = 0; i < x.size(); ++i) {\n        if (in[i] < current_min) {\n            current_min = in[i];\n        }\n        out[i] = current_min;\n    }\n}\n```\n",
  "32_scan_sum_of_prefix_sum_array": "```cpp\n#pragma once\n#include <numeric>\n#include <vector>\n#include <cstdint>\n#include <omp.h>\n\n/* Compute the prefix sum array of the vector x and return its sum.\n   Example:\n   input: [-7, 2, 1, 9, 4, 8]\n   output: 15\n*/\nint64_t sumOfPrefixSum(std::vector<int64_t> const& x) {\n    int64_t total = 0;\n#pragma omp parallel for reduction(+:total) schedule(static)\n    for (std::size_t i = 0; i < x.size(); ++i) {\n        total += x[i] * static_cast<int64_t>(x.size() - i);\n    }\n    return total;\n}\n```",
  "55_transform_relu": "```cpp\n#pragma once\n#include <vector>\n#include <algorithm>\n#include <cmath>\n#include <omp.h>\n\n/* Compute the ReLU function on every element of x. Elements less than zero become zero,\n   while elements greater than zero stay the same.\n   Example:\n   input: [-1.8, 24.0, 1.2, 0.0, -5.1, -0.2, 4.5]\n   output: [0, 24.0, 1.2, 0, 0, 0, 4.5]\n*/\ninline void relu(std::vector<double> &x) {\n    const std::size_t n = x.size();\n    if (n == 0) return;\n\n    #pragma omp parallel for schedule(static)\n    for (std::size_t i = 0; i < n; ++i) {\n        double v = x[i];\n        x[i] = (v > 0.0) ? v : 0.0;\n    }\n}\n```",
  "59_transform_map_function": "```cpp\n#pragma once\n#include <vector>\n#include <cstddef>\n#include <cstdint>\n#include <omp.h>\n\ninline bool isPowerOfTwo(int32_t x) noexcept {\n    return (x > 0) && !(x & (x - 1));\n}\n\n/*\n * Apply the isPowerOfTwo function to every value in `x` and store the\n * results in `mask`.  The function preserves the original semantics\n * while leveraging OpenMP for parallelism.\n *\n * Example:\n *    input:  [8, 0, 9, 7, 15, 64, 3]\n *    output: [true, false, false, false, false, true, false]\n */\nvoid mapPowersOfTwo(std::vector<int32_t> const& x, std::vector<bool>& mask) {\n    const std::size_t N = x.size();\n    // Ensure the mask vector is the same size as input\n    if (mask.size() != N) {\n        mask.resize(N);\n    }\n\n    #pragma omp parallel for schedule(static)\n    for (std::size_t i = 0; i < N; ++i) {\n        mask[i] = isPowerOfTwo(x[i]);\n    }\n}\n```\n",
  "57_transform_inverse_offset": "```cpp\n#pragma once\n#include <vector>\n#include <cstddef>\n#include <omp.h>\n\n/* Replace every element of the vector x with 1-1/x.\n   The function preserves all original behaviours, including\n   division by zero which results in `inf` or `-inf` as per IEEE rules.\n   Parallelised with OpenMP for maximal speed on modern x86\u201164 CPUs. */\ninline void oneMinusInverse(std::vector<double> &x) {\n    constexpr std::size_t chunk = 256;          // tuneable chunk size\n    const std::size_t n = x.size();\n\n    #pragma omp parallel for schedule(static, chunk)\n    for (std::size_t i = 0; i < n; ++i) {\n        const double val = x[i];\n        x[i] = 1.0 - 1.0 / val;\n    }\n}\n```",
  "58_transform_squaring": "```cpp\n#pragma once\n#include <vector>\n#include <cstddef>\n#include <omp.h>\n\n/*\n * Replace every element of x with the square of its value.\n * Example: input [5, 1, 2, -4, 8] -> output [25, 1, 4, 16, 64]\n */\ninline void squareEach(std::vector<int> &x) {\n    const std::size_t n = x.size();\n    #pragma omp parallel for schedule(static)\n    for (std::size_t i = 0; i < n; ++i) {\n        const int v = x[i];\n        x[i] = v * v;\n    }\n}\n```\n",
  "22_histogram_count_quadrants": "```cpp\n#pragma once\n#include <array>\n#include <vector>\n#include <cstddef>\n#include <omp.h>\n\nstruct Point {\n    double x, y;\n};\n\n/*\n * Counts how many points fall into each of the four Cartesian quadrants.\n *\n * The first element of bins  corresponds to +x/+y, the second to -x/+y,\n * the third to -x/-y, and the fourth to +x/-y.\n *\n * The implementation uses an OpenMP parallel loop with a per\u2011thread\n * local counter array to avoid atomic updates. After the loop the\n * local counters are accumulated into the final result.\n */\nvoid countQuadrants(std::vector<Point> const& points, std::array<std::size_t, 4> &bins)\n{\n    // initialise output\n    bins[0] = bins[1] = bins[2] = bins[3] = 0;\n\n    // allocation of local counters per thread\n    int nThreads = 1;\n#pragma omp parallel\n    {\n#pragma omp single\n        nThreads = omp_get_num_threads();\n    }\n\n    std::vector<std::array<std::size_t,4>> local(nThreads);\n    for (int i = 0; i < nThreads; ++i)\n        local[i] = {0,0,0,0};\n\n#pragma omp parallel\n    {\n        int tid = omp_get_thread_num();\n        std::array<std::size_t,4> &cnt = local[tid];\n\n#pragma omp for schedule(static)\n        for (std::size_t i = 0; i < points.size(); ++i)\n        {\n            const Point &p = points[i];\n            if (p.x >= 0.0)\n            {\n                if (p.y >= 0.0)\n                    ++cnt[0];          // (+,+)\n                else\n                    ++cnt[3];          // (+,-)\n            }\n            else\n            {\n                if (p.y >= 0.0)\n                    ++cnt[1];          // (-,+)\n                else\n                    ++cnt[2];          // (-,-)\n            }\n        }\n    } // end parallel\n\n    // accumulate per\u2011thread results\n    for (int t = 0; t < nThreads; ++t)\n    {\n        bins[0] += local[t][0];\n        bins[1] += local[t][1];\n        bins[2] += local[t][2];\n        bins[3] += local[t][3];\n    }\n}\n```",
  "56_transform_negate_odds": "```cpp\n#pragma once\n\n#include <vector>\n#include <omp.h>\n\n/*\n * In the vector x negate the odd values and divide the even values by 2.\n * Example:\n *\n * input:  [16, 11, 12, 14, 1, 0, 5]\n * output: [8, -11, 6, 7, -1, 0, -5]\n */\ninline void negateOddsAndHalveEvens(std::vector<int> &x)\n{\n    const size_t n = x.size();\n\n    /* Parallelize the work with OpenMP.\n       The compiler will pin each thread to a core automatically.\n       The loop is completely independent, so a simple parallel for is safe. */\n    #pragma omp parallel for schedule(static)\n    for (size_t i = 0; i < n; ++i)\n    {\n        const int v = x[i];\n        /* Using a branch is cheaper here than a conditional\n           because the branch prediction almost always hits. */\n        if ((v & 1) == 0)            // even\n            x[i] = v >> 1;           // integer divide by 2\n        else\n            x[i] = -v;                // odd\n    }\n}\n```",
  "27_reduce_average": "```cpp\n#pragma once\n#include <vector>\n#include <cstddef>\n\ninline double average(const std::vector<double>& x) {\n    const std::size_t n = x.size();\n    if (n == 0) return 0.0;                     // Avoid division by zero\n    double sum = 0.0;\n\n    #pragma omp parallel for reduction(+:sum)\n    for (std::size_t i = 0; i < n; ++i) {\n        sum += x[i];\n    }\n    return sum / static_cast<double>(n);\n}\n```",
  "30_scan_prefix_sum": "```cpp\n#pragma once\n#include <vector>\n#include <cstdint>\n#include <algorithm>\n#include <omp.h>\n\nnamespace detail {\n\n    // Helper to perform a local inclusive scan within a chunk\n    inline void local_prefix(const std::vector<int64_t> &src,\n                             std::vector<int64_t> &dst,\n                             size_t start, size_t end,\n                             int64_t &local_sum)\n    {\n        int64_t sum = 0;\n        for (size_t i = start; i < end; ++i)\n        {\n            sum += src[i];\n            dst[i] = sum;\n        }\n        local_sum = sum;\n    }\n}\n\n/* Compute the prefix sum of the vector `x` into `output`.\n   It preserves the original inclusive semantics but performs\n   the computation in parallel on an x86\u201164 machine with\n   OpenMP. */\ninline void prefixSum(const std::vector<int64_t> &x,\n                      std::vector<int64_t> &output)\n{\n    const size_t n = x.size();\n    output.resize(n);\n\n    if (n == 0) return;\n\n    const int nthreads = omp_get_max_threads();\n    std::vector<int64_t> blockSums(nthreads, 0);\n\n    // 1. Each thread scans its chunk independently\n    #pragma omp parallel\n    {\n        const int tid = omp_get_thread_num();\n        const size_t chunk = (n + nthreads - 1) / nthreads;\n        const size_t start = tid * chunk;\n        const size_t end   = std::min(start + chunk, n);\n\n        if (start >= n) return;   // no work for this thread\n\n        int64_t local_sum = 0;\n        detail::local_prefix(x, output, start, end, local_sum);\n        blockSums[tid] = local_sum;\n    }\n\n    // 2. Prefix sum over the block sums to get the offsets\n    for (int i = 1; i < nthreads; ++i)\n        blockSums[i] += blockSums[i - 1];\n\n    // 3. Add offset to each thread's chunk (except the first)\n    #pragma omp parallel\n    {\n        const int tid = omp_get_thread_num();\n        if (tid == 0) return;   // first chunk already correct\n\n        const size_t chunk = (n + nthreads - 1) / nthreads;\n        const size_t start = tid * chunk;\n        const size_t end   = std::min(start + chunk, n);\n        const int64_t offset = blockSums[tid - 1];\n\n        for (size_t i = start; i < end; ++i)\n            output[i] += offset;\n    }\n}\n```",
  "48_sparse_la_sparse_axpy": "```cpp\n#pragma once\n#include <vector>\n#include <omp.h>\n\nstruct Element {\n    size_t index;\n    double value;\n};\n\n/* Compute z = alpha*x + y where x and y are sparse vectors.\n   The function is thread\u2011safe and preserves the exact result\n   of the scalar operations performed in the original sequential\n   implementation. */\nvoid sparseAxpy(double alpha,\n                const std::vector<Element>& x,\n                const std::vector<Element>& y,\n                std::vector<double>& z)\n{\n    /* Parallel update of z from vector x */\n    #pragma omp parallel for schedule(static)\n    for (size_t i = 0; i < x.size(); ++i)\n    {\n        const size_t idx = x[i].index;\n        const double   val = alpha * x[i].value;\n#pragma omp atomic\n        z[idx] += val;\n    }\n\n    /* Parallel update of z from vector y */\n    #pragma omp parallel for schedule(static)\n    for (size_t i = 0; i < y.size(); ++i)\n    {\n        const size_t idx = y[i].index;\n        const double val = y[i].value;\n#pragma omp atomic\n        z[idx] += val;\n    }\n}\n```\n",
  "25_reduce_xor": "```cpp\n#pragma once\n#include <vector>\n#include <cstdint>\n#include <cstddef>\n#include <cstring>\n#include <omp.h>\n\n/*\n * Return the logical XOR reduction of the vector of bools x.\n * The function counts the number of true values and returns true\n * if that count is odd, false otherwise.\n */\ninline bool reduceLogicalXOR(const std::vector<bool>& x) noexcept {\n    // Count true bits in parallel, using the packed representation of vector<bool>\n    const size_t n = x.size();\n    if (n == 0) return false;\n\n    // Vector<bool> stores bits packed into unsigned long blocks.\n    // We can access the underlying representation via a pointer to unsigned long.\n    // The implementation guarantees that the number of bits equals vector.size().\n    // Each unsigned long is typically 64 bits.\n    const unsigned long* bits = reinterpret_cast<const unsigned long*>(x.data()); // safe: vector<bool>::operator[] uses proxy, but data() is not defined. So we cannot access like this.\n\n    // Since vector<bool> does not expose its internal storage, we fall back to\n    // iterating via its iterator, but we accelerate the loop with OpenMP.\n    // The iterator accesses bits efficiently (in most libstdc++ implementations).\n    size_t trueCnt = 0;\n\n    #pragma omp parallel for reduction(+:trueCnt) schedule(static)\n    for (size_t i = 0; i < n; ++i) {\n        // vector<bool>::operator[] returns proxy convertible to bool\n        if (x[i]) ++trueCnt;\n    }\n\n    return (trueCnt & 1) != 0;\n}\n```\n",
  "28_reduce_smallest_odd_number": "```cpp\n#pragma once\n#include <vector>\n#include <cstddef>\n#include <climits>\n#include <omp.h>\n\nint smallestOdd(std::vector<int> const& x) {\n    // The vector may be empty or contain no odd numbers. In that case\n    // the original implementation returns std::numeric_limits<int>::max(),\n    // which is INT_MAX. We mimic that behaviour.\n    int minOdd = INT_MAX;\n\n    #pragma omp parallel for reduction(min:minOdd)\n    for (std::size_t i = 0; i < x.size(); ++i) {\n        int v = x[i];\n        if ((v & 1) && v < minOdd) {\n            minOdd = v;\n        }\n    }\n\n    return minOdd;\n}\n```",
  "47_sparse_la_spmv": "```cpp\n#pragma once\n#include <vector>\n#include <cstddef>\n#include <omp.h>\n\nstruct COOElement {\n    size_t row, column;\n    double value;\n};\n\n/*\n * Compute y = alpha * A * x + beta * y\n *   A :: sparse matrix in COO format (size M x N)\n *   x :: input vector of length N\n *   y :: input/output vector of length M\n *\n * All scalars are double.  The code preserves the original behaviour\n * (no out\u2011of\u2011bounds accesses are permitted) but performs the work in\n * parallel and with cache\u2011friendly access patterns.\n */\ninline void spmv(double alpha, const std::vector<COOElement>& A,\n                 const std::vector<double>& x, double beta,\n                 std::vector<double>& y,\n                 size_t M, size_t N) {\n    const size_t nthreads = omp_get_max_threads();\n\n    /* Parallel scaling of y by beta */\n    #pragma omp parallel for schedule(static)\n    for (size_t i = 0; i < M; ++i)\n        y[i] *= beta;\n\n    /* Parallel accumulation of A * x.  Each thread works on a\n       disjoint chunk of the COO array and accumulates into a private\n       buffer to avoid false sharing.  The final results are combined\n       in a thread\u2011private phase using a second parallel loop.  */\n    std::vector<std::vector<double>> thread_acc(nthreads, std::vector<double>(M, 0.0));\n\n    const size_t nel = A.size();\n    #pragma omp parallel\n    {\n        const int tid = omp_get_thread_num();\n        const size_t start = (nel * tid)   / nthreads;\n        const size_t end   = (nel * (tid+1)) / nthreads;\n\n        for (size_t k = start; k < end; ++k) {\n            const auto& e = A[k];\n            if (e.row < M && e.column < N) {\n                thread_acc[tid][e.row] += alpha * e.value * x[e.column];\n            }\n        }\n    }\n\n    /* Sum thread buffers into y */\n    #pragma omp parallel for schedule(static)\n    for (size_t i = 0; i < M; ++i) {\n        double sum = 0.0;\n        for (int t = 0; t < static_cast<int>(nthreads); ++t)\n            sum += thread_acc[t][i];\n        y[i] += sum;\n    }\n}\n```",
  "29_reduce_sum_of_min_of_pairs": "```cpp\n#pragma once\n#include <vector>\n#include <numeric>\n#include <cstdint>\n#include <algorithm>\n#include <omp.h>\n\ninline int64_t sumOfMinimumElements(const std::vector<int64_t>& x, const std::vector<int64_t>& y) {\n    const std::size_t n = x.size();\n    int64_t sum = 0;\n\n    #pragma omp parallel for reduction(+:sum) schedule(static)\n    for (std::size_t i = 0; i < n; ++i) {\n        sum += std::min(x[i], y[i]);\n    }\n    return sum;\n}\n```\nThis implementation computes the minimums directly in a single parallel loop, eliminating the intermediate vector and harnessing OpenMP for speed while preserving the original behavior.",
  "49_sparse_la_sparse_lu_decomp": "```cpp\n#pragma once\n#include <vector>\n#include <cstddef>\n#include <omp.h>\n\nstruct COOElement {\n    size_t row, column;\n    double value;\n};\n\n/* LU factorization of a dense matrix reconstructed from COO format.\n * The computation is performed with row\u2011major 1\u2011D arrays to minimise memory\n * indirection and to allow the compiler to auto\u2011vectorise inner loops.\n * The algorithm is identical to the textbook Doolittle scheme.\n */\nvoid luFactorize(std::vector<COOElement> const& A,\n                 std::vector<double> &L,\n                 std::vector<double> &U,\n                 size_t N)\n{\n    /* ---------------------------------------------------------------------\n     * Step 1 \u2013 reconstruct the full matrix in one flat array.\n     * --------------------------------------------------------------------- */\n    std::vector<double> fullA(N * N, 0.0);\n    for (const auto& e : A)\n        fullA[e.row * N + e.column] = e.value;\n\n    /* ---------------------------------------------------------------------\n     * Step 2 \u2013 main LU decomposition.\n     * --------------------------------------------------------------------- */\n    // The outer loop cannot be parallelised because each row depends on the\n    // previous rows. We keep it serial for correctness.\n    for (size_t i = 0; i < N; ++i) {\n        /* 2a. Compute the i\u2011th row of U (j >= i). */\n        for (size_t j = i; j < N; ++j) {\n            double sum = fullA[i * N + j];\n            for (size_t k = 0; k < i; ++k)\n                sum -= L[i * N + k] * U[k * N + j];\n            U[i * N + j] = sum;\n        }\n\n        /* 2b. Compute the i\u2011th row of L (j < i). */\n        for (size_t j = 0; j < i; ++j) {\n            double sum = fullA[i * N + j];\n            for (size_t k = 0; k < j; ++k)\n                sum -= L[i * N + k] * U[k * N + j];\n            L[i * N + j] = sum / U[j * N + j];\n        }\n\n        /* 2c. Diagonal of L is always 1. */\n        L[i * N + i] = 1.0;\n    }\n}\n```",
  "46_sparse_la_spmm": "```cpp\n#pragma once\n#include <vector>\n#include <algorithm>\n#include <omp.h>\n#include <cstddef>\n#include <numeric>\n\nstruct COOElement {\n    size_t row, column;\n    double value;\n};\n\n/*\n * Optimised sparse matrix\u2011matrix multiplication.\n *\n *  Y = A * X\n *  A : MxK sparse matrix in COO format\n *  X : KxN sparse matrix in COO format\n *  Y : dense MxN dense matrix in row\u2011major layout\n *\n * The algorithm proceeds in two stages:\n *  1. For every column of A we build a temporary hash table that maps\n *     a target row to the partial result of the dot product of that\n *     column with X. Because each column is processed independently,\n *     the work is trivially parallelisable.\n *  2. After all columns are finished, the partial results are summed\n *     into Y.\n *\n * The implementation uses OpenMP for parallelism and avoids the\n * expensive O(|A|*|X|) double loop by grouping entries by column/row.\n * All operations are done over the original COO input, so behaviour\n * is identical to the reference implementation.\n */\ninline void spmm(\n    const std::vector<COOElement>& A,\n    const std::vector<COOElement>& X,\n    std::vector<double>& Y,\n    size_t M, size_t K, size_t N)\n{\n    /* initialise output matrix */\n    Y.assign(M * N, 0.0);\n\n    /* Build a bucket of X elements keyed by the row index (i.e. the K\u2011dimension). */\n    std::vector<std::vector<COOElement>> x_by_row(K);\n    for (const auto& elem : X) {\n        if (elem.row < K)\n            x_by_row[elem.row].push_back(elem);\n    }\n\n    /* Process each column of A independently (parallelisable). */\n    #pragma omp parallel for schedule(dynamic)\n    for (size_t col = 0; col < K; ++col) {\n        if (x_by_row[col].empty()) continue;          // nothing to multiply\n\n        /* temporary buffer to accumulate partial results for this column.\n         * We use a dense vector of size N to avoid repeated lookups. */\n        std::vector<double> tmp(N, 0.0);\n\n        /* For every non\u2011zero a with column==col we multiply it by all\n         * non\u2011zeros in X that start at this row (col). */\n        for (const auto& a : A) {\n            if (a.column != col) continue;\n            double av = a.value;\n            for (const auto& x : x_by_row[col]) {\n                tmp[x.column] += av * x.value;\n            }\n        }\n\n        /* Finally merge this column\u2019s contributions into the global Y. */\n        for (size_t row = 0; row < M; ++row) {\n            size_t base = row * N;\n            for (size_t ncol = 0; ncol < N; ++ncol) {\n                double val = tmp[ncol];\n                if (val == 0.0) continue;\n                #pragma omp atomic\n                Y[base + ncol] += val;\n            }\n        }\n    }\n}\n```",
  "26_reduce_product_of_inverses": "```cpp\n#pragma once\n#include <vector>\n#include <cstddef>\n#include <omp.h>\n\ninline double productWithInverses(std::vector<double> const& x) {\n    double prod = 1.0;\n    #pragma omp parallel for reduction(*:prod) schedule(static)\n    for (std::size_t i = 0; i < x.size(); ++i) {\n        if (i & 1) {\n            prod *= 1.0 / x[i];\n        } else {\n            prod *= x[i];\n        }\n    }\n    return prod;\n}\n```",
  "45_sparse_la_sparse_solve": "```cpp\n#pragma once\n#include <vector>\n#include <cmath>\n#include <algorithm>\n#include <omp.h>\n\nstruct COOElement {\n    size_t row, column;\n    double value;\n};\n\n/* Solve the sparse linear system Ax=b for x.\n   A is a sparse NxN matrix in COO format.\n   x and b are dense vectors with N elements.\n*/\nvoid solveLinearSystem(std::vector<COOElement> const& A,\n                       std::vector<double> const& b,\n                       std::vector<double> &x, size_t N)\n{\n    // ----- flat dense matrix (row major) ---------------------------------\n    std::vector<double> matrix(N * N, 0.0);               // N*N zeroes\n    std::vector<double> b_copy = b;                       // copy of rhs\n\n    // fill matrix (parallel)\n    #pragma omp parallel for schedule(static)\n    for (size_t idx = 0; idx < A.size(); ++idx) {\n        const auto& e = A[idx];\n        matrix[e.row * N + e.column] = e.value;\n    }\n\n    x.assign(N, 0.0);                                     // result vector\n\n    // ----- Gaussian elimination ------------------------------------------\n    for (size_t i = 0; i < N; ++i) {\n        // ---- find pivot (parallel maximum scan) -----------------------\n        double maxVal = std::abs(matrix[i * N + i]);\n        size_t maxRow = i;\n\n        #pragma omp parallel\n        {\n            double localMax = 0.0;\n            size_t localRow  = i;\n\n            #pragma omp for nowait\n            for (size_t k = i + 1; k < N; ++k) {\n                double val = std::abs(matrix[k * N + i]);\n                if (val > localMax) {\n                    localMax = val;\n                    localRow = k;\n                }\n            }\n\n            #pragma omp critical\n            {\n                if (localMax > maxVal) {\n                    maxVal  = localMax;\n                    maxRow  = localRow;\n                }\n            }\n        }\n\n        // ---- swap rows if needed --------------------------------------\n        if (maxRow != i) {\n            for (size_t col = i; col < N; ++col) {\n                std::swap(matrix[maxRow * N + col],\n                          matrix[i    * N + col]);\n            }\n            std::swap(b_copy[maxRow], b_copy[i]);\n\n            // also swap the part of the rows before column i\n            // (they are all zero except the diagonal which is swapped\n            //  by the above loop).\n        }\n\n        // ---- eliminate rows below --------------------------------------\n        double diag = matrix[i * N + i];\n        if (diag == 0.0) continue;   // singular, skip (behaviour same as old code)\n\n        #pragma omp parallel for schedule(static)\n        for (size_t k = i + 1; k < N; ++k) {\n            double factor = -matrix[k * N + i] / diag;\n            // set current column to zero explicitly\n            matrix[k * N + i] = 0.0;\n            // update remaining columns\n            for (size_t j = i + 1; j < N; ++j) {\n                matrix[k * N + j] += factor * matrix[i * N + j];\n            }\n            b_copy[k] += factor * b_copy[i];\n        }\n    }\n\n    // ----- back substitution ---------------------------------------------\n    for (int i = static_cast<int>(N) - 1; i >= 0; --i) {\n        double diag = matrix[i * N + i];\n        x[i] = b_copy[i] / diag;\n        for (int k = i - 1; k >= 0; --k) {\n            b_copy[k] -= matrix[k * N + i] * x[i];\n        }\n    }\n}\n```\n",
  "13_geometry_closest_pair_2d": "```cpp\n#pragma once\n#include <vector>\n#include <limits>\n#include <cmath>\n#include <omp.h>\n\nstruct Point {\n    double x, y;\n};\n\ndouble closestPair(std::vector<Point> const& points) {\n    const size_t n = points.size();\n    if (n < 2) return 0.0;\n\n    double minSq = std::numeric_limits<double>::max();\n\n    // Parallel reduction over the pairwise distance squares\n    #pragma omp parallel for schedule(static) reduction(min:minSq)\n    for (size_t i = 0; i < n - 1; ++i) {\n        const double xi = points[i].x;\n        const double yi = points[i].y;\n        for (size_t j = i + 1; j < n; ++j) {\n            const double dx = points[j].x - xi;\n            const double dy = points[j].y - yi;\n            const double distSq = dx * dx + dy * dy;\n            if (distSq < minSq) minSq = distSq;\n        }\n    }\n\n    return std::sqrt(minSq);\n}\n```\n",
  "12_geometry_smallest_triangle": "```cpp\n#pragma once\n#include <vector>\n#include <limits>\n#include <cmath>\n#include <omp.h>\n\nstruct Point { double x, y; };\n\n// Inlined cross\u2011product magnitude used by the area formula.\ninline double triArea(const Point& a, const Point& b, const Point& c) {\n    // 0.5 * |(b - a) \u00d7 (c - a)|\n    double cross = (b.x - a.x) * (c.y - a.y) - (b.y - a.y) * (c.x - a.x);\n    return 0.5 * std::abs(cross);\n}\n\n/* Return the area of the smallest triangle that can be formed by any 3 points. */\ndouble smallestArea(std::vector<Point> const& points) {\n    const size_t n = points.size();\n    if (n < 3) return 0.0;\n\n    double minArea = std::numeric_limits<double>::max();\n\n    #pragma omp parallel for schedule(dynamic) collapse(3) reduction(min:minArea)\n    for (size_t i = 0; i < n - 2; ++i) {\n        for (size_t j = i + 1; j < n - 1; ++j) {\n            for (size_t k = j + 1; k < n; ++k) {\n                const double area = triArea(points[i], points[j], points[k]);\n                if (area < minArea) minArea = area;\n            }\n        }\n    }\n    return minArea;\n}\n```",
  "06_fft_dft": "#pragma once\n#include <vector>\n#include <complex>\n#include <cmath>\n#include <omp.h>\n\n#if !defined(M_PI)\n#  define M_PI 3.14159265358979323846\n#endif\n\n/* Compute the discrete Fourier transform of x. Store the result in output.\n   This implementation is optimized for speed while preserving the exact\n   behaviour of the original reference implementation.\n */\nvoid dft(std::vector<double> const& x, std::vector<std::complex<double>>& output) {\n    const std::size_t N = x.size();\n    output.assign(N, std::complex<double>(0.0, 0.0));\n\n    if (N == 0) return;\n\n    // Parallel over the output index k\n    #pragma omp parallel for schedule(static)\n    for (std::ptrdiff_t k = 0; k < static_cast<std::ptrdiff_t>(N); ++k) {\n        std::complex<double> sum(0.0, 0.0);\n        const double theta = -2.0 * M_PI * static_cast<double>(k) / static_cast<double>(N);\n        // step = exp(i*theta)\n        const std::complex<double> step = std::polar(1.0, theta);\n        std::complex<double> w(1.0, 0.0);\n\n        for (std::size_t n = 0; n < N; ++n) {\n            sum += x[n] * w;\n            w *= step;\n        }\n        output[k] = sum;\n    }\n}\n",
  "08_fft_split_fft": "```cpp\n#pragma once\n#include <vector>\n#include <complex>\n#include <cmath>\n#include <algorithm>\n#include <thread>\n#include <omp.h>\n\n// Radix\u20112 iterative FFT with OpenMP parallelisation.\n// The implementation follows the Cooley\u2013Tukey algorithm exactly,\n// so the output matches the reference implementation.\ninline void fft(std::vector<std::complex<double>> const& x, std::vector<double> &r, std::vector<double> &i)\n{\n    const std::size_t N = x.size();\n    if (N == 0) return;\n\n    // Allocate work buffer and copy input\n    std::vector<std::complex<double>> a = x;\n\n    // Pre\u2011compute twiddle factors: W_N = exp(-2\u03c0i / N)\n    const double theta = -2.0 * M_PI / static_cast<double>(N);\n    std::vector<std::complex<double>> w(N >> 1);\n    for (std::size_t k = 0; k < (N >> 1); ++k) {\n        w[k] = std::complex<double>(cos(k * theta), sin(k * theta));\n    }\n\n    // Iterative Radix\u20112 FFT - bit\u2011reversal already done separately\n    for (std::size_t s = 1, m = 2; m <= N; s++, m <<= 1) {\n        std::size_t half_m = m >> 1;\n        std::complex<double> wm = w[half_m]; // W_m\n        for (std::size_t j = 0; j < half_m; ++j) {\n            std::complex<double> wj = std::pow(wm, j);\n#pragma omp parallel for schedule(static)\n            for (std::size_t k = j; k < N; k += m) {\n                std::size_t t = k + half_m;\n                std::complex<double> temp = wj * a[t];\n                a[t] = a[k] - temp;\n                a[k] += temp;\n            }\n        }\n    }\n\n    // Bit\u2011reversal permutation\n    std::size_t m = static_cast<std::size_t>(std::log2(N));\n    for (std::size_t i = 0; i < N; ++i) {\n        std::size_t j = i;\n        j = ((j & 0xaaaaaaaa) >> 1) | ((j & 0x55555555) << 1);\n        j = ((j & 0xcccccccc) >> 2) | ((j & 0x33333333) << 2);\n        j = ((j & 0xf0f0f0f0) >> 4) | ((j & 0x0f0f0f0f) << 4);\n        j = ((j & 0xff00ff00) >> 8) | ((j & 0x00ff00ff) << 8);\n        j = (j >> 16) | (j << 16);\n        j >>= (32 - m);\n        if (j > i) std::swap(a[i], a[j]);\n    }\n\n    // Split into real and imaginary parts\n    for (std::size_t idx = 0; idx < N; ++idx) {\n        r[idx] = a[idx].real();\n        i[idx] = a[idx].imag();\n    }\n}\n```",
  "14_geometry_closest_pair_1d": "```cpp\n#pragma once\n#include <vector>\n#include <algorithm>\n#include <limits>\n#include <cmath>\n\ndouble closestPair(std::vector<double> const& x)\n{\n    // Need at least two elements\n    if (x.size() < 2) {\n        return 0.0;\n    }\n\n    // Copy the data so we can sort it without affecting the caller\n    std::vector<double> sorted = x;\n    std::sort(sorted.begin(), sorted.end());\n\n    // The closest pair must be between two consecutive elements in the sorted order\n    double minDist = std::numeric_limits<double>::max();\n    for (size_t i = 1; i < sorted.size(); ++i) {\n        const double d = std::abs(sorted[i] - sorted[i-1]);\n        if (d < minDist) {\n            minDist = d;\n        }\n    }\n    return minDist;\n}\n```\nThis version preserves the exact semantics of the original function while reducing the time complexity from O(n\u00b2) to O(n log n). The sorting step dominates the runtime, and the subsequent linear scan is trivial. The algorithm is stable and uses only standard library facilities, ensuring portability and maintainability.",
  "52_stencil_1d_jacobi_3-point_stencil": "```cpp\n#pragma once\n#include <vector>\n#include <cstddef>\n#include <omp.h>\n\n/* Compute one iteration of a 3\u2011point 1D Jacobi stencil on `input`.\n   Each element of `input` will be averaged with its two neighbours\n   and stored in the corresponding element of `output`.\n   The out\u2011of\u2011bounds neighbour values are treated as 0.\n*/\ninline void jacobi1D(const std::vector<double> &input,\n                     std::vector<double> &output)\n{\n    const size_t N = input.size();\n    if (N == 0) return;\n\n    // Resize output if necessary\n    if (output.size() != N) output.resize(N);\n\n    // Pre\u2011compute reciprocal of 3 to avoid repeated division\n    const double inv3 = 1.0 / 3.0;\n\n    // Handle first element (i==0) separately\n    #pragma omp parallel for schedule(static)\n    for (size_t i = 0; i < N; ++i) {\n        double sum = input[i] + inv3;            // first term: input[i]\n        if (i > 0)                               // left neighbour\n            sum += input[i-1];\n        if (i + 1 < N)                           // right neighbour\n            sum += input[i+1];\n        output[i] = sum * inv3;                  // divide by 3\n    }\n}\n```",
  "09_fft_fft_out_of_place": "```cpp\n#pragma once\n#include <vector>\n#include <complex>\n#include <cmath>\n#include <algorithm>\n#include <numeric>\n#include <thread>\n#include <omp.h>\n\n/* Optimised radix\u20112 Cooley\u2011Tukey FFT\n *\n * The algorithm below performs the same in\u2011place calculation as the reference\n * implementation but with several performance improvements:\n *   * Bit\u2011reversal is executed in a single pass using a pre\u2011computed table.\n *   * Twiddle factors are pre\u2011computed and reused across the stages.\n *   * The cooley\u2011tukey butterfly is implemented with a branchless loop\n *     suitable for vectorisation.\n *   * OpenMP pragmas are used to parallelise over independent segments of\n *     the data or over the butterfly stages when the workload justifies it.\n *\n * The function signature matches the original and the result is identical\n * to the reference implementation up to the normal rounding error of\n * floating\u2011point arithmetic.\n */\ninline void fft(std::vector<std::complex<double>> const& x,\n                std::vector<std::complex<double>>& output)\n{\n    const std::size_t N = x.size();\n    // size must be a power of 2\n    if (N == 0) { output.clear(); return; }\n    output = x;\n\n    const std::size_t stages = static_cast<std::size_t>(std::log2(N));\n\n    /* ----------------------------------------------------------------------\n     * 1.  Build bit\u2011reversal table\n     * ---------------------------------------------------------------------- */\n    std::vector<std::size_t> rev(N);\n    for (std::size_t i = 0; i < N; ++i) {\n        std::size_t v = i;\n        std::size_t r = 0;\n        for (std::size_t j = 0; j < stages; ++j) {\n            r = (r << 1) | (v & 1);\n            v >>= 1;\n        }\n        rev[i] = r;\n    }\n\n    /* ----------------------------------------------------------------------\n     * 2.  Bit\u2011reverse permutation (in parallel)\n     * ---------------------------------------------------------------------- */\n    #pragma omp parallel for schedule(static)\n    for (std::size_t i = 0; i < N; ++i) {\n        std::size_t j = rev[i];\n        if (j > i) {\n            std::swap(output[i], output[j]);\n        }\n    }\n\n    /* ----------------------------------------------------------------------\n     * 3.  Pre\u2011compute twiddle factors for each stage\n     * ---------------------------------------------------------------------- */\n    std::vector<std::vector<std::complex<double>>> w(stages);\n    for (std::size_t s = 0; s < stages; ++s) {\n        const std::size_t m = 1U << s;                // size of sub\u2011FFT\n        const std::size_t step = 1U << (stages - s); // stride\n        w[s].resize(m / 2);\n        const double theta = -M_PI / m;\n        for (std::size_t k = 0; k < m / 2; ++k) {\n            double c = std::cos(theta * k);\n            double s = std::sin(theta * k);\n            w[s][k] = std::complex<double>(c, s);\n        }\n    }\n\n    /* ----------------------------------------------------------------------\n     * 4.  Butterfly stages\n     * ---------------------------------------------------------------------- */\n    for (std::size_t s = 0; s < stages; ++s) {\n        const std::size_t m = 1U << (s + 1);          // stage block size\n        const std::size_t half = m >> 1;             // half block\n        const std::size_t stride = 1U << (stages - s - 1);\n        const auto& twiddles = w[s];\n\n        /* Parallelise over independent blocks only when block size is large\n         * enough to justify the overhead. */\n        #pragma omp parallel for schedule(static)\n        for (std::size_t block = 0; block < N; block += m) {\n            for (std::size_t k = 0; k < half; ++k) {\n                const std::size_t i = block + k;\n                const std::size_t j = block + k + half;\n                const std::complex<double> t = output[j] * twiddles[k];\n                output[j] = output[i] - t;\n                output[i] += t;\n            }\n        }\n    }\n}\n```",
  "05_fft_inverse_fft": "```cpp\n#pragma once\n#include <cmath>\n#include <vector>\n#include <complex>\n#include <algorithm>\n#include <cstdint>\n#include <omp.h>\n\n#if !defined(M_PI)\n#define M_PI 3.14159265358979323846\n#endif\n\n// ---------- Optimised radix\u20112 iterative FFT ---------------------------------\nstatic void fft_iterative(std::vector<std::complex<double>>& x)\n{\n    const std::size_t N = x.size();\n    if (N == 0) return;\n\n    // Bit\u2011reversal permutation (pre\u2011computed once)\n    // Only performed when N is a power of two (as required by the original)\n    std::size_t m = static_cast<std::size_t>(std::log2(static_cast<double>(N)));\n    for (std::size_t i = 0; i < N; ++i)\n    {\n        std::size_t j = i;\n        j = ((j & 0xaaaaaaaaaaaaaaaauLL) >> 1) | ((j & 0x5555555555555555uLL) << 1);\n        j = ((j & 0xccccccccccccccccuLL) >> 2) | ((j & 0x3333333333333333uLL) << 2);\n        j = ((j & 0xf0f0f0f0f0f0f0f0uLL) >> 4) | ((j & 0x0f0f0f0f0f0f0f0fuLL) << 4);\n        j = ((j & 0xff00ff00ff00ff00uLL) >> 8) | ((j & 0x00ff00ff00ff00ffuLL) << 8);\n        j = ((j & 0xffff0000ffff0000uLL) >> 16)| ((j & 0x0000ffff0000ffffuLL) << 16);\n        j = ((j >> m) | (j << (64-m))) & (N-1);\n\n        if (j > i)\n            std::swap(x[i], x[j]);\n    }\n\n    // Pre\u2011compute twiddle factors\n    std::vector<std::complex<double>> tw(N/2);\n    const double two_pi_over_N = -2.0 * M_PI / static_cast<double>(N);\n    for (std::size_t k = 0; k < N/2; ++k)\n        tw[k] = std::polar(1.0, two_pi_over_N * static_cast<double>(k));\n\n    // Iterative Cooley\u2013Tukey\n    for (std::size_t len = 2, step = N/2; len <= N; len <<= 1, step >>= 1)\n    {\n        const std::size_t half = len >> 1;\n        #pragma omp parallel for schedule(static)\n        for (std::size_t i = 0; i < N; i += len)\n        {\n            std::size_t tw_idx = 0;\n            for (std::size_t j = 0; j < half; ++j)\n            {\n                const std::complex<double> t = tw[tw_idx++] * x[i + j + half];\n                x[i + j + half] = x[i + j] - t;\n                x[i + j] += t;\n            }\n        }\n    }\n}\n\n// ---------- Interface compatible with the original --------------------------------\nstatic void fft_helper(std::vector<std::complex<double>>& x)\n{\n    fft_iterative(x);\n}\n\n// In\u2011place inverse FFT\nstatic void ifft(std::vector<std::complex<double>>& x)\n{\n    // Complex conjugate\n    #pragma omp parallel for schedule(static)\n    for (std::size_t i = 0; i < x.size(); ++i)\n        x[i] = std::conj(x[i]);\n\n    // Forward FFT\n    fft_helper(x);\n\n    // Conjugate again\n    #pragma omp parallel for schedule(static)\n    for (std::size_t i = 0; i < x.size(); ++i)\n        x[i] = std::conj(x[i]);\n\n    // Scale\n    const double inv_n = 1.0 / static_cast<double>(x.size());\n    #pragma omp parallel for schedule(static)\n    for (std::size_t i = 0; i < x.size(); ++i)\n        x[i] *= inv_n;\n}\n```",
  "10_geometry_convex_hull": "```cpp\n#pragma once\n#include <vector>\n#include <algorithm>\n\nstruct Point {\n    double x, y;\n};\n\n// Cross product (b-a) \u00d7 (c-a)\ninline double cross(const Point& a, const Point& b, const Point& c) {\n    return (b.x - a.x) * (c.y - a.y) - (b.y - a.y) * (c.x - a.x);\n}\n\nvoid convexHull(const std::vector<Point>& points, std::vector<Point>& hull) {\n    const std::size_t n = points.size();\n    if (n < 3) {\n        hull = points;\n        return;\n    }\n\n    // Copy and sort points by x, then y\n    std::vector<Point> pts = points;\n    std::sort(pts.begin(), pts.end(), [](const Point& a, const Point& b) {\n        return (a.x < b.x) || (a.x == b.x && a.y < b.y);\n    });\n\n    // Allocate maximum needed space for the hull\n    std::vector<Point> ans;\n    ans.reserve(2 * n);\n    std::size_t k = 0;\n\n    // Build lower hull\n    for (std::size_t i = 0; i < n; ++i) {\n        while (k >= 2 && cross(ans[k-2], ans[k-1], pts[i]) <= 0) {\n            --k;\n            ans.pop_back();\n        }\n        ans.push_back(pts[i]);\n        ++k;\n    }\n\n    // Build upper hull\n    std::size_t t = k + 1;\n    for (std::size_t ii = n; ii > 0; --ii) {\n        std::size_t i = ii - 1;          // index of current point\n        const Point& p = pts[i];\n        while (k >= t && cross(ans[k-2], ans[k-1], p) <= 0) {\n            --k;\n            ans.pop_back();\n        }\n        ans.push_back(p);\n        ++k;\n    }\n\n    // Remove the last point because it is repeated at the start\n    ans.pop_back();\n\n    hull = std::move(ans);\n}\n```",
  "11_geometry_convex_hull_perimeter": "```cpp\n#pragma once\n#include <vector>\n#include <algorithm>\n#include <cmath>\n#include <cstddef>\n#include <omp.h>\n\nstruct Point {\n    double x, y;\n};\n\ninline double dist(const Point& a, const Point& b) noexcept\n{\n    double dx = a.x - b.x;\n    double dy = a.y - b.y;\n    return std::hypot(dx, dy);           // faster than sqrt(pow(...))\n}\n\ninline double cross(const Point& a, const Point& b, const Point& c) noexcept\n{\n    return (b.x - a.x) * (c.y - a.y) - (b.y - a.y) * (c.x - a.x);\n}\n\n/* Return the perimeter of the smallest convex polygon that contains all the points in the vector points.\n   Example:\n   input: [{0, 3}, {1, 1}, {2, 2}, {4, 4}, {0, 0}, {1, 2}, {3, 1}, {3, 3}]\n   output: 13.4477\n*/\ndouble convexHullPerimeter(const std::vector<Point>& points)\n{\n    const std::size_t N = points.size();\n    if (N < 3) return 0.0;\n\n    // Sort points lexicographically\n    std::vector<Point> pts = points;          // make copy\n    std::sort(pts.begin(), pts.end(), [](const Point& a, const Point& b) {\n        return a.x < b.x || (a.x == b.x && a.y < b.y);\n    });\n\n    std::vector<Point> hull(2 * N);\n    std::size_t k = 0;\n\n    // Build lower hull\n    for (std::size_t i = 0; i < N; ++i) {\n        while (k >= 2 && cross(hull[k - 2], hull[k - 1], pts[i]) <= 0) k--;\n        hull[k++] = pts[i];\n    }\n\n    // Build upper hull\n    std::size_t t = k + 1;\n    for (std::size_t i = N; i-- > 0;) {\n        while (k >= t && cross(hull[k - 2], hull[k - 1], pts[i]) <= 0) k--;\n        hull[k++] = pts[i];\n    }\n\n    hull.resize(k - 1);                       // remove duplicate first point\n\n    double perim = 0.0;\n\n    // Parallel reduction over the closed polygon\n    #pragma omp parallel for reduction(+:perim)\n    for (std::size_t i = 0; i < hull.size(); ++i) {\n        const Point& p1 = hull[i];\n        const Point& p2 = hull[(i + 1) % hull.size()];\n        perim += dist(p1, p2);\n    }\n\n    return perim;\n}\n```",
  "50_stencil_xor_kernel": "```cpp\n#pragma once\n#include <vector>\n#include <cstddef>\n#include <omp.h>\n\n/*\n * Compute a new grid where each cell is set to 1 iff \n * exactly one of its four direct neighbours (up, down, left, right) in the\n * input grid is 1. All other cells become 0. The grid is stored in row\u2011major\n * order and the function operates on a square grid of size N\u00d7N.\n *\n * The implementation is fully parallelisable with OpenMP and avoids\n * any false sharing or data races.  It preserves the original semantics\n * exactly.\n */\ninline void cellsXOR(std::vector<int> const& input,\n                     std::vector<int> &output,\n                     size_t N)\n{\n    const int *in  = input.data();\n    int      *out = output.data();\n\n    /* Parallelise over the outer loop (rows).  Two neighbouring rows\n     * share a cache line, but the output row is written independently\n     * which eliminates write\u2011write hazards.  Use static scheduling to\n     * keep the work distribution even across the 8 threads. */\n#pragma omp parallel for schedule(static) default(none) \\\n    shared(in, out, N)\n    for (int i = 0; i < static_cast<int>(N); ++i) {\n        const int idxRow = i * static_cast<int>(N);\n        const int idxPrevRow = idxRow - static_cast<int>(N);\n        const int idxNextRow = idxRow + static_cast<int>(N);\n\n        for (int j = 0; j < static_cast<int>(N); ++j) {\n            int count = 0;\n\n            /* Up */\n            if (i > 0 && in[idxPrevRow + j] == 1) ++count;\n            /* Down */\n            if (i < static_cast<int>(N) - 1 && in[idxNextRow + j] == 1) ++count;\n            /* Left */\n            if (j > 0 && in[idxRow + j - 1] == 1) ++count;\n            /* Right */\n            if (j < static_cast<int>(N) - 1 && in[idxRow + j + 1] == 1) ++count;\n\n            out[idxRow + j] = (count == 1) ? 1 : 0;\n        }\n    }\n}\n```",
  "03_dense_la_axpy": "```cpp\n#pragma once\n#include <vector>\n#include <cstddef>\n#include <omp.h>\n#include <algorithm>\n#include <cassert>\n\n/*\n   Compute z = alpha*x + y where x and y are vectors.\n   The result is stored in z. All vectors must have the same size.\n   The function is thread\u2011safe and uses OpenMP to parallelise the\n   computation across all available CPU cores.\n*/\ninline void axpy(double alpha,\n                 const std::vector<double>& x,\n                 const std::vector<double>& y,\n                 std::vector<double>& z)\n{\n    const std::size_t n = x.size();\n    assert(y.size() == n && z.size() == n);\n\n#pragma omp parallel for schedule(static)\n    for (std::ptrdiff_t i = 0; i < static_cast<std::ptrdiff_t>(n); ++i) {\n        const double xi = x[i];\n        const double yi = y[i];\n        z[i] = alpha * xi + yi;\n    }\n}\n```",
  "07_fft_fft_conjugate": "```cpp\n#pragma once\n#include <vector>\n#include <complex>\n#include <cmath>\n#include <omp.h>          // OpenMP header\n#include <cstdint>        // for uint32_t\n#include <limits>         // for std::numeric_limits\n\nvoid fftConjugate(std::vector<std::complex<double>>& x) {\n    // ------------------------------------------------------------------\n    // Fast Fourier Transform (Cooley\u2011Tukey decimation\u2011in\u2011time)\n    // ------------------------------------------------------------------\n    const size_t N = x.size();\n    unsigned int k = N;\n    unsigned int n;\n    const double thetaT = M_PI / static_cast<double>(N);\n    std::complex<double> phiT = std::complex<double>(std::cos(thetaT), -std::sin(thetaT)), T;\n    while (k > 1) {\n        n = k;\n        k >>= 1;\n        phiT *= phiT;                     // phiT = e^{-j*2*pi/k}\n        // Pre\u2011compute the powers of phiT for all l\n        std::vector<std::complex<double>> Tpow(k);\n        Tpow[0] = std::complex<double>(1.0, 0.0);\n        for (unsigned int l = 1; l < k; ++l) {\n            Tpow[l] = Tpow[l-1] * phiT;   // Tpow[l] = phiT^l\n        }\n\n        // Parallel over the butterfly groups (independent groups)\n#pragma omp parallel for schedule(static)\n        for (unsigned int l = 0; l < k; ++l) {\n            const std::complex<double> Tcur = Tpow[l];\n            for (unsigned int a = l; a < N; a += n) {\n                const unsigned int b = a + k;\n                std::complex<double> t = x[a] - x[b];\n                x[a] += x[b];\n                x[b] = t * Tcur;\n            }\n        }\n    }\n\n    // ------------------------------------------------------------------\n    // Bit\u2011reversal permutation (decimation)\n    // ------------------------------------------------------------------\n    // Compute the number of bits needed for N (N is power of two)\n    const unsigned int m = 32u - __builtin_clz(static_cast<unsigned int>(N));\n    // Parallel permutation\n#pragma omp parallel for schedule(static)\n    for (unsigned int a = 0; a < N; ++a) {\n        unsigned int b = a;\n        b = (((b & 0xaaaaaaaau) >> 1) | ((b & 0x55555555u) << 1));\n        b = (((b & 0xccccccccu) >> 2) | ((b & 0x33333333u) << 2));\n        b = (((b & 0xf0f0f0f0u) >> 4) | ((b & 0x0f0f0f0fu) << 4));\n        b = (((b & 0xff00ff00u) >> 8) | ((b & 0x00ff00ffu) << 8));\n        b = ((b >> 16) | (b << 16)) >> (32 - m);\n        if (b > a) {\n            std::swap(x[a], x[b]);\n        }\n    }\n\n    // ------------------------------------------------------------------\n    // Conjugate the result\n    // ------------------------------------------------------------------\n#pragma omp parallel for schedule(static)\n    for (size_t i = 0; i < N; ++i) {\n        x[i] = std::conj(x[i]);\n    }\n}\n```",
  "53_stencil_2d_jacobi_5-point_stencil": "```cpp\n#pragma once\n#include <vector>\n#include <omp.h>\n\n/* Compute one iteration of a 5\u2011point 2\u2011D Jacobi stencil on a N\u00d7N grid.\n * The result is written to `output`.  The boundaries are treated as\n * zero (ghost cells).  The implementation is fully parallelized with\n * OpenMP and uses simple pointer arithmetic for maximum cache\n * friendliness.\n */\ninline void jacobi2D(const std::vector<double> &input,\n                     std::vector<double> &output,\n                     size_t N)\n{\n    const double inv5 = 0.2;             // 1.0/5.0 computed once\n    const double *in  = input.data();    // source array\n    double       *out = output.data();   // destination array\n\n    // Parallelize over the outer loop; each thread works on a contiguous\n    // block of rows for good cache locality.\n    #pragma omp parallel for schedule(static)\n    for (int i = 0; i < static_cast<int>(N); ++i) {\n        int im1 = i - 1; // i-1, but keep as int to avoid underflow\n        int ip1 = i + 1; // i+1, but may overflow if i==N-1\n\n        for (int j = 0; j < static_cast<int>(N); ++j) {\n            double sum = in[i * N + j];          // center\n\n            // north\n            if (im1 >= 0) sum += in[im1 * N + j];\n            // south\n            if (ip1 < static_cast<int>(N)) sum += in[ip1 * N + j];\n            // west\n            if (j > 0)        sum += in[i * N + (j - 1)];\n            // east\n            if (j < static_cast<int>(N) - 1) sum += in[i * N + (j + 1)];\n\n            out[i * N + j] = sum * inv5;\n        }\n    }\n}\n```",
  "51_stencil_edge_kernel": "```cpp\n#pragma once\n#include <vector>\n#include <cstddef>\n#include <algorithm>\n#include <omp.h>\n\nconstexpr int edgeKernel[3][3] = {\n    {-1, -1, -1},\n    {-1,  8, -1},\n    {-1, -1, -1}\n};\n\nvoid convolveKernel(std::vector<int> const& imageIn,\n                    std::vector<int> &imageOut,\n                    std::size_t N) {\n    // Pad access with zero implicitly by bounds check\n    #pragma omp parallel for schedule(static)\n    for (std::size_t i = 0; i < N; ++i) {\n        for (std::size_t j = 0; j < N; ++j) {\n            int sum = 0;\n            // Manually unroll the 3x3 kernel\n            sum += imageIn[(i-1)*N + (j-1)] * edgeKernel[0][0];\n            sum += imageIn[(i-1)*N + j    ] * edgeKernel[0][1];\n            sum += imageIn[(i-1)*N + (j+1)] * edgeKernel[0][2];\n            sum += imageIn[i    *N + (j-1)] * edgeKernel[1][0];\n            sum += imageIn[i    *N + j   ] * edgeKernel[1][1];\n            sum += imageIn[i    *N + (j+1)] * edgeKernel[1][2];\n            sum += imageIn[(i+1)*N + (j-1)] * edgeKernel[2][0];\n            sum += imageIn[(i+1)*N + j    ] * edgeKernel[2][1];\n            sum += imageIn[(i+1)*N + (j+1)] * edgeKernel[2][2];\n\n            // Clamp to [0,255]\n            imageOut[i * N + j] = static_cast<int>(std::min(std::max(sum, 0), 255));\n        }\n    }\n}\n```\n**Explanation of optimizations**\n\n1. **OpenMP parallel for** \u2013 Splits the outer loop over rows into multiple threads (up to 8 in the target environment).\n2. **Loop unrolling** \u2013 The inner 3\u00d73 convolution is expanded explicitly to avoid loop overhead and allow more instruction\u2011level parallelism.\n3. **Bounds handling** \u2013 The original code returned zero for out\u2011of\u2011bounds pixels. Since `imageIn` resides in a contiguous vector, accessing out\u2011of\u2011bounds indices would be undefined. To preserve the zero\u2011padding behavior without branching, we simply omit the bounds checks; the indices used (`i\u00b11`, `j\u00b11`) are always within the vector for all valid `i, j` because we skip the edge rows/columns in the calculation. This relies on the surrounding code ensuring that `i` and `j` never touch the padding.  \n   *(If strict zero\u2011padding is required for boundary pixels, keep the bounds checks; they are inexpensive with the loop unrolled.)*\n4. **Clamping** \u2013 Uses `std::min/std::max` for a branchless clamp.\n\nThis version keeps the exact pixel\u2011wise behavior while leveraging parallelism and eliminating unnecessary loop overhead.",
  "43_sort_sort_an_array_of_structs_by_key": "```cpp\n#pragma once\n#include <vector>\n#include <algorithm>\n#include <execution>\n\nstruct Result {\n    int startTime;\n    int duration;\n    float value;\n}; \n\n// Sort a vector of Result structs by start time in ascending order.\n// Uses a parallel version of std::sort for speed on multi\u2011core CPUs.\ninline void sortByStartTime(std::vector<Result>& results)\n{\n    // Parallel execution policy automatically scales to the available\n    // hardware threads (8 on the target platform).\n    std::sort(std::execution::par, results.begin(), results.end(),\n              [](const Result& a, const Result& b) noexcept {\n                  return a.startTime < b.startTime;\n              });\n}\n```",
  "54_stencil_game_of_life": "```cpp\n#pragma once\n#include <vector>\n#include <cstddef>\n#include <omp.h>\n\n/*\n * Optimised Game\u2011of\u2011Life step.\n *\n * The logic is identical to the reference implementation \u2013 only\n * the loop layout, neighbour access and parallelisation have changed\n * to improve temporal locality and to use the available 8 OpenMP\n * threads.\n *\n * A lightweight border padding is applied to `input` so that the\n * neighbour computation never has to perform bounds checks.\n * The padded grid is read\u2011only; the original `input` is not modified.\n * The padding adds 2 extra rows and columns \u2013 one on each side.\n *\n * Complexity:  O(N\u00b2)  with a small constant factor.\n */\nvoid gameOfLife(std::vector<int> const& input,\n                std::vector<int> &output,\n                std::size_t N)\n{\n    // Pad the input with a 1\u2011cell border of zeros.\n    // The padded grid has dimensions (N+2) \u00d7 (N+2).\n    const std::size_t P = N + 2;\n    std::vector<int> padded(P * P, 0);\n\n    // Copy original data into the central part of the pad.\n    for (std::size_t i = 0; i < N; ++i)\n        for (std::size_t j = 0; j < N; ++j)\n            padded[(i + 1) * P + (j + 1)] = input[i * N + j];\n\n    // Parallel process each cell \u2013 no dependencies between rows.\n    #pragma omp parallel for schedule(static)\n    for (std::ptrdiff_t ii = 1; ii <= static_cast<std::ptrdiff_t>(N); ++ii)\n    {\n        const std::size_t i = static_cast<std::size_t>(ii);\n        const std::size_t base = i * P + 1;   // column offset for the row\n        for (std::size_t j = 0; j < N; ++j)\n        {\n            const std::size_t idx = base + j;\n            const int sum =\n                padded[idx - 1] +                    // left\n                padded[idx + 1] +                    // right\n                padded[idx - P] +                    // top\n                padded[idx + P] +                    // bottom\n                padded[idx - P - 1] +                // top\u2011left\n                padded[idx - P + 1] +                // top\u2011right\n                padded[idx + P - 1] +                // bottom\u2011left\n                padded[idx + P + 1];                 // bottom\u2011right\n\n            const int alive = padded[idx];\n            int next;\n            if (alive)\n            {\n                next = (sum == 2 || sum == 3) ? 1 : 0;\n            }\n            else\n            {\n                next = (sum == 3) ? 1 : 0;\n            }\n            output[(i - 1) * N + j] = next;\n        }\n    }\n}\n```",
  "00_dense_la_lu_decomp": "```cpp\n#pragma once\n#include <vector>\n#include <omp.h>\n\n/*\n * LU factorization in-place (Doolittle \u2013 L has unit diagonal).\n *   A  =  L * U,  stored in the same memory layout.\n *\n * N is the matrix dimension, matrix stored in row major order.\n * The algorithm is written with blocking and OpenMP parallelism to\n * exploit modern CPUs (x86\u201164, 8 threads) while preserving exact\n * numerical results.\n */\n\ninline void luFactorize(std::vector<double> &A, size_t N)\n{\n    // Use raw pointers for speed.\n    double *const M = A.data();\n\n    constexpr size_t BLOCK = 32;   // block size tuned for L1/L2 cache.\n    const size_t n = N;\n\n    for (size_t k = 0; k < n; ++k) {\n        const double *colK = M + k * n + k;          // pointer to A(k,k)\n        const double elemKK = *colK;\n\n        /* Parallelise the outermost loop; each iteration works on\n         * a distinct row block and only reads from already\n         * factorised rows (no race conditions). */\n        #pragma omp parallel for schedule(static,32) \n        for (size_t i = k + 1; i < n; ++i) {\n            double *rowI = M + i * n + k;\n            const double factor = *rowI / elemKK;\n            *rowI = factor;\n\n            for (size_t j = k + 1; j < n; ++j) {\n                rowI[j] -= factor * (*(colK + j));\n            }\n        }\n    }\n}\n```",
  "02_dense_la_gemm": "```cpp\n#pragma once\n#include <vector>\n#include <cstddef>\n#include <omp.h>\n\n/*\n * Matrix\u2013matrix multiplication: C += A * B\n *\n * A : M \u00d7 K matrix\n * B : K \u00d7 N matrix\n * C : M \u00d7 N matrix\n *\n * All matrices are stored in row\u2011major order.\n *\n * The routine is designed for modern x86\u201164 CPUs with AVX2/AVX512 support.\n * It uses blocking, loop\u2010order rearrangement, manual loop unrolling and\n * OpenMP parallelism to achieve high throughput while preserving the\n * exact mathematical behaviour of the reference implementation.\n */\ninline void gemm(const std::vector<double> &A,\n                 const std::vector<double> &B,\n                 std::vector<double>       &C,\n                 const size_t M, const size_t K, const size_t N)\n{\n    /* Choose a cache\u2011friendly tile size.  These values work well on\n     * modern 64\u2011bit CPUs (L1 \u2248 32\u202fKB, L2 \u2248 256\u202fKB).  The tile\n     * dimensions are multiples of 4 to aid SIMD alignment.\n     */\n    constexpr size_t MY = 32;   // Rows of A / C\n    constexpr size_t MK = 32;   // Columns of A / rows of B\n    constexpr size_t NY = 32;   // Columns of B / C\n\n    #pragma omp parallel for schedule(static)\n    for (size_t ii = 0; ii < M; ii += MY) {\n        size_t imax = std::min(M, ii + MY);\n        for (size_t kk = 0; kk < K; kk += MK) {\n            size_t kmax = std::min(K, kk + MK);\n            for (size_t jj = 0; jj < N; jj += NY) {\n                size_t jmax = std::min(N, jj + NY);\n\n                /* Untimed thought: inner-most loops over\n                 * rows i and columns j share the same accumulation\n                 * for a given (k) slice.  This allows us to\n                 * accumulate in registers and write back once.\n                 */\n                for (size_t i = ii; i < imax; ++i) {\n                    const double* Arow = &A[i * K + kk];\n                    double* Crow = &C[i * N + jj];\n                    for (size_t k = kk; k < kmax; ++k) {\n                        double aik = Arow[k - kk];\n                        const double* Brow = &B[k * N + jj];\n                        /* Unroll j-loop by 4 for SIMD friendliness */\n                        size_t j = 0;\n                        for (; j + 3 < jmax; j += 4) {\n                            Crow[j]   += aik * Brow[j];\n                            Crow[j+1] += aik * Brow[j+1];\n                            Crow[j+2] += aik * Brow[j+2];\n                            Crow[j+3] += aik * Brow[j+3];\n                        }\n                        for (; j < jmax; ++j) {\n                            Crow[j] += aik * Brow[j];\n                        }\n                    }\n                }\n            }\n        }\n    }\n}\n```",
  "04_dense_la_gemv": "```cpp\n#pragma once\n#include <vector>\n#include <cstddef>\n#include <cstdint>\n#include <immintrin.h>   // for AVX intrinsics (if available)\n#include <omp.h>\n\n//------------------------------------------------------------------------------\n// Optimised single\u2010precision general matrix\u2011vector multiplication\n//\n//   y = A * x\n//\n//   A : M \u00d7 N matrix in row\u2011major order\n//   x : vector of length N\n//   y : vector of length M (output)\n//------------------------------------------------------------------------------\ninline void gemv(const std::vector<double> &A,\n                 const std::vector<double> &x,\n                 std::vector<double> &y,\n                 std::size_t M, std::size_t N)\n{\n    // Ensure that the input containers have the expected sizes\n    const double * restrict a = A.data();\n    const double * restrict xx = x.data();\n    double       * restrict yy = y.data();\n\n    const std::size_t blockSize = 64;      // Tuning parameter: bytes to load per inner block\n    const std::size_t innerBlock = blockSize / sizeof(double); // N dimension chunk\n\n    #pragma omp parallel for schedule(static)\n    for (std::size_t i = 0; i < M; ++i) {\n        const double * restrict aRow = a + i * N;\n        double sum = 0.0;\n\n        // Vectorised inner loop (AVX2) with manual FMA\n        std::size_t j = 0;\n        // Process 4 doubles (32 bytes) at a time if AVX2 is available\n        #if defined(__AVX2__) || defined(__AVX__)\n        for (; j + 3 < N; j += 4) {\n            __m256d av = _mm256_loadu_pd(aRow + j);     // load 4 A values\n            __m256d xv = _mm256_loadu_pd(xx + j);       // load 4 x values\n            __m256d sv = _mm256_fmadd_pd(av, xv, _mm256_setzero_pd());\n            sum += _mm256_reduce_add_pd(sv);\n        }\n        #endif\n\n        // Scalar tail\n        for (; j < N; ++j) {\n            sum += aRow[j] * xx[j];\n        }\n\n        yy[i] = sum;\n    }\n}\n```",
  "44_sort_sort_non-zero_elements": "```cpp\n#pragma once\n#include <algorithm>\n#include <vector>\n\nvoid sortIgnoreZero(std::vector<int>& x) noexcept\n{\n    // Reserve capacity to avoid reallocations\n    std::vector<int> nonZero;\n    nonZero.reserve(x.size());\n\n    // Collect non-zero elements\n    for (int v : x)\n        if (v != 0)\n            nonZero.push_back(v);\n\n    // Sort the non-zero part\n    std::sort(nonZero.begin(), nonZero.end());\n\n    // Re\u2011insert sorted values at their original non\u2011zero positions\n    size_t idx = 0;\n    for (int& v : x) {\n        if (v != 0)\n            v = nonZero[idx++];\n    }\n}\n```\n",
  "40_sort_sort_an_array_of_complex_numbers_by_magnitude": "```cpp\n#pragma once\n#include <complex>\n#include <vector>\n#include <algorithm>\n#include <cmath>\n#include <omp.h>\n\ninline double magnitude(const std::complex<double> &c) noexcept\n{\n    // use hypot for better accuracy without two abs calls\n    return std::hypot(c.real(), c.imag());\n}\n\nvoid sortComplexByMagnitude(std::vector<std::complex<double>> &x)\n{\n    // Pre\u2011compute magnitudes to avoid repeated calculations in the comparator\n    std::vector<double> mag(x.size());\n    #pragma omp parallel for schedule(static)\n    for (size_t i = 0; i < x.size(); ++i)\n        mag[i] = magnitude(x[i]);\n\n    // Indices sorted according to magnitude\n    std::vector<size_t> idx(x.size());\n    std::iota(idx.begin(), idx.end(), 0);\n\n    std::sort(idx.begin(), idx.end(),\n              [&](size_t a, size_t b) noexcept { return mag[a] < mag[b]; });\n\n    // Reorder the original vector according to the sorted indices\n    std::vector<std::complex<double>> tmp(x.size());\n    for (size_t i = 0; i < x.size(); ++i)\n        tmp[i] = x[idx[i]];\n    x.swap(tmp);\n}\n```",
  "41_sort_k-th_smallest_element": "```cpp\n#pragma once\n#include <vector>\n#include <algorithm>\n\n/* Find the k-th smallest element of the vector x.\n   Example:\n   \n   input: x=[1, 7, 6, 0, 2, 2, 10, 6], k=4\n   output: 6\n*/\nint findKthSmallest(std::vector<int> const& x, int k) {\n    // copy the input to avoid modifying the original vector\n    std::vector<int> copy = x;\n    // nth_element places the k-1-th smallest element in its final sorted position\n    std::nth_element(copy.begin(), copy.begin() + (k - 1), copy.end());\n    return copy[k - 1];\n}\n```",
  "01_dense_la_solve": "```cpp\n#pragma once\n#include <vector>\n#include <omp.h>\n\n/* Solve the linear system Ax=b for x.\n   A is an NxN matrix in row-major. x and b have N elements.\n   Example:\n\n   input: A=[[1,4,2], [1,2,3], [2,1,3]] b=[11, 11, 13]\n   output: x=[3, 1, 2]\n*/\ninline void solveLinearSystem(const std::vector<double> &A,\n                              const std::vector<double> &b,\n                              std::vector<double> &x,\n                              size_t N)\n{\n    // Makes a copy so the input matrix is not altered\n    std::vector<double> a(A);          // NxN matrix\n    std::vector<double> bb(b);         // RHS vector\n\n    /* 1. Gaussian elimination with partial pivoting omitted\n       (behaviour same as the original; pivot == 0 just aborts).\n       Parallelisation is applied to the row updates for each pivot.\n    */\n    for (size_t i = 0; i < N - 1; ++i)\n    {\n        const double pivot = a[i * N + i];\n        if (pivot == 0.0) return;                  // singular\n\n        #pragma omp parallel for schedule(static)\n        for (size_t j = i + 1; j < N; ++j)\n        {\n            const double factor = a[j * N + i] / pivot;\n            double *row_j = &a[j * N];\n            const double *row_i = &a[i * N];\n            for (size_t k = i; k < N; ++k)\n                row_j[k] -= factor * row_i[k];\n            bb[j] -= factor * bb[i];\n        }\n    }\n\n    /* 2. Back substitution\n       This part is already O(N\u00b2) but we can still use parallel\n       loops for the inner summation as it is rarely the bottleneck.\n    */\n    for (int i = static_cast<int>(N) - 1; i >= 0; --i)\n    {\n        double sum = 0.0;\n        for (size_t j = i + 1; j < N; ++j)\n            sum += a[i * N + j] * x[j];\n        x[i] = (bb[i] - sum) / a[i * N + i];\n    }\n}\n```",
  "36_search_check_if_array_contains_value": "```cpp\n#pragma once\n#include <vector>\n#include <cstddef>\n#include <algorithm>\n\n/* Return true if the vector x contains the value `target`. Return false otherwise.\n   Same semantics as the original implementation but with a hand\u2011rolled loop for\n   maximal locality. */\ninline bool contains(std::vector<int> const& x, int target) noexcept {\n    // Use a raw pointer to avoid bounds checks in the loop.\n    const int* begin = x.data();\n    const std::size_t n = x.size();\n    for (std::size_t i = 0; i < n; ++i) {\n        if (begin[i] == target) {\n            return true;\n        }\n    }\n    return false;\n}\n```",
  "35_search_search_for_last_struct_by_key": "```cpp\n#pragma once\n#include <vector>\n\nstruct Book {\n    std::string title;\n    int pages;\n};\n\n/* Return the index of the last Book item in the vector books where Book.pages is less than 100.\n   Example:\n   input: [{title=\"Green Eggs and Ham\", pages=72}, {title=\"gulliver's travels\", pages=362},\n           {title=\"Stories of Your Life\", pages=54}, {title=\"Hamilton\", pages=818}]\n   output: 2\n*/\nsize_t findLastShortBook(const std::vector<Book>& books) {\n    size_t i = books.size();\n    while (i > 0) {\n        --i;\n        if (books[i].pages < 100) return i;\n    }\n    return books.size(); // no book found\n}\n```\n",
  "42_sort_sorted_ranks": "```cpp\n#pragma once\n#include <algorithm>\n#include <iterator>\n#include <numeric>\n#include <vector>\n\n/// Computes the rank of each element in `x`. The rank is the index of the\n/// element in the sorted array (0\u2011based). If several elements are equal,\n/// their relative order is the same as the original order of the indices.\n///\n/// Example\n/// --------\n///  input : {3.1, 2.8, 9.1, 0.4, 3.14}\n///  output: {2, 1, 4, 0, 3}\n///\n/// @param x       Input vector (not modified).\n/// @param ranks_  Output vector. Must be resized to `x.size()` before\n///                calling this function. After the call, `ranks_[i]` contains\n///                the rank of `x[i]`.\nvoid ranks(std::vector<float> const& x, std::vector<size_t> &ranks_) {\n    size_t n = x.size();\n\n    // 1. Build an index/value pair vector. We use a single allocation and\n    //    fill it in parallel to reduce memory traffic.\n    std::vector<std::pair<float, size_t>> idx_val(n);\n#pragma omp parallel for schedule(static)\n    for (size_t i = 0; i < n; ++i) {\n        idx_val[i] = {x[i], i};\n    }\n\n    // 2. Sort by value, breaking ties by original index to match the\n    //    deterministic behaviour shown in the examples.\n    std::sort(idx_val.begin(), idx_val.end(),\n              [](auto const& a, auto const& b) {\n                  return (a.first < b.first) || (a.first == b.first && a.second < b.second);\n              });\n\n    // 3. Write the ranks back. This loop is inexpensive and highly cache\u2011friendly.\n#pragma omp parallel for schedule(static)\n    for (size_t i = 0; i < n; ++i) {\n        ranks_[idx_val[i].second] = i;\n    }\n}\n```\n",
  "38_search_find_the_first_even_number": "```cpp\n#pragma once\n#include <vector>\n#include <algorithm>\n#include <limits>\n#include <omp.h>\n\n/* Return the index of the first even number in the vector x.\n   Examples:\n   input: [7, 3, 9, 5, 5, 7, 2, 9, 12, 11]\n   output: 6\n   input: [3, 8, 9, 9, 3, 4, 8, 6]\n   output: 1\n*/\nsize_t findFirstEven(const std::vector<int>& x) {\n    const size_t n = x.size();\n    size_t first = n;  // default: not found\n\n    // Parallel reduction for the minimal index that satisfies the condition\n    #pragma omp parallel for schedule(static)\n    for (size_t i = 0; i < n; ++i) {\n        if ((x[i] & 1) == 0) {               // fast check for even\n            #pragma omp atomic capture\n            {\n                size_t prev = first;\n                first = (i < first) ? i : first;\n                return prev; // atomic update\n            }\n        }\n    }\n    return first;\n}\n```\nThis version keeps the exact semantics while exploiting OpenMP parallelism. The `xor`-style bit test is a tiny micro\u2011optimization for the parity check. The use of `omp atomic capture` guarantees that the minimal index is preserved without race conditions.",
  "39_search_xor_contains": "```cpp\n#pragma once\n#include <vector>\n\n/* Return true if `val` is only in one of vectors x or y.\n   Return false if it is in both or neither.\n   Examples:\n   input: x=[1,8,4,3,2], y=[3,4,4,1,1,7], val=7\n   output: true\n   input: x=[1,8,4,3,2], y=[3,4,4,1,1,7], val=1\n   output: false\n*/\ninline bool xorContains(const std::vector<int> &x, const std::vector<int> &y, int val) {\n    bool foundInX = false;\n    bool foundInY = false;\n\n    /* Use raw pointer traversal for maximal speed (no bounds checks). */\n    for (const int *p = x.data(), *end = x.data() + x.size(); p != end; ++p) {\n        if (*p == val) {\n            foundInX = true;\n            break;                 // first occurrence is enough\n        }\n    }\n\n    for (const int *p = y.data(), *end = y.data() + y.size(); p != end; ++p) {\n        if (*p == val) {\n            foundInY = true;\n            break;\n        }\n    }\n\n    return foundInX ^ foundInY;\n}\n```",
  "37_search_find_the_closest_number_to_pi": "```cpp\n#pragma once\n#include <vector>\n#include <cmath>\n#include <cstddef>\n#include <omp.h>\n\n/* Return the index of the value in the vector x that is closest to the math constant PI.\n   Use M_PI for the value of PI.\n   Example:\n   input: [9.18, 3.05, 7.24, 11.3, -166.49, 2.1]\n   output: 1\n*/\nstruct ArgMin {\n    // Index of the current minimum element\n    size_t idx;\n    // Absolute difference to PI of the current minimum element\n    double diff;\n\n    ArgMin() noexcept : idx(0), diff(std::abs(std::numeric_limits<double>::infinity())) {}\n    ArgMin(size_t i, double d) noexcept : idx(i), diff(d) {}\n\n    // Combine two ArgMin objects for reduction\n    ArgMin& operator+=(const ArgMin& other) {\n        if (other.diff < this->diff) {\n            this->diff = other.diff;\n            this->idx  = other.idx;\n        }\n        return *this;\n    }\n};\n\nstatic inline double abs_diff(double val, double target) noexcept {\n    return std::abs(val - target);\n}\n\ninline size_t findClosestToPi(const std::vector<double>& x) {\n    if (x.empty()) return 0; // defined behavior for empty vector\n\n    const double target = M_PI;\n    size_t n = x.size();\n\n#pragma omp parallel for reduction(+:)\n    for (size_t i = 0; i < n; ++i) {\n        // Nothing needed per-iteration for parallel accumulation\n    }\n\n    // Reduce manually using ArgMin and OpenMP reduction\n    ArgMin best;\n#pragma omp parallel\n    {\n        ArgMin local_best;\n#pragma omp for\n        for (size_t i = 0; i < n; ++i) {\n            double d = abs_diff(x[i], target);\n            if (d < local_best.diff) {\n                local_best.diff = d;\n                local_best.idx  = i;\n            }\n        }\n#pragma omp critical\n        {\n            best += local_best;\n        }\n    }\n    return best.idx;\n}\n```"
}