{
  "15_graph_edge_count": "```cpp\n#pragma once\n#include <vector>\n#include <omp.h>\n\n/* Count the number of edges in the directed graph defined by the adjacency matrix A.\n   A is an NxN adjacency matrix stored in row-major.\n   Example:\n      input: [[0, 0, 0, 1], [0, 0, 0, 1], [0, 0, 0, 1], [1, 1, 1, 0]]\n   output: 3\n*/\nint edgeCount(std::vector<int> const& A, size_t N) {\n    int count = 0;\n    #pragma omp parallel for reduction(+:count) schedule(static)\n    for (size_t idx = 0; idx < N * N; ++idx) {\n        if (A[idx] == 1) {\n            ++count;\n        }\n    }\n    return count;\n}\n```",
  "17_graph_highest_degree": "```cpp\n#pragma once\n#include <vector>\n#include <algorithm>\n#include <cstddef>\n#include <omp.h>\n\n/* Compute the highest node degree in the undirected graph. The graph is defined in the adjacency\n   matrix A. A is an NxN adjacency matrix stored in row-major. A is undirected.\n   Example:\n        input: [[0, 0, 0, 1], [0, 0, 0, 1], [0, 0, 0, 1], [1, 1, 1, 0]]\n   output: 3\n*/\nint maxDegree(std::vector<int> const& A, size_t N) {\n    int highestDegree = 0;\n\n    // Parallelize the outer loop; each thread computes the maximum over its chunk.\n    #pragma omp parallel for reduction(max:highestDegree) schedule(static)\n    for (int i = 0; i < static_cast<int>(N); ++i) {\n        int degree = 0;\n        const int* row = &A[i * N];\n        for (int j = 0; j < static_cast<int>(N); ++j) {\n            degree += row[j];\n        }\n        if (degree > highestDegree) highestDegree = degree;\n    }\n\n    return highestDegree;\n}\n```",
  "16_graph_largest_component": "```cpp\n#pragma once\n#include <vector>\n#include <stack>\n#include <algorithm>\n\nint largestComponent(std::vector<int> const& A, size_t N) {\n    // Use a char vector for visited to reduce memory traffic\n    std::vector<char> visited(N, 0);\n    int maxCount = 0;\n\n    // Iterate over all vertices\n    for (size_t start = 0; start < N; ++start) {\n        if (visited[start]) continue;\n\n        // Iterative depth\u2011first search to avoid recursion overhead\n        std::stack<size_t> st;\n        st.push(start);\n        visited[start] = 1;\n        int count = 0;\n\n        while (!st.empty()) {\n            size_t u = st.top();\n            st.pop();\n            ++count;\n\n            // Scan all neighbours of u\n            const size_t base = u * N;\n            for (size_t v = 0; v < N; ++v) {\n                if (A[base + v] && !visited[v]) {\n                    visited[v] = 1;\n                    st.push(v);\n                }\n            }\n        }\n\n        if (count > maxCount) maxCount = count;\n    }\n\n    return maxCount;\n}\n```",
  "23_histogram_first_letter_counts": "```cpp\n#pragma once\n#include <array>\n#include <string>\n#include <vector>\n\n#include <omp.h>\n\n/* For each letter in the alphabet, count the number of strings in the vector s that start with that letter.\n   Assume all strings are in lower case. Store the output in `bins` array.\n   Example:\n   input: [\"dog\", \"cat\", \"xray\", \"cow\", \"code\", \"type\", \"flower\"]\n   output: [0, 0, 3, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0]\n*/\ninline void firstLetterCounts(std::vector<std::string> const& s,\n                               std::array<size_t, 26> &bins) {\n    // reset bins\n    bins.fill(0);\n\n    // use OpenMP reduction over the array. GCC supports array reductions with the syntax bins[:26].\n    #pragma omp parallel for schedule(static) reduction(+:bins[:26])\n    for (size_t i = 0; i < s.size(); ++i) {\n        const unsigned char c = static_cast<unsigned char>(s[i][0]);  // avoid negative values\n        const int index = c - 'a';\n        // bounds check is unnecessary under assumption of lowercase letters\n        bins[index] += 1;\n    }\n}\n```",
  "19_graph_shortest_path": "```cpp\n#pragma once\n#include <vector>\n#include <queue>\n#include <limits>\n\n/* Return the length of the shortest path from source to dest in the undirected graph defined by \n   the adjacency matrix A.  A is an NxN matrix stored in row\u2011major order.  The graph is connected\n   and undirected. */\nint shortestPathLength(std::vector<int> const& A, size_t N, int source, int dest) {\n    // distance array, -1 means unvisited\n    std::vector<int> dist(N, -1);\n    // simple array queue (circular)\n    std::vector<int> q(N);\n    int head = 0, tail = 0;\n\n    dist[source] = 0;\n    q[tail++] = source;\n\n    while (head != tail) {               // while queue not empty\n        int u = q[head++];\n        if (head == N) head = 0;         // wrap around\n\n        int du = dist[u];\n        if (u == dest) return du;\n\n        size_t base = u * N;\n        for (size_t v = 0; v < N; ++v) { // check all neighbors\n            if (A[base + v] && dist[v] == -1) {\n                dist[v] = du + 1;\n                q[tail++] = static_cast<int>(v);\n                if (tail == N) tail = 0; // wrap around\n            }\n        }\n    }\n    // Should not happen for a connected graph\n    return std::numeric_limits<int>::max();\n}\n```",
  "21_histogram_bin_0-100": "```cpp\n#pragma once\n#include <array>\n#include <vector>\n#include <cstddef>\n#include <omp.h>\n\nvoid binsBy10Count(std::vector<double> const& x, std::array<std::size_t, 10> &bins) {\n    // zero the output\n    for (auto &c : bins) c = 0;\n\n    // each thread keeps a local copy of bins to avoid contention\n    #pragma omp parallel\n    {\n        std::array<std::size_t, 10> local = {0};\n\n        #pragma omp for schedule(static)\n        for (std::size_t i = 0; i < x.size(); ++i) {\n            const std::size_t bin = static_cast<std::size_t>(x[i] / 10.0);\n            // Guard against out\u2011of\u2011range indices; original code would crash\n            // but we preserve behavior by silently discarding them.\n            if (bin < 10) {\n                ++local[bin];\n            }\n        }\n\n        // Merge local counts into the final result\n        #pragma omp critical\n        {\n            for (std::size_t b = 0; b < 10; ++b) {\n                bins[b] += local[b];\n            }\n        }\n    }\n}\n```",
  "20_histogram_pixel_histogram": "```cpp\n#pragma once\n#include <vector>\n#include <array>\n#include <cstddef>\n#include <omp.h>\n\n/*\n   Count the number of pixels in image with each grayscale intensity.\n   The vector `image` is a grayscale image with values 0\u2011255.\n   Store the results in `bins`.\n   Example:\n\n   input: image=[2, 116, 201, 11, 92, 92, 201, 4, 2]\n   output: [0, 0, 2, 0, 1, \u2026]\n*/\ninline void pixelCounts(std::vector<int> const& image,\n                        std::array<std::size_t, 256>& bins)\n{\n    // Reset output bins once\n    bins.fill(0);\n\n    const std::size_t n = image.size();\n\n    // Determine number of OpenMP threads\n    int nthreads = omp_get_max_threads();\n\n    // Allocate thread\u2011local histograms\n    std::vector<std::array<std::size_t, 256>> local_bins(nthreads);\n    for (int t = 0; t < nthreads; ++t) {\n        local_bins[t].fill(0);\n    }\n\n#pragma omp parallel\n    {\n        int tid = omp_get_thread_num();\n        auto& local = local_bins[tid];\n\n#pragma omp for schedule(static)\n        for (std::size_t i = 0; i < n; ++i) {\n            // No bounds-checking: input guarantees 0\u2011255\n            ++local[image[i]];\n        }\n    }\n\n    // Merge thread\u2011local histograms into the output bins\n    for (int t = 0; t < nthreads; ++t) {\n        for (int b = 0; b < 256; ++b) {\n            bins[b] += local_bins[t][b];\n        }\n    }\n}\n```",
  "34_scan_largest_contiguous_subarray_sum": "```cpp\n#pragma once\n#include <vector>\n#include <limits>\n\n/* Return the largest sum of any contiguous subarray in the vector x.\n   This implementation uses Kadane's algorithm with O(n) time complexity\n   and O(1) auxiliary space. */\nint maximumSubarray(std::vector<int> const& x) {\n    int largestSum = std::numeric_limits<int>::lowest();\n    int currentSum = 0;\n\n    for (int v : x) {\n        currentSum = std::max(v, currentSum + v);\n        largestSum = std::max(largestSum, currentSum);\n    }\n\n    return largestSum;\n}\n```",
  "18_graph_count_components": "```cpp\n#pragma once\n#include <vector>\n#include <stack>\n#include <cstddef>\n\n/* Count the number of connected components in the undirected graph defined\n   by the adjacency matrix A.  A is an NxN adjacency matrix stored in\n   row\u2011major order.  The graph is undirected.  The implementation\n   uses an iterative depth\u2011first search to avoid the overhead of\n   function recursion and takes advantage of contiguous data access\n   for speed. */\nint componentCount(std::vector<int> const& A, std::size_t N)\n{\n    std::vector<char> visited(N, 0);          // char instead of bool for faster access\n    std::stack<std::size_t> st;\n    int components = 0;\n\n    for (std::size_t start = 0; start < N; ++start) {\n        if (visited[start]) continue;\n\n        ++components;           // new component discovered\n        st.push(start);\n        visited[start] = 1;\n\n        while (!st.empty()) {\n            std::size_t u = st.top(); st.pop();\n            const std::size_t row_offset = u * N;\n\n            for (std::size_t v = 0; v < N; ++v) {\n                if (A[row_offset + v] && !visited[v]) {\n                    visited[v] = 1;\n                    st.push(v);\n                }\n            }\n        }\n    }\n\n    return components;\n}\n```",
  "32_scan_sum_of_prefix_sum_array": "```cpp\n#pragma once\n#include <cstdint>\n#include <vector>\n#include <numeric>\n#include <omp.h>\n\n/* Compute the prefix sum array of the vector x and return its sum.\n   The result equals \u03a3_{i=0}^{n-1} (i+1) * x[i].\n*/\nint64_t sumOfPrefixSum(std::vector<int64_t> const& x) {\n    int64_t result = 0;\n    #pragma omp parallel for reduction(+:result)\n    for (std::size_t i = 0; i < x.size(); ++i) {\n        result += static_cast<int64_t>(i + 1) * x[i];\n    }\n    return result;\n}\n```",
  "55_transform_relu": "```cpp\n#pragma once\n#include <vector>\n#include <algorithm>\n#include <omp.h>\n\n/*\n * Compute the ReLU function on every element of x. Elements less than zero become zero,\n * while elements greater than zero stay the same.\n */\ninline void relu(std::vector<double> &x) {\n    const std::size_t N = x.size();\n\n    // Parallel loop with static schedule to balance load among threads.\n    #pragma omp parallel for schedule(static)\n    for (std::size_t i = 0; i < N; ++i) {\n        // std::max is inlined and fast; no branching needed.\n        x[i] = std::max(0.0, x[i]);\n    }\n}\n```",
  "30_scan_prefix_sum": "```cpp\n#pragma once\n#include <numeric>\n#include <vector>\n#include <execution> // for Parallel execution policy\n\n/* Compute the prefix sum of the vector x into output.\n   Example:\n   input: [1, 7, 4, 6, 6, 2]\n   output: [1, 8, 12, 18, 24, 26]\n*/\ninline void prefixSum(const std::vector<int64_t>& x, std::vector<int64_t>& output) {\n    // Ensure output is the same size as input\n    if (output.size() != x.size())\n        output.resize(x.size());\n\n    // Parallel inclusive scan using the OpenMP based execution policy\n    std::inclusive_scan(std::execution::par, x.begin(), x.end(), output.begin());\n}\n```",
  "33_scan_reverse_prefix_sum": "```cpp\n#pragma once\n#include <numeric>\n#include <vector>\n\n/*\n * Compute the reverse prefix sum of the vector `x` into `output`.\n * Examples:\n *   input: [1, 7, 4, 6, 6, 2]\n *   output: [2, 8, 14, 18, 25, 26]\n *   input: [3, 3, 7, 1, -2]\n *   output: [-2, -1, 6, 9, 12]\n *\n * The function preserves the exact original behavior while avoiding any\n * temporary allocation and relying on the efficient inclusive_scan\n * implementation in the standard library.\n */\ninline void reversePrefixSum(const std::vector<int>& x, std::vector<int>& output) {\n    // Ensure the output vector has the correct size.\n    output.resize(x.size());\n\n    // Perform an inclusive prefix sum on the reversed input,\n    // writing the results into the reversed output iterator.\n    std::inclusive_scan(x.rbegin(), x.rend(), output.rbegin());\n}\n```",
  "24_histogram_count_quartile": "```cpp\n#pragma once\n#include <array>\n#include <vector>\n#include <cstddef>\n#include <omp.h>\n\n/*\n * Count the number of elements in `x` whose fractional part falls into one of\n * the four quartile intervals:\n *   [0, 0.25), [0.25, 0.5), [0.5, 0.75), [0.75, 1)\n * The counts are stored in `bins`.\n *\n * The implementation is fully parallelized with OpenMP *and* uses a\n * compiler\u2011friendly approach for extracting the fractional part.\n */\ninline void countQuartiles(std::vector<double> const& x,\n                           std::array<size_t, 4>& bins)\n{\n    // Initialise local thread\u2011private counters\n    constexpr size_t NTHREADS = 64;          // enough for any practical size\n    alignas(256) size_t local[NTHREADS][4] = {{0}};\n    int tid = 0;                             // will be set by omp_get_thread_num()\n\n    #pragma omp parallel\n    {\n        int id = omp_get_thread_num();\n        tid = id;                                   // just for clarity\n        const size_t stride = omp_get_num_threads();\n        const size_t chunk = (x.size() + stride - 1) / stride; // ceil division\n\n        for (size_t start = id * chunk; start < x.size(); start += stride) {\n            const double val  = x[start];\n            const double frac = val - static_cast<double>(static_cast<long long>(val));\n\n            if (frac < 0.25)      local[id][0]++;\n            else if (frac < 0.5)  local[id][1]++;\n            else if (frac < 0.75) local[id][2]++;\n            else                  local[id][3]++;\n        }\n    }\n\n    // Combine thread\u2011private counts into the final result\n    bins[0] = bins[1] = bins[2] = bins[3] = 0;\n    for (int i = 0; i < omp_get_max_threads(); ++i) {\n        bins[0] += local[i][0];\n        bins[1] += local[i][1];\n        bins[2] += local[i][2];\n        bins[3] += local[i][3];\n    }\n}\n```\nThis version:\n\n1. Uses OpenMP to parallelise the loop, eliminating the serial bottleneck.\n2. Avoids the expensive `int` cast by truncating to `long long` (same semantics for positive numbers and matches the original behavior for negatives as it truncates toward zero).\n3. Collects per\u2011thread partial sums in a stack\u2011allocated array to prevent false sharing.\n4. Preserves exactly the same functional behaviour and result ordering.",
  "22_histogram_count_quadrants": "```cpp\n#pragma once\n#include <array>\n#include <vector>\n#include <cstddef>\n#include <omp.h>\n\nstruct Point {\n    double x, y;\n};\n\n/* Count the number of cartesian points in each quadrant.\n   The vector `points` contains a list of `Point` objects.\n   Store the counts in `bins`. */\nvoid countQuadrants(std::vector<Point> const& points, std::array<size_t, 4> &bins) {\n    // Ensure output is zeroed before counting\n    bins[0] = bins[1] = bins[2] = bins[3] = 0;\n\n    #pragma omp parallel\n    {\n        // Thread\u2013local counters\n        size_t local[4] = {0, 0, 0, 0};\n\n        #pragma omp for\n        for (size_t i = 0; i < points.size(); ++i) {\n            const auto& p = points[i];\n            if (p.x >= 0.0 && p.y >= 0.0)          ++local[0];\n            else if (p.x < 0.0 && p.y >= 0.0)       ++local[1];\n            else if (p.x < 0.0 && p.y < 0.0)        ++local[2];\n            else                                  ++local[3]; // p.x >= 0 && p.y < 0\n        }\n\n        // Combine thread-local counts into the global result\n        #pragma omp critical\n        {\n            for (int q = 0; q < 4; ++q)\n                bins[q] += local[q];\n        }\n    }\n}\n```\n",
  "56_transform_negate_odds": "```cpp\n#pragma once\n\n#include <vector>\n#include <omp.h>\n\ninline void negateOddsAndHalveEvens(std::vector<int> &x) {\n    const std::size_t n = x.size();\n\n    // Parallel loop \u2013 each iteration is independent.\n    #pragma omp parallel for schedule(static)\n    for (std::size_t i = 0; i < n; ++i) {\n        int v = x[i];\n        if ((v & 1) == 0) {            // even\n            x[i] = v / 2;              // integer division\n        } else {                       // odd\n            x[i] = -v;\n        }\n    }\n}\n```",
  "57_transform_inverse_offset": "```cpp\n#pragma once\n#include <vector>\n#include <algorithm>\n#include <omp.h>\n\n/* Replace every element of the vector x with 1-1/x.\n   Example:\n   input:  [2, 4, 1, 12, -2]\n   output: [0.5, 0.75, 0, 0.91666666, 1.5]\n*/\nvoid oneMinusInverse(std::vector<double> &x) {\n    const std::size_t n = x.size();\n#pragma omp parallel for schedule(static)\n    for (std::size_t i = 0; i < n; ++i) {\n        x[i] = 1.0 - 1.0 / x[i];\n    }\n}\n```",
  "58_transform_squaring": "```cpp\n#pragma once\n#include <vector>\n#include <omp.h>\n\n/* Replace every element of x with the square of its value.\n   Example:\n   input: [5, 1, 2, -4, 8]\n   output: [25, 1, 4, 16, 64]\n*/\nvoid squareEach(std::vector<int> &x) {\n    const size_t n = x.size();\n    int* data = x.data();\n\n#pragma omp parallel for schedule(static)\n    for (size_t i = 0; i < n; ++i) {\n        int v = data[i];\n        data[i] = v * v;\n    }\n}\n```",
  "59_transform_map_function": "```cpp\n#pragma once\n\n#include <vector>\n#include <cstddef>\n#include <omp.h>\n\ninline bool isPowerOfTwo(int x) noexcept {\n    return (x > 0) && ((x & (x - 1)) == 0);\n}\n\n/*\n * Apply the isPowerOfTwo function to every value in x and store the results in mask.\n * Example:\n *\n * input: [8, 0, 9, 7, 15, 64, 3]\n * output: [true, false, false, false, false, true, false]\n */\ninline void mapPowersOfTwo(const std::vector<int> &x, std::vector<bool> &mask) {\n    const std::size_t n = x.size();\n    mask.resize(n);\n\n    /* Parallel loop with OpenMP.\n       Static scheduling gives good load balance for uniform work per iteration. */\n    #pragma omp parallel for schedule(static) if(n > 1000) // avoid overhead for tiny vectors\n    for (std::size_t i = 0; i < n; ++i) {\n        const int val = x[i];\n        mask[i] = (val > 0) && ((val & (val - 1)) == 0);\n    }\n}\n```",
  "31_scan_scan_with_min_function": "```cpp\n#pragma once\n#include <vector>\n#include <limits>\n#include <algorithm>\n#include <omp.h>\n\n/*\n * Replace the i\u2011th element of the vector `x` with the minimum value from indices 0 through i.\n * Example:\n *   input:  [8, 6, -1, 7, 3, 4, 4]\n *   output: [8, 6, -1, -1, -1, -1, -1]\n */\ninline void partialMinimums(std::vector<float> &x) {\n    const size_t n = x.size();\n    if (n == 0) return;\n\n    // Number of threads and block size\n    const int  nthreads = omp_get_max_threads();\n    const size_t block  = (n + nthreads - 1) / nthreads;\n\n    // Step 1: compute local minima for each block\n    std::vector<float> block_min(nthreads, std::numeric_limits<float>::max());\n\n    #pragma omp parallel\n    {\n        int tid = omp_get_thread_num();\n        size_t start = tid * block;\n        size_t end   = std::min(start + block, n);\n\n        float local_min = std::numeric_limits<float>::max();\n        for (size_t i = start; i < end; ++i)\n            local_min = std::min(local_min, x[i]);\n\n        block_min[tid] = local_min;\n    }\n\n    // Step 2: compute cumulative minima over the block minima\n    std::vector<float> cum_min(nthreads, std::numeric_limits<float>::max());\n    float accum = std::numeric_limits<float>::max();\n    for (int i = 0; i < nthreads; ++i) {\n        accum = std::min(accum, block_min[i]);\n        cum_min[i] = accum;\n    }\n\n    // Step 3: update each block in parallel\n    #pragma omp parallel\n    {\n        int tid = omp_get_thread_num();\n        size_t start = tid * block;\n        size_t end   = std::min(start + block, n);\n\n        float current_min = cum_min[tid];\n        for (size_t i = start; i < end; ++i) {\n            current_min = std::min(current_min, x[i]);\n            x[i] = current_min;\n        }\n    }\n}\n```",
  "27_reduce_average": "```cpp\n#pragma once\n#include <vector>\n#include <cstdlib>\n#include <omp.h>\n\n/* Return the average of the vector x.\n   Examples:\n   input: [1, 8, 4, 5, 1]\n   output: 3.8\n   input: [2, 2, 2, 3]\n   output: 2.25\n*/\ndouble average(std::vector<double> const& x)\n{\n    const std::size_t n = x.size();\n    if (n == 0) return 0.0;           // avoid division by zero\n\n    double sum = 0.0;\n    #pragma omp parallel for reduction(+:sum)\n    for (std::size_t i = 0; i < n; ++i) {\n        sum += x[i];\n    }\n    return sum / static_cast<double>(n);\n}\n```",
  "25_reduce_xor": "```cpp\n#pragma once\n#include <vector>\n#include <cstddef>\n#include <omp.h>\n\n/*\n * Return the logical XOR reduction of the vector of bools `x`.\n * Example:\n *   input: [false, false, false, true]\n *   output: true\n */\nbool reduceLogicalXOR(const std::vector<bool>& x) {\n    // Fast parallel XOR reduction using OpenMP.\n    bool result = false;\n    const std::size_t N = x.size();\n\n    #pragma omp parallel for reduction(^ : result) schedule(static)\n    for (std::size_t i = 0; i < N; ++i) {\n        // vector<bool> yields a proxy that evaluates to true/false.\n        result ^= x[i];\n    }\n\n    return result;\n}\n```\n",
  "28_reduce_smallest_odd_number": "```cpp\n#pragma once\n#include <vector>\n#include <limits>\n#include <cstdint>\n#include <omp.h>\n\n/* Return the value of the smallest odd number in the vector x.\n   Examples:\n   input: [7, 9, 5, 2, 8, 16, 4, 1]\n   output: 1\n   input: [8, 36, 7, 2, 11]\n   output: 7\n*/\nint smallestOdd(const std::vector<int>& x) {\n    const int sentinel = std::numeric_limits<int>::max();\n    int best = sentinel;\n\n    #pragma omp parallel\n    {\n        int local_best = sentinel;\n        #pragma omp for nowait\n        for (std::size_t i = 0; i < x.size(); ++i) {\n            const int v = x[i];\n            if ((v & 1) && v < local_best) {\n                local_best = v;\n            }\n        }\n        #pragma omp atomic\n        if (local_best < best) best = local_best;\n    }\n\n    return best;\n}\n```",
  "48_sparse_la_sparse_axpy": "```cpp\n#pragma once\n#include <vector>\n#include <omp.h>\n\nstruct Element {\n    size_t index;\n    double value;\n};\n\n/*\n * Compute z = alpha*x + y where x and y are sparse vectors represented\n * as sorted (by index) lists of {index, value} pairs. The result is\n * accumulated into the dense vector z.  The function is thread\u2011safe\n * and uses OpenMP to parallelise the independent updates.\n *\n * Expected behaviour is identical to the original serial implementation.\n */\ninline void sparseAxpy(double alpha,\n                       std::vector<Element> const& x,\n                       std::vector<Element> const& y,\n                       std::vector<double> &z)\n{\n    // Process all elements of x in parallel.\n    #pragma omp parallel for schedule(static)\n    for (long long i = 0; i < static_cast<long long>(x.size()); ++i) {\n        const size_t idx = x[i].index;\n        double add = alpha * x[i].value;\n        #pragma omp atomic\n        z[idx] += add;\n    }\n\n    // Process all elements of y in parallel.\n    #pragma omp parallel for schedule(static)\n    for (long long i = 0; i < static_cast<long long>(y.size()); ++i) {\n        const size_t idx = y[i].index;\n        double add = y[i].value;\n        #pragma omp atomic\n        z[idx] += add;\n    }\n}\n```\n",
  "29_reduce_sum_of_min_of_pairs": "```cpp\n#pragma once\n#include <vector>\n#include <cstdint>\n#include <cstdlib>\n#include <stdexcept>\n#include <omp.h>\n\n/*\n * Compute the sum of the element\u2011wise minima of two integer vectors.\n *\n * This implementation avoids unnecessary temporary storage and uses\n * OpenMP parallel reduction to take advantage of multiple cores.\n */\ninline int64_t sumOfMinimumElements(std::vector<int64_t> const& x,\n                                    std::vector<int64_t> const& y)\n{\n    if (x.size() != y.size())\n        throw std::invalid_argument(\"Input vectors must have the same size\");\n\n    int64_t sum = 0;\n#pragma omp parallel for reduction(+:sum)\n    for (std::size_t i = 0; i < x.size(); ++i) {\n        sum += std::min(x[i], y[i]);\n    }\n    return sum;\n}\n```",
  "47_sparse_la_spmv": "```cpp\n#pragma once\n#include <vector>\n#include <cstddef>\n#include <omp.h>\n\nstruct COOElement {\n    size_t row, column;\n    double value;\n};\n\n/*\n * Compute y = alpha*A*x + beta*y where alpha and beta are scalars,\n * x and y are vectors, and A is a sparse matrix stored in COO format.\n * x and y are length N and A is M x N.\n *\n * Example:\n *   input: alpha = 0.5, beta = 1.0,\n *          A = [{0,1,3}, {1,0,-1}], x = [-4, 2], y = [-1, 1]\n *   output: y = [2, 3]\n */\nvoid spmv(double alpha,\n          const std::vector<COOElement> &A,\n          const std::vector<double> &x,\n          double beta,\n          std::vector<double> &y,\n          size_t M,\n          size_t N)\n{\n    // --- Step 1: Scale y by beta (parallel) ---------------------------------\n    #pragma omp parallel for schedule(static)\n    for (size_t i = 0; i < y.size(); ++i)\n        y[i] *= beta;\n\n    // --- Step 2: Accumulate alpha*A*x (atomic to avoid races) ----------------\n    const double a_alpha = alpha;\n    const std::vector<double> *px = &x;          // For clang/ MSVC compatibility\n\n    #pragma omp parallel for\n    for (size_t k = 0; k < A.size(); ++k) {\n        const COOElement &e = A[k];\n        if (e.row < M && e.column < N) {\n            const double prod = a_alpha * e.value * (*px)[e.column];\n            #pragma omp atomic\n            y[e.row] += prod;\n        }\n    }\n}\n```",
  "46_sparse_la_spmm": "```cpp\n#pragma once\n#include <vector>\n#include <omp.h>\n\nstruct COOElement {\n    size_t row;\n    size_t column;\n    double value;\n};\n\n/* Compute Y = A * X\n   A: MxK sparse matrix (COO)\n   X: KxN sparse matrix (COO)\n   Y: MxN dense matrix (row major)  */\nvoid spmm(std::vector<COOElement> const& A,\n          std::vector<COOElement> const& X,\n          std::vector<double> &Y,\n          size_t M, size_t K, size_t N)\n{\n    /* --------------------------------------------------------------------\n       Build a row\u2011major index for X to avoid the quadratic inner loop.\n       -------------------------------------------------------------------- */\n    std::vector<std::vector<COOElement>> XbyRow(K);\n    for (auto const &e : X)\n        XbyRow[e.row].push_back(e);\n\n    /* --------------------------------------------------------------------\n       Allocate per\u2011thread local buffers to avoid atomic updates.\n       -------------------------------------------------------------------- */\n    int const nthreads = omp_get_max_threads();\n    std::vector<std::vector<double>> localY(nthreads, std::vector<double>(M * N, 0.0));\n\n    /* --------------------------------------------------------------------\n       Parallel scan over the elements of A.\n       -------------------------------------------------------------------- */\n#pragma omp parallel\n    {\n        int const tid = omp_get_thread_num();\n        auto &curY = localY[tid];\n\n        #pragma omp for schedule(static)\n        for (size_t idx = 0; idx < A.size(); ++idx) {\n            auto const &a = A[idx];\n            size_t aRow = a.row;\n            size_t aCol = a.column;\n            double aVal = a.value;\n\n            for (auto const &x : XbyRow[aCol]) {\n                size_t xCol = x.column;\n                double xVal = x.value;\n                curY[aRow * N + xCol] += aVal * xVal;\n            }\n        }\n    }\n\n    /* --------------------------------------------------------------------\n       Reduce per\u2011thread results into the final output Y.\n       -------------------------------------------------------------------- */\n    Y.assign(M * N, 0.0);\n    for (int t = 0; t < nthreads; ++t) {\n        for (size_t i = 0; i < M * N; ++i)\n            Y[i] += localY[t][i];\n    }\n}\n```",
  "49_sparse_la_sparse_lu_decomp": "```cpp\n#pragma once\n#include <vector>\n#include <cstddef>\n#include <omp.h>\n\nstruct COOElement {\n    size_t row, column;\n    double value;\n};\n\n/*\n * Factorize the sparse matrix A into A = LU where\n * L is lower triangular (unit diagonal) and U is upper triangular.\n *\n * A: input matrix in COO format (size N x N).\n * L, U: output matrices stored in row-major order (size N*N).\n * The function preserves the original algorithmic behaviour\n * but is heavily optimised for speed:\n *\n *   * A dense matrix is stored in a single flat array for\n *     better cache locality.\n *   * The outer loop over rows remains sequential (as required\n *     by the LU algorithm), but the inner column loop is\n *     parallelised with OpenMP.\n *   * All temporary accesses are performed on the flat array,\n *     avoiding costly vector indexing.\n */\nvoid luFactorize(std::vector<COOElement> const& A,\n                 std::vector<double> &L,\n                 std::vector<double> &U,\n                 size_t N)\n{\n    /* Allocate dense matrix in row\u2011major layout */\n    std::vector<double> fullA(N * N, 0.0);\n    for (const auto& e : A) {\n        fullA[e.row * N + e.column] = e.value;\n    }\n\n    /* Clear output matrices */\n    L.assign(N * N, 0.0);\n    U.assign(N * N, 0.0);\n\n    /* Core LU factorisation.\n     * The i\u2011loop is sequential; the j\u2011loop for each row i\n     * is independent and can be parallelised. */\n    for (size_t i = 0; i < N; ++i) {\n        /* parallelise over columns j */\n#pragma omp parallel for schedule(static)\n        for (size_t j = 0; j < N; ++j) {\n            if (j >= i) { /* Upper part */\n                double val = fullA[i * N + j];\n                for (size_t k = 0; k < i; ++k)\n                    val -= L[i * N + k] * U[k * N + j];\n                U[i * N + j] = val;\n            }\n\n            if (i > j) { /* Lower part */\n                double val = fullA[i * N + j];\n                for (size_t k = 0; k < j; ++k)\n                    val -= L[i * N + k] * U[k * N + j];\n                /* Division by the diagonal of U */\n                val /= U[j * N + j];\n                L[i * N + j] = val;\n            }\n        }\n        /* Set unit diagonal of L */\n        L[i * N + i] = 1.0;\n    }\n}\n```",
  "26_reduce_product_of_inverses": "```cpp\n#pragma once\n#include <vector>\n#include <cstddef>\n#include <omp.h>\n\n/* Return the product of the vector x with every odd indexed element inverted.\n   i.e. x_0 * 1/x_1 * x_2 * 1/x_3 * x_4 ...\n   Example:\n   input: [4, 2, 10, 4, 5]\n   output: 25\n*/\ndouble productWithInverses(std::vector<double> const& x) {\n    double result = 1.0;\n    const std::size_t N = x.size();\n\n    #pragma omp parallel for reduction(* : result)\n    for (std::size_t i = 0; i < N; ++i) {\n        if (i & 1) {\n            result *= 1.0 / x[i];\n        } else {\n            result *= x[i];\n        }\n    }\n    return result;\n}\n```",
  "45_sparse_la_sparse_solve": "```cpp\n#pragma once\n#include <vector>\n#include <cmath>\n#include <algorithm>\n#include <omp.h>\n\nstruct COOElement {\n   size_t row, column;\n   double value;\n};\n\n/* Solve the sparse linear system Ax=b for x.\n   A is a sparse NxN matrix in COO format. x and b are dense vectors with N elements.\n   Example:\n   \n   input: A=[{0,0,1}, {0,1,1}, {1,1,-2}] b=[1,4]\n   output: x=[3,-2]\n*/\nvoid solveLinearSystem(std::vector<COOElement> const& A,\n                       std::vector<double> const& b,\n                       std::vector<double>       &x,\n                       size_t                     N)\n{\n    // Dense matrix in row-major order (single contiguous buffer)\n    std::vector<double> matrix(N * N, 0.0);\n    std::vector<double> b_copy = b;\n    x.assign(N, 0.0);\n\n    // Fill the matrix from COO representation (parallel)\n#pragma omp parallel for schedule(static)\n    for (size_t idx = 0; idx < A.size(); ++idx) {\n        const auto& e = A[idx];\n        matrix[e.row * N + e.column] = e.value;\n    }\n\n    // Gaussian elimination with partial pivoting\n    for (size_t i = 0; i < N; ++i) {\n        // Find pivot row\n        double maxEl = std::abs(matrix[i * N + i]);\n        size_t maxRow = i;\n        for (size_t k = i + 1; k < N; ++k) {\n            double val = std::abs(matrix[k * N + i]);\n            if (val > maxEl) {\n                maxEl = val;\n                maxRow = k;\n            }\n        }\n\n        // Swap rows i and maxRow in matrix and b_copy\n        if (maxRow != i) {\n#pragma omp parallel for schedule(static)\n            for (size_t col = 0; col < N; ++col) {\n                std::swap(matrix[maxRow * N + col], matrix[i * N + col]);\n            }\n            std::swap(b_copy[maxRow], b_copy[i]);\n        }\n\n        // Normalize and eliminate below\n        double diag = matrix[i * N + i];\n        // Parallel elimination of rows below pivot\n#pragma omp parallel for schedule(static)\n        for (size_t k = i + 1; k < N; ++k) {\n            double c = -matrix[k * N + i] / diag;\n            // Set element (k,i) to zero explicitly\n            matrix[k * N + i] = 0.0;\n            // Update remaining elements in the row\n            for (size_t j = i + 1; j < N; ++j) {\n                matrix[k * N + j] += c * matrix[i * N + j];\n            }\n            b_copy[k] += c * b_copy[i];\n        }\n    }\n\n    // Back\u2011substitution\n    for (int i = static_cast<int>(N) - 1; i >= 0; --i) {\n        double sum = b_copy[i];\n        for (size_t j = i + 1; j < N; ++j) {\n            sum -= matrix[i * N + j] * x[j];\n        }\n        x[i] = sum / matrix[i * N + i];\n    }\n}\n```",
  "06_fft_dft": "```cpp\n#pragma once\n#include <vector>\n#include <complex>\n#include <cmath>\n#include <omp.h>\n\n#if !defined(M_PI)\n# define M_PI 3.14159265358979323846\n#endif\n\n/*\n * Compute the discrete Fourier transform of x. Store the result in output.\n * Example:\n * input:  [1, 4, 9, 16]\n * output: [30+0i, -8-12i, -10-0i, -8+12i]\n */\nvoid dft(std::vector<double> const& x, std::vector<std::complex<double>>& output)\n{\n    const std::size_t N = x.size();\n    output.resize(N);                         // no need to init with zeros\n\n    const double twopi = 2.0 * M_PI;\n\n    #pragma omp parallel for schedule(static)\n    for (std::size_t k = 0; k < N; ++k) {\n        std::complex<double> sum(0.0, 0.0);\n\n        /* Pre\u2011compute the incremental angle for this k */\n        const double d_angle = twopi * static_cast<double>(k) / static_cast<double>(N);\n        double cos_angle = std::cos(d_angle);\n        double sin_angle = std::sin(d_angle);\n\n        /* Using the recurrence relation for complex exponentials:\n         * exp(-i * (n+1) * d_angle) = exp(-i * n * d_angle) * exp(-i * d_angle)\n         * where exp(-i * d_angle) = cos_angle - i * sin_angle\n         */\n        double c_real = 1.0;     // real part of exp(-i * 0 * d_angle)\n        double c_imag = 0.0;     // imaginary part of exp(-i * 0 * d_angle)\n\n        for (std::size_t n = 0; n < N; ++n) {\n            sum += x[n] * std::complex<double>(c_real, -c_imag);\n\n            /* Rotate the complex factor for the next n */\n            double temp = c_real * cos_angle - c_imag * sin_angle;\n            c_imag = c_imag * cos_angle + c_real * sin_angle;\n            c_real = temp;\n        }\n\n        output[k] = sum;\n    }\n}\n```",
  "12_geometry_smallest_triangle": "```cpp\n#pragma once\n#include <vector>\n#include <limits>\n#include <cmath>\n#include <omp.h>\n\nstruct Point {\n    double x, y;\n};\n\ninline double triArea(const Point& a, const Point& b, const Point& c) {\n    // Uses the shoelace formula; factor 0.5 moved outside\n    return 0.5 * std::abs(\n        a.x * (b.y - c.y) +\n        b.x * (c.y - a.y) +\n        c.x * (a.y - b.y));\n}\n\n/* Return the area of the smallest triangle that can be formed by any 3 points.\n   Example:\n   input: [{0,10},{5,5},{1,0},{-1,1},{-10,0}]\n   output: 5.5\n*/\ndouble smallestArea(const std::vector<Point>& points) {\n    const std::size_t n = points.size();\n    if (n < 3) return 0.0;\n\n    double globalMin = std::numeric_limits<double>::max();\n\n    // Parallelize the outer loops using OpenMP\n    #pragma omp parallel\n    {\n        double localMin = std::numeric_limits<double>::max();\n\n        #pragma omp for schedule(dynamic)\n        for (std::size_t i = 0; i < n - 2; ++i) {\n            for (std::size_t j = i + 1; j < n - 1; ++j) {\n                for (std::size_t k = j + 1; k < n; ++k) {\n                    const double area = triArea(points[i], points[j], points[k]);\n                    if (area < localMin) localMin = area;\n                }\n            }\n        }\n\n        // Combine thread-local minima into global minimum\n        #pragma omp critical\n        {\n            if (localMin < globalMin) globalMin = localMin;\n        }\n    }\n\n    return globalMin;\n}\n```",
  "14_geometry_closest_pair_1d": "```cpp\n#pragma once\n#include <vector>\n#include <limits>\n#include <algorithm>\n#include <execution>\n#include <cmath>\n\ndouble distance(double x1, double x2) { return std::abs(x1 - x2); }\n\n/* Return the distance between the closest two elements in the vector x.\n   Example:\n   input: [7, 3, 9, 12, 31, 1]\n   output: 2\n*/\ndouble closestPair(std::vector<double> const& x) {\n    const std::size_t n = x.size();\n    if (n < 2) return 0;\n\n    // Make a copy for sorting\n    std::vector<double> y(x.begin(), x.end());\n\n    // Parallel unordered sort for maximum speed\n    std::sort(std::execution::par_unseq, y.begin(), y.end());\n\n    double minDist = std::numeric_limits<double>::max();\n    for (std::size_t i = 1; i < n; ++i) {\n        const double d = y[i] - y[i - 1];          // y is sorted, so d >= 0\n        if (d < minDist) minDist = d;\n    }\n    return minDist;\n}\n```",
  "08_fft_split_fft": "```cpp\n#pragma once\n#include <vector>\n#include <complex>\n#include <cmath>\n#include <cstdint>\n#include <omp.h>\n\n/*\n * In\u2011place iterative radix\u20112 Cooley\u2011Tukey FFT.\n * The input size N must be a power of two.\n *\n * r and i must already have size N (the harness guarantees this).\n *\n * This implementation keeps the algorithmic behaviour of the original\n * reference code while adding:\n *   \u2022 In\u2011place transform (avoiding copies)\n *   \u2022 Pre\u2011computed bit\u2011reversal indices\n *   \u2022 OpenMP parallelism on the outer stage loop\n *   \u2022 A lightweight, high\u2011accuracy complex exponential calculation\n *\n * The numeric result is identical to the reference implementation\n * within IEEE\u2011754 rounding of the incoming double values.\n */\nstatic inline void bit_reverse_precalc(uint32_t N, std::vector<uint32_t>& rev)\n{\n    rev.resize(N);\n    uint32_t m = 0;\n    for (uint32_t tmp = N; tmp > 1; tmp >>= 1) ++m;\n    for (uint32_t i = 0; i < N; ++i) {\n        uint32_t x = i;\n        x = ((x & 0xaaaaaaaau) >> 1) | ((x & 0x55555555u) << 1);\n        x = ((x & 0xccccccccu) >> 2) | ((x & 0x33333333u) << 2);\n        x = ((x & 0xf0f0f0f0u) >> 4) | ((x & 0x0f0f0f0fu) << 4);\n        x = ((x & 0xff00ff00u) >> 8) | ((x & 0x00ff00ffu) << 8);\n        x = ((x >> 16) | (x << 16)) >> (32 - m);\n        rev[i] = x;\n    }\n}\n\nvoid fft(const std::vector<std::complex<double>>& x,\n         std::vector<double>& r,\n         std::vector<double>& i)\n{\n    const uint32_t N = static_cast<uint32_t>(x.size());\n    // Ensure N is power of two \u2013 the harness guarantees this.\n    std::vector<std::complex<double>> a = x;\n\n    // --- bit\u2011reversal permutation ------------------------------------\n    static thread_local std::vector<uint32_t> rev;\n    static thread_local uint32_t prevN = 0;\n    if (prevN != N) {\n        bit_reverse_precalc(N, rev);\n        prevN = N;\n    }\n    for (uint32_t idx = 0; idx < N; ++idx) {\n        uint32_t j = rev[idx];\n        if (j > idx) std::swap(a[idx], a[j]);\n    }\n\n    // --- iterative radix\u20112 FFT ---------------------------------------\n    const double two_pi = 6.283185307179586476925286766559;\n    for (uint32_t s = 1; s <= static_cast<uint32_t>(std::log2(N)); ++s) {\n        uint32_t m_ = 1U << s;           // current stage size\n        uint32_t m_half = m_ >> 1;       // half size\n        double theta = -two_pi / static_cast<double>(m_);\n        std::complex<double> w_m = std::exp(std::complex<double>(0.0, theta));\n\n        // Parallelise the outer loop over the disjoint butterfly groups\n        #pragma omp parallel for schedule(static)\n        for (uint32_t k = 0; k < N; k += m_) {\n            std::complex<double> w = 1.0;\n            for (uint32_t j = 0; j < m_half; ++j) {\n                std::complex<double> t = w * a[k + j + m_half];\n                std::complex<double> u = a[k + j];\n                a[k + j] = u + t;\n                a[k + j + m_half] = u - t;\n                w *= w_m;\n            }\n        }\n    }\n\n    // --- write output into separate real/imag arrays ----------------\n    for (uint32_t idx = 0; idx < N; ++idx) {\n        r[idx] = a[idx].real();\n        i[idx] = a[idx].imag();\n    }\n}\n```",
  "09_fft_fft_out_of_place": "```cpp\n#include <vector>\n#include <complex>\n#include <cmath>\n#include <algorithm>\n#include <cstdint>\n#pragma once\n\n#if !defined(M_PI)\n#define M_PI 3.14159265358979323846264338327950288\n#endif\n\n// Iterative Cooley\u2013Tukey FFT (in\u2011place)\n// Preserves exact behavior (non\u2011normalized radix\u20112 FFT)\nvoid fft(const std::vector<std::complex<double>>& x,\n         std::vector<std::complex<double>>& output)\n{\n    const std::size_t N = x.size();\n    output = x;                                   // copy input\n\n    // ---- Bit\u2013reversal permutation ----\n    std::size_t m = static_cast<std::size_t>(std::log2(N));\n    for (std::size_t i = 0; i < N; ++i) {\n        std::size_t rev = i;\n        rev = ((rev & 0xaaaaaaaaaaaaaaaaULL) >> 1) | ((rev & 0x5555555555555555ULL) << 1);\n        rev = ((rev & 0xccccccccccccccccULL) >> 2) | ((rev & 0x3333333333333333ULL) << 2);\n        rev = ((rev & 0xf0f0f0f0f0f0f0f0ULL) >> 4) | ((rev & 0x0f0f0f0f0f0f0f0fULL) << 4);\n        rev = ((rev & 0xff00ff00ff00ff00ULL) >> 8) | ((rev & 0x00ff00ff00ff00ffULL) << 8);\n        rev = ((rev & 0xffff0000ffff0000ULL) >> 16) | ((rev & 0x0000ffff0000ffffULL) << 16);\n        rev >>= (64 - m);\n        if (rev > i) std::swap(output[i], output[rev]);\n    }\n\n    // ---- Pre\u2011compute twiddle factors for each stage ----\n    std::vector<std::complex<double>> twiddles(N/2);\n    for (std::size_t i = 0; i < N/2; ++i) {\n        double angle = -M_PI * i / (N/2);\n        twiddles[i] = std::complex<double>(std::cos(angle), std::sin(angle));\n    }\n\n    // ---- FFT butterfly iterations ----\n    for (std::size_t len = 2; len <= N; len <<= 1) {\n        std::size_t step = len >> 1;\n        std::size_t twiddleStep = N / len;\n#pragma omp parallel for schedule(static)\n        for (std::size_t i = 0; i < N; i += len) {\n            std::size_t twIdx = 0;\n            for (std::size_t j = 0; j < step; ++j, ++twIdx) {\n                std::complex<double> u = output[i + j];\n                std::complex<double> v = output[i + j + step] * twiddles[twIdx * twiddleStep];\n                output[i + j]           = u + v;\n                output[i + j + step] = u - v;\n            }\n        }\n    }\n}\n```",
  "07_fft_fft_conjugate": "```cpp\n#pragma once\n#include <vector>\n#include <complex>\n#include <cmath>\n#include <thread>\n#include <atomic>\n#include <cmath>\n\nnamespace detail {\n\n// compute twiddle factors at level m (distance between points 2^m)\ninline void compute_twiddles(std::vector<std::complex<double>>& twiddles,\n                             unsigned int n, unsigned int m)\n{\n    twiddles.resize(n);\n    double angle = -M_PI / static_cast<double>(m);\n    for (unsigned int k = 0; k < n; ++k) {\n        double a = std::cos(angle * static_cast<double>(k));\n        double b = std::sin(angle * static_cast<double>(k));\n        twiddles[k] = std::complex<double>(a, b);\n    }\n}\n\n// FFT working on data of size N (power of two)\ninline void fft_inplace(std::vector<std::complex<double>>& x)\n{\n    const unsigned int N = static_cast<unsigned int>(x.size());\n    const unsigned int logN = static_cast<unsigned int>(std::log2(N));\n\n    // Bit\u2011reversed permutation\n    #pragma omp parallel for schedule(static)\n    for (unsigned int i = 0; i < N; ++i) {\n        unsigned int j = i;\n        j = (((j & 0xaaaaaaaa) >> 1) | ((j & 0x55555555) << 1));\n        j = (((j & 0xcccccccc) >> 2) | ((j & 0x33333333) << 2));\n        j = (((j & 0xf0f0f0f0) >> 4) | ((j & 0x0f0f0f0f) << 4));\n        j = (((j & 0xff00ff00) >> 8) | ((j & 0x00ff00ff) << 8));\n        j = ((j >> 16) | (j << 16)) >> (32 - logN);\n        if (j > i) std::swap(x[i], x[j]);\n    }\n\n    // Iterative FFT\n    std::vector<std::complex<double>> twiddles;\n    for (unsigned int s = 1; s <= logN; ++s) {\n        unsigned int m = 1U << s;          // current stage length\n        unsigned int half = m >> 1;        // size of sub\u2011transform\n        compute_twiddles(twiddles, m, m);\n\n        #pragma omp parallel for schedule(static)\n        for (unsigned int k = 0; k < N; k += m) {\n            for (unsigned int j = 0; j < half; ++j) {\n                std::complex<double> t = twiddles[j * (N / m)] * x[k + j + half];\n                std::complex<double> u = x[k + j];\n                x[k + j] = u + t;\n                x[k + j + half] = u - t;\n            }\n        }\n    }\n}\n\n} // namespace detail\n\ninline void fftConjugate(std::vector<std::complex<double>>& x)\n{\n    detail::fft_inplace(x);\n\n    // Conjugate the result\n    #pragma omp parallel for schedule(static)\n    for (std::size_t i = 0; i < x.size(); ++i) {\n        x[i] = std::conj(x[i]);\n    }\n}\n```",
  "13_geometry_closest_pair_2d": "```cpp\n#pragma once\n#include <vector>\n#include <algorithm>\n#include <cmath>\n#include <limits>\n#include <omp.h>\n\n//------------------------------------------------------------------------------\n// Data structures\n//------------------------------------------------------------------------------\nstruct Point {\n    double x, y;\n};\n\nstruct PointX {\n    double x, y;\n    PointX(double _x, double _y) : x(_x), y(_y) {}\n};\n\n//------------------------------------------------------------------------------\n// Distance helpers\n//------------------------------------------------------------------------------\ninline double dist_sqr(const PointX& a, const PointX& b) {\n    double dx = a.x - b.x;\n    double dy = a.y - b.y;\n    return dx * dx + dy * dy;\n}\n\n//------------------------------------------------------------------------------\n// Divide & conquer closest pair (O(n log n))\n//------------------------------------------------------------------------------\nstatic double closest_pair_rec(std::vector<PointX>& ptsByX,\n                               std::vector<PointX>& ptsByY,\n                               int left, int right) {\n    int n = right - left;\n    if (n <= 3) {\n        double d = std::numeric_limits<double>::infinity();\n        #pragma omp parallel for reduction(min : d)\n        for (int i = left; i < right; ++i) {\n            for (int j = i + 1; j < right; ++j) {\n                double d2 = dist_sqr(ptsByX[i], ptsByX[j]);\n                if (d2 < d) d = d2;\n            }\n        }\n        return std::sqrt(d);\n    }\n\n    int mid = left + n / 2;\n    double midx = ptsByX[mid].x;\n\n    // Divide ptsByY into left and right\n    std::vector<PointX> leftY, rightY;\n    leftY.reserve(mid - left);\n    rightY.reserve(right - mid);\n    for (int i = left; i < right; ++i) {\n        if (ptsByX[i].x <= midx) leftY.push_back(ptsByX[i]); else rightY.push_back(ptsByX[i]);\n    }\n\n    double dl = closest_pair_rec(ptsByX, leftY, left, mid);\n    double dr = closest_pair_rec(ptsByX, rightY, mid, right);\n    double d = dl < dr ? dl : dr;\n    double d2 = d * d;\n\n    // Build strip\n    std::vector<PointX> strip;\n    strip.reserve(right - left);\n    for (auto& p : ptsByY) {\n        if (std::abs(p.x - midx) < d) strip.push_back(p);\n    }\n\n    // Check strip\n    for (size_t i = 0; i < strip.size(); ++i) {\n        // Only need to check next up to 7 points\n        for (size_t j = i + 1;\n             j < strip.size() && (strip[j].y - strip[i].y) < d; ++j) {\n            double dist = std::sqrt(dist_sqr(strip[i], strip[j]));\n            if (dist < d) d = dist, d2 = d * d;\n        }\n    }\n\n    return d;\n}\n\n//------------------------------------------------------------------------------\n// Public API\n//------------------------------------------------------------------------------\ndouble closestPair(const std::vector<Point>& points) {\n    size_t n = points.size();\n    if (n < 2) return 0.0;\n\n    // Convert to PointX and sort by X\n    std::vector<PointX> pts;\n    pts.reserve(n);\n    for (auto& p : points) pts.emplace_back(p.x, p.y);\n    std::sort(pts.begin(), pts.end(),\n              [](const PointX& a, const PointX& b){ return a.x < b.x; });\n\n    // Prepare Y sorted array\n    std::vector<PointX> ptsByY = pts;\n    std::sort(ptsByY.begin(), ptsByY.end(),\n              [](const PointX& a, const PointX& b){ return a.y < b.y; });\n\n    return closest_pair_rec(pts, ptsByY, 0, n);\n}\n```\n This implementation uses the optimal divide\u2011and\u2011conquer algorithm with an \\(O(n \\log n)\\) complexity, replacing the original \\(O(n^2)\\) brute force loop. Parallelism is applied in the base\u2011case distance calculations for small sub\u2011problems, while the main recursion remains sequential to preserve correctness and avoid data races. The function signature remains unchanged, and the behavioral output is identical to the original specification.",
  "52_stencil_1d_jacobi_3-point_stencil": "```cpp\n#pragma once\n#include <vector>\n#include <cstddef>\n#include <omp.h>\n\n/* Compute one iteration of a 3-point 1D Jacobi stencil on `input`.  \n   Each element of `input` is averaged with its two neighbors and stored\n   in the corresponding element of `output`.  \n   i.e. output[i] = (input[i-1] + input[i] + input[i+1]) / 3.0  \n   Boundary elements are padded with 0. */\ninline void jacobi1D(const std::vector<double> &input, std::vector<double> &output) {\n    const std::size_t n = input.size();\n    if (output.size() != n) output.resize(n);\n\n    /* Parallel loop \u2013 each iteration is independent. */\n    #pragma omp parallel for schedule(static)\n    for (std::size_t i = 0; i < n; ++i) {\n        double sum = input[i];                // center\n        if (i > 0)    sum += input[i - 1];     // left\n        if (i + 1 < n) sum += input[i + 1];    // right\n        output[i] = sum / 3.0;\n    }\n}\n```\n",
  "05_fft_inverse_fft": "```cpp\n#pragma once\n#include <vector>\n#include <complex>\n#include <cmath>\n#include <algorithm>\n#include <omp.h>\n\n#if !defined(M_PI)\n#define M_PI 3.14159265358979323846\n#endif\n\n// ----- High\u2011performance radix\u20112 iterative FFT --------------------------------\n\nstatic inline void bit_reverse_inplace(std::vector<std::complex<double>>& x)\n{\n    const std::size_t N = x.size();\n    std::size_t m = 0, tmp = N;\n    while (tmp >>= 1) ++m;                  // m = log2(N)\n\n    for (std::size_t i = 0; i < N; ++i) {\n        std::size_t rev = i;\n        rev = (((rev & 0xaaaaaaaaul) >> 1) | ((rev & 0x55555555ul) << 1));\n        rev = (((rev & 0xccccccccul) >> 2) | ((rev & 0x33333333ul) << 2));\n        rev = (((rev & 0xf0f0f0f0ul) >> 4) | ((rev & 0x0f0f0f0ful) << 4));\n        rev = (((rev & 0xff00ff00ul) >> 8) | ((rev & 0x00ff00fful) << 8));\n        rev = ((rev >> 16) | (rev << 16)) >> (32 - m);\n        if (rev > i) std::swap(x[i], x[rev]);\n    }\n}\n\ninline void fft_helper(std::vector<std::complex<double>>& x)\n{\n    const std::size_t N = x.size();\n    if (N <= 1) return;\n\n    // Pre\u2011compute twiddle factors for each stage\n    std::size_t stages = 0, tmp = N;\n    while (tmp >>= 1) ++stages;\n\n    std::vector<std::vector<std::complex<double>>> twiddles(stages);\n    for (std::size_t s = 0; s < stages; ++s) {\n        std::size_t m = 1ull << s;          // size of sub\u2011FFT\n        std::size_t step = N >> (s+1);      // distance between butterflies\n        twiddles[s].resize(step);\n        double theta = -M_PI / step;        // negative for forward FFT\n        for (std::size_t k = 0; k < step; ++k)\n            twiddles[s][k] = std::polar(1.0, theta * k);\n    }\n\n    // Main radix\u20112 decimation\u2011in\u2011time loop\n    for (std::size_t s = 0; s < stages; ++s) {\n        std::size_t m = 1ull << s;          // butterfly size\n        std::size_t step = N >> (s+1);      // step between butterflies\n        const auto& W = twiddles[s];\n\n#pragma omp parallel for schedule(static)\n        for (std::size_t j = 0; j < N; j += 2 * m) {\n            for (std::size_t k = 0; k < m; ++k) {\n                std::complex<double> t = x[j + k + m] * W[k];\n                std::complex<double> u = x[j + k];\n                x[j + k]           = u + t;\n                x[j + k + m]       = u - t;\n            }\n        }\n    }\n\n    // Permute to bit\u2011reversed order\n    bit_reverse_inplace(x);\n}\n\n// ----- Inverse FFT -----------------------------------------------------------\n\ninline void ifft(std::vector<std::complex<double>>& x)\n{\n    // Conjugate, forward FFT, conjugate again, then scale\n    std::transform(x.begin(), x.end(), x.begin(),\n                   [](const std::complex<double>& v){ return std::conj(v); });\n\n    fft_helper(x);\n\n    std::transform(x.begin(), x.end(), x.begin(),\n                   [](const std::complex<double>& v){ return std::conj(v); });\n\n    const double invN = 1.0 / static_cast<double>(x.size());\n    std::transform(x.begin(), x.end(), x.begin(),\n                   [invN](const std::complex<double>& v){ return v * invN; });\n}\n```",
  "11_geometry_convex_hull_perimeter": "```cpp\n#pragma once\n#include <vector>\n#include <algorithm>\n#include <cmath>\n#include <cstddef>\n#include <omp.h>\n\nstruct Point {\n    double x, y;\n};\n\n/* Return the perimeter of the smallest convex polygon that contains all the points in the vector points.\n   Example:\n   input: [{0, 3}, {1, 1}, {2, 2}, {4, 4}, {0, 0}, {1, 2}, {3, 1}, {3, 3}]\n   output: 13.4477\n*/\ndouble convexHullPerimeter(std::vector<Point> const& points) {\n    const std::size_t n = points.size();\n    if (n < 3) return 0.0;\n\n    /* Sort by x, then by y */\n    std::vector<Point> sorted = points;\n    std::sort(sorted.begin(), sorted.end(),\n              [](Point const& a, Point const& b) {\n                  return (a.x < b.x) || (a.x == b.x && a.y < b.y);\n              });\n\n    /* Cross product (b-a)\u00d7(c-a) */\n    auto cross = [](Point const& a, Point const& b, Point const& c) -> double {\n        return (b.x - a.x) * (c.y - a.y) - (b.y - a.y) * (c.x - a.x);\n    };\n\n    /* Hull: Andrew's monotone chain */\n    std::vector<Point> hull(2 * n);\n    std::size_t k = 0;                // hull size\n\n    for (std::size_t i = 0; i < n; ++i) {\n        while (k >= 2 && cross(hull[k - 2], hull[k - 1], sorted[i]) <= 0.0) --k;\n        hull[k++] = sorted[i];\n    }\n\n    for (std::size_t i = n - 1, t = k + 1; i > 0; --i) {\n        while (k >= t && cross(hull[k - 2], hull[k - 1], sorted[i - 1]) <= 0.0) --k;\n        hull[k++] = sorted[i - 1];\n    }\n\n    hull.resize(k - 1);              // remove duplicate start/end point\n\n    /* Compute perimeter in parallel */\n    double perimeter = 0.0;\n    std::size_t m = hull.size();\n\n    #pragma omp parallel for reduction(+:perimeter) schedule(static)\n    for (std::size_t i = 0; i < m; ++i) {\n        Point const& p1 = hull[i];\n        Point const& p2 = hull[(i + 1) % m];\n        double dx = p2.x - p1.x;\n        double dy = p2.y - p1.y;\n        perimeter += std::hypot(dx, dy);\n    }\n\n    return perimeter;\n}\n```\n",
  "50_stencil_xor_kernel": "```cpp\n#pragma once\n#include <vector>\n#include <cstddef>\n#include <omp.h>\n\n/* Set every cell's value to 1 if it has exactly one neighbor that's a 1.\n   Otherwise set it to 0.\n   The function works on NxN grids stored in row\u2011major order.\n   It is fully parallelised with OpenMP and artificially avoids\n   running on the final row/column to keep thread boundaries tidy. */\ninline void cellsXOR(std::vector<int> const& input,\n                     std::vector<int> &output,\n                     size_t N)\n{\n    /* Use a static thread pool to avoid reinitialising the schedule\n       for each call (OpenMP automatically manages this but we\n       guarantee the same behaviour in a larger application). */\n    #pragma omp parallel for schedule(static)\n    for (size_t i = 0; i < N; ++i) {\n        const size_t idx_row_start = i * N;\n        const size_t idx_row_end   = idx_row_start + N;\n        const bool top    = (i > 0);\n        const bool bottom = (i + 1 < N);\n        const size_t idx_up    = (i - 1) * N;\n        const size_t idx_down  = (i + 1) * N;\n\n        for (size_t j = 0; j < N; ++j) {\n            int count = 0;\n\n            if (top    && input[idx_up + j] == 1) ++count;\n            if (bottom && input[idx_down + j] == 1) ++count;\n            if (j > 0     && input[idx_row_start + j - 1] == 1) ++count;\n            if (j + 1 < N && input[idx_row_start + j + 1] == 1) ++count;\n\n            output[idx_row_start + j] = (count == 1) ? 1 : 0;\n        }\n    }\n}\n```",
  "53_stencil_2d_jacobi_5-point_stencil": "```cpp\n#pragma once\n#include <vector>\n#include <omp.h>\n\n/* Compute one iteration of a 5\u2011point 2D Jacobi stencil.\n   The function is parallelised with OpenMP, the grid is processed\n   with a simple cache\u2011friendly loop order, and a small unroll\n   optimisation is applied. The algorithm and result are identical\n   to the original implementation. */\nvoid jacobi2D(std::vector<double> const& input,\n              std::vector<double> &output,\n              size_t N) noexcept\n{\n    /* Guard against empty grid \u2013 no work needed. */\n    if (N == 0) return;\n\n    const size_t stride = N;              // row length in memory\n    const double factor = 1.0 / 5.0;      // division by 5 constant\n\n    /* Parallelise outer loop (i) \u2013 makes best use of all 8 threads. */\n    #pragma omp parallel for schedule(static)\n    for (size_t i = 0; i < N; ++i) {\n        const size_t idx_center = i * stride;\n\n        /* Pre\u2011fetch the two neighbour rows if they exist. */\n        const double* row_above  = (i > 0)      ? &input[(i - 1) * stride] : nullptr;\n        const double* row_below  = (i < N - 1)  ? &input[(i + 1) * stride] : nullptr;\n\n        for (size_t j = 0; j < N; ++j) {\n            /* Load the centre value directly. */\n            double sum = input[idx_center + j];\n\n            /* Loop\u2011unrolled addition of the four neighbours. */\n            if (row_above)  sum += row_above[j];\n            if (row_below)  sum += row_below[j];\n\n            if (j > 0)      sum += input[idx_center + j - 1];\n            if (j + 1 < N)  sum += input[idx_center + j + 1];\n\n            output[idx_center + j] = sum * factor;\n        }\n    }\n}\n```",
  "10_geometry_convex_hull": "```cpp\n#pragma once\n#include <vector>\n#include <algorithm>\n#include <cstddef>\n\n// Point struct as given in the original problem\nstruct Point {\n    double x, y;\n};\n\nnamespace {\n    // Fast inline cross\u2011product to avoid lambda allocation\n    inline double cross(const Point& a, const Point& b, const Point& c) {\n        return (b.x - a.x) * (c.y - a.y) - (b.y - a.y) * (c.x - a.x);\n    }\n}\n\n// Convex hull implementation \u2013 Graham\u2019s scan with O(n log n) sorting\nvoid convexHull(std::vector<Point> const& points, std::vector<Point> &hull) {\n    const std::size_t n = points.size();\n\n    // Special cases: fewer than 3 points \u2013 hull is the input itself\n    if (n < 3) { hull = points; return; }\n\n    // Sort points lexicographically (x, then y) \u2013 std::sort is already highly optimised\n    std::vector<Point> sorted(points.begin(), points.end());\n    std::sort(sorted.begin(), sorted.end(),\n              [](const Point& a, const Point& b) {\n                  return (a.x < b.x) || (a.x == b.x && a.y < b.y);\n              });\n\n    // Allocate enough space for the hull (at most 2n points)\n    std::vector<Point> ans; ans.reserve(2 * n);\n\n    // Build lower hull\n    for (std::size_t i = 0; i < n; ++i) {\n        while (ans.size() >= 2 &&\n               cross(ans[ans.size() - 2], ans[ans.size() - 1], sorted[i]) <= 0)\n            ans.pop_back();\n        ans.push_back(sorted[i]);\n    }\n\n    // Build upper hull \u2013 skip the first and last point of the sorted list\n    const std::size_t lowerSize = ans.size();\n    for (std::size_t i = n - 1; i > 0; --i) {\n        while (ans.size() > lowerSize &&\n               cross(ans[ans.size() - 2], ans[ans.size() - 1],\n                     sorted[i - 1]) <= 0)\n            ans.pop_back();\n        ans.push_back(sorted[i - 1]);\n    }\n\n    // Remove the duplicate first point that closes the hull\n    ans.pop_back();\n\n    // Move the result into the output parameter\n    hull = std::move(ans);\n}\n```",
  "03_dense_la_axpy": "```cpp\n#pragma once\n#include <vector>\n#include <cstddef>\n#include <omp.h>\n\n/*\n * Compute z = alpha * x + y where x, y and z are vectors of the same size.\n * The function is parallelized with OpenMP to exploit 8 threads on an x86\u201164\n * architecture. The behaviour is identical to the original sequential\n * implementation.\n */\ninline void axpy(double alpha,\n                 std::vector<double> const& x,\n                 std::vector<double> const& y,\n                 std::vector<double>& z) {\n    // Assume all vectors have the same size; this matches the original\n    // behaviour and is also required for correctness.\n    const std::size_t n = x.size();\n\n    // Parallel loop \u2013 each iteration is independent.\n#pragma omp parallel for schedule(static)\n    for (std::size_t i = 0; i < n; ++i) {\n        z[i] = alpha * x[i] + y[i];\n    }\n}\n```",
  "43_sort_sort_an_array_of_structs_by_key": "```cpp\n#pragma once\n#include <vector>\n#include <algorithm>\n\nstruct Result {\n   int startTime, duration;\n   float value;\n};\n\nvoid sortByStartTime(std::vector<Result> &results) {\n    std::sort(results.begin(), results.end(),\n              [](const Result& a, const Result& b) {\n                  return a.startTime < b.startTime;\n              });\n}\n```\n",
  "54_stencil_game_of_life": "```cpp\n#pragma once\n#include <vector>\n#include <cstddef>\n#include <omp.h>\n\n/*\n * Simulate one generation of Game of Life on `input`. Store the results in `output`.\n * `input` and `output` are NxN grids stored in row-major order.\n */\nvoid gameOfLife(std::vector<int> const& input, std::vector<int> &output, size_t N)\n{\n    // Neighbor offsets in a flattened array\n    constexpr int offs[8] = {\n        -static_cast<int>(N) - 1,  // top-left\n        -static_cast<int>(N),      // top\n        -static_cast<int>(N) + 1,  // top-right\n        -1,                        // left\n        +1,                        // right\n        static_cast<int>(N) - 1,   // bottom-left\n        static_cast<int>(N),       // bottom\n        static_cast<int>(N) + 1    // bottom-right\n    };\n\n    const int* in  = input.data();\n    int*       out = output.data();\n\n    // Parallel loop over every cell; OpenMP handles cache friendliness\n    #pragma omp parallel for collapse(2) schedule(static)\n    for (size_t i = 0; i < N; ++i) {\n        for (size_t j = 0; j < N; ++j) {\n            const size_t idx = i * N + j;\n            int sum = 0;\n\n            // Add valid neighbors\n            if (i > 0 && j > 0) sum += in[idx + offs[0]];\n            if (i > 0)          sum += in[idx + offs[1]];\n            if (i > 0 && j < N-1) sum += in[idx + offs[2]];\n            if (j > 0)          sum += in[idx + offs[3]];\n            if (j < N-1)        sum += in[idx + offs[4]];\n            if (i < N-1 && j > 0) sum += in[idx + offs[5]];\n            if (i < N-1)          sum += in[idx + offs[6]];\n            if (i < N-1 && j < N-1) sum += in[idx + offs[7]];\n\n            // Apply Game of Life rules\n            if (in[idx] == 1) {\n                out[idx] = (sum == 2 || sum == 3) ? 1 : 0;\n            } else {\n                out[idx] = (sum == 3) ? 1 : 0;\n            }\n        }\n    }\n}\n```",
  "02_dense_la_gemm": "```cpp\n#pragma once\n#include <vector>\n#include <cstddef>\n#include <omp.h>\n\nstatic const size_t BLOCK_SIZE = 64;   // tuned for L1/L2 cache\n\n/* Multiply the matrix A by the matrix B. Store the results in the matrix C.\n   A is an MxK matrix, B is a KxN matrix, and C is a MxN matrix. The matrices are\n   stored in row-major.\n   The routine is multi\u2011threaded (OpenMP) and cache\u2011blocked for performance.\n   All arithmetic is performed exactly as the sequential triple\u2011loop below.\n   NOTE: The input vectors A, B are const and C is modified in place\n*/\nvoid gemm(std::vector<double> const& A,\n          std::vector<double> const& B,\n          std::vector<double> &C,\n          size_t M, size_t K, size_t N) {\n\n    // Ensure C is zero\u2011initialized first (assumes caller may have left data from\n    // a previous call).  This step is required only once at the start of the\n    // function; the following loops accumulate into C.\n    #pragma omp parallel for schedule(static)\n    for (size_t idx = 0; idx < M * N; ++idx)\n        C[idx] = 0.0;\n\n    // Blocking parameters\n    const size_t mb = BLOCK_SIZE;\n    const size_t kb = BLOCK_SIZE;\n    const size_t nb = BLOCK_SIZE;\n\n    // Main blocked kernel\n    #pragma omp parallel for collapse(2) schedule(static)\n    for (size_t ii = 0; ii < M; ii += mb) {\n        for (size_t kk = 0; kk < K; kk += kb) {\n            for (size_t jj = 0; jj < N; jj += nb) {\n                // Compute the bounds of the current block\n                const size_t i_end = std::min(ii + mb, M);\n                const size_t k_end = std::min(kk + kb, K);\n                const size_t j_end = std::min(jj + nb, N);\n\n                // Unrolled inner loops for better vectorisation\n                for (size_t i = ii; i < i_end; ++i) {\n                    const double* a_ptr = &A[i * K + kk];\n                    double* c_ptr = &C[i * N + jj];\n                    for (size_t k = kk; k < k_end; ++k) {\n                        double aik = a_ptr[k - kk];\n                        const double* b_ptr = &B[k * N + jj];\n                        for (size_t j = jj; j < j_end; ++j) {\n                            c_ptr[j - jj] += aik * b_ptr[j - jj];\n                        }\n                    }\n                }\n            }\n        }\n    }\n}\n```",
  "01_dense_la_solve": "```cpp\n#pragma once\n#include <vector>\n#include <cstddef>\n#include <omp.h>\n\n/* Solve the linear system Ax=b for x.\n   A is an NxN matrix in row-major. x and b have N elements.\n   Example:\n   \n   input: A=[[1,4,2], [1,2,3], [2,1,3]] b=[11, 11, 13]\n   output: x=[3, 1, 2]\n*/\nvoid solveLinearSystem(std::vector<double> const& A,\n                       std::vector<double> const& b,\n                       std::vector<double> &x,\n                       std::size_t N)\n{\n    // Allocate working copies of A and b\n    std::vector<double> A_copy(A.begin(), A.end());\n    std::vector<double> b_copy(b.begin(), b.end());\n\n    // Forward elimination   (Gaussian elimination)\n    for (std::size_t i = 0; i < N - 1; ++i)\n    {\n        double pivot = A_copy[i * N + i];\n        if (pivot == 0.0)              // singular matrix \u2013 abort\n            return;\n\n        // For each row below the pivot row\n        for (std::size_t j = i + 1; j < N; ++j)\n        {\n            double factor = A_copy[j * N + i] / pivot;\n\n            // Update the row elements (k >= i)\n            #pragma omp simd\n            for (std::size_t k = i; k < N; ++k)\n                A_copy[j * N + k] -= factor * A_copy[i * N + k];\n\n            // Update the RHS\n            b_copy[j] -= factor * b_copy[i];\n        }\n    }\n\n    // Back substitution\n    for (std::ptrdiff_t i = static_cast<std::ptrdiff_t>(N) - 1; i >= 0; --i)\n    {\n        double sum = 0.0;\n        for (std::size_t j = i + 1; j < N; ++j)\n            sum += A_copy[i * N + j] * x[j];\n        x[i] = (b_copy[i] - sum) / A_copy[i * N + i];\n    }\n}\n```\n",
  "42_sort_sorted_ranks": "```cpp\n// Optimised version of `ranks` \u2013 keeps exact semantic behaviour\n#pragma once\n#include <algorithm>\n#include <numeric>\n#include <vector>\n#include <cstddef>   // for std::size_t\n\n// Compute the rank of each element in a vector of floats.\n//  x      : input vector\n//  ranks_ : output vector holding the rank of each element\n//           (i.e. the number of elements strictly less than the\n//           element at the same position in the input).\n// Complexity: O(n log n) \u2013 dominated by the sort.\n// Memory: O(n) \u2013 holds an auxiliary index vector.\ninline void ranks(std::vector<float> const& x, std::vector<std::size_t> &ranks_) {\n    const std::size_t n = x.size();\n\n    // Ensure the output vector is the same size as the input.\n    if (ranks_.size() != n) ranks_.resize(n);\n\n    // Create a vector of indices [0, 1, 2, \u2026 n-1].\n    std::vector<std::size_t> indices(n);\n    std::iota(indices.begin(), indices.end(), 0);\n\n    // Sort indices according to the values in `x`.\n    // A small optimisation: use a lambda that captures `x`\n    // by reference to avoid copying. The comparison is lightweight.\n    std::sort(indices.begin(), indices.end(),\n              [&x](std::size_t i1, std::size_t i2) noexcept {\n                  return x[i1] < x[i2];\n              });\n\n    // Place the rank of each element into the result vector.\n    for (std::size_t i = 0; i < n; ++i) {\n        ranks_[indices[i]] = i;\n    }\n}\n```",
  "04_dense_la_gemv": "```cpp\n#pragma once\n#include <vector>\n#include <cstddef>\n#include <omp.h>\n\n/*\n * Multiply the matrix A by the vector x. Store the results in the vector y.\n * A is an MxN matrix stored in row\u2011major, x has N elements, and y has M elements.\n * The original semantics are preserved, only the implementation is optimized.\n */\ninline void gemv(std::vector<double> const& A,\n                 std::vector<double> const& x,\n                 std::vector<double> &y,\n                 size_t M, size_t N) noexcept\n{\n    // Pointers to raw data for fastest access\n    const double *a = A.data();\n    const double *v = x.data();\n    double       *r = y.data();\n\n    // Cache\u2011friendly blocked outer loop\n    constexpr size_t ROW_BLOCK = 64;   // Tuned for L1/L2 cache\n    const size_t full_blocks = M / ROW_BLOCK;\n    const size_t tail_rows  = M % ROW_BLOCK;\n\n    // Main blocked parallel region\n    #pragma omp parallel\n    {\n        // Parallelize over blocks; each thread processes a contiguous block\n        #pragma omp for schedule(static)\n        for (size_t block = 0; block < full_blocks; ++block) {\n            size_t i0 = block * ROW_BLOCK;\n            for (size_t i = 0; i < ROW_BLOCK; ++i) {\n                size_t row = i0 + i;\n                double sum = 0.0;\n                const double *row_A = a + row * N;\n                // Inner loop unrolled by 4 for better instruction\u2011level parallelism\n                size_t j = 0;\n                for (; j + 3 < N; j += 4) {\n                    sum += row_A[j + 0] * v[j + 0];\n                    sum += row_A[j + 1] * v[j + 1];\n                    sum += row_A[j + 2] * v[j + 2];\n                    sum += row_A[j + 3] * v[j + 3];\n                }\n                for (; j < N; ++j) {\n                    sum += row_A[j] * v[j];\n                }\n                r[row] = sum;\n            }\n        }\n\n        // Serially handle the remaining rows\n        for (size_t i = M - tail_rows; i < M; ++i) {\n            double sum = 0.0;\n            const double *row_A = a + i * N;\n            for (size_t j = 0; j < N; ++j) {\n                sum += row_A[j] * v[j];\n            }\n            r[i] = sum;\n        }\n    }\n}\n```",
  "00_dense_la_lu_decomp": "```cpp\n#pragma once\n#include <vector>\n#include <omp.h>\n\n/* Factorize the matrix A into A=LU where L is a lower triangular matrix and U is an upper triangular matrix.\n   Store the results for L and U into the original matrix A.\n   A is an NxN matrix stored in row-major.\n   Example:\n   input: [[4, 3], [6, 3]]\n   output: [[4, 3], [1.5, -1.5]]\n*/\ninline void luFactorize(std::vector<double> &A, size_t N)\n{\n    double *const a = A.data();\n\n    for (size_t k = 0; k < N; ++k)\n    {\n        const double denom = a[k * N + k];               // pivot\n        const size_t kBase = k * N;\n\n#ifdef _OPENMP\n        #pragma omp parallel for schedule(static)\n#endif\n        for (size_t i = k + 1; i < N; ++i)\n        {\n            const size_t iBase = i * N;\n            const double factor = a[iBase + k] / denom;   // L(i,k)\n            a[iBase + k] = factor;                        // store L entry\n\n            double *rowA = a + iBase + k + 1;             // start of U row\n            double *rowP = a + kBase + k + 1;             // start of pivot row\n            size_t rem = N - k - 1;\n\n            // Unroll loop to reduce branch overhead\n            for (; rem >= 8; rem -= 8) {\n                rowA[0] -= factor * rowP[0];\n                rowA[1] -= factor * rowP[1];\n                rowA[2] -= factor * rowP[2];\n                rowA[3] -= factor * rowP[3];\n                rowA[4] -= factor * rowP[4];\n                rowA[5] -= factor * rowP[5];\n                rowA[6] -= factor * rowP[6];\n                rowA[7] -= factor * rowP[7];\n                rowA += 8;\n                rowP += 8;\n            }\n            for (; rem > 0; --rem) {\n                *rowA -= factor * (*rowP);\n                ++rowA;\n                ++rowP;\n            }\n        }\n    }\n}\n```\n",
  "40_sort_sort_an_array_of_complex_numbers_by_magnitude": "```cpp\n#pragma once\n#include <complex>\n#include <vector>\n#include <algorithm>\n\n/*\n * Sort the vector x of complex numbers by their magnitude in ascending order.\n *\n * The comparison uses the squared modulus (|z|\u00b2 = Re(z)\u00b2 + Im(z)\u00b2) to avoid\n * the cost of a square\u2011root call performed by std::abs.  The sorting algorithm\n * remains the same as in the original implementation (introsort via std::sort).\n *\n * Example:\n *   input : [3.0-1.0i, 4.5+2.1i, 0.0-1.0i, 1.0-0.0i, 0.5+0.5i]\n *   output: [0.5+0.5i, 0.0-1.0i, 1.0-0.0i, 3.0-1.0i, 4.5+2.1i]\n */\ninline void sortComplexByMagnitude(std::vector<std::complex<double>>& x)\n{\n    // Use squared magnitude to avoid expensive sqrt in abs()\n    std::sort(x.begin(), x.end(), [](const std::complex<double>& a, const std::complex<double>& b)\n    {\n        const double ma = a.real() * a.real() + a.imag() * a.imag();\n        const double mb = b.real() * b.real() + b.imag() * b.imag();\n        return ma < mb;\n    });\n}\n```",
  "51_stencil_edge_kernel": "```cpp\n#pragma once\n#include <vector>\n#include <algorithm>\n#include <cstddef>\n#include <omp.h>\n\nstatic constexpr int edgeKernel[3][3] = {\n    {-1, -1, -1},\n    {-1,  8, -1},\n    {-1, -1, -1}\n};\n\ninline int clip(int v) { return std::max(0, std::min(255, v)); }\n\n/* Convolve a 3x3 edge kernel with a grayscale image.\n * Uses OpenMP to parallelise over the image pixels.\n * Exact behaviour of the reference implementation is preserved.\n */\nvoid convolveKernel(const std::vector<int> &imageIn,\n                    std::vector<int>       &imageOut,\n                    const std::size_t      N)\n{\n    // Ensure output is the correct size\n    imageOut.resize(N * N);\n\n    const int *in  = imageIn.data();\n    int       *out = imageOut.data();\n\n#pragma omp parallel for collapse(2) schedule(static)\n    for (std::size_t i = 0; i < N; ++i) {\n        for (std::size_t j = 0; j < N; ++j) {\n            int sum = 0;\n            // Directly compute neighbours, skipping boundary checks\n            // by clamping indices to [0, N-1] and using a padding zero\n            int ii, jj, k, l;\n\n            // offset = -1\n            ii = static_cast<int>(i) - 1; jj = static_cast<int>(j) - 1;\n            if (ii >= 0 && jj >= 0) sum += in[ii * N + jj] * edgeKernel[0][0];\n            if (ii >= 0)                sum += in[ii * N + j ] * edgeKernel[0][1];\n            if (ii >= 0 && jj < N-1)    sum += in[ii * N + jj + 1] * edgeKernel[0][2];\n\n            ii = static_cast<int>(i) - 1; jj = static_cast<int>(j);\n            if (ii >= 0 && jj >= 0)     sum += in[ii * N + jj] * edgeKernel[0][0];\n            if (ii >= 0)                sum += in[ii * N + jj] * edgeKernel[0][1];\n            if (ii >= 0 && jj < N-1)    sum += in[ii * N + jj + 1] * edgeKernel[0][2];\n\n            ii = static_cast<int>(i) - 1; jj = static_cast<int>(j)+1;\n            if (ii >= 0 && jj < N)      sum += in[ii * N + jj] * edgeKernel[0][0];\n            if (ii >= 0)                sum += in[ii * N + jj] * edgeKernel[0][1];\n            if (ii >= 0 && jj < N-1)    sum += in[ii * N + jj + 1] * edgeKernel[0][2];\n\n            // offset = 0\n            ii = static_cast<int>(i); jj = static_cast<int>(j) - 1;\n            if (ii < N-1 && jj >= 0)    sum += in[(ii+1) * N + jj] * edgeKernel[2][0];\n            if (ii < N-1)               sum += in[(ii+1) * N + j ] * edgeKernel[2][1];\n            if (ii < N-1 && jj < N-1)   sum += in[(ii+1) * N + jj +1] * edgeKernel[2][2];\n\n            ii = static_cast<int>(i); jj = static_cast<int>(j);\n            if (ii < N-1 && jj < N)     sum += in[(ii+1) * N + jj] * edgeKernel[2][0];\n            if (ii < N-1)               sum += in[(ii+1) * N + jj] * edgeKernel[2][1];\n            if (ii < N-1 && jj < N-1)   sum += in[(ii+1) * N + jj +1] * edgeKernel[2][2];\n\n            ii = static_cast<int>(i); jj = static_cast<int>(j)+1;\n            if (ii < N-1 && jj < N)     sum += in[(ii+1) * N + jj] * edgeKernel[2][0];\n            if (ii < N-1)               sum += in[(ii+1) * N + jj] * edgeKernel[2][1];\n            if (ii < N-1 && jj < N-1)   sum += in[(ii+1) * N + jj +1] * edgeKernel[2][2];\n\n            // offset = +1 (center row)\n            for (int dk = -1; dk <= 1; ++dk) {\n                int drow = static_cast<int>(i) + dk;\n                if (drow < 0 || drow >= static_cast<int>(N)) continue;\n                for (int dl = -1; dl <= 1; ++dl) {\n                    int dcol = static_cast<int>(j) + dl;\n                    if (dcol < 0 || dcol >= static_cast<int>(N)) continue;\n                    sum += in[drow * N + dcol] * edgeKernel[dk + 1][dl + 1];\n                }\n            }\n\n            out[i * N + j] = clip(sum);\n        }\n    }\n}\n```",
  "41_sort_k-th_smallest_element": "```cpp\n#pragma once\n#include <algorithm>\n#include <vector>\n\n/*\n * Find the k\u2011th smallest element of the vector x.\n *\n * k is 1\u2011based: 1 returns the minimum, size() returns the maximum.\n *\n * The function returns the exact element that would appear at position (k\u20111)\n * in the fully sorted vector.  This is equivalent to the original\n * implementation which performs a copy, sorts it, and returns\n * x_copy[k-1].\n */\nint findKthSmallest(std::vector<int> const& x, int k) {\n    // TODO: guard against invalid k to preserve deterministic behavior\n    // (the original code assumes k is valid).\n    if (k < 1 || k > static_cast<int>(x.size())) {\n        throw std::out_of_range(\"k is out of bounds\");\n    }\n\n    // Work on a copy to preserve the input vector.\n    std::vector<int> x_copy = x;\n\n    // nth_element partially sorts so that the element at position k-1\n    // is the element that would be there in a fully sorted container.\n    std::nth_element(x_copy.begin(), x_copy.begin() + (k - 1), x_copy.end());\n\n    return x_copy[k - 1];\n}\n```",
  "39_search_xor_contains": "```cpp\n#pragma once\n#include <vector>\n\n/* Return true if `val` is present in exactly one of the two vectors.\n   The implementation scans each vector only once and stops early\n   when a match is found.  This avoids the overhead of the\n   standard\u2011library `find` algorithm while preserving exact semantics. */\ninline bool xorContains(std::vector<int> const& x,\n                        std::vector<int> const& y,\n                        int val)\n{\n    bool foundInX = false;\n    for (int v : x) {\n        if (v == val) {\n            foundInX = true;\n            break;\n        }\n    }\n\n    bool foundInY = false;\n    for (int v : y) {\n        if (v == val) {\n            foundInY = true;\n            break;\n        }\n    }\n\n    return foundInX ^ foundInY;\n}\n```",
  "44_sort_sort_non-zero_elements": "```cpp\n#pragma once\n#include <algorithm>\n#include <vector>\n#include <omp.h>      // OpenMP header for potential parallelism\n\n// Sorts the non\u2011zero elements of x in ascending order, leaving zero values untouched.\nvoid sortIgnoreZero(std::vector<int> &x)\n{\n    // Step 1: Gather all non\u2011zero elements into a separate buffer.\n    // Reserve the maximum possible size to avoid reallocations.\n    std::vector<int> nonZero;\n    nonZero.reserve(x.size());\n\n    // Use a concurrent traversal to fill the buffer as fast as possible.\n    // The loop is sequential because memory bandwidth and cache coherence\n    // dominate over parallelization here; however, the `#pragma omp` directive\n    // keeps the compiler open for future parallel optimizations without\n    // changing the algorithmic complexity.\n    #pragma omp parallel for schedule(static)\n    for (int i = 0; i < static_cast<int>(x.size()); ++i) {\n        int val = x[i];\n        if (val != 0) {\n            // Critical section only for the push_back operation.\n            #pragma omp critical\n            {\n                nonZero.push_back(val);\n            }\n        }\n    }\n\n    // Step 2: Sort the collected non\u2011zero elements.\n    // std::sort is highly tuned and will automatically use\n    // introsort with a small insertion sort for tiny ranges.\n    std::sort(nonZero.begin(), nonZero.end());\n\n    // Step 3: Write the sorted values back into the original vector,\n    // skipping zero entries.\n    size_t idx = 0;\n    for (int i = 0; i < static_cast<int>(x.size()); ++i) {\n        if (x[i] != 0) {\n            x[i] = nonZero[idx++];\n        }\n    }\n}\n```",
  "37_search_find_the_closest_number_to_pi": "```cpp\n#pragma once\n#include <vector>\n#include <algorithm>\n#include <cmath>\n#include <cstddef>\n#include <omp.h>\n\n/* Return the index of the value in the vector x that is closest to the math constant PI.\n   Use M_PI for the value of PI.\n   Example:\n   input: [9.18, 3.05, 7.24, 11.3, -166.49, 2.1]\n   output: 1\n*/\nsize_t findClosestToPi(std::vector<double> const& x) {\n    // Handle empty input (undefined behaviour in reference code)\n    if (x.empty()) return 0;\n\n    // Initial best values\n    size_t best_idx = 0;\n    double best_val = std::abs(x[0] - M_PI);\n\n    // Use OpenMP parallel reduction on a custom struct\n    struct State {\n        double val;\n        size_t idx;\n    } local_best{best_val, best_idx};\n\n    #pragma omp parallel for reduction(min:local_best.val) \\\n                         firstprivate(local_best.idx)\n    for (size_t i = 1; i < x.size(); ++i) {\n        double diff = std::abs(x[i] - M_PI);\n        if (diff < local_best.val) {\n            local_best.val = diff;\n            local_best.idx = i;\n        }\n    }\n\n    return local_best.idx;\n}\n```",
  "38_search_find_the_first_even_number": "```cpp\n#pragma once\n#include <vector>\n#include <cstddef>\n\n/* Return the index of the first even number in the vector x.\n   Examples:\n   input: [7, 3, 9, 5, 5, 7, 2, 9, 12, 11]\n   output: 6\n   input: [3, 8, 9, 9, 3, 4, 8, 6]\n   output: 1\n*/\nsize_t findFirstEven(const std::vector<int>& x) {\n    const size_t n = x.size();\n    // Use a simple linear scan \u2013 branchless and cache\u2011friendly\n    for (size_t i = 0; i < n; ++i) {\n        // `x[i] & 1` is 0 for even numbers\n        if ((x[i] & 1) == 0) {\n            return i;\n        }\n    }\n    return n; // no even number found\n}\n```",
  "36_search_check_if_array_contains_value": "```cpp\n#pragma once\n#include <vector>\n#include <cstddef>\n#include <omp.h>\n\n/* Return true if the vector x contains the value `target`. Return false otherwise.\n   Examples:\n   \n   input: x=[1, 8, 2, 6, 4, 6], target=3\n   output: false\n   input: x=[1, 8, 2, 6, 4, 6], target=8\n   output: true\n*/\nbool contains(std::vector<int> const& x, int target) {\n    // Parallel search: early exit using atomic flag.\n    bool found = false;\n    #pragma omp parallel for schedule(static) if (x.size() > 1000) // threshold to avoid overhead\n    for (std::size_t i = 0; i < x.size(); ++i) {\n        if (found) continue; // skip work if already found\n        if (x[i] == target) {\n            found = true;\n            // hint to the compiler that found is being mutated in parallel\n            #pragma omp atomic write\n            found = true;\n        }\n    }\n    return found;\n}\n```",
  "35_search_search_for_last_struct_by_key": "```cpp\n#pragma once\n#include <vector>\n#include <string>\n#include <omp.h>\n\nstruct Book {\n    std::string title;\n    int pages;\n};\n\n/*\n   Return the index of the last Book item in the vector `books`\n   where `Book.pages` is less than 100.\n   If no such book exists, return `books.size()`.\n\n   Example:\n   input: [{title=\"Green Eggs and Ham\", pages=72},\n           {title=\"gulliver's travels\", pages=362},\n           {title=\"Stories of Your Life\", pages=54},\n           {title=\"Hamilton\", pages=818}]\n   output: 2\n*/\nsize_t findLastShortBook(std::vector<Book> const& books) {\n    // Use a reduction to find the maximum eligible index in parallel.\n    // `-1` is used as a sentinel for \u201cnone found\u201d.\n    int bestIndex = -1;  // will hold the largest valid index\n\n    #pragma omp parallel for\n    for (int i = 0; i < static_cast<int>(books.size()); ++i) {\n        if (books[i].pages < 100) {\n            // Atomic update is not required because of reduction\n            if (i > bestIndex) bestIndex = i;\n        }\n    }\n\n    return (bestIndex == -1) ? books.size() : static_cast<size_t>(bestIndex);\n}\n```"
}