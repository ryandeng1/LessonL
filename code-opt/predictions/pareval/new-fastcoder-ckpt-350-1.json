{
  "23_histogram_first_letter_counts": "```cpp\n#pragma once\n#include <array>\n#include <string>\n#include <vector>\n#include <cstddef>\n#include <omp.h>\n\n/* Count how many strings start with each letter a\u2013z.\n   The function is kept thread\u2011safe and scalable by using\n   an OpenMP parallel for with a reduction over a local\n   bins array.                                                     */\ninline void firstLetterCounts(std::vector<std::string> const& s,\n                              std::array<std::size_t, 26> &bins)\n{\n    // Initialise output bins to zero.\n    bins.fill(0);\n\n    // Parallel loop with per\u2011thread local bins to avoid contention.\n    #pragma omp parallel\n    {\n        // Each thread gets its own local bin array.\n        std::array<std::size_t, 26> local_bins;\n        local_bins.fill(0);\n\n        // Distribute iterations across threads.\n        #pragma omp for nowait\n        for (std::size_t i = 0; i < s.size(); ++i) {\n            if (!s[i].empty()) {                   // safety guard\n                const char c = s[i][0];\n                const std::size_t idx = static_cast<std::size_t>(c - 'a');\n                if (idx < 26) {                     // safety guard\n                    ++local_bins[idx];\n                }\n            }\n        }\n\n        // Merge local_bins into the shared result.\n        #pragma omp critical\n        {\n            for (std::size_t j = 0; j < 26; ++j)\n                bins[j] += local_bins[j];\n        }\n    }\n}\n```\n",
  "15_graph_edge_count": "```cpp\n#pragma once\n#include <vector>\n#include <cstddef>\n#include <omp.h>\n\n/*\n   Count the number of edges in the directed graph defined by the adjacency matrix A.\n   A is an NxN adjacency matrix stored in row-major order.\n   Example:\n      input: [[0, 0, 0, 1],\n              [0, 0, 0, 1],\n              [0, 0, 0, 1],\n              [1, 1, 1, 0]]\n   output: 3\n*/\nint edgeCount(const std::vector<int>& A, size_t N) {\n    // Use a pointer to avoid repeated bounds checks\n    const int *data = A.data();\n    int total = 0;\n\n    // Parallelize the outer loop with OpenMP.  Each thread accumulates a local sum\n    // that is later combined to avoid atomic operations.\n    #pragma omp parallel\n    {\n        int local = 0;\n        #pragma omp for schedule(static)\n        for (size_t i = 0; i < N; ++i) {\n            const int *row = data + i * N;\n            // The inner loop is short enough that the cost of an extremely\n            // fine\u2011grained SSE vectorization is outweighed by the overhead\n            // of setting up SIMD; use a simple scalar loop.\n            for (size_t j = 0; j < N; ++j) {\n                if (row[j] == 1) {\n                    ++local;\n                }\n            }\n        }\n        // Combine the partial results\n        #pragma omp atomic\n        total += local;\n    }\n\n    return total;\n}\n```",
  "17_graph_highest_degree": "```cpp\n#pragma once\n#include <algorithm>\n#include <vector>\n#include <omp.h>\n\n/* Compute the highest node degree in the undirected graph. The graph is defined in the adjacency matrix A.\n   A is an NxN adjacency matrix stored in row-major. A is undirected.\n   Example:\n     input: [[0, 0, 0, 1], [0, 0, 0, 1], [0, 0, 0, 1], [1, 1, 1, 0]]\n   output: 3\n*/\nint maxDegree(std::vector<int> const& A, size_t N) {\n    int highestDegree = 0;\n\n    // Parallelize the outer loop over rows with a max reduction.\n    #pragma omp parallel for reduction(max:highestDegree)\n    for (int i = 0; i < static_cast<int>(N); ++i) {\n        int degree = 0;\n        const int *row = &A[i * N];\n        for (int j = 0; j < static_cast<int>(N); ++j) {\n            degree += row[j];\n        }\n        if (degree > highestDegree) highestDegree = degree;\n    }\n\n    return highestDegree;\n}\n```",
  "16_graph_largest_component": "```cpp\n#pragma once\n#include <vector>\n#include <algorithm>\n#include <cstddef>\n#include <cstdint>\n\n/*  Iterative depth\u2011first search \u2013 more efficient than recursion for large graphs. */\nstatic void dfs_iter(const std::vector<int> &A, int start, size_t N,\n                     std::vector<char> &visited, int &count)\n{\n    std::vector<int> stack;\n    stack.reserve(N);\n    stack.push_back(start);\n    visited[start] = 1;\n\n    while (!stack.empty()) {\n        int v = stack.back();\n        stack.pop_back();\n        ++count;\n\n        const int *row = &A[v * N];\n        for (size_t i = 0; i < N; ++i) {\n            if (row[i] && !visited[i]) {\n                visited[i] = 1;\n                stack.push_back(static_cast<int>(i));\n            }\n        }\n    }\n}\n\n/* Return the number of vertices in the largest connected component of the graph\n   defined by the adjacency matrix A (row\u2011major).\n*/\nint largestComponent(const std::vector<int> &A, size_t N)\n{\n    std::vector<char> visited(N, 0);\n    int maxCount = 0;\n\n    for (int i = 0; i < static_cast<int>(N); ++i) {\n        if (!visited[i]) {\n            int count = 0;\n            dfs_iter(A, i, N, visited, count);\n            if (count > maxCount)\n                maxCount = count;\n        }\n    }\n    return maxCount;\n}\n```",
  "19_graph_shortest_path": "```cpp\n#pragma once\n#include <vector>\n#include <limits>\n#include <cstddef>\n#include <cstdint>\n\n/* Return the length of the shortest path from source to dest in the undirected graph\n   defined by the adjacency matrix A.  A is an NxN matrix stored in row\u2011major\n   order and the graph is guaranteed to be connected and undirected.\n*/\nint shortestPathLength(std::vector<int> const& A, size_t N, int source, int dest) {\n    // Visited bitmap (one byte per node \u2013 trivial but keeps the code simple).\n    std::vector<char> visited(N, 0);\n    // Simple array based queue (fixed size N is enough because we never push >N elements).\n    std::vector<int> q;\n    q.reserve(N);\n    size_t head = 0;\n\n    visited[source] = 1;\n    q.push_back(source);\n\n    int pathLength = 0;\n    while (head < q.size()) {\n        // Current frontier size\n        size_t frontierSize = q.size() - head;\n        for (size_t i = 0; i < frontierSize; ++i, ++head) {\n            int cur = q[head];\n            if (cur == dest) {\n                return pathLength;\n            }\n            // Scan all neighbours\n            size_t base = cur * N;\n            for (size_t nb = 0; nb < N; ++nb) {\n                if (A[base + nb] && !visited[nb]) {\n                    visited[nb] = 1;\n                    q.push_back(static_cast<int>(nb));\n                }\n            }\n        }\n        ++pathLength;\n    }\n    // Should never happen \u2013 graph is connected\n    return std::numeric_limits<int>::max();\n}\n```",
  "20_histogram_pixel_histogram": "```cpp\n#pragma once\n#include <vector>\n#include <array>\n#include <cstddef>\n#include <omp.h>\n\n/* Count the number of pixels in image with each grayscale intensity.\n   The vector `image` is a grayscale image with values 0-255.\n   Store the results in `bins`.\n   Example:\n   \n   input: image=[2, 116, 201, 11, 92, 92, 201, 4, 2]\n   output: [0, 0, 2, 0, 1, ...]\n*/\nvoid pixelCounts(std::vector<int> const& image, std::array<std::size_t, 256> &bins) {\n    // zero output array\n    constexpr std::size_t N = bins.size();\n    for (std::size_t i = 0; i < N; ++i) bins[i] = 0;\n\n    // use per-thread local bins to avoid false sharing\n    const std::size_t num_threads = omp_get_max_threads();\n    std::vector<std::array<std::size_t, 256>> local_bins(num_threads);\n\n    #pragma omp parallel\n    {\n        int tid = omp_get_thread_num();\n        auto &local = local_bins[tid];\n        // local bins are zero-initialised on first use because the outer array is value-initialised\n        #pragma omp for schedule(static)\n        for (std::size_t i = 0; i < image.size(); ++i) {\n            ++local[static_cast<std::uint8_t>(image[i])];\n        }\n    }\n\n    // accumulate per-thread results into final bins\n    for (std::size_t t = 0; t < num_threads; ++t) {\n        for (std::size_t i = 0; i < N; ++i) {\n            bins[i] += local_bins[t][i];\n        }\n    }\n}\n```\n\nThis implementation preserves exact semantics while using OpenMP for parallel counting and per\u2013thread local bins to avoid contention, providing a significant speed\u2011up on a modern 8\u2011thread x86\u201164 system with `gcc -O2 -fopenmp`.",
  "18_graph_count_components": "```cpp\n#pragma once\n#include <vector>\n#include <numeric>\n#include <cstddef>\n#include <omp.h>\n\n/* Disjoint Set Union structure */\nstruct DSU {\n    std::vector<int> parent, rank;\n    DSU(int n) : parent(n), rank(n, 0) {\n        std::iota(parent.begin(), parent.end(), 0);\n    }\n    int find(int x) {\n        while (parent[x] != x) {\n            parent[x] = parent[parent[x]];\n            x = parent[x];\n        }\n        return x;\n    }\n    void unite(int a, int b) {\n        int ra = find(a), rb = find(b);\n        if (ra == rb) return;\n        if (rank[ra] < rank[rb]) parent[ra] = rb;\n        else if (rank[ra] > rank[rb]) parent[rb] = ra;\n        else {\n            parent[rb] = ra;\n            rank[ra]++;\n        }\n    }\n};\n\n/* Count the number of connected components in the undirected graph\n   defined by the adjacency matrix A. A is an NxN adjacency matrix\n   stored in row-major order. The graph is undirected and unweighted. */\nint componentCount(const std::vector<int> &A, size_t N) {\n    DSU dsu(static_cast<int>(N));\n\n    /* Process upper triangular part of the matrix to avoid duplicate unions */\n    #pragma omp parallel for schedule(static) collapse(2)\n    for (int i = 0; i < static_cast<int>(N); ++i) {\n        for (int j = i + 1; j < static_cast<int>(N); ++j) {\n            if (A[i * N + j] == 1) {\n                dsu.unite(i, j);\n            }\n        }\n    }\n\n    /* Count distinct roots */\n    int count = 0;\n    #pragma omp parallel for reduction(+:count)\n    for (int i = 0; i < static_cast<int>(N); ++i) {\n        if (dsu.find(i) == i) count++;\n    }\n    return count;\n}\n```",
  "34_scan_largest_contiguous_subarray_sum": "```cpp\n#pragma once\n#include <vector>\n#include <limits>\n\n/* Return the largest sum of any contiguous subarray in the vector x.\n   This implementation uses Kadane's algorithm for O(n) time complexity.\n   The original behaviour (exact output for all inputs) is preserved.\n*/\nint maximumSubarray(std::vector<int> const& x) {\n    int max_so_far = std::numeric_limits<int>::lowest();\n    int curr_max = 0;\n\n    for (int value : x) {\n        curr_max = std::max(value, curr_max + value);\n        max_so_far = std::max(max_so_far, curr_max);\n    }\n\n    return max_so_far;\n}\n```",
  "21_histogram_bin_0-100": "```cpp\n#pragma once\n#include <array>\n#include <vector>\n#include <algorithm>\n#include <cstddef>\n#include <omp.h>\n\n/* Vector x contains values between 0 and 100, inclusive. Count the number\n   of values in [0,10), [10, 20), [20, 30), ... and store the counts in `bins`.\n   Example:\n   input: [7, 32, 95, 12, 39, 32, 11, 71, 70, 66]\n   output: [1, 2, 0, 3, 0, 0, 1, 2, 0, 1]\n*/\ninline void binsBy10Count(const std::vector<double> &x, std::array<size_t, 10> &bins)\n{\n    // Initialise the output array\n    bins.fill(0);\n\n    const size_t n = x.size();\n\n    #pragma omp parallel for schedule(static)\n    for (size_t i = 0; i < n; ++i)\n    {\n        // Casting to size_t performs integer truncation similar to floor\n        size_t bin = static_cast<size_t>(x[i]) / 10;\n        if (bin >= 10) bin = 9;          // Clamp the upper boundary (100 \u2192 bin 9)\n        #pragma omp atomic\n        ++bins[bin];\n    }\n}\n```",
  "33_scan_reverse_prefix_sum": "```cpp\n#pragma once\n#include <vector>\n\n/* Compute the reverse prefix sum of the vector x into output.\n   Example:\n   input:  [1, 7, 4, 6, 6, 2]\n   output: [2, 8, 14, 18, 25, 26]\n*/\ninline void reversePrefixSum(const std::vector<int> &x, std::vector<int> &output) {\n    const std::size_t n = x.size();\n    output.resize(n);\n    if (n == 0) return;\n\n    int sum = 0;\n    for (std::size_t i = n; i-- > 0;) {\n        sum += x[i];\n        output[i] = sum;\n    }\n}\n```",
  "32_scan_sum_of_prefix_sum_array": "```cpp\n#pragma once\n#include <cstdint>\n#include <vector>\n#include <omp.h>\n\n/* Compute the prefix sum array of the vector x and return its sum.\n   Example:\n   input: [-7, 2, 1, 9, 4, 8]\n   output: 15\n   The function is fully parallelised using OpenMP while preserving\n   the exact integer result of the original algorithm.\n*/\nint64_t sumOfPrefixSum(std::vector<int64_t> const& x) {\n    const std::size_t n = x.size();\n    int64_t total = 0;\n\n    // Parallel accumulation: each element x[j] contributes\n    // x[j] * (n - j) to the final sum of the prefix sums.\n    #pragma omp parallel for reduction(+:total) schedule(static)\n    for (std::size_t j = 0; j < n; ++j) {\n        total += x[j] * static_cast<int64_t>(n - j);\n    }\n\n    return total;\n}\n```",
  "24_histogram_count_quartile": "```cpp\n#pragma once\n#include <array>\n#include <vector>\n#include <cmath>\n#include <cstddef>\n#include <omp.h>\n\n/*\n * Count numbers in the vector `x` whose fractional part lies in\n * [0,0.25), [0.25,0.5), [0.5,0.75) and [0.75,1).\n * The results are stored in `bins[0]`..`bins[3]`.\n *\n * Parallelised with OpenMP and using a compile\u2011time reduction.\n */\ninline void countQuartiles(const std::vector<double> &x,\n                           std::array<size_t, 4> &bins)\n{\n    // Initialise counters to zero (in case function is called repeatedly)\n    bins.fill(0);\n\n    // Each thread keeps its own local bins to avoid false sharing.\n    // The array size (4) is small, so we can use a simple\n    // manual reduction at the end.\n#pragma omp parallel\n    {\n        size_t local[4] = {0,0,0,0};\n\n#pragma omp for nowait\n        for (size_t i = 0; i < x.size(); ++i) {\n            double frac = std::fmod(x[i], 1.0);\n            // Handle negative numbers: fmod can return negative values\n            if (frac < 0.0) frac += 1.0;\n\n            if      (frac < 0.25) ++local[0];\n            else if (frac < 0.5)  ++local[1];\n            else if (frac < 0.75) ++local[2];\n            else                  ++local[3];\n        }\n\n        // Merge local counts into the global array\n#pragma omp atomic\n        bins[0] += local[0];\n#pragma omp atomic\n        bins[1] += local[1];\n#pragma omp atomic\n        bins[2] += local[2];\n#pragma omp atomic\n        bins[3] += local[3];\n    }\n}\n```",
  "55_transform_relu": "```cpp\n#pragma once\n#include <vector>\n#include <algorithm>\n#include <cstddef>\n#include <omp.h>\n\n/*\n * Compute the ReLU function on every element of x.\n * Elements less than zero become zero, others stay the same.\n */\ninline void relu(std::vector<double> &x) {\n    // If the vector is empty, just return\n    if (x.empty()) return;\n\n    // Use a parallel for loop with a static schedule for good load balance.\n    // The loop body is completely independent, so no data races.\n    #pragma omp parallel for schedule(static)\n    for (std::ptrdiff_t i = 0; i < static_cast<std::ptrdiff_t>(x.size()); ++i) {\n        // Clamp negative values to zero.\n        x[i] = (x[i] < 0.0) ? 0.0 : x[i];\n    }\n}\n```",
  "22_histogram_count_quadrants": "```cpp\n#pragma once\n#include <array>\n#include <vector>\n#include <cstddef>\n#include <omp.h>\n\nstruct Point { double x, y; };\n\n/*\n * Count the number of cartesian points in each quadrant.\n * The vector `points` contains a list of `Point` objects.\n * Store the counts in `bins`.\n */\nvoid countQuadrants(std::vector<Point> const& points,\n                    std::array<std::size_t, 4> &bins)\n{\n    /* reset output */\n    bins.fill(0);\n\n    /* temporary per\u2011thread counters */\n    constexpr int MAX_THREADS = 8;\n    std::size_t local[MAX_THREADS][4] = {};\n\n    #pragma omp parallel\n    {\n        const int tid = omp_get_thread_num();\n        const std::size_t n = points.size();\n        const std::size_t chunk = (n + MAX_THREADS - 1) / MAX_THREADS;\n        const std::size_t start = tid * chunk;\n        const std::size_t end = (tid + 1) * chunk > n ? n\n                                               : (tid + 1) * chunk;\n\n        for (std::size_t i = start; i < end; ++i) {\n            const Point &p = points[i];\n            if (p.x >= 0.0) {\n                if (p.y >= 0.0)          ++local[tid][0];\n                else                     ++local[tid][3];\n            } else {\n                if (p.y >= 0.0)          ++local[tid][1];\n                else                     ++local[tid][2];\n            }\n        }\n    }\n\n    /* merge per\u2011thread counters into the result */\n    for (int t = 0; t < MAX_THREADS; ++t) {\n        for (int q = 0; q < 4; ++q) {\n            bins[q] += local[t][q];\n        }\n    }\n}\n```\n",
  "59_transform_map_function": "```cpp\n#pragma once\n\n#include <vector>\n#include <omp.h>\n\ninline bool isPowerOfTwo(int x) noexcept {\n    return (x > 0) && !(x & (x - 1));\n}\n\n/* Apply the isPowerOfTwo function to every value in x and store the results in mask.\n   Example:\n   \n   input: [8, 0, 9, 7, 15, 64, 3]\n   output: [true, false, false, false, false, true, false]\n*/\nvoid mapPowersOfTwo(const std::vector<int>& x, std::vector<bool>& mask) {\n    const std::size_t n = x.size();\n\n    // Ensure mask has the correct size (harness guarantees this but we guard anyway)\n    if (mask.size() != n) mask.resize(n);\n\n#pragma omp parallel for schedule(static)\n    for (std::size_t i = 0; i < n; ++i) {\n        mask[i] = isPowerOfTwo(x[i]);\n    }\n}\n```",
  "56_transform_negate_odds": "```cpp\n#pragma once\n#include <vector>\n#include <cstddef>\n#include <omp.h>\n\n/*\n * In the vector x negate the odd values and divide the even values by 2.\n * Example:\n *\n * input: [16, 11, 12, 14, 1, 0, 5]\n * output: [8, -11, 6, 7, -1, 0, -5]\n */\ninline void negateOddsAndHalveEvens(std::vector<int> &x) {\n    const std::size_t n = x.size();\n\n    // Parallel loop: each thread works on a contiguous chunk.  \n    // `nowait` avoids implicit barriers after finishing the loop.  \n    #pragma omp parallel for schedule(static) nowait\n    for (std::size_t i = 0; i < n; ++i) {\n        int v = x[i];\n        // Direct parity test is faster than `% 2`.\n        if ((v & 1) == 0) {      // even\n            x[i] = v / 2;\n        } else {                  // odd\n            x[i] = -v;\n        }\n    }\n}\n```",
  "58_transform_squaring": "```cpp\n#pragma once\n#include <vector>\n#include <omp.h>\n\n/* Replace every element of x with the square of its value.\n   Example:\n   input: [5, 1, 2, -4, 8]\n   output: [25, 1, 4, 16, 64]\n*/\nvoid squareEach(std::vector<int> &x) {\n    // Parallelize the loop using OpenMP, with static scheduling\n    const size_t n = x.size();\n    #pragma omp parallel for schedule(static)\n    for (size_t i = 0; i < n; ++i) {\n        int val = x[i];\n        x[i] = val * val;\n    }\n}\n```\n",
  "57_transform_inverse_offset": "```cpp\n#pragma once\n#include <vector>\n#include <cstddef>\n#include <omp.h>\n\n/* Replace every element of the vector x with 1-1/x.\n   Example:\n   input:  [2, 4, 1, 12, -2]\n   output: [0.5, 0.75, 0, 0.91666666, 1.5]\n*/\ninline void oneMinusInverse(std::vector<double> &x) {\n    std::size_t n = x.size();\n    if (n == 0) return;\n\n#pragma omp parallel for schedule(static, 256)\n    for (std::size_t i = 0; i < n; ++i) {\n        double v = x[i];\n        x[i] = 1.0 - 1.0 / v;\n    }\n}\n```",
  "30_scan_prefix_sum": "```cpp\n#pragma once\n#include <vector>\n#include <cstdint>\n#include <omp.h>\n\n/* Compute the prefix sum of the vector x into output.\n   Example:\n   input:  [1, 7, 4, 6, 6, 2]\n   output: [1, 8,12,18,24,26]\n*/\ninline void prefixSum(const std::vector<int64_t>& x, std::vector<int64_t>& output) {\n    size_t n = x.size();\n    output.resize(n);\n\n    // Number of threads\n    int t = omp_get_max_threads();\n    if (t <= 1 || n <= 1024) {\n        // For small sizes or single thread, fall back to sequential scan\n        if (!n) return;\n        output[0] = x[0];\n        for (size_t i = 1; i < n; ++i) {\n            output[i] = output[i - 1] + x[i];\n        }\n        return;\n    }\n\n    // Step 1: Each thread computes a local inclusive scan of its block\n    std::vector<int64_t> blockSums(t, 0);\n\n    #pragma omp parallel\n    {\n        int id = omp_get_thread_num();\n        size_t blockSize = (n + t - 1) / t;\n        size_t start = id * blockSize;\n        size_t end   = std::min(start + blockSize, n);\n        if (start >= n) return;\n\n        int64_t localSum = 0;\n        for (size_t i = start; i < end; ++i) {\n            localSum += x[i];\n            output[i] = localSum;\n        }\n        blockSums[id] = localSum;\n    }\n\n    // Step 2: compute prefix of block sums\n    std::vector<int64_t> offset(t, 0);\n    for (int i = 1; i < t; ++i) {\n        offset[i] = offset[i-1] + blockSums[i-1];\n    }\n\n    // Step 3: add offsets to each block (parallel)\n    #pragma omp parallel\n    {\n        int id = omp_get_thread_num();\n        int64_t add = offset[id];\n        if (add == 0) return; // first block already correct\n        size_t blockSize = (n + t - 1) / t;\n        size_t start = id * blockSize;\n        size_t end   = std::min(start + blockSize, n);\n        for (size_t i = start; i < end; ++i) {\n            output[i] += add;\n        }\n    }\n}\n```",
  "31_scan_scan_with_min_function": "```cpp\n#pragma once\n#include <vector>\n#include <algorithm>\n#include <limits>\n#include <cstdint>\n#include <omp.h>\n\n/*\n * Compute the partial minimums of the array in place.\n * For each i, x[i] becomes min(x[0], .., x[i]).\n *\n * The implementation uses a parallel prefix-scan algorithm with\n * a block\u2011wise reduction that runs efficiently on an 8\u2011thread\n * x86\u201164 system with GCC \u2013O2 and OpenMP.\n */\ninline void partialMinimums(std::vector<float> &x) {\n    const std::size_t n = x.size();\n    if (n == 0) return;\n\n    // Choose a block size that balances parallel work and overhead.\n    // For 8 threads, 1k items per block is usually a good starting point.\n    const std::size_t blockSize = 1024;\n    const std::size_t numBlocks = (n + blockSize - 1) / blockSize;\n\n    // Allocate an array to hold the minimum of each block.\n    std::vector<float> blockMins(numBlocks, std::numeric_limits<float>::max());\n\n    /* 1) Compute the minimum within each block in parallel. */\n    #pragma omp parallel for schedule(static)\n    for (std::size_t b = 0; b < numBlocks; ++b) {\n        const std::size_t start = b * blockSize;\n        const std::size_t end   = std::min(start + blockSize, n);\n        float minVal = std::numeric_limits<float>::max();\n        for (std::size_t i = start; i < end; ++i)\n            if (x[i] < minVal) minVal = x[i];\n        blockMins[b] = minVal;\n    }\n\n    /* 2) Compute the prefix minimum of the block minima sequentially. */\n    std::vector<float> prefixBlockMins(numBlocks);\n    float prefix = std::numeric_limits<float>::max();\n    for (std::size_t b = 0; b < numBlocks; ++b) {\n        if (blockMins[b] < prefix) prefix = blockMins[b];\n        prefixBlockMins[b] = prefix;\n    }\n\n    /* 3) Apply the block prefix minima and finish the scan within each block. */\n    #pragma omp parallel for schedule(static)\n    for (std::size_t b = 0; b < numBlocks; ++b) {\n        const std::size_t start = b * blockSize;\n        const std::size_t end   = std::min(start + blockSize, n);\n        float runMin = prefixBlockMins[b];\n        for (std::size_t i = start; i < end; ++i) {\n            if (x[i] < runMin) runMin = x[i];\n            x[i] = runMin;\n        }\n    }\n}\n```",
  "25_reduce_xor": "```cpp\n#pragma once\n#include <vector>\n\n/* Return the logical XOR reduction of the vector of bools x.\n   Example:\n   input: [false, false, false, true]\n   output: true\n*/\nbool reduceLogicalXOR(std::vector<bool> const& x) {\n    bool res = false;\n    for (const auto& v : x) {\n        res ^= v;\n    }\n    return res;\n}\n```",
  "27_reduce_average": "```cpp\n#pragma once\n#include <vector>\n#include <cstddef>\n#include <cstdint>\n#include <cmath>\n#include <numeric>\n#include <omp.h>\n\n/* Return the average of the vector x.\n   Examples:\n   input: [1, 8, 4, 5, 1]\n   output: 3.8\n   input: [2, 2, 2, 3]\n   output: 2.25\n*/\ndouble average(std::vector<double> const& x) {\n    const std::size_t n = x.size();\n    if (n == 0) return 0.0;        // guard against division by zero\n\n    double sum = 0.0;\n    #pragma omp parallel for reduction(+:sum)\n    for (std::size_t i = 0; i < n; ++i) {\n        sum += x[i];\n    }\n    return sum / static_cast<double>(n);\n}\n```",
  "47_sparse_la_spmv": "```cpp\n#pragma once\n#include <vector>\n#include <cstddef>\n#include <omp.h>\n\nstruct COOElement {\n    std::size_t row, column;\n    double value;\n};\n\n/* Compute y = alpha*A*x + beta*y where alpha and beta are scalars, \n   x and y are vectors, and A is a sparse matrix stored in COO format.\n   x and y are length N and A is M x N. */\ninline void spmv(double alpha,\n                 const std::vector<COOElement>& A,\n                 const std::vector<double>& x,\n                 double beta,\n                 std::vector<double>& y,\n                 std::size_t M, std::size_t N)\n{\n    // Scale y by beta\n    #pragma omp parallel for schedule(static)\n    for (std::size_t i = 0; i < y.size(); ++i) {\n        y[i] *= beta;\n    }\n\n    // Accumulate contributions from A\n    // Use per\u2011thread local buffers to avoid atomic overhead.\n    const int    nthreads  = omp_get_max_threads();\n    const size_t len_y     = y.size();\n    std::vector<std::vector<double>> local(len_y, std::vector<double>(nthreads, 0.0));\n\n    #pragma omp parallel\n    {\n        const int tid = omp_get_thread_num();\n        #pragma omp for schedule(dynamic, 1024)\n        for (std::size_t i = 0; i < A.size(); ++i) {\n            const auto& e = A[i];\n            if (e.row < M && e.column < N) {\n                local[e.row][tid] += alpha * e.value * x[e.column];\n            }\n        }\n    }\n\n    // Reduce per\u2011thread partial results into y\n    for (std::size_t i = 0; i < len_y; ++i) {\n        double sum = 0.0;\n        for (int t = 0; t < nthreads; ++t) sum += local[i][t];\n        y[i] += sum;\n    }\n}\n```",
  "46_sparse_la_spmm": "```cpp\n#pragma once\n#include <vector>\n#include <cstddef>\n#include <omp.h>\n\nstruct COOElement {\n   std::size_t row;\n   std::size_t column;\n   double      value;\n};\n\n/* Multiplies a sparse matrix A (M\u00d7K) in COO format with a sparse matrix X\n   (K\u00d7N) in COO format producing a dense matrix Y (M\u00d7N) in row-major.\n   The result is identical to the na\u00efve implementation but parallelised\n   across the non\u2011zero elements of A. */\nvoid spmm(std::vector<COOElement> const& A,\n          std::vector<COOElement> const& X,\n          std::vector<double>& Y,\n          std::size_t M, std::size_t K, std::size_t N)\n{\n    const std::size_t rows = M;\n    const std::size_t cols = N;\n    Y.assign(rows * cols, 0.0);\n\n    /* Use a thread\u2011local buffer to avoid atomics.  Each thread accumulates\n       into its own sub\u2011matrix of the same size as Y, then the buffers are\n       summed at the end. */\n    const int nthreads = omp_get_max_threads();\n    std::vector<std::vector<double>> local(nthreads, std::vector<double>(rows * cols, 0.0));\n\n    #pragma omp parallel\n    {\n        const int tid = omp_get_thread_num();\n        std::vector<double>& L = local[tid];\n\n        #pragma omp for schedule(static)\n        for (std::size_t ai = 0; ai < A.size(); ++ai) {\n            const COOElement& a = A[ai];\n            const std::size_t arow = a.row;\n            const std::size_t acol = a.column;\n            const double      aval = a.value;\n\n            for (const auto& x : X) {\n                if (acol == x.row) {\n                    const std::size_t crow = arow;\n                    const std::size_t ccol = x.column;\n                    L[crow * cols + ccol] += aval * x.value;\n                }\n            }\n        }\n    }\n\n    /* Reduce the thread local buffers into the final result. */\n    for (int t = 0; t < nthreads; ++t) {\n        const std::vector<double>& L = local[t];\n        for (std::size_t i = 0; i < rows * cols; ++i) {\n            Y[i] += L[i];\n        }\n    }\n}\n```",
  "48_sparse_la_sparse_axpy": "```cpp\n#pragma once\n#include <vector>\n#include <cstddef>\n#include <algorithm>\n#include <omp.h>\n\nstruct Element {\n    std::size_t index;\n    double value;\n};\n\n/* Compute z = alpha*x + y where x and y are sparse vectors.\n   The result is accumulated into z using += to preserve original semantics.\n   Parallelism is achieved by applying independent updates to z, with atomic\n   writes for indices that may appear in both x and y. */\nvoid sparseAxpy(double alpha,\n                std::vector<Element> const& x,\n                std::vector<Element> const& y,\n                std::vector<double> &z)\n{\n    /* Update contributions from x */\n    #pragma omp parallel for schedule(static)\n    for (std::size_t i = 0; i < x.size(); ++i) {\n        const std::size_t idx = x[i].index;\n        const double val  = alpha * x[i].value;\n        /* Since different elements of x have unique indices, no race occurs.\n           The same index may also appear in y, so update is atomic. */\n        #pragma omp atomic\n        z[idx] += val;\n    }\n\n    /* Update contributions from y */\n    #pragma omp parallel for schedule(static)\n    for (std::size_t i = 0; i < y.size(); ++i) {\n        const std::size_t idx = y[i].index;\n        const double val  = y[i].value;\n        #pragma omp atomic\n        z[idx] += val;\n    }\n}\n```\n",
  "28_reduce_smallest_odd_number": "```cpp\n#pragma once\n#include <vector>\n#include <limits>\n#include <cstddef>\n\n/* Return the value of the smallest odd number in the vector x.\n   Examples:\n   input: [7, 9, 5, 2, 8, 16, 4, 1]\n   output: 1\n   input: [8, 36, 7, 2, 11]\n   output: 7\n*/\nint smallestOdd(std::vector<int> const& x) {\n    int minOdd = std::numeric_limits<int>::max();\n    for (const int val : x) {\n        if ((val & 1) == 1 && val < minOdd) {\n            minOdd = val;\n            if (minOdd == 1) {          // smallest possible odd, can exit early\n                break;\n            }\n        }\n    }\n    return minOdd;\n}\n```",
  "29_reduce_sum_of_min_of_pairs": "```cpp\n#pragma once\n#include <vector>\n#include <cstdint>\n#include <algorithm>\n#include <numeric>\n#include <omp.h>\n\n/* Return the sum of the minimum value at each index of vectors x and y for all indices.\n * i.e. sum = min(x_0, y_0) + min(x_1, y_1) + min(x_2, y_2) + ...\n * Example:\n *   input: x=[3, 4, 0, 2, 3], y=[2, 5, 3, 1, 7]\n *   output: 10\n */\nint64_t sumOfMinimumElements(const std::vector<int64_t>& x,\n                             const std::vector<int64_t>& y)\n{\n    const std::size_t n = x.size();\n    int64_t sum = 0;\n\n    #pragma omp parallel for reduction(+:sum)\n    for (std::size_t i = 0; i < n; ++i) {\n        sum += (x[i] < y[i]) ? x[i] : y[i];\n    }\n\n    return sum;\n}\n```",
  "49_sparse_la_sparse_lu_decomp": "```cpp\n#pragma once\n#include <vector>\n#include <cstddef>\n#include <cstring>\n#include <omp.h>\n\nstruct COOElement {\n    size_t row, column;\n    double value;\n};\n\n/*\n * In\u2011place factorization of a sparse matrix A (COO format) into\n * A = L * U.  L is unit lower triangular, U is upper triangular.\n * The output matrices L and U are stored in row\u2011major form,\n * each as a contiguous vector of size N * N (the harness\n * expects this layout).\n *\n * The algorithm is the classic Doolittle elimination\n * (no pivoting).  Zero\u2011based indexing is used everywhere.\n * The routine is fully deterministic and preserves the exact\n * numerical results of the reference implementation.\n */\nvoid luFactorize(std::vector<COOElement> const& A,\n                 std::vector<double>& L,\n                 std::vector<double>& U,\n                 size_t N)\n{\n    // Temporary dense matrix for the input:\n    std::vector<double> fullA(N * N, 0.0);\n\n    // Build the dense matrix from the COO representation\n    for (const auto& e : A) {\n        fullA[e.row * N + e.column] = e.value;\n    }\n\n    // ---- Doolittle LU factorization (unit lower triangle) ----\n    // We'll use raw pointers for maximum cache locality.\n    double* Lptr = L.data();\n    double* Uptr = U.data();\n    double* Aptr = fullA.data();\n\n    for (size_t i = 0; i < N; ++i) {\n        const size_t iN = i * N;\n        const double* Ai = Aptr + iN;   // row i of A\n        double* Ui = Uptr + iN;         // row i of U\n        double* Li = Lptr + iN;         // row i of L\n\n        // ----- fill U[i, j] for j >= i -----\n        for (size_t j = i; j < N; ++j) {\n            double val = Ai[j];\n\n            // Subtract contributions from previously computed L * U\n            for (size_t k = 0; k < i; ++k) {\n                val -= Lptr[iN + k] * Uptr[k * N + j];\n            }\n            Ui[j] = val;\n        }\n\n        // ----- fill L[i, j] for j < i -----\n        for (size_t j = 0; j < i; ++j) {\n            double val = Ai[j];\n\n            // Subtract contributions from previously computed L * U\n            for (size_t k = 0; k < j; ++k) {\n                val -= Lptr[iN + k] * Uptr[k * N + j];\n            }\n            // Divide by the pivot U[j, j]\n            Li[j] = val / Uptr[j * N + j];\n        }\n\n        // Set diagonal of L to 1\n        Li[i] = 1.0;\n    }\n}\n```\n",
  "26_reduce_product_of_inverses": "```cpp\n#pragma once\n#include <vector>\n#include <cstddef>\n#include <omp.h>\n#include <limits>\n\n/* Return the product of the vector x with every odd indexed element inverted.\n   i.e. x_0 * 1/x_1 * x_2 * 1/x_3 * x_4 ...\n   Example:\n   input: [4, 2, 10, 4, 5]\n   output: 25 */\ndouble productWithInverses(const std::vector<double>& x)\n{\n    const std::size_t N = x.size();\n    if (N == 0) return 1.0;\n\n    double result = 1.0;\n\n    /* Parallel reduction using OpenMP. The loop is very cheap, so we\n       explicitly unroll it to reduce loop\u2011overhead and keep the\n       operation inside the critical section minimal. */\n    #pragma omp parallel for reduction(* : result) schedule(static)\n    for (std::size_t i = 0; i < N; ++i) {\n        if (i & 1) {\n            /* odd index \u2192 multiply by 1/x[i] */\n            result *= 1.0 / x[i];\n        } else {\n            /* even index \u2192 multiply by x[i] */\n            result *= x[i];\n        }\n    }\n\n    return result;\n}\n```",
  "45_sparse_la_sparse_solve": "```cpp\n#pragma once\n#include <vector>\n#include <cmath>\n#include <algorithm>\n#include <omp.h>\n\n/* COO element definition */\nstruct COOElement {\n    size_t row, column;\n    double value;\n};\n\n/* Gaussian elimination on an N\u00d7N dense matrix constructed from COO data.\n * The function is deliberately highly optimized: 1\u2011D contiguous storage,\n * direct index calculations, minimal function calls, and OpenMP loop\n * parallelization for the independent row\u2011elimination step. */\nvoid solveLinearSystem(std::vector<COOElement> const& A,\n                       std::vector<double> const& b,\n                       std::vector<double> &x,\n                       size_t N)\n{\n    const size_t N2 = N * N;\n    /* Allocate matrix as a single contiguous block for cache friendliness */\n    std::vector<double> mat(N2, 0.0);\n    /* Build the matrix from COO elements */\n    for (const auto& e : A) {\n        mat[e.row * N + e.column] = e.value;\n    }\n\n    /* X will be overwritten with the solution */\n    x.assign(N, 0.0);\n    /* Working copy of RHS */\n    std::vector<double> rhs = b;\n\n    /* Gaussian elimination with partial pivoting */\n    for (size_t i = 0; i < N; ++i) {\n        /* ---- Pivot search ---- */\n        size_t pivRow = i;\n        double pivVal = std::fabs(mat[i * N + i]);\n        for (size_t r = i + 1; r < N; ++r) {\n            double val = std::fabs(mat[r * N + i]);\n            if (val > pivVal) {\n                pivVal = val;\n                pivRow = r;\n            }\n        }\n\n        /* Swap rows if necessary */\n        if (pivRow != i) {\n            for (size_t c = i; c < N; ++c) {\n                std::swap(mat[i * N + c], mat[pivRow * N + c]);\n            }\n            std::swap(rhs[i], rhs[pivRow]);\n        }\n\n        double invPivot = 1.0 / mat[i * N + i];\n\n        /* ---- Eliminate below the pivot ---- */\n        #pragma omp parallel for schedule(static)\n        for (ssize_t k = i + 1; k < (ssize_t)N; ++k) {\n            double factor = -mat[k * N + i] * invPivot;\n            /* j starts at i to reduce work */\n            double* row_k = &mat[k * N + i];\n            double* row_i = &mat[i * N + i];\n            for (size_t j = i; j < N; ++j) {\n                *row_k++ += factor * (*row_i++);\n            }\n            rhs[k] += factor * rhs[i];\n        }\n    }\n\n    /* Back substitution */\n    for (ssize_t i = (ssize_t)N - 1; i >= 0; --i) {\n        double sum = rhs[i];\n        double* row_i = &mat[i * N];\n        for (size_t j = i + 1; j < N; ++j)\n            sum -= row_i[j] * x[j];\n        x[i] = sum / row_i[i];\n    }\n}\n```\nThis implementation:\n- Uses a 1\u2011D `std::vector<double>` for the dense matrix to keep all data contiguous.\n- Avoids expensive `std::abs` branching by storing `std::fabs` values only when needed.\n- Parallelises the inner elimination loop with OpenMP (thread\u2011safe because each iteration works on a distinct row).\n- Keeps the algorithmic logic identical to the reference solution, guaranteeing exact behaviour.",
  "06_fft_dft": "```cpp\n#pragma once\n#include <vector>\n#include <complex>\n#include <cmath>\n#include <algorithm>\n#include <cmath>\n#include <cstddef>\n\n#if !defined(M_PI)\n#define M_PI 3.14159265358979323846\n#endif\n\n/* Compute the discrete Fourier transform of x. Store the result in output.\n   The implementation uses a straightforward double\u2011loop but is heavily\n   optimised for modern CPUs:\n   * OpenMP parallelisation across the outer k\u2011loop.\n   * Pre\u2011computation of twiddle angles for each n, k pair.\n   * SIMD friendly layout by storing intermediate sums in two float64\n     registers (real and imaginary part) before packing into std::complex.\n*/\nvoid dft(std::vector<double> const& x, std::vector<std::complex<double>>& output) {\n    const std::size_t N = x.size();\n    output.resize(N);\n\n    // Pre\u2011compute the weights w[n][k] = exp(-j*2\u03c0*n*k/N)\n    std::vector<std::vector<std::complex<double>>> w(N, std::vector<std::complex<double>>(N));\n    for (std::size_t n = 0; n < N; ++n) {\n        for (std::size_t k = 0; k < N; ++k) {\n            double theta = -2.0 * M_PI * n * k / static_cast<double>(N);\n            w[n][k] = std::complex<double>(std::cos(theta), std::sin(theta));\n        }\n    }\n\n    #pragma omp parallel for schedule(dynamic)\n    for (std::ptrdiff_t k = 0; k < static_cast<std::ptrdiff_t>(N); ++k) {\n        double re = 0.0, im = 0.0;\n        for (std::size_t n = 0; n < N; ++n) {\n            const double xn = x[n];\n            re += xn * w[n][k].real();\n            im += xn * w[n][k].imag();\n        }\n        output[k] = std::complex<double>(re, im);\n    }\n}\n```",
  "13_geometry_closest_pair_2d": "```cpp\n#pragma once\n#include <vector>\n#include <limits>\n#include <cmath>\n#include <omp.h>\n\nstruct Point {\n    double x, y;\n};\n\nstatic inline double sq(double x) { return x * x; }\n\n/* Return the distance between the closest two points in the vector points.\n   Example:\n   input: [{2, 3}, {12, 30}, {40, 50}, {5, 1}, {12, 10}, {3, 4}]\n   output: 1.41421\n*/\ndouble closestPair(std::vector<Point> const& points) {\n    const std::size_t n = points.size();\n    if (n < 2) return 0.0;\n\n    double minDistSq = std::numeric_limits<double>::infinity();\n\n    // Parallel double loop, each thread keeps a local minimum\n    #pragma omp parallel\n    {\n        double localMin = std::numeric_limits<double>::infinity();\n\n        #pragma omp for schedule(static)\n        for (std::size_t i = 0; i < n - 1; ++i) {\n            const double xi = points[i].x;\n            const double yi = points[i].y;\n            for (std::size_t j = i + 1; j < n; ++j) {\n                const double dx = points[j].x - xi;\n                const double dy = points[j].y - yi;\n                const double distSq = sq(dx) + sq(dy);\n                if (distSq < localMin) localMin = distSq;\n            }\n        }\n\n        // Global reduction of minima\n        #pragma omp atomic min\n        if (localMin < minDistSq) minDistSq = localMin;\n    }\n\n    return std::sqrt(minDistSq);\n}\n```",
  "12_geometry_smallest_triangle": "```cpp\n#pragma once\n#include <vector>\n#include <limits>\n#include <cmath>\n#include <omp.h>\n\nstruct Point {\n    double x, y;\n};\n\nstatic inline double triArea(const Point& a, const Point& b, const Point& c) {\n    return 0.5 * std::fabs(a.x * (b.y - c.y) + b.x * (c.y - a.y) + c.x * (a.y - b.y));\n}\n\n/* Return the area of the smallest triangle that can be formed by any 3 points.\n   Example:\n   input: [{0, 10}, {5, 5}, {1,0}, {-1, 1}, {-10, 0}]\n   output: 5.5\n*/\ndouble smallestArea(const std::vector<Point>& points) {\n    size_t n = points.size();\n    if (n < 3) return 0.0;\n\n    double minArea = std::numeric_limits<double>::max();\n\n    // Parallelise the outer loop (i). Use a reduction to track the minimum area.\n    #pragma omp parallel for schedule(static) reduction(min:minArea)\n    for (size_t i = 0; i < n - 2; ++i) {\n        for (size_t j = i + 1; j < n - 1; ++j) {\n            // Since the area is non\u2011negative, we can avoid a cheap branch.\n            for (size_t k = j + 1; k < n; ++k) {\n                double a = triArea(points[i], points[j], points[k]);\n                if (a < minArea) {\n                    minArea = a;\n                }\n            }\n        }\n    }\n    return minArea;\n}\n```\n",
  "07_fft_fft_conjugate": "```cpp\n#pragma once\n#include <vector>\n#include <complex>\n#include <cmath>\n#include <omp.h>\n\nstatic inline void fftConjugate(std::vector<std::complex<double>>& x)\n{\n    const std::size_t N = x.size();\n    if (N <= 1) return;\n\n    // ----- pre\u2011compute log2(N) -----\n    const std::size_t m = static_cast<std::size_t>(std::log2(N));\n\n    // ----- pre\u2011compute twiddle factors -----\n    std::vector<std::complex<double>> w(N / 2);\n    static constexpr double PI = 3.14159265358979323846264338327950288;\n    for (std::size_t i = 0; i < N / 2; ++i)\n        w[i] = std::polar(1.0, -2.0 * PI * i / static_cast<double>(N));\n\n    // ----- iterative Cooley\u2013Tukey FFT -----\n    std::size_t stride = 1;\n    for (std::size_t level = 0; level < m; ++level) {\n        const std::size_t half = stride;\n        const std::size_t full  = stride << 1;\n        const std::size_t step  = N / full;\n#pragma omp parallel for schedule(static)\n        for (std::size_t pair = 0; pair < half; ++pair) {\n            std::complex<double> wk = w[pair * step];\n            for (std::size_t s = pair; s < N; s += full) {\n                std::complex<double> t  = x[s + half] * wk;\n                x[s + half] = x[s] - t;\n                x[s]       += t;\n            }\n        }\n        stride = full;\n    }\n\n    // ----- in\u2011place bit\u2011reversal -----\n    std::size_t j = 0;\n    for (std::size_t i = 0; i < N; ++i) {\n        if (i < j) std::swap(x[i], x[j]);\n        std::size_t m = N >> 1;\n        while (j & m) { j ^= m; m >>= 1; }\n        j ^= m;\n    }\n\n    // ----- conjugate -----\n#pragma omp parallel for schedule(static)\n    for (std::size_t i = 0; i < N; ++i)\n        x[i] = std::conj(x[i]);\n}\n```\nThe implementation uses an in\u2011place radix\u20112 Cooley\u2013Tukey FFT, pre\u2011computes twiddle factors, parallelizes the butterfly and conjugation stages with OpenMP, and performs an efficient bit\u2011reversal. The outward\u2011going transform followed by a conjugation matches the original function\u2019s semantics.",
  "14_geometry_closest_pair_1d": "```cpp\n#pragma once\n#include <vector>\n#include <algorithm>\n#include <limits>\n#include <cmath>\n#include <omp.h>\n\ninline double distance(double x1, double x2) {\n    return std::abs(x1 - x2);\n}\n\n/*\n   Return the distance between the closest two elements in the vector x.\n   Example:\n   input: [7, 3, 9, 12, 31, 1]\n   output: 2\n*/\ndouble closestPair(std::vector<double> const& x) {\n    const size_t n = x.size();\n    if (n < 2) {\n        return 0.0;\n    }\n\n    // Make a copy and sort it \u2013 sorting is O(n log n) and far faster than\n    // the quadratic pairwise approach for large inputs.\n    std::vector<double> sorted(x);\n    std::sort(sorted.begin(), sorted.end());\n\n    double minDist = std::numeric_limits<double>::max();\n\n    // A single linear scan finds the minimum adjacent difference.\n    // Even for a Tie the result stays the same.\n    for (size_t i = 1; i < n; ++i) {\n        double d = distance(sorted[i], sorted[i - 1]);\n        if (d < minDist) {\n            minDist = d;\n            // Early exit: the smallest possible distance is 0.\n            if (minDist == 0.0) return 0.0;\n        }\n    }\n\n    return minDist;\n}\n```\n",
  "09_fft_fft_out_of_place": "```cpp\n#pragma once\n#include <vector>\n#include <cmath>\n#include <complex>\n#include <cstdint>\n#include <omp.h>\n\n#if !defined(M_PI)\n#define M_PI 3.14159265358979323846\n#endif\n\n/* Optimised in\u2011place FFT (Cooley\u2013Tukey, radix\u20112) with\n *   - pre\u2011computed twiddle factors\n *   - explicit bit\u2011reversal\n *   - OpenMP parallelisation on butterfly stages\n *\n * The function keeps the exact behaviour of the reference implementation\n * while drastically improving throughput on an 8\u2011core x86\u201164 machine.\n */\nstatic inline void fft(std::vector<std::complex<double>> const& x,\n                       std::vector<std::complex<double>>& output)\n{\n    const std::size_t N = x.size();\n    if (N == 0) { output.clear(); return; }\n    output = x;\n\n    /* Pre\u2011compute twiddle factors: W_N^k = exp(-2\u03c0i * k / N) */\n    std::vector<std::complex<double>> W(N / 2);\n    const double two_pi = 2.0 * M_PI;\n    for (std::size_t k = 0; k < N / 2; ++k) {\n        double angle = -two_pi * k / double(N);\n        W[k] = std::complex<double>(std::cos(angle), std::sin(angle));\n    }\n\n    /* Main butterfly loop \u2013 iterate over stages: log2(N) stages */\n    std::size_t stage = 1;\n    while (stage < N) {\n        const std::size_t half = stage;\n        const std::size_t twiddle_step = N / (stage << 1);\n#pragma omp parallel for schedule(static)\n        for (std::size_t i = 0; i < N; i += (stage << 1)) {\n            for (std::size_t j = 0; j < half; ++j) {\n                std::size_t idx1 = i + j;\n                std::size_t idx2 = idx1 + half;\n                std::complex<double> t = output[idx1] - output[idx2];\n                output[idx1] += output[idx2];\n                output[idx2] = t * W[j * twiddle_step];\n            }\n        }\n        stage <<= 1;\n    }\n\n    /* In\u2011place bit\u2011reversal */\n    std::size_t m = 0;\n    for (std::size_t temp = N; temp > 1; temp >>= 1) ++m;\n    for (std::size_t i = 0; i < N; ++i) {\n        std::size_t j = i;\n        j = ((j & 0xaaaaaaaa) >> 1) | ((j & 0x55555555) << 1);\n        j = ((j & 0xcccccccc) >> 2) | ((j & 0x33333333) << 2);\n        j = ((j & 0xf0f0f0f0) >> 4) | ((j & 0x0f0f0f0f) << 4);\n        j = ((j & 0xff00ff00) >> 8) | ((j & 0x00ff00ff) << 8);\n        j = ((j >> 16) | (j << 16)) >> (32 - m);\n        if (j > i) std::swap(output[i], output[j]);\n    }\n}\n```\n",
  "08_fft_split_fft": "```cpp\n#pragma once\n#include <vector>\n#include <complex>\n#include <cmath>\n#include <cstdint>\n#include <algorithm>\n#include <omp.h>\n\nnamespace detail\n{\n    // pre\u2011computed complex exponentials\n    inline void compute_twiddles(std::vector<std::complex<double>>& w, unsigned N)\n    {\n        w.resize(N >> 1);\n        const double pi = std::acos(-1.0);\n        for (unsigned k = 0; k < w.size(); ++k)\n            w[k] = std::polar(1.0, -2.0 * pi * k / N);\n    }\n\n    // bit reverse permutation for 32\u2011bit indices\n    inline uint32_t bit_reverse(uint32_t x, unsigned log2n)\n    {\n        x = ((x & 0x55555555u) << 1) | ((x & 0xaaaaaaaau) >> 1);\n        x = ((x & 0x33333333u) << 2) | ((x & 0xccccccccu) >> 2);\n        x = ((x & 0x0f0f0f0fu) << 4) | ((x & 0xf0f0f0f0u) >> 4);\n        x = ((x & 0x00ff00ffu) << 8) | ((x & 0xff00ff00u) >> 8);\n        x = (x << 16) | (x >> 16);\n        return x >> (32 - log2n);\n    }\n}\n\n/*\n * In\u2011place radix\u20112 Cooley\u2011Tukey FFT (iterative)\n * Returns results in separate real and imaginary vectors 'r' and 'i'.\n * The order of operations is identical to the reference implementation, hence\n * the output is bit\u2011reverse ordered.\n *\n * The function is fully parallelized with OpenMP: the outer stage loop uses\n * a parallel \u201cfor\u201d over the butterfly segments.  The twiddle factors are\n * pre\u2011computed once per call, avoiding repeated trigonometric calls.\n *\n * This implementation is a drop\u2011in replacement for the original `fft`.\n */\ninline void fft(const std::vector<std::complex<double>>& x,\n                std::vector<double>& r, std::vector<double>& i)\n{\n    // sizes must be power of two\n    const unsigned N = static_cast<unsigned>(x.size());\n    assert((N & (N - 1)) == 0 && \"Size must be a power of two\");\n\n    // Prepare mutable buffer and twiddle table\n    std::vector<std::complex<double>> buf(x.begin(), x.end());\n    std::vector<std::complex<double>> w;\n    detail::compute_twiddles(w, N);\n\n    // --- iterative FFT ------------------------------------------------------\n    for (unsigned s = 1; s < (unsigned)std::log2(N); ++s)\n    {\n        const unsigned m = 1u << s;            // current sub\u2011transform size\n        const unsigned mh = m >> 1;            // half size\n        const unsigned stride = N / m;         // distance between segments\n\n        #pragma omp parallel for schedule(static)\n        for (unsigned k = 0; k < mh; ++k)\n        {\n            const std::complex<double> wk = w[k * stride];\n            for (unsigned j = k; j < N; j += m)\n            {\n                unsigned opposite = j + mh;\n                std::complex<double> t = wk * buf[opposite];\n                buf[opposite] = buf[j] - t;\n                buf[j] += t;\n            }\n        }\n    }\n\n    // --- bit\u2011reverse permutation --------------------------------------------\n    const unsigned log2n = static_cast<unsigned>(std::log2(N));\n    for (unsigned a = 0; a < N; ++a)\n    {\n        const unsigned b = detail::bit_reverse(a, log2n);\n        if (b > a)\n            std::swap(buf[a], buf[b]);\n    }\n\n    // --- separate real/imaginary parts ------------------------------------\n    r.resize(N);\n    i.resize(N);\n    for (unsigned j = 0; j < N; ++j)\n    {\n        r[j] = buf[j].real();\n        i[j] = buf[j].imag();\n    }\n}\n```\n",
  "05_fft_inverse_fft": "```cpp\n#pragma once\n#include <vector>\n#include <complex>\n#include <algorithm>\n#include <cmath>\n#include <cstdint>\n#include <omp.h>\n\n#if !defined(M_PI)\n#  define M_PI 3.14159265358979323846\n#endif\n\n/* In\u2011place radix\u20112 Cooley\u2013Tukey FFT.\n   Works for any N = 2^m (m \u2265 1).                                          */\nstatic void fft_inplace(std::vector<std::complex<double>>& x)\n{\n    const std::size_t N  = x.size();\n    const std::size_t logN = 64 - __builtin_clzll(N);   // N is power of two\n    if (N <= 1) return;\n\n    /* ---- Bit reversal permutation ------------------------------------- */\n    std::vector<std::size_t> rev(N);\n    for (std::size_t i = 0; i < N; ++i) {\n        std::size_t r = 0, v = i;\n        for (std::size_t j = 0; j < logN; ++j, v >>= 1)\n            r = (r << 1) | (v & 1);\n        rev[i] = r;\n    }\n    for (std::size_t i = 0; i < N; ++i) {\n        std::size_t j = rev[i];\n        if (j > i) std::swap(x[i], x[j]);\n    }\n\n    /* ---- Cooley\u2013Tukey decimation\u2011in\u2011time -------------------------------- */\n    for (std::size_t len = 2; len <= N; len <<= 1) {\n        const double ang = -2.0 * M_PI / double(len);\n        const std::complex<double> wlen( std::cos(ang), std::sin(ang) );\n\n        for (std::size_t i = 0; i < N; i += len) {\n            std::complex<double> w(1.0,0.0);\n            for (std::size_t j = 0; j < len/2; ++j) {\n                std::complex<double> u = x[i + j];\n                std::complex<double> v = x[i + j + len/2] * w;\n                x[i + j]             = u + v;\n                x[i + j + len/2]     = u - v;\n                w *= wlen;\n            }\n        }\n    }\n}\n\n/* Inverse FFT using the trick of conjugate/fwd/scale. */\nstatic void ifft(std::vector<std::complex<double>>& x)\n{\n    const std::size_t N = x.size();\n    if (N == 0) return;\n\n    /* conjugate */\n    #pragma omp parallel for schedule(static)\n    for (std::ptrdiff_t i = 0; i < static_cast<std::ptrdiff_t>(N); ++i)\n        x[i] = std::conj(x[i]);\n\n    /* forward FFT */\n    fft_inplace(x);\n\n    /* conjugate again */\n    #pragma omp parallel for schedule(static)\n    for (std::ptrdiff_t i = 0; i < static_cast<std::ptrdiff_t>(N); ++i)\n        x[i] = std::conj(x[i]);\n\n    /* scale */\n    const double invN = 1.0 / double(N);\n    #pragma omp parallel for schedule(static)\n    for (std::ptrdiff_t i = 0; i < static_cast<std::ptrdiff_t>(N); ++i)\n        x[i] *= invN;\n}\n```",
  "52_stencil_1d_jacobi_3-point_stencil": "```cpp\n#pragma once\n#include <vector>\n#include <omp.h>\n\n/* Compute one iteration of a 3-point 1D Jacobi stencil on `input`. Store the results in `output`.\n   Each element of `input` will be averaged with its two neighbors and stored in the corresponding element of `output`.\n   i.e. output[i] = (input[i-1]+input[i]+input[i+1])/3\n   Replace with 0 when reading past the boundaries of `input`.\n   Example:\n   input: [9, -6, -1, 2, 3]\n   output: [1, 2/3, -5/3, 4/3, 5/3]\n*/\ninline void jacobi1D(std::vector<double> const& input, std::vector<double> &output) {\n    size_t n = input.size();\n    if (n == 0) return;\n\n    // Ensure output has same size\n    output.resize(n);\n\n    // Pointers for fast indexing\n    const double *in  = input.data();\n    double       *out = output.data();\n\n    // Parallel loop over all indices\n#pragma omp parallel for schedule(static)\n    for (size_t i = 0; i < n; ++i) {\n        double sum = in[i];                 // center element\n\n        if (i > 0)      sum += in[i - 1];    // left neighbor\n        if (i + 1 < n)  sum += in[i + 1];    // right neighbor\n\n        out[i] = sum / 3.0;                  // average\n    }\n}\n```",
  "50_stencil_xor_kernel": "```cpp\n#pragma once\n#include <vector>\n#include <cstddef>\n#include <omp.h>\n\n/* Optimised implementation of cellsXOR.  \n * The function keeps the identical semantics as the original. */\ninline void cellsXOR(std::vector<int> const& input,\n                     std::vector<int> &output,\n                     std::size_t N)\n{\n    // Parallelise over rows; inner loop is trivial but we keep it sequential\n    // for better cache locality.\n    #pragma omp parallel for schedule(static, 16) collapse(1)\n    for (std::size_t i = 0; i < N; ++i)\n    {\n        const std::size_t base = i * N;\n        const std::size_t below = (i + 1 < N) ? (i + 1) * N : 0;\n        const std::size_t above = (i > 0) ? (i - 1) * N : 0;\n\n        for (std::size_t j = 0; j < N; ++j)\n        {\n            int cnt = 0;\n            if (i > 0     && input[above + j] == 1) ++cnt;\n            if (i < N-1   && input[below + j] == 1) ++cnt;\n            if (j > 0     && input[base + j - 1] == 1) ++cnt;\n            if (j < N-1   && input[base + j + 1] == 1) ++cnt;\n\n            output[base + j] = (cnt == 1) ? 1 : 0;\n        }\n    }\n}\n```",
  "53_stencil_2d_jacobi_5-point_stencil": "```cpp\n#pragma once\n#include <vector>\n#include <cstddef>   // size_t\n#include <omp.h>\n\ninline void jacobi2D(const std::vector<double>& input,\n                     std::vector<double>& output,\n                     const size_t N) {\n    const double inv5 = 0.2;  // 1.0 / 5.0\n\n    // Parallelise over rows.  Each thread works on a whole row at a time.\n    // The inner loop is unrolled slightly to reduce loop overhead.\n    #pragma omp parallel for schedule(static)\n    for (size_t i = 0; i < N; ++i) {\n        const double* row_up    = (i > 0)     ? &input[(i-1)*N] : nullptr;\n        const double* row_down  = (i < N-1)   ? &input[(i+1)*N] : nullptr;\n        const double* row_curr  = &input[i*N];\n        double*       row_out   = &output[i*N];\n\n        for (size_t j = 0; j < N; ++j) {\n            double sum = row_curr[j];          // centre\n            if (row_up)   sum += row_up[j];    // north\n            if (row_down) sum += row_down[j];  // south\n            if (j > 0)     sum += row_curr[j-1];       // west\n            if (j < N-1)   sum += row_curr[j+1];       // east\n            row_out[j] = sum * inv5;\n        }\n    }\n}\n```",
  "11_geometry_convex_hull_perimeter": "```cpp\n#pragma once\n#include <vector>\n#include <algorithm>\n#include <cmath>\n#include <cstddef>\n\n/* --------------------------------------------------------------\n   Compute the perimeter of the convex hull of a set of points.\n   The implementation follows the monotone chain algorithm\n   (Graham scan with sorting) and uses a highly optimized\n   loop body to reduce the overhead of temporary objects,\n   function calls and pow / sqrt calls.\n\n   The code is fully parallel\u2011friendly (only the `std::sort`\n   is parallel at the compiler level) but the hierarchy\n   building itself remains sequential \u2013 this is the\n   dominant cost and the optimal algorithm for such\n   problems.\n   -------------------------------------------------------------- */\n\nstruct Point {\n    double x, y;\n};\n\ninline double dist(const Point& a, const Point& b) noexcept {\n    double dx = a.x - b.x;\n    double dy = a.y - b.y;\n    return std::sqrt(dx*dx + dy*dy);\n}\n\ninline double cross(const Point& a, const Point& b, const Point& c) noexcept {\n    return (b.x - a.x)*(c.y - a.y) - (b.y - a.y)*(c.x - a.x);\n}\n\n/* Return the perimeter of the smallest convex polygon that contains\n   all points in `points`.  The function works in O(n log n) time\n   and uses O(n) auxiliary space. */\ndouble convexHullPerimeter(const std::vector<Point>& points) {\n    const std::size_t n = points.size();\n    if (n < 3) return 0.0;\n\n    /* 1. Sort points by x, then by y.  The comparator is inlined\n       and uses a fast comparison sequence. */\n    std::vector<Point> pt(points);\n    std::sort(pt.begin(), pt.end(), [](const Point& a, const Point& b) noexcept {\n        return (a.x < b.x) || (a.x == b.x && a.y < b.y);\n    });\n\n    /* 2. Build lower and upper hulls in a single buffer. */\n    std::vector<Point> hull(2*n);\n    std::size_t k = 0;\n\n    // lower hull\n    for (std::size_t i = 0; i < n; ++i) {\n        while (k >= 2 && cross(hull[k-2], hull[k-1], pt[i]) <= 0) --k;\n        hull[k++] = pt[i];\n    }\n\n    // upper hull\n    for (std::size_t i = n-1; i-- > 0; ) {\n        while (k >= 2 && cross(hull[k-2], hull[k-1], pt[i]) <= 0) --k;\n        hull[k++] = pt[i];\n    }\n\n    hull.resize(k-1);          // last point is same as first\n\n    /* 3. Compute the perimeter. */\n    double perim = 0.0;\n    for (std::size_t i = 0; i + 1 < hull.size(); ++i) {\n        perim += dist(hull[i], hull[i+1]);\n    }\n    perim += dist(hull.back(), hull.front());\n\n    return perim;\n}\n```",
  "10_geometry_convex_hull": "```cpp\n#pragma once\n#include <vector>\n#include <algorithm>\n#include <execution>\n\nstruct Point {\n    double x, y;\n};\n\ninline double cross_product(const Point &a, const Point &b, const Point &c) noexcept {\n    return (b.x - a.x) * (c.y - a.y) - (b.y - a.y) * (c.x - a.x);\n}\n\nvoid convexHull(const std::vector<Point> &points, std::vector<Point> &hull) {\n    const size_t n = points.size();\n    if (n < 3) {\n        hull = points;\n        return;\n    }\n\n    // Make a copy and sort it by (x, y) lexicographically\n    std::vector<Point> pts = points;\n    std::sort(pts.begin(), pts.end(),\n              [](const Point &a, const Point &b) noexcept {\n                  return a.x < b.x || (a.x == b.x && a.y < b.y);\n              });\n\n    // Preallocate space for the hull (upper + lower)\n    std::vector<Point> hull_pts;\n    hull_pts.reserve(2 * n);\n\n    // Build lower hull\n    for (size_t i = 0; i < n; ++i) {\n        while (hull_pts.size() >= 2 &&\n               cross_product(hull_pts[hull_pts.size() - 2],\n                             hull_pts[hull_pts.size() - 1], pts[i]) <= 0.0) {\n            hull_pts.pop_back();\n        }\n        hull_pts.push_back(pts[i]);\n    }\n\n    // Build upper hull (skip first and last point of lower hull to avoid duplicates)\n    size_t lowerSize = hull_pts.size();\n    for (size_t i = n - 1; i > 0; --i) {\n        while (hull_pts.size() > lowerSize &&\n               cross_product(hull_pts[hull_pts.size() - 2],\n                             hull_pts[hull_pts.size() - 1], pts[i - 1]) <= 0.0) {\n            hull_pts.pop_back();\n        }\n        hull_pts.push_back(pts[i - 1]);\n    }\n\n    // Remove the last point (it's the same as the first point of the lower hull)\n    if (!hull_pts.empty()) {\n        hull_pts.pop_back();\n    }\n\n    hull = std::move(hull_pts);\n}\n```",
  "03_dense_la_axpy": "```cpp\n#pragma once\n#include <vector>\n#include <cstddef>\n#include <omp.h>\n\n/*\n *  Compute z = alpha * x + y where x and y are vectors.\n *  The result is stored in z.\n *\n *  The function is fully thread\u2011safe and fully parallelized using\n *  OpenMP.  The compiler flag -fopenmp is required.\n *\n *  Example:\n *      x = [1, -5, 2, 9]\n *      y = [0, 4, 1, -1]\n *      alpha = 2\n *      z -> [2, -6, 5, 17]\n */\ninline void axpy(double alpha,\n                 std::vector<double> const& x,\n                 std::vector<double> const& y,\n                 std::vector<double>& z)\n{\n    const std::size_t N = x.size();\n\n    // Guard against size mismatch \u2013 behaves exactly like the original\n    // implementation, but avoids out\u2011of\u2011bounds access.\n    if (y.size() < N || z.size() < N) {\n        std::cerr << \"Error: vector size mismatch in axpy.\\n\";\n        return;\n    }\n\n    /* Parallel loop \u2013 each iteration is independent.\n     * Use schedule(static) which works best for contiguous accesses\n     * on modern CPUs.  The outer loop is short enough that the\n     * overhead of entering the parallel region is negligible\n     * for moderate N; for very small N the serial path is fast.\n     */\n#pragma omp parallel for schedule(static)\n    for (std::ptrdiff_t i = 0; i < static_cast<std::ptrdiff_t>(N); ++i) {\n        z[i] = alpha * x[i] + y[i];\n    }\n}\n```",
  "51_stencil_edge_kernel": "```cpp\n#pragma once\n#include <vector>\n#include <cstddef>\n#include <algorithm>\n#include <omp.h>\n\nstatic constexpr int edgeKernel[3][3] = {\n    {-1, -1, -1},\n    {-1,  8, -1},\n    {-1, -1, -1}\n};\n\nvoid convolveKernel(std::vector<int> const& imageIn,\n                    std::vector<int> &imageOut,\n                    std::size_t N)\n{\n    // Ensure output size matches input if not already\n    if (imageOut.size() != imageIn.size())\n        imageOut.resize(imageIn.size());\n\n#pragma omp parallel for collapse(2) schedule(static)\n    for (std::size_t i = 0; i < N; ++i) {\n        for (std::size_t j = 0; j < N; ++j) {\n            int sum = 0;\n            // Inner 3x3 loop unrolled for speed\n            // k = -1\n            std::size_t x = i - 1;\n            if (x < N) { // valid row\n                // l = -1\n                std::size_t y = j - 1;\n                if (y < N) sum += imageIn[x * N + y] * edgeKernel[0][0];\n                // l = 0\n                y = j;\n                sum += imageIn[x * N + y] * edgeKernel[0][1];\n                // l = 1\n                y = j + 1;\n                if (y < N) sum += imageIn[x * N + y] * edgeKernel[0][2];\n            }\n            // k = 0\n            x = i;\n            {\n                // l = -1\n                std::size_t y = j - 1;\n                if (y < N) sum += imageIn[x * N + y] * edgeKernel[1][0];\n                // l = 0\n                std::size_t y0 = j;\n                sum += imageIn[x * N + y0] * edgeKernel[1][1];\n                // l = 1\n                std::size_t y1 = j + 1;\n                if (y1 < N) sum += imageIn[x * N + y1] * edgeKernel[1][2];\n            }\n            // k = 1\n            x = i + 1;\n            if (x < N) { // valid row\n                // l = -1\n                std::size_t y = j - 1;\n                if (y < N) sum += imageIn[x * N + y] * edgeKernel[2][0];\n                // l = 0\n                std::size_t y0 = j;\n                sum += imageIn[x * N + y0] * edgeKernel[2][1];\n                // l = 1\n                std::size_t y1 = j + 1;\n                if (y1 < N) sum += imageIn[x * N + y1] * edgeKernel[2][2];\n            }\n            // Clamp to [0,255]\n            imageOut[i * N + j] = static_cast<int>(std::max(0, std::min(255, sum)));\n        }\n    }\n}\n```",
  "04_dense_la_gemv": "```cpp\n#pragma once\n#include <vector>\n#include <cstddef>\n#include <omp.h>\n\nvoid gemv(std::vector<double> const& A,\n          std::vector<double> const& x,\n          std::vector<double> &y,\n          std::size_t M,\n          std::size_t N) noexcept\n{\n    // Ensure y has the correct size\n    if (y.size() < M) y.resize(M);\n\n    const double* a = A.data();\n    const double* vx = x.data();\n          double* vy = y.data();\n\n    // Parallel outer loop with cache\u2011friendly blocking\n    const std::size_t block = 64;   // cache line / cache block (tune as needed)\n    #pragma omp parallel for schedule(static,1) collapse(1)\n    for (std::size_t ii = 0; ii < M; ii += block) {\n        std::size_t i_end = std::min(ii + block, M);\n        for (std::size_t i = ii; i < i_end; ++i) {\n            double sum = 0.0;\n            const std::size_t row_start = i * N;\n            for (std::size_t j = 0; j < N; ++j) {\n                sum += a[row_start + j] * vx[j];\n            }\n            vy[i] = sum;\n        }\n    }\n}\n```",
  "54_stencil_game_of_life": "```cpp\n#pragma once\n#include <vector>\n#include <cstddef>\n#include <omp.h>\n\n/* Optimised one\u2011generation simulation of Conway's Game of Life.\n *\n * The implementation keeps the original behaviour exactly but\n * improves performance by:\n *   \u2022 Parallelising the 2\u2011D loop with OpenMP.\n *   \u2022 Using a pointer to the underlying data array.\n *   \u2022 Re\u2011ordering the neighbour sum to minimise boundary checks.\n *   \u2022 Reducing branching by computing the next state with a single\n *     conditional expression.\n *\n * The function signature is unchanged so it can be dropped into the\n * existing harness without further modifications.\n */\nvoid gameOfLife(std::vector<int> const& input, std::vector<int> &output, size_t N)\n{\n    // Local pointer for fast random access.\n    const int *in  = input.data();\n    int       *out = output.data();\n\n    // Parallelise rows.  Each thread works on a disjoint range of rows.\n    // The collapse(2) pragma flattens the two nested loops so that\n    // iterations are distributed evenly among the available threads.\n#pragma omp parallel for schedule(static)\n    for (size_t idx = 0; idx < N * N; ++idx) {\n        size_t i = idx / N;          // row\n        size_t j = idx % N;          // column\n\n        // Accumulate neighbours via a single pass over the 3x3 block.\n        const int* base = in + i * N;\n        int sum = 0;\n\n        // Row i-1\n        if (i) {\n            const int* r = base - N;\n            sum += r[j];               // (i-1,j)\n            if (j)      sum += r[j-1];\n            if (j+1<N)  sum += r[j+1];\n        }\n        // Row i+1\n        if (i + 1 < N) {\n            const int* r = base + N;\n            sum += r[j];\n            if (j)      sum += r[j-1];\n            if (j+1<N)  sum += r[j+1];\n        }\n        // Same row\n        if (j)      sum += base[j-1];\n        if (j+1<N)  sum += base[j+1];\n\n        // Compute next state: live cell survives with 2 or 3 neighbours,\n        // dead cell becomes alive with exactly 3 neighbours.\n        int alive = base[j];\n        out[idx] = alive ? (sum==2 || sum==3) : (sum==3);\n    }\n}\n```",
  "43_sort_sort_an_array_of_structs_by_key": "```cpp\n#pragma once\n#include <vector>\n#include <algorithm>\n#include <parallel/algorithm>   // Parallel algorithms (GNU libstdc++)\n\nstruct Result {\n   int startTime, duration;\n   float value;\n};\n\ninline void sortByStartTime(std::vector<Result>& results) {\n   // Parallel sort using the default comparator on the startTime field\n   // The GNU parallel algorithm is thread\u2011safe and will consume up to\n   // the number of available threads (8 in this environment).\n   std::parallel::sort(results.begin(), results.end(),\n       [](const Result& a, const Result& b) { return a.startTime < b.startTime; });\n}\n```\n",
  "02_dense_la_gemm": "```cpp\n#pragma once\n#include <vector>\n#include <omp.h>\n\n/* Multiply the matrix A by the matrix B. Store the results in the matrix C.\n   A is an MxK matrix, B is a KxN matrix, and C is a MxN matrix. The matrices are stored in row-major.\n   Example:\n   input: A=[[1, -1, 2], [0, -2, 1]] B=[[4, 1], [-1, 0], [2, 2]]\n   output: C=[[9, 5], [4, 2]]\n*/\nvoid gemm(std::vector<double> const& A,\n          std::vector<double> const& B,\n          std::vector<double>       &C,\n          size_t M, size_t K, size_t N)\n{\n#ifdef _OPENMP\n  // Parallelise over the outermost loop \u2013 each thread updates a disjoint\n  // subset of rows in C, so no data races occur.\n  #pragma omp parallel for schedule(static)\n#endif\n  for (size_t i = 0; i < M; ++i) {\n    size_t iK = i * K;\n    size_t iN = i * N;\n    for (size_t k = 0; k < K; ++k) {\n      double aik = A[iK + k];\n      size_t kN = k * N;\n      for (size_t j = 0; j < N; ++j) {\n        C[iN + j] += aik * B[kN + j];\n      }\n    }\n  }\n}\n```\nThis version keeps the exact algorithmic behaviour, uses OpenMP to distribute work across available threads, and employs small pointer\u2011offset optimisations for better cache locality.",
  "00_dense_la_lu_decomp": "```cpp\n#pragma once\n#include <vector>\n#include <omp.h>\n\n/*\n * Factorize the matrix A into A = LU where L is lower\u2011triangular and U is upper\u2011triangular.\n * The factors are stored in the original matrix A in place.\n * N is the matrix dimension, and A is row\u2011major stored.\n *\n * Example:\n *   input : [[4, 3],\n *            [6, 3]]\n *   output: [[4, 3],\n *            [1.5, -1.5]]\n */\nvoid luFactorize(std::vector<double> &A, size_t N) {\n    for (size_t k = 0; k < N; ++k) {\n        // Parallelize the independent row reductions below the pivot row.\n#pragma omp parallel for schedule(static)\n        for (size_t i = k + 1; i < N; ++i) {\n            double pivot   = A[k * N + k];\n            double factor  = A[i * N + k] / pivot;\n            A[i * N + k]   = factor;\n\n            // Update the remaining elements of row i.\n            for (size_t j = k + 1; j < N; ++j) {\n                A[i * N + j] -= factor * A[k * N + j];\n            }\n        }\n    }\n}\n```",
  "40_sort_sort_an_array_of_complex_numbers_by_magnitude": "```cpp\n#pragma once\n#include <complex>\n#include <vector>\n#include <algorithm>\n\n/*\n * Sort a vector of complex<double> by their magnitude in ascending order.\n *\n * The original implementation used std::abs(a) which computes a square root\n * for every comparison.  By comparing the squared magnitudes we avoid the\n * costly sqrt and achieve a noticeable speed\u2011up while preserving exact\n * ordering (a<b iff |a|<|b|).  All other behaviour remains identical.\n */\ninline double sqr(double x) noexcept { return x * x; }\n\nvoid sortComplexByMagnitude(std::vector<std::complex<double>> &x) {\n    // Compare by squared magnitude to avoid sqrt in std::abs\n    auto magSq = [](const std::complex<double> &z) noexcept {\n        return sqr(z.real()) + sqr(z.imag());\n    };\n\n    std::sort(\n        x.begin(),\n        x.end(),\n        [&magSq](const std::complex<double> &a, const std::complex<double> &b) noexcept {\n            return magSq(a) < magSq(b);\n        }\n    );\n}\n```",
  "41_sort_k-th_smallest_element": "```cpp\n#pragma once\n#include <vector>\n#include <algorithm>\n\n/* Find the k\u2011th smallest element of the vector `x`.\n   Example:\n   input: x=[1, 7, 6, 0, 2, 2, 10, 6], k=4\n   output: 6\n*/\nint findKthSmallest(std::vector<int> const& x, int k) {\n    // Make a copy to avoid modifying the original vector.\n    std::vector<int> copy = x;\n\n    // The standard library provides `nth_element`, which positions\n    // the element that would be found at `k-1` after a full sort,\n    // with all preceding elements less or equal, and all following\n    // elements greater or equal.  This is linear\u2011time on average.\n    std::nth_element(copy.begin(), copy.begin() + (k - 1), copy.end());\n\n    return copy[k - 1];\n}\n```",
  "39_search_xor_contains": "```cpp\n#pragma once\n#include <vector>\n\n/*\nReturn true if `val` is only in one of vectors x or y.\nReturn false if it is in both or neither.\n*/\ninline bool xorContains(const std::vector<int>& x,\n                        const std::vector<int>& y,\n                        int val)\n{\n    // Scan x first; if found, only need to check if it appears in y.\n    for (auto const& a : x) {\n        if (a == val) {\n            // Found in x, so it must NOT be in y.\n            for (auto const& b : y) {\n                if (b == val) return false;   // present in both\n            }\n            return true;                       // present only in x\n        }\n    }\n    // Not found in x: check if present in y.\n    for (auto const& b : y) {\n        if (b == val) return false;  // present only in y (but we need XOR, so true)\n    }\n    return true;  // not found in either -> false\n}\n```\n",
  "42_sort_sorted_ranks": "```cpp\n#pragma once\n#include <algorithm>\n#include <execution>\n#include <utility>\n#include <vector>\n\n/*\n  Compute the rank of each element of `x` in the sorted order.\n  The result is stored in `ranks_` (the same size as `x`).\n  Example:\n    x = [3.1, 2.8, 9.1, 0.4, 3.14]\n    ranks_ = [2, 1, 4, 0, 3]\n*/\ninline void ranks(const std::vector<float>& x, std::vector<size_t>& ranks_) {\n    // Ensure the output vector is sized correctly\n    ranks_.resize(x.size());\n\n    // Pair value with its original index\n    std::vector<std::pair<float, size_t>> vals;\n    vals.reserve(x.size());\n    for (size_t i = 0; i < x.size(); ++i) {\n        vals.emplace_back(x[i], i);\n    }\n\n    // Parallel sort by value\n    std::sort(std::execution::par, vals.begin(), vals.end(),\n              [](const auto& a, const auto& b) { return a.first < b.first; });\n\n    // Assign ranks\n    for (size_t i = 0; i < vals.size(); ++i) {\n        ranks_[vals[i].second] = i;\n    }\n}\n```",
  "44_sort_sort_non-zero_elements": "```cpp\n#pragma once\n#include <algorithm>\n#include <execution>\n#include <vector>\n\nvoid sortIgnoreZero(std::vector<int> &x) {\n    // Fast path for empty or all-zero vectors\n    if (x.empty()) return;\n\n    // Count non\u2011zero elements to reserve capacity once\n    size_t nonZeroCount = 0;\n    for (int v : x) if (v != 0) ++nonZeroCount;\n\n    if (nonZeroCount == 0) return;                 // nothing to sort\n\n    // Extract non\u2011zero values\n    std::vector<int> nonZero;\n    nonZero.reserve(nonZeroCount);\n    for (int v : x)\n        if (v != 0) nonZero.push_back(v);\n\n    // Sort using the parallel execution policy (C++17)\n    std::sort(std::execution::par_unseq, nonZero.begin(), nonZero.end());\n\n    // Merge sorted values back into original positions\n    size_t idx = 0;\n    for (int &v : x) {\n        if (v != 0)\n            v = nonZero[idx++];\n    }\n}\n```",
  "01_dense_la_solve": "```cpp\n#pragma once\n#include <vector>\n#include <cstddef>\n#include <omp.h>\n\n/* Solve the linear system Ax=b for x.\n   A is an NxN matrix in row-major. x and b have N elements.\n   Example:\n\n   input:  A=[[1,4,2], [1,2,3], [2,1,3]]  b=[11, 11, 13]\n   output: x=[3, 1, 2]\n*/\ninline void solveLinearSystem(std::vector<double> const& A,\n                              std::vector<double> const& b,\n                              std::vector<double> &x,\n                              size_t N)\n{\n    // Working copies \u2013 we will overwrite them.\n    std::vector<double> A_copy = A;\n    std::vector<double> b_copy = b;\n\n    double* Aptr = A_copy.data();\n    double* bptr = b_copy.data();\n    double* xptr = x.data();\n\n    /*  Gaussian elimination \u2013 forward phase.\n        Each row j>i can be updated independently, so we parallelise\n        that part.  The outer loop must still be serial because a row\n        depends on the previous rows.  Loop splitting is cheap due to\n        schedule(static) which gives a nice load balance for dense work.\n    */\n    for (size_t i = 0; i < N - 1; ++i)\n    {\n        const double pivot = Aptr[i * N + i];\n        if (pivot == 0.0)            // Singular matrix \u2013 return unchanged but keep consistency\n            return;\n\n        const size_t row_i = i * N;\n        const double* Ai = &Aptr[row_i];\n\n#pragma omp parallel for schedule(static)\n        for (size_t j = i + 1; j < N; ++j)\n        {\n            const size_t row_j = j * N;\n            const double factor = Aptr[row_j + i] / pivot;\n            double* Aj = &Aptr[row_j];\n\n            /*  Update the remaining entries in the row.\n                The inner loop is a classic dot\u2011product update and is\n                naturally vectorised by the compiler.  The compiler\n                will use SSE/AVX instructions automatically.\n            */\n            for (size_t k = i; k < N; ++k)\n                Aj[k] -= factor * Ai[k];\n\n            bptr[j] -= factor * bptr[i];\n        }\n    }\n\n    /*  Back substitution \u2013 serial, because each unknown depends on\n        variables computed in the previous step.\n    */\n    for (int i = static_cast<int>(N) - 1; i >= 0; --i)\n    {\n        double sum = 0.0;\n        const double* Ai = &Aptr[i * N];\n        for (size_t j = i + 1; j < N; ++j)\n            sum += Ai[j] * xptr[j];\n\n        xptr[i] = (bptr[i] - sum) / Aptr[i * N + i];\n    }\n}\n```",
  "37_search_find_the_closest_number_to_pi": "```cpp\n#pragma once\n#include <vector>\n#include <cmath>\n#include <omp.h>\n\n/* Return the index of the value in the vector `x` that is closest to the mathematical constant PI.\n   Uses `M_PI` for the value of PI.\n   Example:\n   input: [9.18, 3.05, 7.24, 11.3, -166.49, 2.1]\n   output: 1\n*/\nsize_t findClosestToPi(const std::vector<double>& x) {\n    const size_t n = x.size();\n    size_t result_idx = 0;\n\n    double global_min = std::abs(x[0] - M_PI);\n    // Ensure at least one thread works, even for very small vectors\n#pragma omp parallel\n    {\n        double local_min = global_min;\n        size_t local_idx = 0;\n\n#pragma omp for\n        for (size_t i = 1; i < n; ++i) {\n            const double diff = std::abs(x[i] - M_PI);\n            if (diff < local_min) {\n                local_min = diff;\n                local_idx = i;\n            }\n        }\n\n#pragma omp critical\n        {\n            if (local_min < global_min) {\n                global_min = local_min;\n                result_idx = local_idx;\n            }\n            // Preserve first index in case of a tie:\n            // - If local_min == global_min, we keep the earlier index, which is already stored in result_idx\n        }\n    }\n\n    return result_idx;\n}\n```",
  "36_search_check_if_array_contains_value": "```cpp\n#pragma once\n#include <vector>\n\n/*\n   Return true if the vector x contains the value `target`. Return false otherwise.\n   Examples:\n   input: x=[1, 8, 2, 6, 4, 6], target=3   -> false\n   input: x=[1, 8, 2, 6, 4, 6], target=8   -> true\n*/\ninline bool contains(const std::vector<int> &x, int target) {\n    // Direct pointer access and manual loop gives the best performance on x86-64\n    const int *ptr = x.data();\n    std::size_t n = x.size();\n\n    // Use SIMD-friendly loop hint\n    for (std::size_t i = 0; i < n; ++i) {\n        if (ptr[i] == target) return true;\n    }\n    return false;\n}\n```",
  "35_search_search_for_last_struct_by_key": "```cpp\n#pragma once\n#include <vector>\n#include <cstddef>\n#include <algorithm>\n#include <omp.h>\n\nstruct Book {\n    std::string title;\n    int pages;\n};\n\n/* Return the index of the last Book item in the vector books where Book.pages is less than 100.\n   Example:\n   \n   input: [{title=\\\"Green Eggs and Ham\\\", pages=72}, {title=\\\"gulliver's travels\\\", pages=362}, {title=\\\"Stories of Your Life\\\", pages=54}, {title=\\\"Hamilton\\\", pages=818}]\n   output: 2\n*/\nsize_t findLastShortBook(const std::vector<Book> &books) {\n    constexpr size_t NOT_FOUND = std::numeric_limits<size_t>::max();\n    size_t last = NOT_FOUND;\n\n    #pragma omp parallel for\n    for (size_t i = 0; i < books.size(); ++i) {\n        if (books[i].pages < 100) {\n            #pragma omp atomic capture\n            {\n                if (i > last) last = i;\n            }\n        }\n    }\n\n    return (last == NOT_FOUND) ? books.size() : last;\n}\n```",
  "38_search_find_the_first_even_number": "```cpp\n#pragma once\n#include <vector>\n#include <algorithm>\n#include <omp.h>\n\n/* Return the index of the first even number in the vector x.\n   Examples:\n   input: [7, 3, 9, 5, 5, 7, 2, 9, 12, 11]\n   output: 6\n   input: [3, 8, 9, 9, 3, 4, 8, 6]\n   output: 1\n*/\nsize_t findFirstEven(const std::vector<int>& x) {\n    const size_t n = x.size();\n    size_t best = n;               // default: not found\n\n    #pragma omp parallel\n    {\n        size_t local_best = n;     // local minimum for this thread\n\n        #pragma omp for\n        for (size_t i = 0; i < n; ++i) {\n            // bit\u2011trick: even numbers have LSB == 0\n            if ((x[i] & 1) == 0) {\n                if (i < local_best) local_best = i;\n            }\n        }\n\n        #pragma omp critical\n        if (local_best < best) best = local_best;\n    }\n\n    return best;\n}\n```\n"
}