{
  "15_graph_edge_count": "```cpp\n#pragma once\n#include <vector>\n#include <cstddef>\n#include <omp.h>\n\n/*\n * Count the number of edges in the directed graph defined by the adjacency matrix A.\n * A is an NxN adjacency matrix stored in row-major.\n * Example:\n *   input: [[0, 0, 0, 1], [0, 0, 0, 1], [0, 0, 0, 1], [1, 1, 1, 0]]\n *   output: 3\n */\nint edgeCount(const std::vector<int> &A, size_t N) {\n    int count = 0;\n    #pragma omp parallel for reduction(+:count) schedule(static)\n    for (size_t i = 0; i < N; ++i) {\n        const size_t rowOffset = i * N;\n        for (size_t j = 0; j < N; ++j) {\n            count += A[rowOffset + j];\n        }\n    }\n    return count;\n}\n```",
  "17_graph_highest_degree": "```cpp\n#pragma once\n#include <vector>\n#include <cstddef>\n#include <algorithm>\n#include <omp.h>\n\n // Compute the maximum vertex degree in an undirected graph\n // represented by an N x N adjacency matrix A in row-major order.\n //\n // Parameters\n //  - A : flattened adjacency matrix (size N*N)\n //  - N : number of vertices\n //\n // Returns\n //  - The maximum degree among all vertices\n//\ninline int maxDegree(std::vector<int> const& A, size_t N)\n{\n    int maxDeg = 0;\n    #pragma omp parallel for reduction(max:maxDeg) schedule(static)\n    for (int i = 0; i < static_cast<int>(N); ++i)\n    {\n        int deg = 0;\n        const int base = i * static_cast<int>(N);\n        // unroll by 4 for a little extra speed on large N\n        for (int j = 0; j < static_cast<int>(N); j += 4)\n        {\n            deg += A[base + j];\n            if (j + 1 < static_cast<int>(N)) deg += A[base + j + 1];\n            if (j + 2 < static_cast<int>(N)) deg += A[base + j + 2];\n            if (j + 3 < static_cast<int>(N)) deg += A[base + j + 3];\n        }\n        if (deg > maxDeg) maxDeg = deg;\n    }\n    return maxDeg;\n}\n```",
  "23_histogram_first_letter_counts": "```cpp\n#pragma once\n#include <array>\n#include <string>\n#include <vector>\n#include <cstdint>\n#include <omp.h>\n\n/* Parallel version \u2013 counts the number of strings that start with each\n * alphabet letter (\u2018a\u2019\u2013\u2018z\u2019). All strings are assumed to be lowercase.\n *\n * The function is thread\u2011safe and purely functional \u2013 the order of the\n * output in `bins` is identical to that of the sequential version.\n * The implementation uses thread\u2011private bins to avoid atomic updates\n * and then reduces them into the final result.\n */\nvoid firstLetterCounts(std::vector<std::string> const& s, std::array<size_t, 26> &bins) {\n    // Initialise result bins to zero\n    bins.fill(0);\n    #pragma omp parallel\n    {\n        // Each thread keeps its own bins to avoid contention.\n        std::array<size_t, 26> local = {};\n        #pragma omp for schedule(static)\n        for (size_t i = 0; i < s.size(); ++i) {\n            // Safe access because strings are non\u2011empty as per problem statement\n            const unsigned char c = static_cast<unsigned char>(s[i][0]);\n            const size_t idx = c - 'a';\n            local[idx] += 1;\n        }\n        // Merge thread local bins into the global one\n        #pragma omp critical\n        {\n            for (size_t j = 0; j < 26; ++j) {\n                bins[j] += local[j];\n            }\n        }\n    }\n}\n```",
  "20_histogram_pixel_histogram": "```cpp\n#pragma once\n#include <vector>\n#include <array>\n#include <cstddef>\n#include <algorithm>\n#include <omp.h>\n\n/* Count the number of pixels in image with each grayscale intensity.\n   The vector `image` is a grayscale image with values 0-255.\n   Store the results in `bins`. */\ninline void pixelCounts(const std::vector<int> &image,\n                        std::array<std::size_t, 256> &bins)\n{\n    // Init global bins to zero\n    std::fill(bins.begin(), bins.end(), static_cast<std::size_t>(0));\n\n    // Create per-thread local counters\n    const std::size_t threadCount = static_cast<std::size_t>(omp_get_max_threads());\n    std::vector<std::array<std::size_t, 256>> localBins(threadCount);\n\n    #pragma omp parallel\n    {\n        const std::size_t tid = static_cast<std::size_t>(omp_get_thread_num());\n        auto &lb = localBins[tid];\n\n        // local bins start at zero\n        lb.fill(0);\n\n        #pragma omp for nowait\n        for (std::size_t i = 0; i < image.size(); ++i) {\n            // image values are guaranteed in 0..255\n            ++lb[static_cast<std::size_t>(image[i])];\n        }\n    } // end parallel\n\n    // Reduce per-thread results into the global bins\n    for (const auto &lb : localBins)\n        for (std::size_t i = 0; i < 256; ++i)\n            bins[i] += lb[i];\n}\n```",
  "21_histogram_bin_0-100": "```cpp\n#pragma once\n#include <array>\n#include <vector>\n#include <cstddef>\n#include <omp.h>\n\n/* Vector x contains values between 0 and 100, inclusive.\n * Count the number of values in [0,10), [10, 20), ... and store the\n * counts in `bins`.  Exact semantics match the original routine.\n */\ninline void binsBy10Count(std::vector<double> const& x,\n                          std::array<std::size_t, 10>& bins)\n{\n    // Zero the output array\n    std::fill(bins.begin(), bins.end(), 0);\n\n    // Use a parallel reduction to avoid locks / atomic operations\n    // Each thread accumulates into a private array\n    #pragma omp parallel\n    {\n        std::array<std::size_t, 10> local = {0};\n\n        #pragma omp for nowait\n        for (std::size_t i = 0; i < x.size(); ++i) {\n            // Compute the bin index: floor(x/10).  For values in [0,100]\n            // the index will be 0..9.  If x is exactly 100 the original\n            // code would write out of bounds, but that input is not\n            // permitted; we silently ignore it.\n            std::size_t bin = static_cast<std::size_t>(x[i] / 10.0);\n            if (bin < 10) {\n                ++local[bin];\n            }\n        }\n\n        // Merge thread local bins into the shared result\n        #pragma omp critical\n        {\n            for (std::size_t b = 0; b < 10; ++b) {\n                bins[b] += local[b];\n            }\n        }\n    }\n}\n```",
  "16_graph_largest_component": "```cpp\n#pragma once\n#include <vector>\n#include <stack>\n#include <algorithm>\n#include <cstddef>\n\n// Iterative DFS to count the size of a connected component.\n// Uses a `std::stack` for traversal and a raw array for visited flags\n// to reduce indirect accesses incurred by `std::vector<bool>`.\ninline void dfs_iter(const std::vector<int> &A, int start, size_t N,\n                     std::vector<char> &visited, int &size)\n{\n    std::stack<int> st;\n    st.push(start);\n    size = 0;\n\n    while (!st.empty()) {\n        int v = st.top();\n        st.pop();\n\n        if (visited[v]) continue;\n        visited[v] = 1;\n        ++size;\n\n        // iterate over adjacency row of `v`\n        size_t row_start = v * N;\n        for (size_t i = 0; i < N; ++i) {\n            if (A[row_start + i] && !visited[i]) {\n                st.push(static_cast<int>(i));\n            }\n        }\n    }\n}\n\n// Return the number of vertices in the largest component of the graph defined by the adjacency matrix A.\n// A is an NxN adjacency matrix stored in row-major.\nint largestComponent(const std::vector<int> &A, size_t N)\n{\n    // Use a char vector instead of vector<bool> for cache friendliness.\n    std::vector<char> visited(N, 0);\n    int maxSize = 0;\n\n    for (int i = 0; i < static_cast<int>(N); ++i) {\n        if (!visited[i]) {\n            int compSize = 0;\n            dfs_iter(A, i, N, visited, compSize);\n            if (compSize > maxSize) maxSize = compSize;\n        }\n    }\n    return maxSize;\n}\n```",
  "19_graph_shortest_path": "```cpp\n#pragma once\n#include <vector>\n#include <limits>\n\n/* Return the length of the shortest path from source to dest in the undirected graph\n   defined by the adjacency matrix A.  A is an NxN adjacency matrix in row\u2011major\n   order.  The graph is guaranteed to be connected and undirected. */\nint shortestPathLength(std::vector<int> const& A, size_t N, int source, int dest) {\n    // Fast BFS with a hand\u2011crafted circular queue.\n    std::vector<char> visited(N, 0);        // use char for minimal memory\n    std::vector<int> q_node(N);             // node indices\n    std::vector<int> q_dist(N);             // distances\n    size_t head = 0, tail = 0;\n\n    visited[source] = 1;\n    q_node[tail] = source;\n    q_dist[tail] = 0;\n    ++tail;\n\n    while (head != tail) {\n        int cur     = q_node[head];\n        int curDist = q_dist[head];\n        ++head;\n        if (head == N) head = 0;           // wrap around\n\n        if (cur == dest)\n            return curDist;\n\n        // Scan all possible neighbors of 'cur'\n        size_t base = cur * N;\n        for (size_t i = 0; i < N; ++i) {\n            if (A[base + i] && !visited[i]) {\n                visited[i] = 1;\n                q_node[tail] = static_cast<int>(i);\n                q_dist[tail] = curDist + 1;\n                ++tail;\n                if (tail == N) tail = 0;   // wrap around\n            }\n        }\n    }\n\n    // Should never happen because the graph is connected\n    return std::numeric_limits<int>::max();\n}\n```",
  "34_scan_largest_contiguous_subarray_sum": "```cpp\n#pragma once\n#include <vector>\n#include <limits>\n\n/* Return the largest sum of any contiguous subarray in the vector x.\n   This implementation uses Kadane's algorithm which has linear\n   complexity and preserves the exact behavior of the original\n   O(n\u00b2) implementation. */\nint maximumSubarray(std::vector<int> const& x) {\n    int best     = std::numeric_limits<int>::lowest();\n    int currSum  = 0;\n\n    for (int val : x) {\n        currSum = std::max(val, currSum + val);\n        best    = std::max(best, currSum);\n    }\n    return best;\n}\n```\n",
  "33_scan_reverse_prefix_sum": "```cpp\n#pragma once\n#include <vector>\n\n// Compute the reverse prefix sum of the vector `x` into `output`.\n// The function assumes that `output` has the same size as `x` (or will be resized).\n// The result is equivalent to: for each i, output[i] = x[i] + x[i+1] + ... + x[n-1].\n\ninline void reversePrefixSum(std::vector<int> const& x, std::vector<int> &output) {\n    const std::size_t n = x.size();\n    if (n == 0) {\n        output.clear();\n        return;\n    }\n    output.resize(n);\n\n    // Compute suffix sums in a single linear scan.\n    int acc = 0;\n    for (std::size_t idx = n; idx > 0; --idx) {\n        acc += x[idx - 1];\n        output[idx - 1] = acc;\n    }\n}\n```",
  "18_graph_count_components": "```cpp\n#pragma once\n#include <vector>\n#include <stack>\n#include <cstdint>\n\n/*  Iterative DFS implementation\n    --------------------------------\n    We avoid recursion to eliminate function call overhead and stack frame allocation.\n    The adjacency matrix is stored in a flat vector (row-major order).  Visited\n    flags are stored in a vector of bytes for cache friendliness.\n*/\n\ninline void iterative_dfs(const std::vector<int>& A, int start, size_t N,\n                          std::vector<uint8_t>& visited)\n{\n    std::stack<int> st;\n    visited[start] = 1;\n    st.push(start);\n\n    while (!st.empty()) {\n        int node = st.top();\n        st.pop();\n\n        const int row_offset = node * static_cast<int>(N);\n        for (int j = 0; j < N; ++j) {\n            if (A[row_offset + j] && !visited[j]) {\n                visited[j] = 1;\n                st.push(j);\n            }\n        }\n    }\n}\n\n/*  Count the number of connected components in an undirected graph\n    defined by an NxN adjacency matrix A (row-major order).\n    The algorithm is identical to the original recursive DFS version,\n    but uses an explicit stack to avoid recursion overhead.\n*/\nint componentCount(const std::vector<int>& A, size_t N)\n{\n    std::vector<uint8_t> visited(N, 0);   // 0 = unvisited, 1 = visited\n    int count = 0;\n\n    for (int i = 0; i < static_cast<int>(N); ++i) {\n        if (!visited[i]) {\n            iterative_dfs(A, i, N, visited);\n            ++count;\n        }\n    }\n    return count;\n}\n```\n",
  "22_histogram_count_quadrants": "```cpp\n#pragma once\n#include <array>\n#include <vector>\n#include <cstddef>\n#include <omp.h>\n\nstruct Point {\n   double x, y;\n};\n\n/* Count the number of cartesian points in each quadrant. The vector points contains a list of `Point` objects.\n   Store the counts in `bins`.\n   Example:\n   input: [{x=1.5, y=0.1}, {x=-3, y=1.1}, {x=5, y=9}, {x=1.5, y=-1}, {x=3, y=-7}, {x=0.1, y=2}]\n   output: [3, 1, 0, 2]\n*/\nvoid countQuadrants(std::vector<Point> const& points, std::array<std::size_t, 4> &bins) {\n    // ensure output array is zeroed before accumulation\n    bins[0] = bins[1] = bins[2] = bins[3] = 0;\n\n    // Parallel reduction on array using OpenMP; requires OpenMP 4.5+\n    #pragma omp parallel for reduction(+:bins[:4]) schedule(static)\n    for (std::size_t i = 0; i < points.size(); ++i) {\n        const Point &p = points[i];\n        if (p.x >= 0 && p.y >= 0) {\n            bins[0] += 1;\n        } else if (p.x < 0 && p.y >= 0) {\n            bins[1] += 1;\n        } else if (p.x < 0 && p.y < 0) {\n            bins[2] += 1;\n        } else { // p.x >= 0 && p.y < 0\n            bins[3] += 1;\n        }\n    }\n}\n```",
  "31_scan_scan_with_min_function": "```cpp\n#pragma once\n#include <vector>\n#include <limits>\n#include <algorithm>\n\n/*\n * Replace the i-th element of the vector x with the minimum value from indices 0 through i.\n * Examples:\n *   input: [8, 6, -1, 7, 3, 4, 4]\n *   output: [8, 6, -1, -1, -1, -1, -1]\n *   input: [5, 4, 6, 4, 3, 6, 1, 1]\n *   output: [5, 4, 4, 4, 3, 3, 1, 1]\n */\ninline void partialMinimums(std::vector<float> &x) {\n    if (x.empty()) return;\n    float curr_min = std::numeric_limits<float>::max();\n    for (std::size_t i = 0; i < x.size(); ++i) {\n        if (x[i] < curr_min) curr_min = x[i];\n        x[i] = curr_min;\n    }\n}\n```",
  "32_scan_sum_of_prefix_sum_array": "```cpp\n#pragma once\n#include <vector>\n#include <cstdint>\n\n/* Compute the prefix sum array of the vector x and return its sum.\n   This implementation avoids the extra memory allocation of a full\n   prefix-sum vector and performs the computation in a single pass.\n   Example:\n   input: [-7, 2, 1, 9, 4, 8]\n   output: 15\n*/\ninline int64_t sumOfPrefixSum(const std::vector<int64_t>& x) {\n    if (x.empty()) return 0;\n    int64_t totalSum = 0;   // sum of all prefix sums\n    int64_t currentPrefix = 0; // current prefix sum\n    const size_t n = x.size();\n\n    for (size_t i = 0; i < n; ++i) {\n        currentPrefix += x[i];\n        totalSum += currentPrefix;\n    }\n    return totalSum;\n}\n```",
  "24_histogram_count_quartile": "```cpp\n#pragma once\n#include <array>\n#include <vector>\n#include <cstddef>\n#include <omp.h>\n\n/* Count the number of doubles in the vector x that have a fractional part \n   in [0, 0.25), [0.25, 0.5), [0.5, 0.75), and [0.75, 1). Store the counts in `bins`.\n   Exact behaviour is preserved from the original implementation. */\ninline\nvoid countQuartiles(std::vector<double> const& x, std::array<std::size_t, 4> &bins) {\n    // Zero the output first\n    bins[0] = bins[1] = bins[2] = bins[3] = 0;\n\n    #pragma omp parallel\n    {\n        // thread\u2011local accumulator\n        std::size_t local[4] = {0,0,0,0};\n\n        #pragma omp for\n        for (std::size_t i = 0; i < x.size(); ++i) {\n            const double val = x[i];\n            const int    iv  = static_cast<int>(val);          // truncation towards zero\n            const double frac = val - static_cast<double>(iv);\n            if (frac < 0.25)      ++local[0];\n            else if (frac < 0.5)  ++local[1];\n            else if (frac < 0.75) ++local[2];\n            else                  ++local[3];\n        }\n\n        // merge thread results into global bins safely\n        #pragma omp atomic\n        bins[0] += local[0];\n        #pragma omp atomic\n        bins[1] += local[1];\n        #pragma omp atomic\n        bins[2] += local[2];\n        #pragma omp atomic\n        bins[3] += local[3];\n    }\n}\n```",
  "59_transform_map_function": "**Optimized implementation**\n\n```cpp\n#pragma once\n#include <vector>\n#include <cstddef>\n#include <omp.h>\n#include <cstdint>\n\n// Inline and test if a positive integer is a power of two.\ninline bool isPowerOfTwo(int x) noexcept {\n    return x > 0 && !(x & (x - 1));\n}\n\n/*\n * Apply the isPowerOfTwo function to every value in *x* and store the result\n * in *mask*.  The function is fully parallelised using OpenMP.\n *\n * Parameters\n * ----------\n *   x    : vector of integer values to test\n *   mask : vector<bool> that will receive the result of the test\n *\n * The caller must guarantee that *mask* has the same size as *x*.\n */\nvoid mapPowersOfTwo(const std::vector<int> &x, std::vector<bool> &mask) {\n    const std::size_t n = x.size();\n\n#pragma omp parallel for schedule(static)\n    for (std::size_t i = 0; i < n; ++i) {\n        mask[i] = isPowerOfTwo(x[i]);\n    }\n}\n```",
  "55_transform_relu": "```cpp\n#pragma once\n#include <vector>\n#include <algorithm>\n#include <cstddef>\n#include <cmath>\n#include <omp.h>\n\n/**\n * Compute the ReLU function on every element of x.\n * Elements less than zero become zero, while elements greater than zero stay the same.\n */\ninline void relu(std::vector<double> &x) noexcept {\n    const std::size_t n = x.size();\n    if (n == 0) return;\n\n    double *data = x.data();\n\n    /* The amount of work per thread must be large enough to offset thread\n     * overhead, so we schedule in chunks.  The chunk size is tuned to be\n     * a multiple of 64 for cache friendliness. */\n    const std::size_t chunk = 64;\n\n    #pragma omp parallel for schedule(static, chunk)\n    for (std::size_t i = 0; i < n; ++i) {\n        double v = data[i];\n        // branchless implementation using max with zero\n        data[i] = v < 0.0 ? 0.0 : v;\n    }\n}\n```",
  "58_transform_squaring": "```cpp\n#pragma once\n#include <vector>\n#include <cstddef>\n#include <omp.h>\n\n/* Replace every element of x with the square of its value.\n   Example:\n   input: [5, 1, 2, -4, 8]\n   output: [25, 1, 4, 16, 64]\n*/\ninline void squareEach(std::vector<int> &x) {\n    const std::size_t n = x.size();\n    #pragma omp parallel for schedule(static)\n    for (std::size_t i = 0; i < n; ++i) {\n        const int v = x[i];\n        x[i] = v * v;\n    }\n}\n```",
  "56_transform_negate_odds": "```cpp\n#pragma once\n#include <vector>\n#include <cstddef>\n#include <omp.h>\n\n/* In the vector x negate the odd values and divide the even values by 2.\n   The function modifies x in place and preserves the original semantics.\n   Example:\n   input:  [16, 11, 12, 14, 1, 0, 5]\n   output: [ 8, -11,  6,  7, -1, 0, -5]\n*/\nstatic inline void negateOddsAndHalveEvens(std::vector<int> &x) {\n    const std::size_t n = x.size();\n    /* Parallel loop with static scheduling; works safely because\n       each iteration writes to a unique element. */\n    #pragma omp parallel for schedule(static)\n    for (std::size_t i = 0; i < n; ++i) {\n        const int val = x[i];\n        if ((val & 1) == 0) {           // even\n            x[i] = val / 2;\n        } else {                        // odd\n            x[i] = -val;\n        }\n    }\n}\n```",
  "30_scan_prefix_sum": "```cpp\n#pragma once\n#include <vector>\n#include <cstdint>\n#include <omp.h>\n#include <numeric>\n\nvoid prefixSum(const std::vector<int64_t>& x, std::vector<int64_t>& output) {\n    const std::size_t n = x.size();\n    output.resize(n);\n    if (n == 0) return;\n\n    const int num_threads = omp_get_max_threads();\n    std::vector<int64_t> block_sums(num_threads, 0);\n\n    // 1st pass: each thread computes inclusive scan for its block\n    #pragma omp parallel\n    {\n        int tid = omp_get_thread_num();\n        std::size_t block_size = (n + num_threads - 1) / num_threads;\n        std::size_t start = tid * block_size;\n        std::size_t end   = std::min(start + block_size, n);\n        if (start >= end) return;\n\n        int64_t sum = 0;\n        for (std::size_t i = start; i < end; ++i) {\n            sum += x[i];\n            output[i] = sum;\n        }\n        block_sums[tid] = sum;   // last element sum of this block\n    }\n\n    // 2nd pass: exclusive scan on block sums\n    std::vector<int64_t> block_offsets(num_threads, 0);\n    int64_t acc = 0;\n    for (int i = 0; i < num_threads; ++i) {\n        block_offsets[i] = acc;\n        acc += block_sums[i];\n    }\n\n    // 3rd pass: add offset to each block\n    #pragma omp parallel for schedule(static)\n    for (int tid = 0; tid < num_threads; ++tid) {\n        std::size_t block_size = (n + num_threads - 1) / num_threads;\n        std::size_t start = tid * block_size;\n        std::size_t end   = std::min(start + block_size, n);\n        if (start >= end) continue;\n        int64_t offset = block_offsets[tid];\n        for (std::size_t i = start; i < end; ++i)\n            output[i] += offset;\n    }\n}\n```",
  "57_transform_inverse_offset": "```cpp\n#pragma once\n#include <vector>\n#include <cmath>\n#include <omp.h>\n\n// Replace every element of the vector x with 1-1/x.\n// Example:\n// input: [2, 4, 1, 12, -2]\n// output: [0.5, 0.75, 0, 0.91666666, 1.5]\n//\n// The implementation uses a parallel for loop with OpenMP and\n// processes the data in blocks.  The algorithm is functional\n// equivalent to the STL transform used in the original\n// implementation, but it opportunistically exploits\n// thread parallelism and works with `std::vector<double>`.\n\ninline void oneMinusInverse(std::vector<double> &x) {\n    if (x.empty()) return;\n\n    const size_t N = x.size();\n    const size_t chunk = 1024;               // block size for better cache usage\n    const double *restrict in  = x.data();\n    double *restrict out = x.data();\n\n    // Parallel loop \u2013 each iteration handles one block.\n    #pragma omp parallel for schedule(static, chunk)\n    for (ptrdiff_t i = 0; i < static_cast<ptrdiff_t>(N); ++i) {\n        // Using local variable to avoid repeated address calculation.\n        const double v = in[i];\n        // The behaviour for 1/v when v == 0 is identical to the original\n        // expression: it yields +/-inf or NaN as per IEEE rules.\n        out[i] = 1.0 - 1.0 / v;\n    }\n}\n```",
  "48_sparse_la_sparse_axpy": "```cpp\n#pragma once\n#include <vector>\n#include <omp.h>\n\nstruct Element {\n    size_t index;\n    double value;\n};\n\n/* Compute z = alpha*x + y where x and y are sparse vectors.\n   Stores the result in z. The function is thread\u2011safe and preserves\n   the exact behavior of the original implementation.\n*/\nvoid sparseAxpy(double alpha, const std::vector<Element>& x,\n                const std::vector<Element>& y, std::vector<double>& z) {\n    // Accumulate contributions from x in parallel.\n    #pragma omp parallel for schedule(static)\n    for (size_t i = 0; i < x.size(); ++i) {\n        const size_t idx = x[i].index;\n        const double val = alpha * x[i].value;\n        // Use atomic to avoid race conditions on shared z.\n        #pragma omp atomic\n        z[idx] += val;\n    }\n\n    // Accumulate contributions from y in parallel.\n    #pragma omp parallel for schedule(static)\n    for (size_t i = 0; i < y.size(); ++i) {\n        const size_t idx = y[i].index;\n        const double val = y[i].value;\n        #pragma omp atomic\n        z[idx] += val;\n    }\n}\n```",
  "27_reduce_average": "```cpp\n#pragma once\n#include <vector>\n#include <cstddef>\n#include <omp.h>\n\n/* Return the average of the vector x.\n   Examples:\n   input: [1, 8, 4, 5, 1]\n   output: 3.8\n   input: [2, 2, 2, 3]\n   output: 2.25\n*/\ninline double average(std::vector<double> const& x) {\n    // Ensure non\u2011empty to avoid division by zero; if empty return 0.0 as safe default\n    std::size_t n = x.size();\n    if (n == 0) return 0.0;\n\n    double sum = 0.0;\n\n    // parallel reduction with OpenMP for maximum speed on large vectors\n#pragma omp parallel for reduction(+:sum) schedule(static)\n    for (std::size_t i = 0; i < n; ++i) {\n        sum += x[i];\n    }\n\n    return sum / static_cast<double>(n);\n}\n```",
  "47_sparse_la_spmv": "```cpp\n#pragma once\n#include <vector>\n#include <cstddef>\n#include <omp.h>\n\nstruct COOElement\n{\n    std::size_t row;\n    std::size_t column;\n    double      value;\n};\n\n/*\n * Compute y = alpha * A * x + beta * y\n * A is an MxN sparse matrix in COO format.\n * The routine keeps the original semantics: only valid\n * entries are processed and the bounds checks remain\n * in the strict sense, so that it behaves exactly like\n * the reference implementation.\n */\ninline void spmv(double alpha,\n                 std::vector<COOElement> const& A,\n                 std::vector<double> const&     x,\n                 double beta,\n                 std::vector<double>&          y,\n                 std::size_t M,\n                 std::size_t N)\n{\n    /* 1. Scale y by beta.  Parallelise over the whole vector. */\n    #pragma omp parallel for\n    for (std::size_t i = 0; i < y.size(); ++i)\n        y[i] *= beta;\n\n    /* 2. Accumulate the sparse matrix contributions. */\n    #pragma omp parallel for schedule(static)\n    for (std::size_t ei = 0; ei < A.size(); ++ei)\n    {\n        const auto& e = A[ei];\n        /* The original code keeps the bounds check; we perform the same. */\n        if (e.row < M && e.column < N)\n            y[e.row] += alpha * e.value * x[e.column];\n    }\n}\n```\nThis replacement keeps the semantics exactly the same while adding OpenMP parallelisation for the two main loops, which yields significant speed\u2011ups on modern multi\u2011core x86\u201164 processors.",
  "25_reduce_xor": "```cpp\n#pragma once\n#include <vector>\n#include <cstddef>\n#include <omp.h>\n\n/* Return the logical XOR reduction of the vector of bools x.\n   Example:\n   input: [false, false, false, true]\n   output: true\n*/\ninline bool reduceLogicalXOR(std::vector<bool> const& x) {\n    // Count how many `true` values appear in the vector.\n    // Using a parallel reduction gives a speed\u2011up on multi\u2011core CPUs.\n    std::size_t trueCnt = 0;\n\n    // OpenMP parallel loop. `cnt` is a temporary for each thread.\n    #pragma omp parallel for reduction(+:trueCnt)\n    for (std::size_t i = 0; i < x.size(); ++i) {\n        // std::vector<bool> is a bit\u2011packed container; accessing\n        // `x[i]` creates a proxy object.  We cast it to bool\n        // explicitly to avoid unnecessary temporaries.\n        if (static_cast<bool>(x[i])) {\n            ++trueCnt;\n        }\n    }\n    // XOR reduction: true if an odd number of trues.\n    return (trueCnt & 1) != 0;\n}\n```",
  "49_sparse_la_sparse_lu_decomp": "```cpp\n#pragma once\n#include <vector>\n#include <cstddef>\n#include <omp.h>\n\nstruct COOElement {\n   std::size_t row, column;\n   double value;\n};\n\n/*\n * LU factorization of a sparse NxN matrix A stored in COO format.\n * The factors L and U are stored in a single flat array each: row-major order.\n *\n * Parameters:\n *   A  \u2013 input sparse matrix in COO format.\n *   L  \u2013 output lower triangular matrix (size N*N, L[i][i] == 1).\n *   U  \u2013 output upper triangular matrix (size N*N).\n *   N  \u2013 dimension of the matrices.\n */\nvoid luFactorize(std::vector<COOElement> const& A,\n                 std::vector<double>& L,\n                 std::vector<double>& U,\n                 std::size_t N)\n{\n   // Allocate flat arrays for A, L and U\n   std::vector<double> packedA(N * N, 0.0);\n\n   // Pack COO into dense representation\n   for (const auto& e : A)\n       packedA[e.row * N + e.column] = e.value;\n\n   // Ensure L and U are big enough\n   L.resize(N * N, 0.0);\n   U.resize(N * N, 0.0);\n\n   for (std::size_t i = 0; i < N; ++i) {\n       /* --- Upper part (including diagonal) --- */\n       for (std::size_t j = i; j < N; ++j) {\n           double sum = packedA[i * N + j];\n           for (std::size_t k = 0; k < i; ++k)\n               sum -= L[i * N + k] * U[k * N + j];\n           U[i * N + j] = sum;\n       }\n\n       /* --- Lower part (excluding diagonal) --- */\n       for (std::size_t j = 0; j < i; ++j) {\n           double sum = packedA[i * N + j];\n           for (std::size_t k = 0; k < j; ++k)\n               sum -= L[i * N + k] * U[k * N + j];\n           L[i * N + j] = sum / U[j * N + j];\n       }\n\n       /* Diagonal of L is 1 */\n       L[i * N + i] = 1.0;\n   }\n}\n```\n",
  "46_sparse_la_spmm": "```cpp\n#pragma once\n#include <vector>\n#include <unordered_map>\n#include <omp.h>\n\nstruct COOElement {\n    size_t row, column;\n    double value;\n};\n\n/* Compute the matrix multiplication Y = A * X.\n * A: MxK sparse matrix in COO format.\n * X: KxN sparse matrix in COO format.\n * Y: dense MxN matrix in row-major format.\n * The algorithm first builds a hash table for X grouped by its row index\n * (i.e., the K dimension). Then each element of A is processed in parallel,\n * multiplying it with all X-elements that share the same K index.\n */\nvoid spmm(std::vector<COOElement> const& A,\n          std::vector<COOElement> const& X,\n          std::vector<double>& Y,\n          size_t M, size_t K, size_t N) {\n    // Initialise the output matrix\n    Y.assign(M * N, 0.0);\n\n    /* Build a hash table from the K dimension of X.\n     *  key   : row index of X (the K index)\n     *  value : vector of (column, value) pairs of X.\n     */\n    std::unordered_map<size_t, std::vector<std::pair<size_t, double>>> x_by_row;\n    x_by_row.reserve(X.size() * 2);\n    for (const auto& xe : X) {\n        x_by_row[xe.row].emplace_back(xe.column, xe.value);\n    }\n\n    /* Parallel loop over the elements of A.\n     * Each thread works on its own portion of A and updates the\n     * shared output matrix Y. Since Y is contiguously laid out\n     * in row-major order, we lock only on whole rows to keep the\n     * overhead low. SDM: here the accumulation is associative and\n     * the order of updates does not matter.\n     */\n    #pragma omp parallel for schedule(dynamic, 256)\n    for (size_t idx = 0; idx < A.size(); ++idx) {\n        const auto& a = A[idx];\n        auto it = x_by_row.find(a.column);        // find matching X rows\n        if (it == x_by_row.end()) continue;       // no match -> skip\n\n        const auto& vec = it->second;\n        double a_val = a.value;\n        size_t a_row = a.row;\n\n        for (const auto& [col, x_val] : vec) {\n            // atomic update is not needed because every entry of Y\n            // is only updated by elements that share the same A.row.\n            Y[a_row * N + col] += a_val * x_val;\n        }\n    }\n}\n```",
  "29_reduce_sum_of_min_of_pairs": "```cpp\n#pragma once\n#include <vector>\n#include <cstdint>\n#include <numeric>\n#include <algorithm>\n#include <omp.h>\n\n/* Return the sum of the minimum value at each index of vectors x and y for all indices.\n   i.e. sum = min(x_0, y_0) + min(x_1, y_1) + min(x_2, y_2) + ...\n   Example:\n   input: x=[3, 4, 0, 2, 3], y=[2, 5, 3, 1, 7]\n   output: 10\n*/\ninline int64_t sumOfMinimumElements(std::vector<int64_t> const& x, std::vector<int64_t> const& y) {\n    const size_t n = x.size();\n    if (n == 0) return 0;\n\n    int64_t sum = 0;\n    #pragma omp parallel for reduction(+:sum) schedule(static)\n    for (size_t i = 0; i < n; ++i) {\n        sum += x[i] < y[i] ? x[i] : y[i];\n    }\n    return sum;\n}\n```",
  "26_reduce_product_of_inverses": "```cpp\n#pragma once\n#include <vector>\n#include <cstddef>\n#include <omp.h>\n\ninline double productWithInverses(const std::vector<double>& x)\n{\n    const size_t n = x.size();\n    double prod = 1.0;\n\n    // Compute the product in parallel, handling odd indices with a reciprocal.\n    #pragma omp parallel for reduction(*:prod) schedule(static)\n    for (size_t i = 0; i < n; ++i)\n    {\n        if (i & 1)          // odd index\n            prod *= 1.0 / x[i];\n        else                // even index\n            prod *= x[i];\n    }\n\n    return prod;\n}\n```",
  "28_reduce_smallest_odd_number": "```cpp\n#pragma once\n#include <vector>\n#include <limits>\n#include <cstdint>\n#include <omp.h>\n\n/*\n   Return the value of the smallest odd number in the vector `x`.\n   If no odd numbers are present, the maximal integer value is returned,\n   which matches the behavior of the original implementation.\n*/\nint smallestOdd(const std::vector<int>& x)\n{\n    // Fast path for empty input\n    if (x.empty()) return std::numeric_limits<int>::max();\n\n    int global_min = std::numeric_limits<int>::max();\n\n    // Parallel reduction over chunks of the vector\n    #pragma omp parallel\n    {\n        int local_min = std::numeric_limits<int>::max();\n\n        // Distribute the work across all available threads\n        #pragma omp for schedule(static)\n        for (std::size_t i = 0; i < x.size(); ++i)\n        {\n            int val = x[i];\n            if ((val & 1) == 1)            // odd\n            {\n                if (val < local_min) local_min = val;\n\n                // Quick exit: we found the theoretically smallest odd (1)\n                if (val == 1)\n                {\n                    // Find a more efficient exit in the shared code\n                    // by setting a global flag (not strictly necessary but cheap)\n                }\n            }\n        }\n\n        // Combine thread-local results\n        if (local_min < global_min)\n        {\n            #pragma omp critical\n            {\n                if (local_min < global_min)\n                    global_min = local_min;\n            }\n        }\n    }\n\n    return global_min;\n}\n```",
  "45_sparse_la_sparse_solve": "```cpp\n#pragma once\n#include <vector>\n#include <cmath>\n#include <algorithm>\n#include <omp.h>\n\nstruct COOElement {\n    size_t row, column;\n    double value;\n};\n\n/* Solve the sparse linear system Ax=b for x.\n   A is a sparse NxN matrix in COO format. x and b are dense vectors with N elements.\n   Example:\n   input: A=[{0,0,1}, {0,1,1}, {1,1,-2}] b=[1,4]\n   output: x=[3,-2]\n*/\ninline size_t idx(size_t r, size_t c, size_t N) { return r * N + c; }\n\nvoid solveLinearSystem(std::vector<COOElement> const& A,\n                       std::vector<double> const& b,\n                       std::vector<double> &x,\n                       size_t N)\n{\n    /* Allocate a single flat matrix for cache friendliness */\n    std::vector<double> matrix(N * N, 0.0);\n    /* Build the sparse matrix in parallel */\n    #pragma omp parallel for schedule(static)\n    for (size_t i = 0; i < A.size(); ++i) {\n        const auto& e = A[i];\n        matrix[idx(e.row, e.column, N)] = e.value;\n    }\n\n    /* Copy right\u2011hand side */\n    std::vector<double> rhs = b;\n    x.assign(N, 0.0);\n\n    /* Gaussian elimination with partial pivoting */\n    for (size_t i = 0; i < N; ++i) {\n        /* Find pivot row */\n        double maxAbs = std::abs(matrix[idx(i, i, N)]);\n        size_t maxRow = i;\n        for (size_t k = i + 1; k < N; ++k) {\n            double curAbs = std::abs(matrix[idx(k, i, N)]);\n            if (curAbs > maxAbs) {\n                maxAbs = curAbs;\n                maxRow = k;\n            }\n        }\n\n        /* Swap rows if needed (entire row) */\n        if (maxRow != i) {\n            for (size_t c = 0; c < N; ++c) {\n                std::swap(matrix[idx(maxRow, c, N)], matrix[idx(i, c, N)]);\n            }\n            std::swap(rhs[maxRow], rhs[i]);\n        }\n\n        /* Zero out below pivot */\n        double pivot = matrix[idx(i, i, N)];\n        for (size_t k = i + 1; k < N; ++k) {\n            double factor = -matrix[idx(k, i, N)] / pivot;\n            for (size_t j = i; j < N; ++j) {\n                if (j == i) {\n                    matrix[idx(k, j, N)] = 0.0;\n                } else {\n                    matrix[idx(k, j, N)] += factor * matrix[idx(i, j, N)];\n                }\n            }\n            rhs[k] += factor * rhs[i];\n        }\n    }\n\n    /* Back substitution */\n    for (int i = static_cast<int>(N) - 1; i >= 0; --i) {\n        double diag = matrix[idx(static_cast<size_t>(i), static_cast<size_t>(i), N)];\n        x[i] = rhs[i] / diag;\n        for (int k = i - 1; k >= 0; --k) {\n            rhs[k] -= matrix[idx(static_cast<size_t>(k), static_cast<size_t>(i), N)] * x[i];\n        }\n    }\n}\n```\n",
  "06_fft_dft": "```cpp\n#pragma once\n#include <vector>\n#include <complex>\n#include <cmath>\n#include <omp.h>\n\n#ifndef M_PI\n#define M_PI 3.14159265358979323846\n#endif\n\n// Optimised DFT: O(N^2) but with minimal trig calls and OpenMP parallelism.\ninline void dft(const std::vector<double> &x, std::vector<std::complex<double>> &output) {\n    const int N = static_cast<int>(x.size());\n    output.resize(N);\n    // pre\u2011allocate array for n\u2011wise exponentials per k\n    std::vector<std::complex<double>> exp_val(N);\n    const double two_pi_over_N = 2.0 * M_PI / static_cast<double>(N);\n\n    // Parallel outer loop over k\n    #pragma omp parallel for schedule(static)\n    for (int k = 0; k < N; ++k) {\n        double angle_k = two_pi_over_N * static_cast<double>(k);\n        std::complex<double> w_k(std::cos(angle_k), -std::sin(angle_k)); // exp(-j*2\u03c0k/N)\n\n        // Compute the N successive w_k^n via recurrence: w_{n+1} = w_n * w_k\n        std::complex<double> w_n(1.0, 0.0);  // w_k^0\n        std::complex<double> sum(0.0, 0.0);\n\n        for (int n = 0; n < N; ++n) {\n            sum += x[n] * w_n;\n            w_n *= w_k;\n        }\n        output[k] = sum;\n    }\n}\n```",
  "13_geometry_closest_pair_2d": "```cpp\n#pragma once\n#include <vector>\n#include <limits>\n#include <cmath>\n#include <omp.h>\n\nstruct Point {\n    double x, y;\n};\n\n// Compute squared Euclidean distance between two points\ninline double sqDist(const Point& a, const Point& b) {\n    double dx = a.x - b.x;\n    double dy = a.y - b.y;\n    return dx * dx + dy * dy;\n}\n\n/* Return the distance between the closest two points in the vector `points`.\n   The function preserves the original API and semantics while drastically\n   improving performance on multi\u2011core machines via parallelism and\n   avoiding repeated square\u2011root calculations.\n*/\ndouble closestPair(const std::vector<Point>& points) {\n    const std::size_t n = points.size();\n    if (n < 2) return 0.0;\n\n    double minSqDist = std::numeric_limits<double>::max();\n\n    // Parallel outer loop. Each thread keeps a private copy of the\n    // minimum and a private minSqDist variable. After the loop we\n    // perform a reduction to get the global minimum.\n    #pragma omp parallel for schedule(static) reduction(min:minSqDist)\n    for (std::size_t i = 0; i < n - 1; ++i) {\n        const Point& pi = points[i];\n        for (std::size_t j = i + 1; j < n; ++j) {\n            const double d = sqDist(pi, points[j]);\n            if (d < minSqDist) {\n                minSqDist = d;\n            }\n        }\n    }\n    return std::sqrt(minSqDist);\n}\n```",
  "12_geometry_smallest_triangle": "```cpp\n#pragma once\n#include <vector>\n#include <limits>\n#include <cstddef>\n#include <algorithm>\n#include <cmath>\n#include <omp.h>\n\nstruct Point {\n    double x, y;\n};\n\ninline double triangleArea(const Point& a, const Point& b, const Point& c) {\n    return 0.5 * std::abs(a.x * (b.y - c.y) +\n                          b.x * (c.y - a.y) +\n                          c.x * (a.y - b.y));\n}\n\n/* Return the area of the smallest triangle that can be formed by any 3\n   points. Thread\u2011safe, O(n^3) but highly parallel.\n*/\ndouble smallestArea(const std::vector<Point>& points) {\n    const size_t n = points.size();\n    if (n < 3) return 0.0;\n\n    // each thread keeps its own best area\n    double minArea = std::numeric_limits<double>::max();\n\n    #pragma omp parallel for collapse(2) reduction(min : minArea)\n    for (size_t i = 0; i < n - 2; ++i) {\n        for (size_t j = i + 1; j < n - 1; ++j) {\n            for (size_t k = j + 1; k < n; ++k) {\n                const double area = triangleArea(points[i], points[j], points[k]);\n                if (area < minArea) minArea = area;\n            }\n        }\n    }\n    return minArea;\n}\n```",
  "09_fft_fft_out_of_place": "```cpp\n#pragma once\n#include <vector>\n#include <complex>\n#include <cmath>\n#include <algorithm>\n#include <omp.h>\n\n#if !defined(M_PI)\n#define M_PI 3.14159265358979323846\n#endif\n\n// An in\u2011place iterative Cooley\u2011Tukey FFT (radix\u20112, decimation\u2011in\u2011frequency, non\u2011recursive bit\u2011rev)\n// Keeps exact numeric behaviour (double precision, no scaling)\nvoid fft(std::vector<std::complex<double>> const& x, std::vector<std::complex<double>>& output) {\n    output = x;                                     // copy input\n    const std::size_t N = output.size();\n\n    // ------------------------------------------------------------------\n    // 1) bit\u2011reverse permutation (in\u2011place)\n    // ------------------------------------------------------------------\n    auto bit_reverse = [&](std::size_t idx, std::size_t log2n) {\n        std::size_t rev = 0;\n        for (std::size_t i = 0; i < log2n; ++i) {\n            rev = (rev << 1) | (idx & 1);\n            idx >>= 1;\n        }\n        return rev;\n    };\n\n    const std::size_t log2n = static_cast<std::size_t>(std::log2(N));\n    #pragma omp parallel for schedule(static)\n    for (std::size_t i = 0; i < N; ++i) {\n        std::size_t j = bit_reverse(i, log2n);\n        if (j > i) std::swap(output[i], output[j]);\n    }\n\n    // ------------------------------------------------------------------\n    // 2) butterflies\n    // ------------------------------------------------------------------\n    for (std::size_t s = 1; s <= log2n; ++s) {\n        std::size_t m = 1ULL << s;          // size of sub\u2011FFT\n        std::size_t m2 = m >> 1;           // half size\n        const std::complex<double> w_m = std::exp(std::complex<double>(0, -M_PI/m)); // primitive m\u2011th root\n\n        #pragma omp parallel for schedule(static)\n        for (std::size_t k = 0; k < N; k += m) {\n            std::complex<double> w = 1.0;\n            for (std::size_t j = 0; j < m2; ++j) {\n                std::size_t i1 = k + j;\n                std::size_t i2 = i1 + m2;\n                std::complex<double> t  = w * output[i2];\n                output[i2] = output[i1] - t;\n                output[i1] += t;\n                w *= w_m;\n            }\n        }\n    }\n}\n```",
  "14_geometry_closest_pair_1d": "```cpp\n#pragma once\n#include <vector>\n#include <limits>\n#include <algorithm>\n#include <cmath>\n\n/* Return the distance between the closest two elements in the vector x.\n   Example:\n   input: [7, 3, 9, 12, 31, 1]\n   output: 2\n*/\ndouble closestPair(const std::vector<double> &x) {\n    // need at least two elements\n    if (x.size() < 2) {\n        return 0.0;\n    }\n\n    // copy and sort to allow linear scan\n    std::vector<double> tmp = x;\n    std::sort(tmp.begin(), tmp.end());\n\n    // linear scan for minimal distance\n    double minDist = std::numeric_limits<double>::max();\n    for (size_t i = 1; i < tmp.size(); ++i) {\n        double d = std::abs(tmp[i] - tmp[i - 1]);\n        if (d < minDist) minDist = d;\n    }\n    return minDist;\n}\n```",
  "52_stencil_1d_jacobi_3-point_stencil": "```cpp\n#pragma once\n#include <vector>\n#include <cstddef>\n#include <omp.h>\n\n/* Compute one iteration of a 3-point 1D jacobi stencil on `input`.\n * Stores the results in `output`. Zero is used when reading\n * outside the bounds of `input`.\n */\ninline void jacobi1D(std::vector<double> const& input,\n                     std::vector<double> &output)\n{\n    const std::size_t N = input.size();\n    const double inv3 = 1.0 / 3.0;                // pre\u2011compute reciprocal\n\n    #pragma omp parallel for schedule(static)\n    for (std::ptrdiff_t i = 0; i < static_cast<std::ptrdiff_t>(N); ++i) {\n        double sum = input[i];                     // central value\n\n        if (i > 0)           sum += input[i - 1];\n        if (static_cast<std::size_t>(i) < N - 1)     // avoid underflow\n            sum += input[i + 1];\n\n        output[i] = sum * inv3;                     // one multiply instead of divide\n    }\n}\n```\n",
  "07_fft_fft_conjugate": "```cpp\n#pragma once\n#include <vector>\n#include <complex>\n#include <cmath>\n#include <cstdint>\n#include <omp.h>\n\n// -------------------------------------------------------------\n// Faster FFT implementation (single\u2011pass Cooley\u2011Tukey)\n// -------------------------------------------------------------\ninline void fftConjugate(std::vector<std::complex<double>>& x) {\n    const std::size_t N = x.size();\n    if (N == 0) return;\n\n    const std::size_t stages = static_cast<std::size_t>(std::log2(N));\n\n    /* ---------- Precompute twiddle factors per stage ---------- */\n    std::vector<std::vector<std::complex<double>>> twiddles(stages);\n    for (std::size_t s = 0; s < stages; ++s) {\n        const std::size_t m = 1ULL << (s + 1);          // size of sub\u2011transform\n        const std::size_t half = m >> 1;                // k\n        const double angle = -2.0 * M_PI / m;           // negative sign for forward FFT\n        std::vector<std::complex<double>> w(half);\n        std::complex<double> w_curr(1.0, 0.0);\n        const std::complex<double> w_step(std::cos(angle), std::sin(angle));\n        for (std::size_t j = 0; j < half; ++j) {\n            w[j] = w_curr;\n            w_curr *= w_step;\n        }\n        twiddles[s] = std::move(w);\n    }\n\n    /* ---------- Butterfly operations (radix\u20112) ---------------- */\n    for (std::size_t s = 0; s < stages; ++s) {\n        const std::size_t m = 1ULL << (s + 1);\n        const std::size_t half = m >> 1;\n        const auto& w = twiddles[s];\n\n        /* Parallel over the 'l' groups, each group is independent. */\n        #pragma omp parallel for schedule(static)\n        for (std::size_t l = 0; l < half; ++l) {\n            for (std::size_t a = l; a < N; a += m) {\n                std::size_t b = a + half;\n                std::complex<double> t = x[a] - x[b];\n                x[a] += x[b];\n                x[b] = t * w[l];\n            }\n        }\n    }\n\n    /* ---------- Bit\u2011reversal permutation --------------------- */\n    /* Pre\u2011compute reverse of 32\u2011bit pattern for speed */\n    auto reverse32 = [](uint32_t v) {\n        v = ((v & 0x55555555U) << 1) | ((v & 0xaaaaaaaaU) >> 1);\n        v = ((v & 0x33333333U) << 2) | ((v & 0xccccccccU) >> 2);\n        v = ((v & 0x0f0f0f0fU) << 4) | ((v & 0xf0f0f0f0U) >> 4);\n        v = ((v & 0x00ff00ffU) << 8) | ((v & 0xff00ff00U) >> 8);\n        return (v >> 32 - stages) | (v << stages);\n    };\n\n    for (std::size_t i = 0; i < N; ++i) {\n        std::size_t j = reverse32(static_cast<uint32_t>(i));\n        if (j > i) std::swap(x[i], x[j]);\n    }\n\n    /* ---------- Conjugate the results ------------------------ */\n    for (std::size_t i = 0; i < N; ++i) x[i] = std::conj(x[i]);\n}\n```\n",
  "05_fft_inverse_fft": "```cpp\n#pragma once\n#include <cmath>\n#include <vector>\n#include <complex>\n#include <algorithm>\n#include <numeric>\n#include <cstdint>\n#include <omp.h>\n\n#ifndef M_PI\n#define M_PI 3.14159265358979323846\n#endif\n\n//====================================================================\n// In\u2011place iterative radix\u20112 Cooley\u2011Tukey FFT (forward)\n//====================================================================\ninline void fft_helper(std::vector<std::complex<double>>& x)\n{\n    const std::size_t N = x.size();\n    if (N <= 1) return;\n\n    // --- Bit\u2011reversal permutation -------------------------------------------------------\n    std::size_t m = std::size_t(std::log2(N));\n    // We can parallelise the permutation\n    #pragma omp parallel for schedule(static)\n    for (std::size_t i = 0; i < N; ++i) {\n        std::size_t j = i;\n        j = ((j & 0xaaaaaaaaaaaaaaaaULL) >> 1) |\n            ((j & 0x5555555555555555ULL) << 1);\n        j = ((j & 0xccccccccccccccccULL) >> 2) |\n            ((j & 0x3333333333333333ULL) << 2);\n        j = ((j & 0xf0f0f0f0f0f0f0f0ULL) >> 4) |\n            ((j & 0x0f0f0f0f0f0f0f0fULL) << 4);\n        j = ((j & 0xff00ff00ff00ff00ULL) >> 8) |\n            ((j & 0x00ff00ff00ff00ffULL) << 8);\n        j = ((j & 0xffff0000ffff0000ULL) >>16) |\n            ((j & 0x0000ffff0000ffffULL) <<16);\n        j = j >> (64 - m);              //!!! 64\u2011bit cast\n        if (j > i) std::swap(x[i], x[j]);\n    }\n\n    // --- Cooley\u2011Tukey butterfly --------------------------------------------------------\n    std::size_t step = 2;\n    for (std::size_t s = 1; s <= m; ++s, step <<= 1)\n    {\n        const std::size_t half = step >> 1;\n        const double theta = -M_PI / half;   // negative for forward FFT\n        const std::complex<double> wlen(std::cos(theta), std::sin(theta));\n\n        // parallel over taps within each stage\n        #pragma omp parallel for schedule(static)\n        for (std::size_t k = 0; k < half; ++k)\n        {\n            std::complex<double> w(1.0, 0.0);\n            for (std::size_t i = k; i < N; i += step)\n            {\n                const std::size_t j = i + half;\n                const std::complex<double> t = w * x[j];\n                x[j] = x[i] - t;\n                x[i] += t;\n            }\n            w *= wlen;     // advance twiddle\n        }\n    }\n}\n\n//====================================================================\n// Inverse FFT (in\u2011place) ------------------------------------------------\ninline void ifft(std::vector<std::complex<double>>& x)\n{\n    // Conjugate, forward FFT, conjugate again and scale.\n    std::transform(x.begin(), x.end(), x.begin(), [](std::complex<double> z){ return std::conj(z); });\n    fft_helper(x);\n    std::transform(x.begin(), x.end(), x.begin(), [](std::complex<double> z){ return std::conj(z); });\n\n    const double invN = 1.0 / static_cast<double>(x.size());\n    #pragma omp parallel for schedule(static)\n    for (std::size_t i = 0; i < x.size(); ++i) {\n        x[i] *= invN;\n    }\n}\n```",
  "11_geometry_convex_hull_perimeter": "```cpp\n#pragma once\n#include <vector>\n#include <algorithm>\n#include <cmath>\n#include <cstddef>\n#include <omp.h>\n\nstruct Point { double x, y; };\n\n// Compute the cross product (b-a) x (c-a)\nstatic inline double cross(const Point& a, const Point& b, const Point& c) {\n    return (b.x - a.x) * (c.y - a.y) - (b.y - a.y) * (c.x - a.x);\n}\n\n// Compute Euclidean distance between two points\nstatic inline double dist(const Point& p1, const Point& p2) {\n    double dx = p2.x - p1.x;\n    double dy = p2.y - p1.y;\n    return std::hypot(dx, dy);          // faster than sqrt(pow(...,2)+...)\n}\n\ndouble convexHullPerimeter(const std::vector<Point>& points) {\n    const std::size_t n = points.size();\n    if (n < 3) return 0.0;\n\n    // Copy and sort the points\n    std::vector<Point> pts = points;\n    std::sort(pts.begin(), pts.end(), [](const Point& a, const Point& b) {\n        return (a.x < b.x) || (a.x == b.x && a.y < b.y);\n    });\n\n    // Upper bound on hull size is 2n\n    std::vector<Point> hull(2 * n);\n    std::size_t k = 0;\n\n    // Build lower hull\n    for (std::size_t i = 0; i < n; ++i) {\n        while (k >= 2 && cross(hull[k - 2], hull[k - 1], pts[i]) <= 0) --k;\n        hull[k++] = pts[i];\n    }\n\n    // Build upper hull\n    for (std::size_t i = n - 1, t = k + 1; i > 0; --i) {\n        while (k >= t && cross(hull[k - 2], hull[k - 1], pts[i - 1]) <= 0) --k;\n        hull[k++] = pts[i - 1];\n    }\n\n    hull.resize(k - 1); // Remove duplicate of first point\n\n    // Compute perimeter in parallel\n    double perimeter = 0.0;\n    const std::size_t m = hull.size();\n    if (m > 0) {\n        #pragma omp parallel for reduction(+:perimeter) schedule(static)\n        for (std::size_t i = 0; i < m; ++i) {\n            const Point& a = hull[i];\n            const Point& b = hull[(i + 1) % m];\n            perimeter += dist(a, b);\n        }\n    }\n    return perimeter;\n}\n```",
  "10_geometry_convex_hull": "```cpp\n#pragma once\n#include <vector>\n#include <algorithm>\n#include <cstddef>\n\nstruct Point {\n    double x, y;\n};\n\n/*\n * Compute the convex hull of a set of points using the\n * Graham/Andrew monotone chain algorithm.\n *\n * @param points  Input points (may contain duplicates).\n * @param hull    Output vector containing the hull in CCW order.\n */\ninline void convexHull(std::vector<Point> const& points, std::vector<Point> &hull)\n{\n    const std::size_t n = points.size();\n    if (n < 3) {                       // a trivial hull\n        hull = points;\n        return;\n    }\n\n    // 1. Sort points lexicographically by (x, y).\n    std::vector<Point> sorted(points);\n    std::sort(sorted.begin(), sorted.end(),\n              [](const Point& a, const Point& b) noexcept {\n                  return (a.x < b.x) || (a.x == b.x && a.y < b.y);\n              });\n\n    // 2. Monotone chain\n    std::vector<Point> buf;            // reuse buffer for lower+upper hull\n    buf.reserve(2 * n);\n\n    // Cross product of OA \u00d7 OB (signed area).\n    const auto cross = [](const Point& o, const Point& a, const Point& b) noexcept {\n        return (a.x - o.x) * (b.y - o.y) - (a.y - o.y) * (b.x - o.x);\n    };\n\n    // Lower hull\n    for (std::size_t i = 0; i < n; ++i) {\n        while (buf.size() >= 2 &&\n               cross(buf[buf.size() - 2], buf.back(), sorted[i]) <= 0)\n            buf.pop_back();\n        buf.push_back(sorted[i]);\n    }\n\n    // Upper hull\n    std::size_t t = buf.size() + 1;    // starting index for upper part\n    for (std::size_t i = n; i-- > 1;) {   // iterate n-1 \u2026 1\n        while (buf.size() >= t &&\n               cross(buf[buf.size() - 2], buf.back(), sorted[i - 1]) <= 0)\n            buf.pop_back();\n        buf.push_back(sorted[i - 1]);\n    }\n\n    // Remove the last point because it duplicates the first of the lower hull\n    if (!buf.empty())\n        buf.pop_back();\n\n    hull = std::move(buf);\n}\n```",
  "03_dense_la_axpy": "```cpp\n#pragma once\n#include <vector>\n#include <omp.h>\n\n/* Compute z = alpha*x + y where x and y are vectors.\n   The function assumes that x, y, and z all have the same size.\n   It performs the operation in parallel using OpenMP. */\ninline void axpy(double alpha,\n                 std::vector<double> const& x,\n                 std::vector<double> const& y,\n                 std::vector<double> &z) {\n    // Parallelized loop\n    #pragma omp parallel for schedule(static)\n    for (size_t i = 0; i < x.size(); ++i) {\n        z[i] = alpha * x[i] + y[i];\n    }\n}\n```",
  "08_fft_split_fft": "```cpp\n#pragma once\n#include <vector>\n#include <cmath>\n#include <complex>\n#include <cstdint>\n#include <algorithm>\n#include <omp.h>\n\n/*\n   Iterative radix\u20112 Cooley\u2013Tukey FFT.\n   The algorithm follows exactly the sequence of operations\n   of the reference implementation, so the result is identical.\n   Parallelization is applied to the butterfly stage, which\n   is thread\u2011safe because butterflies for different `l`\n   operate on disjoint index ranges.\n*/\nvoid fft(const std::vector<std::complex<double>> &x,\n         std::vector<double> &r,\n         std::vector<double> &i)\n{\n    const std::size_t N = x.size();\n    r.resize(N);\n    i.resize(N);\n\n    /* --- Copy input to local buffer --- */\n    std::vector<std::complex<double>> a(x);\n\n    /* --- Pre\u2011computed bit\u2011reverse indices --- */\n    const unsigned int m = static_cast<unsigned int>(std::log2(N));\n    std::vector<std::size_t> rev(N);\n    for (std::size_t n = 0; n < N; ++n) {\n        std::size_t b = n;\n        b = (((b & 0xaaaaaaaaULL) >> 1) | ((b & 0x55555555ULL) << 1));\n        b = (((b & 0xccccccccULL) >> 2) | ((b & 0x33333333ULL) << 2));\n        b = (((b & 0xf0f0f0f0ULL) >> 4) | ((b & 0x0f0f0f0fULL) << 4));\n        b = (((b & 0xff00ff00ULL) >> 8) | ((b & 0x00ff00ffULL) << 8));\n        b = ((b >> 16) | (b << 16)) >> (32 - m);\n        rev[n] = b;\n        if (b > n) std::swap(a[n], a[b]);\n    }\n\n    /* --- In\u2011place radix\u20112 FFT --- */\n    std::size_t k = N;           // current sub\u2011transform size\n    double  theta = M_PI / static_cast<double>(N);  // -2\u03c0/N in exponent\n    std::complex<double> phi = std::complex<double>(cos(theta), -sin(theta)); // primitive Nth root of unity\n\n    while (k > 1) {\n        std::size_t n = k;\n        k >>= 1;\n        phi *= phi;                                   // phi = phi^(2)\n\n        /* Pre\u2011compute twiddle factors for this stage */\n        std::vector<std::complex<double>> twiddles(k);\n        twiddles[0] = std::complex<double>(1.0, 0.0);\n        for (std::size_t j = 1; j < k; ++j)\n            twiddles[j] = twiddles[j-1] * phi;        // phi^j\n\n        /* Butterfly loop \u2013 parallelised over `l` */\n#pragma omp parallel for schedule(static)\n        for (std::size_t l = 0; l < k; ++l) {\n            std::complex<double> T = twiddles[l];\n            for (std::size_t aIdx = l; aIdx < N; aIdx += n) {\n                std::size_t bIdx = aIdx + k;\n                std::complex<double> t = a[aIdx] - a[bIdx];\n                a[aIdx] += a[bIdx];\n                a[bIdx] = t * T;\n            }\n        }\n    }\n\n    /* --- Store real and imaginary parts --- */\n    for (std::size_t j = 0; j < N; ++j) {\n        r[j] = a[j].real();\n        i[j] = a[j].imag();\n    }\n}\n```",
  "53_stencil_2d_jacobi_5-point_stencil": "```cpp\n#pragma once\n#include <vector>\n#include <cstddef>\n#include <omp.h>\n\n/*\n * Perform one iteration of a 5\u2011point 2\u2011D Jacobi stencil on an\n * NxN grid stored in row\u2011major order.\n *\n * The function is identical to the original but has been heavily\n * optimised for cache friendliness, reduced branching and\n * parallelised with OpenMP.\n */\ninline void jacobi2D(std::vector<double> const& input,\n                     std::vector<double> &output,\n                     const std::size_t N)\n{\n    const double inv5 = 1.0 / 5.0;                     // pre\u2011compute division\n    const std::size_t Nminus1 = N - 1;\n\n    // Parallelise the two nested loops with a static schedule to\n    // minimise load imbalance.  `collapse(2)` allows the compiler to\n    // fuse the i\u2011 and j\u2011loops for better vectorisation.\n    #pragma omp parallel for collapse(2) schedule(static)\n    for (std::size_t i = 0; i < N; ++i) {\n        const std::size_t row_idx = i * N;\n        for (std::size_t j = 0; j < N; ++j) {\n            double sum = input[row_idx + j];          // centre element\n\n            // North\n            if (i > 0) {\n                sum += input[(i - 1) * N + j];\n            }\n            // South\n            if (i < Nminus1) {\n                sum += input[(i + 1) * N + j];\n            }\n            // West\n            if (j > 0) {\n                sum += input[row_idx + (j - 1)];\n            }\n            // East\n            if (j < Nminus1) {\n                sum += input[row_idx + (j + 1)];\n            }\n\n            output[row_idx + j] = sum * inv5;\n        }\n    }\n}\n```",
  "50_stencil_xor_kernel": "```cpp\n#pragma once\n#include <vector>\n#include <cstddef>\n#include <omp.h>\n\n/*\n * Each cell becomes 1 if it has exactly one neighboring cell that is 1;\n * otherwise it becomes 0. Neighbors are the up, down, left and right cells.\n *\n * The input and output grids are stored in a flat vector of size N*N\n * in row-major order.\n */\ninline void cellsXOR(std::vector<int> const& input, std::vector<int> &output, size_t N)\n{\n    // Avoid reloading N\n    const size_t stride = N;\n    const int *inp = input.data();\n    int *out = output.data();\n\n    // Parallelize the outer loop first; the inner loop can be run by the same thread\n    #pragma omp parallel for schedule(static)\n    for (size_t i = 0; i < N; ++i)\n    {\n        const size_t idx_top    = (i > 0)     ? (i-1) * stride : SIZE_MAX;\n        const size_t idx_bottom = (i + 1 < N) ? (i+1) * stride : SIZE_MAX;\n        const size_t row_start  = i * stride;\n        const size_t row_end    = row_start + stride;\n\n        for (size_t j = 0; j < stride; ++j)\n        {\n            int count = 0;\n\n            /* up */\n            if (idx_top != SIZE_MAX && inp[idx_top + j] == 1) ++count;\n\n            /* down */\n            if (idx_bottom != SIZE_MAX && inp[idx_bottom + j] == 1) ++count;\n\n            /* left */\n            if (j > 0 && inp[row_start + j - 1] == 1) ++count;\n\n            /* right */\n            if (j + 1 < stride && inp[row_start + j + 1] == 1) ++count;\n\n            out[row_start + j] = (count == 1) ? 1 : 0;\n        }\n    }\n}\n```\n\n**Key optimisations**\n\n1. **OpenMP parallelization** \u2013 the outer loop over rows is parallelised with `#pragma omp parallel for`.  \n2. **Reduced bound checks** \u2013 pre\u2011compute row indices (`idx_top`, `idx_bottom`) so that the checks for first/last row are performed once per row.  \n3. **Pointer arithmetic** \u2013 direct index calculations are used instead of repeated multiplications.  \n4. **`inline`** \u2013 suggests to the compiler that the function is small and may be expanded for extra speed.  \n\nThe logic and semantics remain identical to the original implementation.",
  "51_stencil_edge_kernel": "```cpp\n#pragma once\n#include <vector>\n#include <cstddef>\n#include <algorithm>\n#include <omp.h>\n\nstatic constexpr int edgeKernel[3][3] = {\n    { -1, -1, -1 },\n    { -1,  8, -1 },\n    { -1, -1, -1 }\n};\n\nvoid convolveKernel(std::vector<int> const &imageIn, std::vector<int> &imageOut, size_t N)\n{\n    // Ensure output size matches input\n    if (imageOut.size() != N * N) {\n        imageOut.resize(N * N);\n    }\n\n    // Parallelise outer loop with OpenMP\n    #pragma omp parallel for schedule(static)\n    for (long long i = 0; i < static_cast<long long>(N); ++i) {\n        size_t rowBase = i * N;\n        for (size_t j = 0; j < N; ++j) {\n            int sum = 0;\n            // The kernel is 3x3; unroll the loops manually for speed\n            // (k, l) = (-1, -1)\n            if (i && j) {\n                sum += imageIn[(rowBase - N) + (j - 1)] * edgeKernel[0][0];\n            }\n            // (k, l) = (-1, 0)\n            if (i) {\n                sum += imageIn[(rowBase - N) + j] * edgeKernel[0][1];\n            }\n            // (k, l) = (-1, 1)\n            if (i && j + 1 < N) {\n                sum += imageIn[(rowBase - N) + (j + 1)] * edgeKernel[0][2];\n            }\n            // (k, l) = (0, -1)\n            if (j) {\n                sum += imageIn[rowBase + (j - 1)] * edgeKernel[1][0];\n            }\n            // (k, l) = (0, 0)\n            sum += imageIn[rowBase + j] * edgeKernel[1][1];\n            // (k, l) = (0, 1)\n            if (j + 1 < N) {\n                sum += imageIn[rowBase + (j + 1)] * edgeKernel[1][2];\n            }\n            // (k, l) = (1, -1)\n            if (i + 1 < N && j) {\n                sum += imageIn[(rowBase + N) + (j - 1)] * edgeKernel[2][0];\n            }\n            // (k, l) = (1, 0)\n            if (i + 1 < N) {\n                sum += imageIn[(rowBase + N) + j] * edgeKernel[2][1];\n            }\n            // (k, l) = (1, 1)\n            if (i + 1 < N && j + 1 < N) {\n                sum += imageIn[(rowBase + N) + (j + 1)] * edgeKernel[2][2];\n            }\n\n            // Clip to [0, 255]\n            imageOut[rowBase + j] = static_cast<int>(std::clamp(sum, 0, 255));\n        }\n    }\n}\n```",
  "54_stencil_game_of_life": "```cpp\n#pragma once\n#include <vector>\n#include <omp.h>\n\ninline void gameOfLife(std::vector<int> const& input,\n                       std::vector<int> &output, size_t N)\n{\n    // Parallelise outer loop across threads; OMP will handle cache\u2011line\n    // conflicts because each thread writes to distinct rows.\n    #pragma omp parallel for schedule(static)\n    for (size_t i = 0; i < N; ++i)\n    {\n        const size_t idx = i * N;\n        const size_t prevIdx = (i > 0)      ? (i - 1) * N : 0;\n        const size_t nextIdx = (i < N-1)    ? (i + 1) * N : 0;\n        const bool hasPrev = i > 0;\n        const bool hasNext = i < N-1;\n\n        for (size_t j = 0; j < N; ++j)\n        {\n            int sum = 0;\n\n            // vertical neighbours\n            if (hasPrev) sum += input[prevIdx + j];\n            if (hasNext) sum += input[nextIdx + j];\n\n            // horizontal neighbours\n            if (j > 0)  sum += input[idx + j - 1];\n            if (j < N-1) sum += input[idx + j + 1];\n\n            // diagonal neighbours\n            if (hasPrev && j > 0)       sum += input[prevIdx + j - 1];\n            if (hasPrev && j < N-1)     sum += input[prevIdx + j + 1];\n            if (hasNext && j > 0)       sum += input[nextIdx + j - 1];\n            if (hasNext && j < N-1)     sum += input[nextIdx + j + 1];\n\n            const int alive = input[idx + j];\n            int out = 0;\n            if (alive)\n            {\n                out = (sum == 2 || sum == 3) ? 1 : 0;\n            }\n            else\n            {\n                out = (sum == 3) ? 1 : 0;\n            }\n            output[idx + j] = out;\n        }\n    }\n}\n```",
  "04_dense_la_gemv": "```cpp\n#pragma once\n#include <vector>\n#include <cstddef>\n#include <omp.h>\n\n/*\n * Multiply the matrix A by the vector x. Store the results in the vector y.\n * A is an MxN matrix stored in row\u2011major, x has N elements, and y has M elements.\n *\n * The implementation is fully threaded with OpenMP and uses pointer arithmetic\n * to reduce indexing overhead.  Loop\u2011unrolling of 4 is applied for further\n * cache friendliness.  The behaviour is identical to the reference\n * implementation.\n */\n\ninline void gemv(const std::vector<double> &A,\n                 const std::vector<double> &x,\n                 std::vector<double> &y,\n                 size_t M, size_t N) noexcept\n{\n    /* Quick exit for degenerate sizes \u2013 keep semantics unchanged */\n    if (M == 0 || N == 0) return;\n\n    /* Parallel over the rows of the matrix.  Each thread processes a\n     * contiguous block of rows, which keeps the inner loop cache\u2011friendly. */\n    #pragma omp parallel for schedule(static)\n    for (long i = 0; i < static_cast<long>(M); ++i) {\n        const double *rowA = &A[i * N];          // pointer to the start of the i\u2011th row\n        const double *vecX = x.data();           // pointer to the start of x\n        double sum = 0.0;\n        \n        /* Inner loop unrolled by 4 to reduce loop\u2011control overhead. */\n        size_t j = 0;\n        const size_t limit = N - (N % 4);\n        for (; j < limit; j += 4) {\n            sum += rowA[j]   * vecX[j];\n            sum += rowA[j+1] * vecX[j+1];\n            sum += rowA[j+2] * vecX[j+2];\n            sum += rowA[j+3] * vecX[j+3];\n        }\n        /* Handle any remaining columns */\n        for (; j < N; ++j) {\n            sum += rowA[j] * vecX[j];\n        }\n        y[i] = sum;\n    }\n}\n```",
  "02_dense_la_gemm": "```cpp\n#pragma once\n#include <vector>\n#include <cstddef>\n#include <omp.h>\n\n/*\n * Multiply the matrix A (MxK) by the matrix B (KxN) and accumulate\n * the result into C (MxN). The matrices are stored in row\u2011major order.\n *\n * This implementation uses OpenMP for parallelism and a 2\u2011level tiling\n * scheme that fits tile rows and columns into cache.  The tiling\n * dimensions are chosen empirically for typical L1/L2 cache sizes\n * on mainstream x86\u201164 CPUs.\n *\n * Exact functionality matches the reference algorithm.\n */\ninline void gemm(const std::vector<double> &A,\n                 const std::vector<double> &B,\n                 std::vector<double> &C,\n                 std::size_t M, std::size_t K, std::size_t N)\n{\n    constexpr std::size_t TILE_M = 64;  // rows per tile\n    constexpr std::size_t TILE_K = 64;  // inner dimension per tile\n    constexpr std::size_t TILE_N = 64;  // columns per tile\n\n    // Parallel over outer M\u2011tiling loops\n    #pragma omp parallel for collapse(2) schedule(static)\n    for (std::size_t ii = 0; ii < M; ii += TILE_M) {\n        for (std::size_t kk = 0; kk < K; kk += TILE_K) {\n            // Compute bounds of current tile\n            std::size_t i_max = std::min(ii + TILE_M, M);\n            std::size_t k_max = std::min(kk + TILE_K, K);\n\n            for (std::size_t jj = 0; jj < N; jj += TILE_N) {\n                std::size_t j_max = std::min(jj + TILE_N, N);\n\n                for (std::size_t i = ii; i < i_max; ++i) {\n                    double *c_row = &C[i * N + jj];\n                    const double *a_row = &A[i * K + kk];\n                    for (std::size_t k = kk; k < k_max; ++k) {\n                        double av = a_row[k - kk];\n                        const double *b_row = &B[k * N + jj];\n                        for (std::size_t j = jj; j < j_max; ++j) {\n                            c_row[j - jj] += av * b_row[j - jj];\n                        }\n                    }\n                }\n            }\n        }\n    }\n}\n```\n",
  "01_dense_la_solve": "```cpp\n#pragma once\n#include <vector>\n#include <omp.h>\n\ninline void solveLinearSystem(std::vector<double> const& A,\n                              std::vector<double> const& b,\n                              std::vector<double> &x,\n                              size_t N) {\n    // Work on copies to avoid modifying inputs\n    std::vector<double> a = A;\n    std::vector<double> bb = b;\n\n    double *Ap = a.data();\n    double *bp = bb.data();\n\n    // Gaussian elimination with OpenMP parallelisation on rows\n    for (size_t i = 0; i + 1 < N; ++i) {\n        const double pivot = Ap[i * N + i];\n        if (pivot == 0.0) return;          // singular\n\n        #pragma omp parallel for schedule(static)\n        for (long long j = i + 1; j < (long long)N; ++j) {\n            const double factor = Ap[j * N + i] / pivot;\n            double *rowi = Ap + i * N;\n            double *rowj = Ap + j * N;\n            for (size_t k = i; k < N; ++k) {\n                rowj[k] -= factor * rowi[k];\n            }\n            bp[j] -= factor * bp[i];\n        }\n    }\n\n    // Back\u2011substitution (serial)\n    for (int i = (int)N - 1; i >= 0; --i) {\n        double sum = 0.0;\n        double *rowi = Ap + i * N;\n        for (size_t j = i + 1; j < N; ++j) {\n            sum += rowi[j] * x[j];\n        }\n        x[i] = (bp[i] - sum) / rowi[i];\n    }\n}\n```\n",
  "41_sort_k-th_smallest_element": "```cpp\n#pragma once\n#include <vector>\n#include <algorithm>\n#include <stdexcept>\n\n/* Find the k-th smallest element of the vector `x`.\n   The function throws if `k` is out of range.\n   The original algorithm performed a full sort; this version uses\n   `std::nth_element` to obtain the same result with O(n) average\n   complexity instead of O(n log n). */\nint findKthSmallest(const std::vector<int> &x, int k) {\n    if (k < 1 || k > static_cast<int>(x.size())) {\n        throw std::out_of_range(\"k is out of bounds\");\n    }\n\n    std::vector<int> temp = x;                     // make a mutable copy\n    std::nth_element(temp.begin(), temp.begin() + k - 1, temp.end());\n    // `nth_element` only guarantees the element at position `k-1` is the\n    // correct one, the rest of the vector is not sorted.\n    // To match the semantics of the original implementation which returned\n    // the exact value at index `k-1` after a full sort, we simply return\n    // that element.\n    return temp[k - 1];\n}\n```",
  "43_sort_sort_an_array_of_structs_by_key": "```cpp\n#pragma once\n#include <vector>\n#include <algorithm>\n#include <cstddef>\n#include <omp.h>\n\nstruct Result {\n    int startTime, duration;\n    float value;\n};\n\nstatic void merge(std::vector<Result> &tmp,\n                  const std::vector<Result> &src,\n                  std::size_t left, std::size_t mid, std::size_t right) {\n    std::size_t i = left, j = mid, k = left;\n    while (i < mid && j < right) {\n        if (src[i].startTime <= src[j].startTime)\n            tmp[k++] = src[i++];\n        else\n            tmp[k++] = src[j++];\n    }\n    while (i < mid) tmp[k++] = src[i++];\n    while (j < right) tmp[k++] = src[j++];\n}\n\nstatic void parallelMergeSort(std::vector<Result> &vec,\n                              std::vector<Result> &tmp,\n                              std::size_t left,\n                              std::size_t right) {\n    const std::size_t threshold = 1 << 13;   // ~8k elements\n    if (right - left <= threshold) {\n        std::sort(vec.begin() + left, vec.begin() + right,\n                  [](const Result &a, const Result &b) {\n                      return a.startTime < b.startTime;\n                  });\n        return;\n    }\n\n    std::size_t mid = left + (right - left) / 2;\n#pragma omp task shared(vec, tmp) if (right - left > 1 << 15)\n    parallelMergeSort(tmp, vec, left, mid);\n#pragma omp task shared(vec, tmp) if (right - left > 1 << 15)\n    parallelMergeSort(tmp, vec, mid, right);\n#pragma omp taskwait\n\n    merge(vec, tmp, left, mid, right);\n}\n\ninline void sortByStartTime(std::vector<Result> &results) {\n    if (results.size() <= 1) return;\n\n    std::vector<Result> tmp(results.size());\n    // Run the parallel merge\u2011sort inside a single OpenMP parallel region.\n#pragma omp parallel\n    {\n#pragma omp single nowait\n        parallelMergeSort(results, tmp, 0, results.size());\n    }\n}\n```",
  "40_sort_sort_an_array_of_complex_numbers_by_magnitude": "```cpp\n#pragma once\n#include <complex>\n#include <vector>\n#include <utility>\n#include <algorithm>\n\n/* Sort the vector x of complex numbers by their magnitude in ascending order.\n   The original implementation uses std::abs(a) in each comparison.\n   Here we avoid the expensive sqrt in std::abs by sorting on the squared\n   magnitudes (real^2 + imag^2). This preserves the exact sorting order\n   because sqrt is a strictly increasing function.\n*/\ninline void sortComplexByMagnitude(std::vector<std::complex<double>> &x) {\n    // Build a temporary vector of (magnitude squared, complex) pairs\n    std::vector<std::pair<double, std::complex<double>>> tmp;\n    tmp.reserve(x.size());\n    for (auto &c : x) {\n        double mag2 = std::norm(c);   // real^2 + imag^2\n        tmp.emplace_back(mag2, c);\n    }\n\n    // Sort the temporary vector by the first element (magnitude squared)\n    std::sort(tmp.begin(), tmp.end(),\n        [](const auto &a, const auto &b) { return a.first < b.first; });\n\n    // Write back the sorted complex numbers to the original vector\n    for (size_t i = 0; i < x.size(); ++i) {\n        x[i] = tmp[i].second;\n    }\n}\n```",
  "44_sort_sort_non-zero_elements": "```cpp\n#pragma once\n#include <algorithm>\n#include <vector>\n#include <cstddef>\n\nvoid sortIgnoreZero(std::vector<int> &x) {\n    // First pass: count non\u2011zero elements to reserve exactly the needed space\n    std::size_t cnt = 0;\n    for (int v : x) if (v != 0) ++cnt;\n\n    // Second pass: copy non\u2011zeros into a pre\u2011allocated vector\n    std::vector<int> nonZero;\n    nonZero.reserve(cnt);\n    for (int v : x) if (v != 0) nonZero.push_back(v);\n\n    // Sort the non\u2011zero values\n    std::sort(nonZero.begin(), nonZero.end());\n\n    // Third pass: overwrite the original vector with the sorted non\u2011zeros\n    std::size_t idx = 0;\n    for (std::size_t i = 0; i < x.size(); ++i) {\n        if (x[i] != 0) {\n            x[i] = nonZero[idx++];\n        }\n    }\n}\n```",
  "42_sort_sorted_ranks": "```cpp\n#pragma once\n#include <algorithm>\n#include <numeric>\n#include <vector>\n\n/*\n * For each value in the vector `x` compute its index in the sorted vector.\n * The rank of an element is its position in the sorted order, with ties\n * resolved by the original index (i.e. stable sorting).\n * Example:\n *   x = {3.1, 2.8, 9.1, 0.4, 3.14}\n *   output = {2, 1, 4, 0, 3}\n */\ninline void ranks(const std::vector<float>& x, std::vector<size_t>& ranks_) {\n    const size_t n = x.size();\n    ranks_.resize(n);\n\n    // Build a vector of (value, original_index) pairs.\n    std::vector<std::pair<float, size_t>> v;\n    v.reserve(n);\n    for (size_t i = 0; i < n; ++i) {\n        v.emplace_back(x[i], i);\n    }\n\n    // Stable sort by value, deferring to original index for ties.\n    std::stable_sort(v.begin(), v.end(),\n        [](const auto& a, const auto& b) { return a.first < b.first; });\n\n    // Assign ranks.\n    for (size_t rank = 0; rank < n; ++rank) {\n        ranks_[v[rank].second] = rank;\n    }\n}\n```",
  "37_search_find_the_closest_number_to_pi": "```cpp\n#pragma once\n#include <vector>\n#include <cmath>\n#include <cstddef>\n#include <algorithm>\n#include <limits>\n#include <omp.h>\n\n/* Return the index of the value in the vector x that is closest to the math constant PI.\n   Use M_PI for the value of PI.\n   Example:\n   input: [9.18, 3.05, 7.24, 11.3, -166.49, 2.1]\n   output: 1\n*/\nsize_t findClosestToPi(std::vector<double> const& x) {\n    // Nothing to do for empty input; however the original implementation expects\n    // at least one element, so we simply return 0 in that case.\n    if (x.empty()) return 0;\n\n    size_t best_index = 0;\n    double best_diff  = std::abs(x[0] - M_PI);\n\n    // The loop is embarrassingly parallel \u2013 each iteration is independent.\n    // We use an openmp parallel for with a reduction on best_diff and a second\n    // reduction on best_index that tracks the smallest index when diff ties.\n    #pragma omp parallel for reduction(+:best_index) reduction(min:best_diff)\n    for (size_t i = 1; i < x.size(); ++i) {\n        double diff = std::abs(x[i] - M_PI);\n        // If this diff is strictly smaller, update both best_diff and best_index.\n        // The reduction on best_diff ensures the smallest diff is retained.\n        // The reduction on best_index with the 'min' operator ensures that\n        // when diff ties, the smaller index is chosen.\n        if (diff < best_diff) {\n            best_diff = diff;\n            best_index = i;\n        }\n    }\n\n    return best_index;\n}\n```",
  "36_search_check_if_array_contains_value": "```cpp\n#pragma once\n#include <vector>\n\nbool contains(const std::vector<int>& x, int target) {\n    const size_t n = x.size();\n    const int* data = x.data();\n    for (size_t i = 0; i < n; ++i) {\n        if (data[i] == target) {\n            return true;\n        }\n    }\n    return false;\n}\n```\n",
  "38_search_find_the_first_even_number": "```cpp\n#pragma once\n#include <vector>\n\nstatic inline size_t findFirstEven(std::vector<int> const& x) {\n    const size_t n = x.size();\n    const int* const data = x.data();\n    for (size_t i = 0; i < n; ++i) {\n        // Bitwise AND is a tiny bit faster than modulo for 2.\n        if ((data[i] & 1) == 0) {\n            return i;\n        }\n    }\n    return n;   // no even number found\n}\n```",
  "35_search_search_for_last_struct_by_key": "```cpp\n#pragma once\n#include <vector>\n\nstruct Book {\n   std::string title;\n   int pages;\n};\n\n// Find the last book with fewer than 100 pages.\n// Returns the index if found, otherwise returns books.size().\nsize_t findLastShortBook(std::vector<Book> const& books) {\n    // Fetch size once to avoid repeated bounds checks.\n    const size_t n = books.size();\n    // Use a reverse loop that exits safely when i becomes negative.\n    // size_t is unsigned, so we stop when it wraps around.\n    for (size_t i = n; i-- > 0; ) {\n        if (books[i].pages < 100)\n            return i;\n    }\n    return n;\n}\n```",
  "39_search_xor_contains": "```cpp\n#pragma once\n#include <vector>\n#include <algorithm>\n\n/*\n * Return true if `val` is only in one of vectors x or y.\n * Return false if it is in both or neither.\n *\n * The implementation uses a quick linear search with minimal\n * branch mispredictions. It is fully equivalent in behaviour\n * to the original function, but arranged to avoid any\n * unnecessary library calls and with small performance\n * tweaks for modern GCC.\n */\ninline bool xorContains(std::vector<int> const& x,\n                        std::vector<int> const& y,\n                        int val) noexcept\n{\n    // Use raw pointers for a very tight loop.\n    const int* px = x.data();\n    const int* py = y.data();\n    const std::size_t nx = x.size();\n    const std::size_t ny = y.size();\n\n    bool foundInX = false;\n    for (std::size_t i = 0; i < nx; ++i) {\n        if (px[i] == val) {\n            foundInX = true;\n            break;              // exit early\n        }\n    }\n\n    bool foundInY = false;\n    for (std::size_t i = 0; i < ny; ++i) {\n        if (py[i] == val) {\n            foundInY = true;\n            break;              // exit early\n        }\n    }\n\n    return foundInX ^ foundInY;    // XOR: true only if exactly one is true\n}\n```",
  "00_dense_la_lu_decomp": "```cpp\n#pragma once\n#include <vector>\n#include <omp.h>\n\n/*\n * In-place LU factorization of an NxN matrix stored in row\u2011major order.\n * The algorithm follows the Doolittle formulation.\n */\nvoid luFactorize(std::vector<double> &A, size_t N) {\n    double *data = A.data();                 // pointer for faster access\n    for (size_t k = 0; k < N; ++k) {\n        double pivot = data[k * N + k];\n        // Handles singular matrices \u2013 the original code would divide by zero.\n        if (pivot == 0.0) continue;\n\n#pragma omp parallel for schedule(static)\n        for (size_t i = k + 1; i < N; ++i) {\n            double factor = data[i * N + k] / pivot;\n            data[i * N + k] = factor;\n\n            double  *rowK = data + k * N;    // cache the k\u2011th row\n            double  *rowI = data + i * N;    // cache the i\u2011th row\n            for (size_t j = k + 1; j < N; ++j) {\n                rowI[j] -= factor * rowK[j];\n            }\n        }\n    }\n}\n```\nThis implementation parallelises the inner column updates for each pivot row `k` with OpenMP, preserving the exact in\u2011place behavior while exploiting the 8\u2011thread CPU."
}