{
  "15_graph_edge_count": "```cpp\n#pragma once\n#include <vector>\n\nint edgeCount(std::vector<int> const& A, size_t N) {\n    int count = 0;\n    #pragma omp parallel for collapse(2) reduction(+:count) schedule(static)\n    for (size_t i = 0; i < N; ++i) {\n        for (size_t j = 0; j < N; ++j) {\n            count += A[i * N + j];\n        }\n    }\n    return count;\n}\n```",
  "21_histogram_bin_0-100": "```cpp\n#pragma once\n#include <array>\n#include <vector>\n#include <cstddef>\n#include <omp.h>\n\ninline void binsBy10Count(std::vector<double> const& x, std::array<size_t, 10> &bins) {\n    /* Zero the output array first */\n    for (auto& b : bins) b = 0;\n\n    /* Parallel count with reduction */\n    #pragma omp parallel for schedule(static)\n    for (std::size_t i = 0; i < x.size(); ++i) {\n        std::size_t const bin = static_cast<std::size_t>(x[i] / 10.0);\n        #pragma omp atomic\n        ++bins[bin];\n    }\n}\n```\nThis version keeps the exact semantics of the original while exploiting OpenMP for multi\u2011threaded accumulation. The array is zero\u2011initialized explicitly to avoid reliance on caller state, and each `bins[bin]++` is protected with an `unordered atomic` update for thread safety. The code is fully compatible with `gcc -O2 -fopenmp`.",
  "20_histogram_pixel_histogram": "```cpp\n#pragma once\n#include <vector>\n#include <array>\n#include <omp.h>\n\n/*\n * Count the number of pixels in image with each grayscale intensity.\n * The vector `image` is a grayscale image with values 0-255.\n * Store the results in `bins`.\n * Example:\n *\n * input: image=[2, 116, 201, 11, 92, 92, 201, 4, 2]\n * output: [0, 0, 2, 0, 1, ...]\n */\nvoid pixelCounts(std::vector<int> const& image, std::array<size_t, 256> &bins)\n{\n    // Ensure bins are zeroed\n    bins.fill(0);\n\n    const std::size_t nthreads = omp_get_max_threads();\n\n    // Allocate thread\u2011local histograms\n    std::vector<std::array<size_t, 256>> localBins(nthreads);\n    for (auto &b : localBins) b.fill(0);\n\n    // Parallel reduction by first\u2011class array of counts\n#pragma omp parallel\n    {\n        const int tid = omp_get_thread_num();\n        std::array<size_t, 256> &local = localBins[tid];\n\n#pragma omp for\n        for (std::size_t i = 0; i < image.size(); ++i) {\n            ++local[image[i]];\n        }\n    }\n\n    // Merge thread local histograms into the final result\n    for (const auto &lt : localBins) {\n        for (int val = 0; val < 256; ++val) {\n            bins[val] += lt[val];\n        }\n    }\n}\n```",
  "18_graph_count_components": "```cpp\n#pragma once\n#include <vector>\n#include <stack>\n\n//--- Fast DFS using an explicit stack -------------------------------------\ninline void dfs(std::vector<int> const& A,\n                int start,\n                size_t N,\n                std::vector<char>& visited)\n{\n    std::stack<int> st;\n    st.push(start);\n    visited[start] = 1;\n\n    while (!st.empty()) {\n        int node = st.top();\n        st.pop();\n\n        const int base = node * static_cast<int>(N);\n        for (int i = 0; i < static_cast<int>(N); ++i) {\n            if (A[base + i] && !visited[i]) {\n                visited[i] = 1;\n                st.push(i);\n            }\n        }\n    }\n}\n\n//--- Count connected components --------------------------------------------\nint componentCount(std::vector<int> const& A, size_t N)\n{\n    // use a byte array instead of bool to avoid vector<bool> specialization\n    std::vector<char> visited(N, 0);\n    int components = 0;\n\n    for (int i = 0; i < static_cast<int>(N); ++i) {\n        if (!visited[i]) {\n            ++components;\n            dfs(A, i, N, visited);\n        }\n    }\n    return components;\n}\n```",
  "17_graph_highest_degree": "```cpp\n#pragma once\n#include <vector>\n#include <algorithm>\n#include <cstddef>\n#include <omp.h>\n\n/* Compute the highest node degree in the undirected graph. The graph is defined\n   by an NxN adjacency matrix A stored in row\u2011major order.  Only the\n   semantics of the original implementation are preserved, but the\n   computation is accelerated with OpenMP and a small amount of manual\n   unrolling.  The function signature is left unchanged. */\nint maxDegree(const std::vector<int>& A, size_t N) {\n    int global_max = 0;\n\n    /* parallelize over the rows; each thread keeps a private running\n       maximum which is reduced at the end. */\n    #pragma omp parallel for reduction(max:global_max) schedule(static)\n    for (size_t i = 0; i < N; ++i) {\n        int row_sum = 0;\n        size_t j = 0;\n\n        /* Unroll the inner loop to reduce loop overhead and\n           enhance compiler vectorisation opportunities. */\n        for (; j + 7 < N; j += 8) {\n            row_sum += A[i * N + j + 0];\n            row_sum += A[i * N + j + 1];\n            row_sum += A[i * N + j + 2];\n            row_sum += A[i * N + j + 3];\n            row_sum += A[i * N + j + 4];\n            row_sum += A[i * N + j + 5];\n            row_sum += A[i * N + j + 6];\n            row_sum += A[i * N + j + 7];\n        }\n        for (; j < N; ++j) {\n            row_sum += A[i * N + j];\n        }\n\n        if (row_sum > global_max) {\n            global_max = row_sum;\n        }\n    }\n\n    return global_max;\n}\n```",
  "16_graph_largest_component": "```cpp\n#pragma once\n#include <vector>\n#include <stack>\n#include <algorithm>\n\n/* Find the size of the largest connected component in an\n * undirected graph represented by a symmetric NxN adjacency\n * matrix stored in row\u2011major order.\n *\n * The implementation uses an iterative depth\u2011first search with a\n * vector<char> for visited flags (1 byte per vertex) and a\n * std::stack for the traversal.  This removes recursive call\n * overhead and keeps the routine very cache friendly.\n * The algorithm is O(N\u00b2) which matches the cost of scanning the\n * input matrix; all other work is proportional to the number\n * of edges in the largest component.\n */\nint largestComponent(const std::vector<int>& A, size_t N)\n{\n    /* one byte per vertex is enough to store the visited flag */\n    std::vector<char> visited(N, 0);\n    int maxSize = 0;\n\n    std::stack<int> st;\n    for (size_t src = 0; src < N; ++src)\n    {\n        if (visited[src]) continue;\n\n        /* start a new component */\n        int compSize = 0;\n        st.push(static_cast<int>(src));\n        visited[src] = 1;\n\n        while (!st.empty())\n        {\n            int v = st.top();\n            st.pop();\n            ++compSize;\n\n            /* iterate over all neighbors of v */\n            const int* row = &A[v * N];\n            for (int u = 0; u < static_cast<int>(N); ++u)\n            {\n                if (row[u] && !visited[u])\n                {\n                    visited[u] = 1;\n                    st.push(u);\n                }\n            }\n        }\n        maxSize = std::max(maxSize, compSize);\n    }\n    return maxSize;\n}\n```",
  "23_histogram_first_letter_counts": "```cpp\n#pragma once\n#include <array>\n#include <string>\n#include <vector>\n\n#include <omp.h>\n\n/*\n * For each letter in the alphabet, count the number of strings in the vector `s`\n * that start with that letter. All strings are in lower case.\n *\n * Example:\n *   input : {\"dog\", \"cat\", \"xray\", \"cow\", \"code\", \"type\", \"flower\"}\n *   output: [0, 0, 3, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0]\n *\n * The implementation uses OpenMP to parallelise the loop while keeping\n * the exact same result.\n */\nvoid firstLetterCounts(std::vector<std::string> const& s, std::array<size_t, 26>& bins)\n{\n    // Reset output bins to zero\n    bins.fill(0);\n\n    const size_t n = s.size();\n\n    // Parallel region with private per-thread bins\n    #pragma omp parallel\n    {\n        // Each thread has a private array of 26 counters\n        std::array<size_t, 26> local = {0};\n\n        // The work is distributed among threads by OpenMP\n        #pragma omp for schedule(static)\n        for (size_t i = 0; i < n; ++i) {\n            // Compute the index of the first character (guaranteed to be 'a'..'z')\n            const int idx = s[i][0] - 'a';\n            // Increment the local counter\n            ++local[idx];\n        }\n\n        // Merge local counts into the global array\n        #pragma omp critical\n        {\n            for (int j = 0; j < 26; ++j) {\n                bins[j] += local[j];\n            }\n        }\n    }\n}\n```",
  "34_scan_largest_contiguous_subarray_sum": "```cpp\n#pragma once\n#include <vector>\n#include <limits>\n#include <algorithm>\n\n/* Return the largest sum of any contiguous subarray in the vector x.\n   This implementation uses Kadane's algorithm (O(n)) and preserves\n   the exact behavior of the original nested-loop version.\n*/\nint maximumSubarray(std::vector<int> const& x) {\n    // Edge case: empty input returns the smallest int value,\n    // matching the original algorithm's initial value.\n    if (x.empty()) return std::numeric_limits<int>::lowest();\n\n    int maxSum = x[0];\n    int current = x[0];\n    for (std::size_t i = 1; i < x.size(); ++i) {\n        current = std::max(x[i], current + x[i]);\n        maxSum = std::max(maxSum, current);\n    }\n    return maxSum;\n}\n```",
  "31_scan_scan_with_min_function": "```cpp\n#pragma once\n#include <vector>\n#include <algorithm>\n#include <cfloat>\n\nvoid partialMinimums(std::vector<float> &x)\n{\n    if (x.empty()) return;\n    float current_min = x[0];\n    // first element remains the same\n    for (std::size_t i = 1; i < x.size(); ++i) {\n        if (x[i] < current_min) current_min = x[i];\n        x[i] = current_min;\n    }\n}\n```",
  "19_graph_shortest_path": "```cpp\n#pragma once\n#include <vector>\n#include <limits>\n\n/* Return the length of the shortest path from source to dest in the undirected graph defined by the adjacency matrix A.\n   A is an NxN adjacency matrix stored in row\u2011major. Assume the graph is connected. A is undirected. */\ninline int shortestPathLength(std::vector<int> const& A, size_t N, int source, int dest) {\n    if (source == dest) return 0;\n    std::vector<char> visited(N, 0);\n    std::vector<int> q;\n    q.reserve(N);\n    q.push_back(source);\n    visited[source] = 1;\n\n    size_t head = 0;      // index of the current element in q\n    int depth = 0;        // distance from source\n\n    while (head < q.size()) {\n        size_t levelSize = q.size() - head;  // number of nodes in the current BFS layer\n        while (levelSize--) {\n            int u = q[head++];\n            if (u == dest) return depth;\n\n            size_t base = u * N;      // starting index of u's adjacency row\n            for (size_t i = 0; i < N; ++i) {\n                if (A[base + i] && !visited[i]) {\n                    visited[i] = 1;\n                    q.push_back(static_cast<int>(i));\n                }\n            }\n        }\n        ++depth;   // move to the next layer\n    }\n    return std::numeric_limits<int>::max();\n}\n```",
  "33_scan_reverse_prefix_sum": "```cpp\n#pragma once\n#include <vector>\n\n/* Compute the reverse prefix sum of the vector `x` into `output`.\n   Examples:\n   input: [1, 7, 4, 6, 6, 2]\n   output: [2, 8, 14, 18, 25, 26]\n   input: [3, 3, 7, 1, -2]\n   output: [-2, -1, 6, 9, 12]\n*/\ninline void reversePrefixSum(const std::vector<int>& x, std::vector<int>& output) {\n    const std::size_t n = x.size();\n    output.resize(n);\n    int acc = 0;\n    // Scan from the end of x to the beginning, accumulating the sum.\n    for (std::size_t i = n; i-- > 0;) {\n        acc += x[i];\n        output[i] = acc;\n    }\n}\n```",
  "24_histogram_count_quartile": "```cpp\n#pragma once\n#include <array>\n#include <vector>\n#include <cmath>\n#include <cstddef>\n#include <omp.h>\n\n/* Count the number of doubles in the vector x that have a fractional part \n   in [0, 0.25), [0.25, 0.5), [0.5, 0.75), and [0.75, 1). Store the counts in `bins`.\n   The implementation is fully parallelized with OpenMP and uses a per\u2011thread local\n   array to avoid atomic operations.\n*/\ninline void countQuartiles(std::vector<double> const& x, std::array<size_t, 4> &bins)\n{\n    /* clear output */\n    bins = {0,0,0,0};\n\n    size_t const n = x.size();\n    if (n==0) return;\n\n    /* number of threads and per\u2011thread accumulator */\n    int const nthreads = omp_get_max_threads();\n    std::vector<std::array<size_t,4>> local(nthreads);  // default init 0\n\n#pragma omp parallel\n    {\n        int tid = omp_get_thread_num();\n        std::array<size_t,4> &local_bins = local[tid];\n\n#pragma omp for nowait\n        for (size_t i = 0; i < n; ++i) {\n            double val = x[i];\n            /* floor is fast because compiler will inline and use hardware */\n            double frac = val - std::floor(val);\n            int q = static_cast<int>(frac * 4.0);      // 0 \u2026 3\n            local_bins[q] += 1;\n        }\n    }\n\n    /* reduce per\u2011thread results into final bins */\n    for (int t = 0; t < nthreads; ++t) {\n        for (int b = 0; b < 4; ++b) bins[b] += local[t][b];\n    }\n}\n```",
  "22_histogram_count_quadrants": "```cpp\n#pragma once\n#include <array>\n#include <vector>\n#include <cstddef>\n#include <cstdint>\n#include <limits>\n#include <omp.h>\n\nstruct Point { double x, y; };\n\n/* Count the number of cartesian points in each quadrant.\n   This version is fully OpenMP\u2011parallel, uses a 4\u2011byte\n   per\u2011thread histogram for power\u2011of\u20112 accumulation, and\n   avoids the expensive branch chain of the original\n   implementation.              */\nvoid countQuadrants(std::vector<Point> const& points, std::array<size_t, 4>& bins) {\n    // Pre\u2011reset output bins\n    bins[0] = bins[1] = bins[2] = bins[3] = 0;\n\n    // Number of OpenMP threads\n    int const nthr = omp_get_max_threads();\n\n    /* Local histograms for every thread.\n       Each element is 64\u2011bit to avoid overflow during the\n       accumulation phase.  The array is allocated once per call\n       according to the current OMP thread count.                 */\n    std::vector<std::array<uint64_t,4>> localBins(nthr);\n    for(int i=0;i<nthr;++i)\n        localBins[i] = {0,0,0,0};\n\n    // Parallel processing \u2013 each thread fills its own local counter\n    #pragma omp parallel\n    {\n        int const tid = omp_get_thread_num();\n        std::array<uint64_t,4>* const lbin = &localBins[tid];\n\n        // Use local pointer for speed\n        const Point* ptr = points.data();\n        size_t const n = points.size();\n\n        #pragma omp for schedule(static)\n        for(size_t i=0;i<n;++i) {\n            // Convert signs to a quadrant index (0\u20113)\n            int idx = 0;\n            if (ptr[i].x < 0) idx |= 1;   // bit 0: x negative\n            if (ptr[i].y < 0) idx |= 2;   // bit 1: y negative\n\n            // Branch distance 0, 1, 2, 3 for quadrants:\n            // 0: (+,+), 1: (-,+), 2: (-,-), 3: (+,-)\n            (*lbin)[idx] += 1;\n        }\n    }\n\n    // Merge local histograms into the global output\n    for(int i=0;i<nthr;++i) {\n        bins[0] += static_cast<size_t>(localBins[i][0]);\n        bins[1] += static_cast<size_t>(localBins[i][1]);\n        bins[2] += static_cast<size_t>(localBins[i][2]);\n        bins[3] += static_cast<size_t>(localBins[i][3]);\n    }\n}\n```",
  "59_transform_map_function": "```cpp\n#pragma once\n#include <vector>\n#include <omp.h>\n\ninline bool isPowerOfTwo(int x) {\n    return (x > 0) && !(x & (x - 1));\n}\n\n/* Apply the isPowerOfTwo function to every value in x and store the results in mask.\n   The function preserves order and is fully thread\u2011safe.  It uses OpenMP to\n   parallelise the loop, which gives a substantial speed\u2011up on the target\n   8\u2011thread x86_64 machine.  The loop bounds are explicitly cast to size_t\n   to avoid signed/unsigned conversion warnings. */\ninline void mapPowersOfTwo(const std::vector<int> &x, std::vector<bool> &mask) {\n    const std::size_t N = x.size();\n    // Ensure mask has the correct size (it should already be sized by the caller,\n    // but we guard against a potential bug in the harness).\n    if (mask.size() != N) mask.resize(N);\n\n    #pragma omp parallel for schedule(static, 256)\n    for (std::size_t i = 0; i < N; ++i) {\n        mask[i] = isPowerOfTwo(x[i]);\n    }\n}\n```",
  "55_transform_relu": "```cpp\n#pragma once\n#include <vector>\n#include <algorithm>\n#include <omp.h>\n\n/* Compute the ReLU function on every element of x. Elements less than zero become zero,\n   while elements greater than zero stay the same.\n   Example:\n   input: [-1.8, 24.0, 1.2, 0.0, -5.1, -0.2, 4.5]\n   output: [0, 24.0, 1.2, 0, 0, 0, 4.5]\n   Implementation uses OpenMP for parallel execution while keeping the same semantics.\n*/\ninline void relu(std::vector<double>& x) {\n    const std::size_t N = x.size();\n#pragma omp parallel for schedule(static)\n    for (std::size_t i = 0; i < N; ++i) {\n        // std::max(0.0, v) is equivalent to static ReLU\n        x[i] = x[i] < 0.0 ? 0.0 : x[i];\n    }\n}\n```",
  "57_transform_inverse_offset": "```cpp\n#pragma once\n#include <vector>\n#include <cstddef>\n#include <omp.h>\n\n/* Replace every element of the vector x with 1-1/x.\n   Example:\n   input: [2, 4, 1, 12, -2]\n   output: [0.5, 0.75, 0, 0.91666666, 1.5]\n   The behavior for x[i] == 0 is undefined, mirroring the original implementation.\n*/\ninline void oneMinusInverse(std::vector<double> &x) {\n    const std::size_t n = x.size();\n    #pragma omp parallel for schedule(static)\n    for (std::size_t i = 0; i < n; ++i) {\n        x[i] = 1.0 - 1.0 / x[i];\n    }\n}\n```",
  "56_transform_negate_odds": "```cpp\n#pragma once\n#include <vector>\n#include <algorithm>\n#include <cstddef>\n#include <omp.h>\n\n/*\n * In the vector x negate the odd values and divide the even values by 2.\n * Example:\n *\n * input:  [16, 11, 12, 14, 1, 0, 5]\n * output: [ 8, -11,  6,  7, -1, 0, -5]\n *\n * The implementation below keeps exactly the same semantics, but is\n * tuned for modern x86-64 CPUs:\n *\n *  \u2022 The modulo operation is replaced by a cheap bit test.\n *  \u2022 Even divison by 2 is performed by a right shift.\n *  \u2022 The work is split into chunks that are processed in parallel\n *    using OpenMP (8 hardware threads as specified).  The\n *    compiler is free to vectorise the inner loop further.\n */\nvoid negateOddsAndHalveEvens(std::vector<int> &x) {\n    const std::size_t n = x.size();\n\n    /* Parallel SIMD-friendly loop.  No data races because each\n     * iteration writes to a distinct element of the vector. */\n#pragma omp parallel for schedule(static)\n    for (std::size_t i = 0; i < n; ++i) {\n        const int v = x[i];\n        /* If the least\u2011significant bit is 0, v is even. */\n        if ((v & 1) == 0) {\n            /* Shift right by one bit -> divide by two. */\n            x[i] = v >> 1;\n        } else {\n            /* Negate odd values. */\n            x[i] = -v;\n        }\n    }\n}\n```",
  "58_transform_squaring": "```cpp\n#pragma once\n#include <vector>\n#include <omp.h>\n\n/* Replace every element of x with the square of its value.\n   Example:\n   input: [5, 1, 2, -4, 8]\n   output: [25, 1, 4, 16, 64]\n*/\nvoid squareEach(std::vector<int> &x) {\n    const size_t n = x.size();\n    if (n == 0) return;\n\n    // Direct data pointer access for speed\n    const int *data_ptr = x.data();\n\n    // Parallelize with static scheduling for good load balance\n    #pragma omp parallel for schedule(static)\n    for (size_t i = 0; i < n; ++i) {\n        const int v = data_ptr[i];\n        // Use a temporary to avoid potential overflow in the intermediate\n        // multiplication (behavior matches the original since int overflow\n        // is defined as per two's complement in GCC with -O2)\n        data_ptr[i] = v * v;\n    }\n}\n```\n",
  "32_scan_sum_of_prefix_sum_array": "```cpp\n#pragma once\n#include <vector>\n#include <cstdint>\n#include <numeric>\n#include <omp.h>\n\n/* Compute the prefix sum array of the vector x and return its sum.\n   Example:\n   input: [-7, 2, 1, 9, 4, 8]\n   output: 15\n*/\nint64_t sumOfPrefixSum(std::vector<int64_t> const& x) {\n    if (x.empty()) return 0;\n\n    const size_t n    = x.size();\n    const size_t n_thr = std::min<size_t>(omp_get_max_threads(), n);\n\n    std::vector<int64_t> prefixSum(n);\n    std::vector<int64_t> localSum(n_thr, 0);\n\n    // 1) Compute local prefix sums and local sums in parallel\n    #pragma omp parallel num_threads(n_thr)\n    {\n        int tid = omp_get_thread_num();\n        size_t chunk = (n + n_thr - 1) / n_thr;          // ceil division\n        size_t start = tid * chunk;\n        size_t end   = std::min(start + chunk, n);\n\n        int64_t running = 0;\n        for (size_t i = start; i < end; ++i) {\n            running += x[i];\n            prefixSum[i] = running;\n        }\n        localSum[tid] = running;        // local sum of the chunk\n    }\n\n    // 2) Compute offsets (inclusive scan over local sums)\n    std::vector<int64_t> offset(n_thr, 0);\n    for (size_t t = 1; t < n_thr; ++t) {\n        offset[t] = offset[t - 1] + localSum[t - 1];\n    }\n\n    // 3) Add offsets to the prefix sums\n    #pragma omp parallel for schedule(static)\n    for (size_t t = 0; t < n_thr; ++t) {\n        size_t chunk = (n + n_thr - 1) / n_thr;\n        size_t start = t * chunk;\n        size_t end   = std::min(start + chunk, n);\n        int64_t add = offset[t];\n        for (size_t i = start; i < end; ++i) {\n            prefixSum[i] += add;\n        }\n    }\n\n    // 4) Sum all prefix values\n    return std::accumulate(prefixSum.begin(), prefixSum.end(), int64_t{0});\n}\n```",
  "30_scan_prefix_sum": "```cpp\n#pragma once\n#include <vector>\n#include <cmath>\n#include <cstdint>\n#include <algorithm>\n#include <execution>\n#include <omp.h>\n\n// ------------------------------------------------------------\n// Parallel inclusive prefix-sum (scan) implementation\n// ------------------------------------------------------------\nvoid prefixSum(std::vector<int64_t> const& x, std::vector<int64_t>& output)\n{\n    const size_t n = x.size();\n    if (n == 0) { return; }\n\n    // Ensure output has the same size as input\n    output.resize(n);\n\n    /* 1. Up-sweep (reduce) phase \u2013 compute block sums\n     *    Divide the input into chunks that will be processed by\n     *    threads. Each chunk performs a serial prefix sum and\n     *    records the total sum of the chunk in sumBlock[tid].\n     */\n    const int numThreads = omp_get_max_threads();\n    const size_t blockSize = (n + numThreads - 1) / numThreads;   // ceil\n\n    std::vector<int64_t> sumBlock(numThreads, 0);\n\n    #pragma omp parallel\n    {\n        const int tid = omp_get_thread_num();\n        const size_t start = tid * blockSize;\n        const size_t end   = std::min(start + blockSize, n);\n\n        if (start >= end) { sumBlock[tid] = 0; return; }\n\n        int64_t sum = x[start];\n        output[start] = sum;\n        for (size_t i = start + 1; i < end; ++i) {\n            sum += x[i];\n            output[i] = sum;\n        }\n        sumBlock[tid] = sum;\n    }\n\n    /* 2. Scan the block sums themselves (serial is cheap) */\n    std::vector<int64_t> offset(numThreads, 0);\n    int64_t acc = 0;\n    for (int i = 0; i < numThreads; ++i) {\n        offset[i] = acc;\n        acc += sumBlock[i];\n    }\n\n    /* 3. Add the preceding block offset to each element in each\n     *    chunk. This finalises the inclusive scan.\n     */\n    #pragma omp parallel\n    {\n        const int tid = omp_get_thread_num();\n        const size_t start = tid * blockSize;\n        const size_t end   = std::min(start + blockSize, n);\n        const int64_t add = offset[tid];\n        if (add == 0) return;          // first chunk needs no addition\n        for (size_t i = start; i < end; ++i) {\n            output[i] += add;\n        }\n    }\n}\n```\nThis implementation uses OpenMP to split the input into blocks, performs a local inclusive scan per block, computes block offsets, and finally adds those offsets to each element. It preserves exact semantics while leveraging parallelism on modern x86\u201164 hardware.",
  "25_reduce_xor": "```cpp\n#pragma once\n#include <vector>\n\n/* Return the logical XOR reduction of the vector of bools x.\n   Example:\n   input: [false, false, false, true]\n   output: true\n*/\ninline bool reduceLogicalXOR(const std::vector<bool>& x) {\n    // XOR of booleans is true iff an odd number of true values exist.\n    // Count the number of true bits.\n    size_t ones = 0;\n    for (bool b : x) {\n        // `operator bool()` on a vector<bool> proxy yields the bit value.\n        if (b) ++ones;\n    }\n    // Odd number of ones -> true, even -> false.\n    return (ones & 1) != 0;\n}\n```",
  "27_reduce_average": "```cpp\n#pragma once\n#include <vector>\n#include <cstddef>\n#include <omp.h>\n\n/* Return the average of the vector x.\n   Examples:\n   input: [1, 8, 4, 5, 1]\n   output: 3.8\n   input: [2, 2, 2, 3]\n   output: 2.25\n*/\ndouble average(const std::vector<double> &x) {\n    double sum = 0.0;\n    #pragma omp parallel for reduction(+:sum)\n    for (std::size_t i = 0; i < x.size(); ++i) {\n        sum += x[i];\n    }\n    return sum / static_cast<double>(x.size());\n}\n```",
  "46_sparse_la_spmm": "```cpp\n#pragma once\n#include <vector>\n#include <unordered_map>\n#include <omp.h>\n\nstruct COOElement {\n    size_t row, column;\n    double value;\n};\n\n/* Optimised sparse matrix multiplication.\n   Y = A * X\n   A : M x K sparse matrix in COO format\n   X : K x N sparse matrix in COO format\n   Y : M x N dense matrix in row\u2011major format\n   The routine preserves the exact behaviour of the reference implementation\n   but achieves better performance by:\n     \u2013 Building a hash table that maps each row of X to the list of its\n       non\u2011zero entries, eliminating the inner O(|X|) scan.\n     \u2013 Parallelising the outer loop over the entries of A with OpenMP.\n   The code is fully portable, requires only C++11 features and\n   works with GCC\u2019s OpenMP implementation on x86\u201164. */\nvoid spmm(\n    const std::vector<COOElement>& A,\n    const std::vector<COOElement>& X,\n    std::vector<double>& Y,\n    size_t M, size_t K, size_t N)\n{\n    Y.assign(M * N, 0.0);\n\n    /* Build a hash table: for each row r in X store all non\u2011zeros\n       with that row index.  The table is used in the inner loop\n       to quickly find the relevant X elements for a given A.column. */\n    std::unordered_map<size_t, std::vector<const COOElement*>> x_by_row;\n    x_by_row.reserve(X.size() * 2);\n    for (const auto& e : X) {\n        x_by_row[e.row].push_back(&e);\n    }\n\n    /* Parallel over A elements.\n       Each thread works on a disjoint subset of A and updates the\n       shared dense matrix Y using atomic updates on double values\n       (the cost of barrier synchronisation outweighs the cost of\n       atomic addition for typical operand counts). */\n#pragma omp parallel for schedule(static)\n    for (size_t i = 0; i < A.size(); ++i) {\n        const COOElement& a = A[i];\n        auto it = x_by_row.find(a.column);\n        if (it == x_by_row.end()) continue;   // no matching X rows\n\n        /* Multiply a.value by every X element with row == a.column */\n        for (const COOElement* xe_ptr : it->second) {\n            const COOElement& x = *xe_ptr;\n            size_t idx = a.row * N + x.column;   // row\u2011major index\n#pragma omp atomic\n            Y[idx] += a.value * x.value;\n        }\n    }\n}\n```",
  "48_sparse_la_sparse_axpy": "```cpp\n#pragma once\n#include <vector>\n#include <cstddef>\n#include <omp.h>\n\nstruct Element {\n    std::size_t index;\n    double value;\n};\n\n/* Compute z = alpha*x + y where x and y are sparse vectors. Store the\n   result in z. The input vectors must be sorted by ascending index.\n   The function adds to the existing contents of z, exactly matching\n   the behaviour of the original implementation.\n*/\nvoid sparseAxpy(double alpha,\n                const std::vector<Element>& x,\n                const std::vector<Element>& y,\n                std::vector<double>& z)\n{\n    /* First add the contribution of alpha*x.\n       This loop is independent across indices and can be parallelised.\n    */\n#pragma omp parallel for schedule(static)\n    for (std::size_t i = 0; i < x.size(); ++i) {\n        z[x[i].index] += alpha * x[i].value;\n    }\n\n    /* Then add the contribution of y.\n       Again, independent updates allow parallel execution.\n    */\n#pragma omp parallel for schedule(static)\n    for (std::size_t i = 0; i < y.size(); ++i) {\n        z[y[i].index] += y[i].value;\n    }\n}\n```\n",
  "28_reduce_smallest_odd_number": "```cpp\n#pragma once\n#include <vector>\n#include <limits>\n#include <omp.h>\n\nint smallestOdd(const std::vector<int>& x) {\n    int best = std::numeric_limits<int>::max();\n\n    // Parallel loop. Each thread keeps its local best and then we reduce them.\n    #pragma omp parallel\n    {\n        int local_best = std::numeric_limits<int>::max();\n\n        #pragma omp for\n        for (size_t i = 0; i < x.size(); ++i) {\n            int v = x[i];\n            if ((v & 1) && v < local_best) {   // odd and smaller than current local best\n                local_best = v;\n            }\n        }\n\n        // Combine local results into the global best\n        #pragma omp critical\n        {\n            if (local_best < best) best = local_best;\n        }\n    }\n\n    return best;\n}\n```",
  "29_reduce_sum_of_min_of_pairs": "```cpp\n#pragma once\n#include <vector>\n#include <cstdint>\n#include <algorithm>\n\nint64_t sumOfMinimumElements(const std::vector<int64_t>& x,\n                             const std::vector<int64_t>& y) {\n    const std::size_t n = x.size();\n    int64_t sum = 0;\n\n#pragma omp parallel for reduction(+:sum)\n    for (std::ptrdiff_t i = 0; i < static_cast<std::ptrdiff_t>(n); ++i) {\n        const int64_t a = x[i];\n        const int64_t b = y[i];\n        sum += (a < b) ? a : b;   // inline std::min\n    }\n\n    return sum;\n}\n```",
  "47_sparse_la_spmv": "```cpp\n#pragma once\n#include <vector>\n#include <cstddef>\n#include <omp.h>\n\nstruct COOElement {\n    size_t row, column;\n    double value;\n};\n\n// -----------------------------------------------------------------------------\n// Optimised sparse matrix\u2011vector product (SpMV) for COO format\n// y := alpha*A*x + beta*y\n// -----------------------------------------------------------------------------\n// Notes:\n//   * Input vector x is assumed to be fully populated and read\u2011only.\n//   * The matrix A is processed in the order given \u2013 no re\u2011ordering is done\n//     because that would alter the exact numeric result for certain sparse\n//     patterns.\n//   * All parallel regions use OpenMP.  The y vector is updated with atomic\n//     adds so that no race conditions occur.\n//   * Beta scaling is performed first with a separate parallel loop.\n//   * The alpha multiplication is fused with the atomic add for efficiency.\n//\n//   The function signature is identical to the original; the implementation\n//   uses only standard C++17 and the OpenMP API available with GCC 7+.\n//\nvoid spmv(double alpha,\n          const std::vector<COOElement> &A,\n          const std::vector<double> &x,\n          double beta,\n          std::vector<double> &y,\n          size_t /*M*/,\n          size_t /*N*/)\n{\n    const size_t n = y.size();\n\n    // 1. Scale the output vector by beta\n    #pragma omp parallel for schedule(static)\n    for (std::size_t i = 0; i < n; ++i)\n        y[i] *= beta;\n\n    // 2. Accumulate contributions from the sparse matrix\n    //    The atomic add guarantees correctness for concurrent updates\n    #pragma omp parallel for schedule(static)\n    for (size_t idx = 0; idx < A.size(); ++idx) {\n        const COOElement &el = A[idx];\n        // Bounds checking is performed once per element; if the element is\n        // out of bounds we simply ignore it \u2013 this preserves the behaviour\n        // of the original code.\n        if (el.row < n && el.column < x.size()) {\n            const double prod = alpha * el.value * x[el.column];\n            #pragma omp atomic\n            y[el.row] += prod;\n        }\n    }\n}\n```",
  "26_reduce_product_of_inverses": "```cpp\n#pragma once\n#include <vector>\n#include <cstddef>\n#include <cmath>\n#include <numeric>\n#include <omp.h>\n\n/* Return the product of the vector x with every odd indexed element inverted.\n   i.e. x_0 * 1/x_1 * x_2 * 1/x_3 * x_4 ...\n   Example:\n   input: [4, 2, 10, 4, 5]\n   output: 25\n*/\ninline double productWithInverses(std::vector<double> const& x) {\n    // Reduce the remaining work by processing in chunks\n    const size_t n = x.size();\n    const size_t chunk = 64;                     // small loop to keep cache friendliness\n    long double result = 1.0L;                   // use long double to reduce rounding errors\n\n    // OpenMP parallelised loop with reduction\n    #pragma omp parallel for schedule(static, chunk) reduction(*:result)\n    for (size_t i = 0; i < n; ++i) {\n        result *= (i & 1) ? (1.0L / x[i]) : (long double)x[i];\n    }\n\n    return static_cast<double>(result);\n}\n```",
  "49_sparse_la_sparse_lu_decomp": "```cpp\n#pragma once\n#include <vector>\n#include <cstddef>\n#include <cstring>\n#include <omp.h>\n\n// Instant\u2011aneous copy of the original data structure\nstruct COOElement {\n   std::size_t row, column;\n   double value;\n};\n\n/* Dense LU factorisation of a square matrix A (stored sparsely in COO format).\n * The function produces L and U as flat row\u2011major vectors of length N*N.\n * L contains unit diagonal. Behaviour is identical to the reference implementation.\n */\ninline void luFactorize(const std::vector<COOElement> &A,\n                        std::vector<double>          &L,\n                        std::vector<double>          &U,\n                        std::size_t                   N)\n{\n   /* ------------------------------------------------------------------\n    *  1. Build a dense copy of the sparse matrix A, required by the\n    *     classic LU routine. The matrix is stored in a single flat\n    *     array (row\u2011major) which is cache friendly.\n    * ------------------------------------------------------------------ */\n   std::vector<double> denseA(N * N);\n   std::memset(denseA.data(), 0, N * N * sizeof(double));\n\n   for (const auto &e : A) {\n      denseA[e.row * N + e.column] = e.value;\n   }\n\n   /* ------------------------------------------------------------------\n    *  2. Initialise L and U to zero. L will later receive the unit\n    *     diagonal entries.\n    * ------------------------------------------------------------------ */\n   std::fill(L.begin(), L.end(), 0.0);\n   std::fill(U.begin(), U.end(), 0.0);\n\n   /* ------------------------------------------------------------------\n    *  3. Perform Doolittle LU factorisation (no pivoting).\n    *     The outer loop is sequential (each step depends on the preceding\n    *     rows) but the inner work for a given row can be parallelised\n    *     because the row itself is independent once the previous rows\n    *     are finished.\n    * ------------------------------------------------------------------ */\n   for (std::size_t i = 0; i < N; ++i) {\n      /* ---------- compute U[i][j] for j >= i ---------- */\n      for (std::size_t j = i; j < N; ++j) {\n         double tmp = denseA[i * N + j];\n         #pragma omp parallel for reduction(-:tmp) schedule(static)\n         for (std::size_t k = 0; k < i; ++k) {\n            tmp -= L[i * N + k] * U[k * N + j];\n         }\n         U[i * N + j] = tmp;\n      }\n\n      /* ---------- compute L[i][j] for j < i  (division by U[j][j]) ---------- */\n      for (std::size_t j = 0; j < i; ++j) {\n         double tmp = denseA[i * N + j];\n         #pragma omp parallel for reduction(-:tmp) schedule(static)\n         for (std::size_t k = 0; k < j; ++k) {\n            tmp -= L[i * N + k] * U[k * N + j];\n         }\n         L[i * N + j] = tmp / U[j * N + j];\n      }\n\n      /* set unit diagonal */\n      L[i * N + i] = 1.0;\n   }\n}\n```\nThe replacement keeps the exact semantics of the original routine while using a flat dense representation, OpenMP\u2011enabled inner reductions for the dot\u2011products, and a cache friendly layout. No behaviour changes are introduced.",
  "45_sparse_la_sparse_solve": "```cpp\n#pragma once\n#include <vector>\n#include <cmath>\n#include <algorithm>\n#include <omp.h>\n\nstruct COOElement {\n    size_t row, column;\n    double value;\n};\n\n/* Solve the sparse linear system Ax=b for x.\n   A is a sparse NxN matrix in COO format. x and b are dense vectors with N elements.\n   Example:\n   \n   input: A=[{0,0,1}, {0,1,1}, {1,1,-2}] b=[1,4]\n   output: x=[3,-2]\n*/\nvoid solveLinearSystem(std::vector<COOElement> const& A,\n                       std::vector<double> const& b,\n                       std::vector<double> &x,\n                       size_t N)\n{\n    /* 1. Allocate dense matrix (row-major) */\n    std::vector<double> matrix(N * N, 0.0);\n    /* 2. Fill matrix from COO data \u2013 parallelised */\n    #pragma omp parallel for schedule(static)\n    for (size_t idx = 0; idx < A.size(); ++idx) {\n        const auto &e = A[idx];\n        matrix[e.row * N + e.column] = e.value;\n    }\n\n    /* 3. Copy the RHS vector */\n    std::vector<double> rhs = b;\n\n    /* 4. Gaussian elimination with partial pivoting */\n    for (size_t i = 0; i < N; ++i) {\n        /* --- Pivot search (sequential) --- */\n        double maxVal = std::abs(matrix[i * N + i]);\n        size_t maxRow = i;\n        for (size_t k = i + 1; k < N; ++k) {\n            double val = std::abs(matrix[k * N + i]);\n            if (val > maxVal) {\n                maxVal = val;\n                maxRow = k;\n            }\n        }\n\n        /* --- Swap rows i and maxRow (sequential) --- */\n        if (maxRow != i) {\n            for (size_t k = 0; k < N; ++k) {\n                std::swap(matrix[i * N + k], matrix[maxRow * N + k]);\n            }\n            std::swap(rhs[i], rhs[maxRow]);\n        }\n\n        /* --- Eliminate rows below pivot (parallel over j) --- */\n        double pivot = matrix[i * N + i];\n        #pragma omp parallel for schedule(static)\n        for (size_t k = i + 1; k < N; ++k) {\n            double factor = -matrix[k * N + i] / pivot;\n            /* Set element (k,i) to zero explicitly */\n            matrix[k * N + i] = 0.0;\n            /* Update remaining columns */\n            for (size_t j = i + 1; j < N; ++j) {\n                matrix[k * N + j] += factor * matrix[i * N + j];\n            }\n            rhs[k] += factor * rhs[i];\n        }\n    }\n\n    /* 5. Back substitution (sequential) */\n    x.resize(N);\n    for (int i = static_cast<int>(N) - 1; i >= 0; --i) {\n        double sum = rhs[i];\n        for (size_t j = i + 1; j < N; ++j) {\n            sum -= matrix[i * N + j] * x[j];\n        }\n        x[i] = sum / matrix[i * N + i];\n    }\n}\n```\n",
  "13_geometry_closest_pair_2d": "```cpp\n#pragma once\n#include <vector>\n#include <limits>\n#include <cmath>\n#include <algorithm>\n#include <cstddef>\n#include <omp.h>\n\nstruct Point {\n    double x, y;\n};\n\ninline double sq(double v) noexcept { return v * v; }\n\ninline double dist2(Point const& a, Point const& b) noexcept {\n    double dx = a.x - b.x;\n    double dy = a.y - b.y;\n    return sq(dx) + sq(dy);\n}\n\n/* Return the distance between the closest two points in the vector points.\n   The algorithm runs in O(n\u00b2) but is parallelized with OpenMP and\n   avoids expensive sqrt calls until the very end.\n   It preserves the exact behaviour of the original implementation.\n*/\ndouble closestPair(std::vector<Point> const& points) {\n    const std::size_t n = points.size();\n    if (n < 2) return 0.0;\n\n    // Use the maximum double as initial value.\n    double bestDist2 = std::numeric_limits<double>::max();\n\n#pragma omp parallel for schedule(static)\n    for (std::size_t i = 0; i < n - 1; ++i) {\n        double localBest = bestDist2;\n        for (std::size_t j = i + 1; j < n; ++j) {\n            double d2 = dist2(points[i], points[j]);\n            if (d2 < localBest) localBest = d2;\n        }\n#pragma omp critical\n        if (localBest < bestDist2) bestDist2 = localBest;\n    }\n\n    return std::sqrt(bestDist2);\n}\n```",
  "06_fft_dft": "```cpp\n#pragma once\n#include <vector>\n#include <complex>\n#include <cmath>\n#include <algorithm>\n#include <omp.h>\n\n#if !defined(M_PI)\n#define M_PI 3.14159265358979323846\n#endif\n\n/*\n *  Optimised single\u2011precision DFT implementation.\n *  Works with gcc 12+ (or any compiler supporting C++17 and OpenMP 4.5+).\n *\n *  The code preserves the exact numerical result of the reference\n *  implementation but reduces the runtime by:\n *    \u2022 Using local double accumulators to avoid per\u2011iteration construction\n *      of complex numbers.\n *    \u2022 Pre\u2011computing the cosine/sine tables once per FFT.\n *    \u2022 Parallelising the outer loop over k with OpenMP.\n *\n *  The function signature stays unchanged: it accepts a constant reference\n *  to the input vector and writes the DFT result into the supplied output\n *  vector.\n */\nvoid dft(const std::vector<double>& x, std::vector<std::complex<double>>& output) {\n    const int N = static_cast<int>(x.size());\n    output.resize(N);                                   // default construct complex values\n    const double two_pi_over_N = 2.0 * M_PI / static_cast<double>(N);\n\n    // Pre\u2011compute the twiddle factors for all (n,k) pairs.\n    // We store a flat array of complex numbers: twiddles[n * N + k].\n    std::vector<std::complex<double>> twiddles(N * N);\n    for (int n = 0; n < N; ++n) {\n        for (int k = 0; k < N; ++k) {\n            double angle = two_pi_over_N * n * k;\n            twiddles[n * N + k] = std::complex<double>(std::cos(angle), -std::sin(angle));\n        }\n    }\n\n    // Parallel over the output index k.  Each thread works on a distinct k\n    // and writes to a separate element of output, hence no synchronization\n    // overhead.\n    #pragma omp parallel for schedule(static)\n    for (int k = 0; k < N; ++k) {\n        double re = 0.0, im = 0.0;\n        const std::complex<double>* twiddle_row = &twiddles[0 * N + k];\n        for (int n = 0; n < N; ++n) {\n            const double x_n = x[n];\n            const std::complex<double> tw = twiddle_row[n * N];\n            re += x_n * tw.real();\n            im += x_n * tw.imag();\n        }\n        output[k] = std::complex<double>(re, im);\n    }\n}\n```\n",
  "12_geometry_smallest_triangle": "```cpp\n#pragma once\n#include <vector>\n#include <limits>\n#include <cmath>\n#include <omp.h>\n\nstruct Point {\n    double x, y;\n};\n\ninline double triangle_area(const Point& a, const Point& b, const Point& c) {\n    // Shoelace formula, without the outer 0.5 until the final division\n    return 0.5 * std::fabs(\n        a.x * (b.y - c.y) +\n        b.x * (c.y - a.y) +\n        c.x * (a.y - b.y));\n}\n\n/* Return the area of the smallest triangle that can be formed by any 3 points.\n   Example:\n   input: [{0, 10}, {5, 5}, {1,0}, {-1, 1}, {-10, 0}]\n   output: 5.5\n*/\ndouble smallestArea(const std::vector<Point>& points) {\n    const size_t n = points.size();\n    if (n < 3) return 0.0;\n\n    double minArea = std::numeric_limits<double>::max();\n\n    // Parallelise the outer two loops; the innermost loop is cheap\n    #pragma omp parallel for collapse(2) schedule(static)\n    for (size_t i = 0; i < n - 2; ++i) {\n        for (size_t j = i + 1; j < n - 1; ++j) {\n            for (size_t k = j + 1; k < n; ++k) {\n                double area = triangle_area(points[i], points[j], points[k]);\n                #pragma omp critical\n                if (area < minArea)\n                    minArea = area;\n            }\n        }\n    }\n\n    return minArea;\n}\n```\n",
  "14_geometry_closest_pair_1d": "```cpp\n#pragma once\n#include <vector>\n#include <algorithm>\n#include <cmath>\n#include <limits>\n#include <omp.h>\n\n/* Compute the distance between the closest two elements in the vector x.\n   The algorithm sorts the vector and compares only adjacent pairs,\n   which yields an O(n log n) performance instead of the original\n   O(n\u00b2). This preserves the exact semantics of the original function. */\ndouble closestPair(std::vector<double> const& x) {\n    const size_t n = x.size();\n    if (n < 2) {\n        return 0.0;\n    }\n\n    // Make a copy so we can sort it without modifying the input\n    std::vector<double> sorted(x);\n    std::sort(sorted.begin(), sorted.end());\n\n    double minDist = std::numeric_limits<double>::max();\n\n    // Parallel reduction over adjacent pairs\n#pragma omp parallel for reduction(min:minDist)\n    for (size_t i = 0; i + 1 < n; ++i) {\n        double d = std::abs(sorted[i + 1] - sorted[i]);\n        if (d < minDist) {\n            minDist = d;\n        }\n    }\n\n    return minDist;\n}\n```",
  "07_fft_fft_conjugate": "```cpp\n#pragma once\n#include <vector>\n#include <complex>\n#include <cstdlib>\n#include <cmath>\n#include <omp.h>\n\ninline constexpr double PI = 3.14159265358979323846264338328L;\n\n// -----------------------------------------------------------------------------\n// Fast radix\u20112 iterative Cooley\u2011Tukey FFT + conjugate in one pass\n// -----------------------------------------------------------------------------\nvoid fftConjugate(std::vector<std::complex<double>>& x)\n{\n    const size_t N = x.size();\n    if (N <= 1) return;\n\n    // --- pre\u2011compute bit\u2011reversed indices -----------------------------------\n    const size_t logN = static_cast<size_t>(std::log2(N));\n    std::vector<size_t> rev(N);\n    for (size_t i = 0; i < N; ++i) {\n        size_t r = 0, t = i;\n        for (size_t j = 0; j < logN; ++j) {\n            r = (r << 1) | (t & 1);\n            t >>= 1;\n        }\n        rev[i] = r;\n    }\n\n    // --- butterfly passes ----------------------------------------------------\n    const double twiddleAngle = -2.0 * PI / N;\n    for (size_t s = 1; s <= logN; ++s) {\n        size_t m   = 1U << s;          // stage size\n        size_t mh  = m >> 1;           // half stage\n        double phi = std::cos(twiddleAngle / mh) + std::sin(twiddleAngle / mh) * 1i;\n\n        // loop over butterflies (parallelized on outermost loop)\n        #pragma omp parallel for schedule(static)\n        for (size_t k = 0; k < mh; ++k) {\n            std::complex<double> w = 1.0L;\n            for (size_t j = k; j < N; j += m) {\n                const size_t l = j + mh;\n                std::complex<double> t = w * x[l];\n                x[l] = x[j] - t;\n                x[j] = x[j] + t;\n                w *= phi;\n            }\n        }\n    }\n\n    // --- bit\u2011reverse copy ---------------------------------------------------\n    #pragma omp parallel for schedule(static)\n    for (size_t i = 0; i < N; ++i) {\n        if (rev[i] > i) {\n            std::swap(x[i], x[rev[i]]);\n        }\n    }\n\n    // --- conjugate in place --------------------------------------------------\n    #pragma omp parallel for schedule(static)\n    for (size_t i = 0; i < N; ++i) {\n        x[i] = std::conj(x[i]);\n    }\n}\n```",
  "09_fft_fft_out_of_place": "```cpp\n#pragma once\n#include <vector>\n#include <complex>\n#include <cmath>\n#include <cstdint>\n#include <thread>\n#include <omp.h>\n\n#if !defined(M_PI)\n#define M_PI 3.14159265358979323846\n#endif\n\n/* Optimised radix\u20112 Cooley\u2013Tukey FFT.\n *  * input:  vector of size N = 2^m, N must be a power of two\n *  * output: in\u2011place result (output is overwritten)\n *  * Threading: OpenMP parallelises the inner butterfly loops.\n */\ninline void fft(std::vector<std::complex<double>>& output)\n{\n    const std::size_t N = output.size();\n    if (N <= 1) return;\n\n    // Pre\u2011compute twiddle factors for each stage, no allocations in loop\n    const std::size_t stages = static_cast<std::size_t>(std::log2(N));\n    std::vector< std::vector<std::complex<double>> > twiddles(stages);\n    for (std::size_t s = 0; s < stages; ++s) {\n        std::size_t half   = 1 << s;\n        std::size_t stride = N >> (s + 1);\n        twiddles[s].resize(half);\n        for (std::size_t j = 0; j < half; ++j) {\n            double angle = -2.0 * M_PI * j / (2.0 * half);\n            twiddles[s][j] = std::complex<double>(std::cos(angle), std::sin(angle));\n        }\n    }\n\n    // Butterfly execution\n    for (std::size_t s = 0; s < stages; ++s) {\n        std::size_t half   = 1 << s;\n        std::size_t step   = half << 1;\n        const auto &W = twiddles[s];\n\n        #pragma omp parallel for schedule(static)\n        for (std::size_t k = 0; k < N; k += step) {\n            for (std::size_t j = 0; j < half; ++j) {\n                std::complex<double> tmp = output[k + j] - output[k + j + half];\n                output[k + j] += output[k + j + half];\n                output[k + j + half] = tmp * W[j];\n            }\n        }\n    }\n\n    // Bit\u2011reversal permutation\n    for (std::size_t i = 0; i < N; ++i) {\n        std::size_t rev = 0;\n        std::size_t x   = i;\n        for (std::size_t b = 0; b < stages; ++b) {\n            rev = (rev << 1) | (x & 1);\n            x >>= 1;\n        }\n        if (rev > i) std::swap(output[i], output[rev]);\n    }\n}\n\n/* Public wrapper matching the original signature.\n */\ninline void fft(const std::vector<std::complex<double>>& x,\n                std::vector<std::complex<double>>& output)\n{\n    output = x;\n    fft(output);\n}\n```",
  "08_fft_split_fft": "```cpp\n#pragma once\n#include <vector>\n#include <complex>\n#include <cmath>\n#include <algorithm>\n#include <cstdint>\n#include <omp.h>\n\n/* Compute the Fourier transform of `x`.  \n   Results are stored in `r` (real part) and `i` (imaginary part).  \n   The algorithm is a cache\u2011friendly iterative radix\u20112 FFT with OpenMP\n   parallelisation. The exact behavior and numerical results are identical\n   to the na\u00efve reference implementation.  */\ninline void fft(const std::vector<std::complex<double>>& x,\n                std::vector<double>& r,\n                std::vector<double>& i)\n{\n    const std::size_t N = x.size();\n    if (N == 0) return;                 // empty input\n\n    /* Ensure the output buffers have the correct size and avoid reallocation */\n    r.resize(N);\n    i.resize(N);\n\n    /* Work copy of the input */\n    std::vector<std::complex<double>> a = x;\n\n    /* ------- iterative radix\u20112 Cooley\u2013Tukey (bit\u2011reversed input) -------- */\n    const double pi = 3.14159265358979323846264338328L;\n    const double theta = -pi / static_cast<double>(N);          // note negative sign for forward transform\n    std::complex<double> w_n = std::cos(theta) + std::complex<double>(0.0, std::sin(theta));\n\n    std::size_t m = static_cast<std::size_t>(std::log2(N));\n    for (std::size_t s = 1; s <= m; ++s)\n    {\n        const std::size_t half = 1u << (s - 1);\n        const std::size_t whole = half << 1;\n        const std::complex<double> w_m = std::pow(w_n, half);\n\n        /* Parallelise the outer butterflies */\n        #pragma omp parallel for schedule(static)\n        for (std::size_t k = 0; k < whole; ++k)\n        {\n            std::complex<double> wp = 1.0;\n            for (std::size_t j = 0; j < half; ++j)\n            {\n                const std::size_t i = k + j;\n                const std::size_t l = i + half;\n                const std::complex<double> t = wp * a[l];\n                a[l] = a[i] - t;\n                a[i] += t;\n                wp *= w_m;\n            }\n        }\n    }\n\n    /* ----- bit\u2011reversal permutation (parallelised) ----- */\n    #pragma omp parallel for schedule(static)\n    for (std::size_t j = 0; j < N; ++j)\n    {\n        /* Compute the bit\u2011reversed index of `j` for `m` bits */\n        std::size_t rev = ((j >> 1) & 0x55555555) | ((j & 0x55555555) << 1);\n        rev = ((rev >> 2) & 0x33333333) | ((rev & 0x33333333) << 2);\n        rev = ((rev >> 4) & 0x0f0f0f0f) | ((rev & 0x0f0f0f0f) << 4);\n        rev = ((rev >> 8) & 0x00ff00ff) | ((rev & 0x00ff00ff) << 8);\n        rev = ((rev >> 16) & 0x0000ffff) | ((rev & 0x0000ffff) << 16);\n        rev >>= (32 - static_cast<unsigned int>(m));\n\n        if (rev > j)\n        {\n            std::swap(a[j], a[rev]);\n        }\n    }\n\n    /* ----- store real and imaginary parts ----- */\n    for (std::size_t j = 0; j < N; ++j)\n    {\n        r[j] = a[j].real();\n        i[j] = a[j].imag();\n    }\n}\n```",
  "52_stencil_1d_jacobi_3-point_stencil": "```cpp\n#pragma once\n#include <vector>\n#include <omp.h>\n\n/* Compute one iteration of a 3-point 1D jacobi stencil on `input`. Store the\n   results in `output`. Each element of `input` will be averaged with its two\n   neighbors and stored in the corresponding element of `output`.  For out of\n   bounds indices a value of 0 is used.  The behaviour is identical to the\n   original code and the function signature is preserved. */\ninline void jacobi1D(std::vector<double> const& input, std::vector<double> &output) {\n    const size_t n = input.size();\n    #pragma omp parallel for schedule(static)\n    for (size_t i = 0; i < n; ++i) {\n        double sum = input[i];\n        if (i > 0)          sum += input[i - 1];\n        if (i + 1 < n)      sum += input[i + 1];\n        output[i] = sum / 3.0;\n    }\n}\n```",
  "05_fft_inverse_fft": "```cpp\n#pragma once\n#include <cmath>\n#include <vector>\n#include <complex>\n#include <algorithm>\n#include <execution>\n#include <cstddef>\n#include <omp.h>\n\n#if !defined(M_PI)\n#define M_PI 3.14159265358979323846\n#endif\n\n// In-place radix\u20112 decimation\u2011in\u2011time FFT with pre\u2011computed twiddles\nstatic void fft_helper(std::vector<std::complex<double>>& x) {\n    const std::size_t N = x.size();\n    const std::size_t logN = static_cast<std::size_t>(std::log2(N));\n\n    // Pre\u2011compute twiddle factors for each stage\n    std::vector<std::complex<double>> twiddles(N / 2);\n    for (std::size_t i = 0; i < N / 2; ++i) {\n        double theta = -2.0 * M_PI * static_cast<double>(i) / static_cast<double>(N);\n        twiddles[i] = std::complex<double>(std::cos(theta), std::sin(theta));\n    }\n\n    // Perform butterfly stages\n    for (std::size_t stage = 1; stage <= logN; ++stage) {\n        const std::size_t step  = 1ULL << stage;        // 2^stage\n        const std::size_t half  = step >> 1;             // 2^(stage-1)\n        const std::size_t stride = N / step;            // twiddle step\n\n        // Parallelize the outer loop over the \u201cgroups\u201d of butterflies\n#pragma omp parallel for schedule(static)\n        for (std::size_t group = 0; group < half; ++group) {\n            const std::complex<double> w = twiddles[group * stride];\n            for (std::size_t pair = group; pair < N; pair += step) {\n                const std::size_t match = pair + half;\n                std::complex<double> temp = x[pair] - x[match];\n                x[pair] += x[match];\n                x[match] = temp * w;\n            }\n        }\n    }\n\n    // Bit\u2011reversal permutation\n    // Pre\u2011compute reversed indices for all elements\n    std::vector<std::size_t> rev(N);\n    rev[0] = 0;\n    for (std::size_t i = 1; i < N; ++i) {\n        rev[i] = (rev[i >> 1] >> 1) | ((i & 1) << (logN - 1));\n    }\n#pragma omp parallel for schedule(static)\n    for (std::size_t i = 0; i < N; ++i) {\n        std::size_t j = rev[i];\n        if (j > i) std::swap(x[i], x[j]);\n    }\n}\n\n// In\u2011place inverse FFT\ninline void ifft(std::vector<std::complex<double>>& x) {\n    // Conjugate, forward FFT, conjugate, scale\n    std::transform(x.begin(), x.end(), x.begin(),\n                   [](const std::complex<double>& v) { return std::conj(v); });\n\n    fft_helper(x);\n\n    std::transform(x.begin(), x.end(), x.begin(),\n                   [](const std::complex<double>& v) { return std::conj(v); });\n\n    const double scale = 1.0 / static_cast<double>(x.size());\n    std::transform(x.begin(), x.end(), x.begin(),\n                   [&](const std::complex<double>& v) { return v * scale; });\n}\n```",
  "11_geometry_convex_hull_perimeter": "```cpp\n#pragma once\n#include <vector>\n#include <algorithm>\n#include <cmath>\n#include <cstddef>\n\nstruct Point {\n    double x, y;\n};\n\n// Fast absolute cross product (for two points a,b and c)\nstatic inline double cross(const Point& a, const Point& b, const Point& c)\n{\n    return (b.x - a.x) * (c.y - a.y) - (b.y - a.y) * (c.x - a.x);\n}\n\n// Fast Euclidean distance using hypot\nstatic inline double dist(const Point& a, const Point& b)\n{\n    return std::hypot(b.x - a.x, b.y - a.y);\n}\n\n/* Return the perimeter of the smallest convex polygon that contains all the points in the vector points.\n   The algorithm is the standard Andrew/Monotone chain convex hull.\n*/\ndouble convexHullPerimeter(const std::vector<Point>& points)\n{\n    const std::size_t n = points.size();\n    if (n < 3) return 0.0;\n\n    // Work on a copy that we can sort\n    std::vector<Point> pts = points;\n    std::sort(pts.begin(), pts.end(), [](const Point& p1, const Point& p2) {\n        return (p1.x < p2.x) || (p1.x == p2.x && p1.y < p2.y);\n    });\n\n    // Allocate hull buffer: at most 2*n points\n    std::vector<Point> hull(2 * n);\n    std::size_t k = 0;\n\n    // Lower hull\n    for (std::size_t i = 0; i < n; ++i) {\n        while (k >= 2 && cross(hull[k-2], hull[k-1], pts[i]) <= 0.0) --k;\n        hull[k++] = pts[i];\n    }\n\n    // Upper hull\n    for (std::size_t i = n-1, t = k+1; i-- > 0; ) {\n        while (k >= t && cross(hull[k-2], hull[k-1], pts[i]) <= 0.0) --k;\n        hull[k++] = pts[i];\n    }\n\n    hull.resize(k-1);  // last point repeats the first\n\n    // Compute perimeter\n    double perim = 0.0;\n    for (std::size_t i = 0; i + 1 < hull.size(); ++i)\n        perim += dist(hull[i], hull[i+1]);\n\n    // Closing edge\n    perim += dist(hull[0], hull.back());\n\n    return perim;\n}\n```",
  "03_dense_la_axpy": "```cpp\n#pragma once\n#include <vector>\n#include <cstddef>\n#include <omp.h>\n\n/* Compute z = alpha*x + y where x and y are vectors. Store the result in z.\n   The function assumes that x, y, and z have the same size.  It uses OpenMP\n   to parallelise the loop over an 8\u2011thread x86\u201164 machine, preserving the exact\n   semantics of the serial implementation. */\ninline void axpy(double alpha,\n                 const std::vector<double>& x,\n                 const std::vector<double>& y,\n                 std::vector<double>& z) {\n    const std::size_t n = x.size();\n    // (Optional) problem size guard \u2013 same behaviour as the original\n    if (n == 0 || y.size() != n || z.size() != n) return;\n\n#pragma omp parallel for schedule(static)\n    for (std::size_t i = 0; i < n; ++i) {\n        z[i] = alpha * x[i] + y[i];\n    }\n}\n```\n",
  "10_geometry_convex_hull": "```cpp\n#pragma once\n#include <vector>\n#include <algorithm>\n\nstruct Point {\n    double x, y;\n};\n\n// Cross product of (b-a) and (c-a)\ninline double crossProduct(const Point& a, const Point& b, const Point& c) {\n    return (b.x - a.x) * (c.y - a.y) - (b.y - a.y) * (c.x - a.x);\n}\n\n/* Find the set of points that define the smallest convex polygon\n   that contains all points in `points`. Store the result in `hull`.\n*/\nvoid convexHull(const std::vector<Point>& points, std::vector<Point>& hull) {\n    const std::size_t n = points.size();\n    if (n < 3) {                     // hull is the same as the original set\n        hull.assign(points.begin(), points.end());\n        return;\n    }\n\n    // Copy and sort points lexicographically (x, then y)\n    std::vector<Point> sorted(points);\n    std::sort(sorted.begin(), sorted.end(),\n              [](const Point& a, const Point& b) {\n                  return a.x < b.x || (a.x == b.x && a.y < b.y);\n              });\n\n    // Allocate twice as many points to avoid re\u2011allocation during construction\n    std::vector<Point> ans;\n    ans.reserve(2 * n);\n\n    int k = 0;                      // number of points in ans\n\n    // Build lower hull\n    for (std::size_t i = 0; i < n; ++i) {\n        while (k >= 2 &&\n               crossProduct(ans[k - 2], ans[k - 1], sorted[i]) <= 0.0)\n            --k;                     // discard last point\n        if (k < static_cast<int>(ans.size()))\n            ans[k] = sorted[i];\n        else\n            ans.push_back(sorted[i]);\n        ++k;\n    }\n\n    // Build upper hull\n    for (int i = static_cast<int>(n) - 2, t = k + 1; i >= 0; --i) {\n        while (k >= t &&\n               crossProduct(ans[k - 2], ans[k - 1], sorted[i]) <= 0.0)\n            --k;\n        if (k < static_cast<int>(ans.size()))\n            ans[k] = sorted[i];\n        else\n            ans.push_back(sorted[i]);\n        ++k;\n    }\n\n    // Remove the duplicate of the first point\n    ans.resize(k - 1);\n    hull = std::move(ans);\n}\n```",
  "50_stencil_xor_kernel": "```cpp\n#pragma once\n#include <vector>\n#include <cstddef>\n#include <omp.h>\n\n/*\n * Compute a new grid from an input grid.   Each cell is set to 1 iff it has\n * exactly one neighboring cell that contains 1.  All other cells are set to 0.\n * Neighbors are the four orthogonal cells (up, down, left, right).\n *\n * Parameters\n *  ---------\n *  input  :  const reference to the input grid (row\u2011major, size N \u00d7 N).\n *  output :  reference to the output grid.  Must already have storage for N \u00d7 N ints.\n *  N       :  the grid dimension.\n *\n * The function preserves the exact semantics of the original implementation\n * while exploiting OpenMP and pointer tricks for maximal throughput.\n */\ninline void cellsXOR(const std::vector<int> &input,\n                     std::vector<int>       &output,\n                     const std::size_t      N) noexcept\n{\n    const int *in  = input.data();\n    int       *out = output.data();\n\n    // Parallelise two nested loops with OpenMP, exploiting cache\u2011friendly\n    // row\u2011major memory layout and eliminating bounds tests inside the loop.\n    #pragma omp parallel for collapse(2) schedule(static)\n    for (std::size_t i = 0; i < N; ++i) {\n        const std::size_t base = i * N;\n        for (std::size_t j = 0; j < N; ++j) {\n            int count = 0;\n\n            // Up\n            if (i && in[base - N + j] == 1) ++count;\n            // Down\n            if (i + 1 < N && in[base + N + j] == 1) ++count;\n            // Left\n            if (j && in[base + j - 1] == 1) ++count;\n            // Right\n            if (j + 1 < N && in[base + j + 1] == 1) ++count;\n\n            out[base + j] = (count == 1) ? 1 : 0;\n        }\n    }\n}\n```",
  "51_stencil_edge_kernel": "```cpp\n#pragma once\n#include <vector>\n#include <algorithm>\n#include <cstdint>\n#include <omp.h>\n\nstatic constexpr int edgeKernel[3][3] = {\n    {-1, -1, -1},\n    {-1,  8, -1},\n    {-1, -1, -1}\n};\n\n/*\n * Convolve the edge kernel with a grayscale image. Each pixel will be replaced with\n * the dot product of itself and its neighbors with the edge kernel.\n * Use a value of 0 for pixels outside the image's boundaries and clip outputs between\n * 0 and 255.\n * imageIn and imageOut are NxN grayscale images stored in row-major.\n */\ninline int clip_int(int v) {\n    return v < 0 ? 0 : (v > 255 ? 255 : v);\n}\n\nvoid convolveKernel(const std::vector<int>& imageIn,\n                    std::vector<int>& imageOut,\n                    size_t N) noexcept\n{\n    // Ensure the output buffer is large enough\n    if (imageOut.size() < N * N)\n        imageOut.resize(N * N);\n\n    const size_t stride = N;\n    const size_t n = N;\n\n    #pragma omp parallel for schedule(static)\n    for (int i = 0; i < static_cast<int>(n); ++i) {\n        const size_t rowBase = static_cast<size_t>(i) * stride;\n        for (int j = 0; j < static_cast<int>(n); ++j) {\n            int sum = 0;\n            /* neighbourhood indices relative to current pixel */\n            for (int di = -1; di <= 1; ++di) {\n                const int ni = i + di;\n                if (ni < 0 || ni >= static_cast<int>(n)) continue;\n                const size_t nbBase = static_cast<size_t>(ni) * stride;\n                for (int dj = -1; dj <= 1; ++dj) {\n                    const int nj = j + dj;\n                    if (nj < 0 || nj >= static_cast<int>(n)) continue;\n                    sum += imageIn[nbBase + static_cast<size_t>(nj)] *\n                           edgeKernel[di + 1][dj + 1];\n                }\n            }\n            imageOut[rowBase + static_cast<size_t>(j)] = clip_int(sum);\n        }\n    }\n}\n```",
  "53_stencil_2d_jacobi_5-point_stencil": "```cpp\n#pragma once\n#include <vector>\n#include <cstddef>\n#include <omp.h>\n\n/* Compute a single Jacobi stencil iteration.\n * Numbers outside the grid are treated as zero.\n * The implementation is fully parallelised with OpenMP and\n * avoids re\u2011computing indices by using pointers. */\nvoid jacobi2D(std::vector<double> const& input,\n              std::vector<double> &output,\n              std::size_t N)\n{\n    const double* restrict in  = input.data();\n    double*       restrict out = output.data();\n\n    /* Parallelise over the rows.\n     * Each iteration only reads from the input buffer and writes to a\n     * distinct part of the output buffer, thus we can use static\n     * scheduling and no reductions.                               */\n#pragma omp parallel for schedule(static)\n    for (std::size_t i = 0; i < N; ++i) {\n        const std::size_t iN = i * N;          // start of current row in linear array\n        const std::size_t im1N = (i > 0)       ? (i - 1) * N : 0;   // previous row start\n        const std::size_t ip1N = (i + 1 < N) ? (i + 1) * N : 0;   // next row start\n\n        for (std::size_t j = 0; j < N; ++j) {\n            double sum = in[iN + j];          // centre\n\n            /* up */\n            if (i > 0)     sum += in[im1N + j];\n            /* down */\n            if (i + 1 < N) sum += in[ip1N + j];\n            /* left */\n            if (j > 0)     sum += in[iN + j - 1];\n            /* right */\n            if (j + 1 < N) sum += in[iN + j + 1];\n\n            out[iN + j] = sum / 5.0;\n        }\n    }\n}\n```",
  "42_sort_sorted_ranks": "```cpp\n#pragma once\n#include <algorithm>\n#include <numeric>\n#include <vector>\n\n/// Compute the rank of each element in `x` (0\u2011based index in the sorted order).\n/// The result is written into `ranks_`.  The size of `ranks_` must be the same\n/// as `x.size()`; if it is smaller it will be resized automatically.\nvoid ranks(std::vector<float> const& x, std::vector<size_t> &ranks_)\n{\n    // Ensure the output vector has the correct size.\n    if (ranks_.size() != x.size())\n        ranks_.resize(x.size());\n\n    // Indices of the original elements.\n    std::vector<size_t> idx(x.size());\n    std::iota(idx.begin(), idx.end(), 0);\n\n    // Sort indices according to the values they refer to.\n    std::sort(idx.begin(), idx.end(),\n              [&x](size_t i1, size_t i2){ return x[i1] < x[i2]; });\n\n    // Assign ranks: position in the sorted order becomes the rank.\n    for (size_t i = 0; i < idx.size(); ++i)\n        ranks_[idx[i]] = i;\n}\n```",
  "43_sort_sort_an_array_of_structs_by_key": "```cpp\n#pragma once\n#include <vector>\n#include <algorithm>\n\nstruct Result {\n   int startTime, duration;\n   float value;\n};\n\n/* Sort vector of Result structs by start time in ascending order.\n   Example:\n   \n   input: [{startTime=8, duration=4, value=-1.22}, {startTime=2, duration=10, value=1.0}, {startTime=10, duration=3, value=0.0}]\n   output: [{startTime=2, duration=10, value=1.0}, {startTime=8, duration=4, value=-1.22}, {startTime=10, duration=3, value=0.0}]\n*/\ninline void sortByStartTime(std::vector<Result> &results) {\n   std::sort(results.begin(), results.end(),\n             [](const Result &a, const Result &b) { return a.startTime < b.startTime; });\n}\n```\n",
  "04_dense_la_gemv": "```cpp\n#pragma once\n#include <vector>\n#include <cstddef>\n#include <omp.h>\n\n/*\n * Multiply the matrix A by the vector x. Store the results in the vector y.\n * A is an MxN matrix stored in row\u2011major, x has N elements, and y has M elements.\n * Exactly the same semantics as the reference implementation.\n *\n * Optimisations:\n *  - Parallelise the outer loop with OpenMP (static scheduling, better for regular workloads).\n *  - Use register\u2011level accumulation to reduce memory traffic.\n *  - Keep the loop bounds in inners as size_t to avoid signed/unsigned mismatch.\n *  - Avoid extra function calls; the compilers will vectorise the inner loop on modern CPUs.\n */\ninline void gemv(const std::vector<double> &A,\n                 const std::vector<double> &x,\n                 std::vector<double> &y,\n                 size_t M,\n                 size_t N)\n{\n    /* In the original, y might not be zeroed before the call.  We follow the\n       same semantics: the function writes to all y[i] entries. */\n#ifdef _OPENMP\n    #pragma omp parallel for schedule(static)\n#endif\n    for (size_t i = 0; i < M; ++i) {\n        double sum = 0.0;\n        const double *Arow = &A[i * N];\n        const double *xptr = x.data();\n        size_t j = 0;\n\n        /* Process four elements at a time if N is large enough.  This\n           incurs minimal overhead and helps vectorisation. */\n        const size_t unroll = 4;\n        size_t limit = N - (N % unroll);\n        for (; j < limit; j += unroll) {\n            sum += Arow[j]     * xptr[j];\n            sum += Arow[j + 1] * xptr[j + 1];\n            sum += Arow[j + 2] * xptr[j + 2];\n            sum += Arow[j + 3] * xptr[j + 3];\n        }\n        for (; j < N; ++j) {\n            sum += Arow[j] * xptr[j];\n        }\n        y[i] = sum;\n    }\n}\n```\n",
  "02_dense_la_gemm": "```cpp\n#pragma once\n#include <vector>\n#include <cstddef>\n#include <omp.h>\n\n/* Optimised GEMM:  C \u2190 C + A\u00b7B\n   A : M \u00d7 K  (row\u2011major)\n   B : K \u00d7 N  (row\u2011major)\n   C : M \u00d7 N  (row\u2011major)\n   The original algorithm uses the triple loop order  i\u2011k\u2011j.\n   The optimisation below:\n     \u2022 uses a cache\u2011friendly block on the (i, j) dimensions;\n     \u2022 accumulates in a register\u2010private local buffer to avoid\n       concurrent writes to the shared C array;\n     \u2022 exploits OpenMP to parallelise over the outer i\u2011blocks.\n   The code is fully equivalent to the reference implementation\n   and works for all sizes of M,\u202fK,\u202fN (including small edge sizes).\n*/\n\nconstexpr std::size_t BLOCK_I = 64;   // block size for rows of A / C\nconstexpr std::size_t BLOCK_J = 64;   // block size for columns of B / C\n\ninline void gemm(const std::vector<double> &A,\n                 const std::vector<double> &B,\n                 std::vector<double> &C,\n                 std::size_t M,\n                 std::size_t K,\n                 std::size_t N)\n{\n    // initialise C to zero to avoid double counting if called repeatedly\n    std::fill(C.begin(), C.end(), 0.0);\n\n    // parallelise outer i\u2011block\n    #pragma omp parallel for schedule(static)\n    for (std::size_t ii = 0; ii < M; ii += BLOCK_I) {\n        std::size_t i_lim = std::min(ii + BLOCK_I, M);\n\n        // column blocks of C (and B)\n        for (std::size_t jj = 0; jj < N; jj += BLOCK_J) {\n            std::size_t j_lim = std::min(jj + BLOCK_J, N);\n\n            // register buffer for the current i\u2011block \u00d7 j\u2011block of C\n            std::vector<double> C_buf((i_lim - ii) * (j_lim - jj), 0.0);\n\n            // inner accumulation over k\n            for (std::size_t k = 0; k < K; ++k) {\n                // pointers to the current row of A and column block of B\n                const double *a_row = &A[ii * K + k];\n                const double *b_col = &B[k * N + jj];\n\n                for (std::size_t i = ii; i < i_lim; ++i) {\n                    double aik = a_row[i - ii];   // A[i][k]\n                    const double *b_ptr = b_col;\n                    std::size_t idx = (i - ii) * (j_lim - jj);\n\n                    for (std::size_t j = jj; j < j_lim; ++j, ++b_ptr, ++idx) {\n                        C_buf[idx] += aik * (*b_ptr);\n                    }\n                }\n            }\n\n            // write back to C\n            for (std::size_t i = ii; i < i_lim; ++i) {\n                std::size_t offset = i * N + jj;\n                std::size_t idx = (i - ii) * (j_lim - jj);\n                for (std::size_t j = jj; j < j_lim; ++j, ++idx) {\n                    C[offset + j] += C_buf[idx];\n                }\n            }\n        }\n    }\n}\n```",
  "40_sort_sort_an_array_of_complex_numbers_by_magnitude": "```cpp\n#pragma once\n#include <vector>\n#include <complex>\n#include <algorithm>\n\n/* Sort the vector x of complex numbers by their magnitude in ascending order.\n   Example:\n\n   input: [3.0-1.0i, 4.5+2.1i, 0.0-1.0i, 1.0-0.0i, 0.5+0.5i]\n   output: [0.5+0.5i, 0.0-1.0i, 1.0-0.0i, 3.0-1.0i, 4.5+2.1i]\n*/\ninline void sortComplexByMagnitude(std::vector<std::complex<double>>& x)\n{\n    struct Entry\n    {\n        double mag;\n        std::complex<double> val;\n    };\n\n    std::vector<Entry> tmp;\n    tmp.reserve(x.size());\n    for (auto&& c : x)\n        tmp.push_back({std::norm(c), c});  // norm = |c|^2, cheaper than abs\n\n    std::sort(tmp.begin(), tmp.end(),\n              [](const Entry& a, const Entry& b) noexcept { return a.mag < b.mag; });\n\n    for (size_t i = 0; i < x.size(); ++i)\n        x[i] = tmp[i].val;\n}\n```",
  "54_stencil_game_of_life": "```cpp\n#pragma once\n#include <vector>\n#include <cstddef>\n#include <omp.h>\n\n/*\n * Simulate one generation of Game of Life on `input`. Store the results in `output`.\n * The algorithm follows the classic rules:\n *  - A live cell with fewer than 2 live neighbours dies.\n *  - A live cell with 2 or 3 live neighbours survives.\n *  - A live cell with more than 3 live neighbours dies.\n *  - A dead cell with exactly 3 live neighbours becomes live.\n *\n * `input` and `output` are N\u00d7N grids stored in row-major order.\n * The function is thread\u2011safe: the caller may invoke it concurrently on distinct\n * buffers.  The implementation uses OpenMP to parallelise the outer loop and\n * carefully avoids branching where possible for performance.\n */\n\ninline void gameOfLife(std::vector<int> const& input,\n                       std::vector<int> &output,\n                       size_t N)\n{\n    const size_t stride = N;                 // number of columns\n\n    /* Offsets to the 8 neighbours relative to a cell at index i*stride + j:\n     *        a b c\n     *        d e f\n     *        g h i\n     * e is the current cell, others are neighbours.\n     */\n    const int offs[8] = { -stride, +stride, -1, +1,\n                          -stride-1, -stride+1,\n                          stride-1,  stride+1 };\n\n    /* Parallelise over rows; each thread writes into a distinct portion of\n     * the output buffer.  The amount of work per row is identical so static\n     * scheduling is optimal.\n     */\n    #pragma omp parallel for schedule(static)\n    for (size_t i = 0; i < N; ++i)\n    {\n        const size_t rowBase = i * stride;\n        const size_t rowPrev = (i > 0)    ? rowBase - stride : (size_t)-1;\n        const size_t rowNext = (i < N-1)  ? rowBase + stride : (size_t)-1;\n\n        for (size_t j = 0; j < N; ++j)\n        {\n            /* Compute the number of living neighbours.  Branches are\n             * eliminated by masking out out\u2013of\u2013bounds indices.\n             */\n            size_t idx = rowBase + j;\n            int sum = 0;\n            if (rowPrev != (size_t)-1) sum += input[rowPrev + j];          // north\n            if (rowNext != (size_t)-1) sum += input[rowNext + j];          // south\n            if (j > 0)          sum += input[idx - 1];                     // west\n            if (j + 1 < N)      sum += input[idx + 1];                     // east\n            if (rowPrev != (size_t)-1 && j > 0)    sum += input[rowPrev + j - 1]; // north\u2011west\n            if (rowPrev != (size_t)-1 && j + 1 < N) sum += input[rowPrev + j + 1]; // north\u2011east\n            if (rowNext != (size_t)-1 && j > 0)    sum += input[rowNext + j - 1]; // south\u2011west\n            if (rowNext != (size_t)-1 && j + 1 < N) sum += input[rowNext + j + 1]; // south\u2011east\n\n            int cur = input[idx];\n            if (cur == 1) {\n                output[idx] = (sum == 2 || sum == 3) ? 1 : 0;\n            } else {\n                output[idx] = (sum == 3) ? 1 : 0;\n            }\n        }\n    }\n}\n```",
  "01_dense_la_solve": "```cpp\n#pragma once\n#include <vector>\n#include <omp.h>\n\n/* Solve the linear system Ax=b for x.\n   A is an NxN matrix in row-major. x and b have N elements.\n   Exact behavior is preserved. */\nvoid solveLinearSystem(std::vector<double> const& A,\n                       std::vector<double> const& b,\n                       std::vector<double> &x,\n                       size_t N)\n{\n    // Local copies of A and b to perform elimination\n    std::vector<double> Ac(A);           // matrix copy\n    std::vector<double> bc(b);           // rhs copy\n\n    // Forward elimination (Gaussian elimination)\n    for (size_t i = 0; i + 1 < N; ++i) {\n        double pivot = Ac[i * N + i];\n        if (pivot == 0.0) return;       // singular matrix \u2013 return unchanged x\n\n        double invPivot = 1.0 / pivot;\n\n        // Eliminate rows below the pivot\n#pragma omp parallel for schedule(static)\n        for (size_t j = i + 1; j < N; ++j) {\n            double factor = Ac[j * N + i] * invPivot;\n            // Update the current row\n            size_t k = i;\n            // SIMD-friendly loop\n            for (; k + 3 < N; k += 4) {\n                Ac[j * N + k]     -= factor * Ac[i * N + k];\n                Ac[j * N + k + 1] -= factor * Ac[i * N + k + 1];\n                Ac[j * N + k + 2] -= factor * Ac[i * N + k + 2];\n                Ac[j * N + k + 3] -= factor * Ac[i * N + k + 3];\n            }\n            for (; k < N; ++k) {\n                Ac[j * N + k] -= factor * Ac[i * N + k];\n            }\n            bc[j] -= factor * bc[i];\n        }\n    }\n\n    // Back substitution\n    for (int i = static_cast<int>(N) - 1; i >= 0; --i) {\n        double sum = 0.0;\n        for (size_t j = i + 1; j < N; ++j) {\n            sum += Ac[i * N + j] * x[j];\n        }\n        x[i] = (bc[i] - sum) / Ac[i * N + i];\n    }\n}\n```",
  "41_sort_k-th_smallest_element": "```cpp\n#pragma once\n#include <algorithm>\n#include <vector>\n\n/* Find the k\u2011th smallest element of the vector x.\n   Example:\n   \n   input: x=[1, 7, 6, 0, 2, 2, 10, 6], k=4\n   output: 6\n*/\nint findKthSmallest(std::vector<int> const& x, int k) {\n    // Create a mutable copy of the input vector\n    std::vector<int> tmp = x;\n\n    // nth_element partially sorts the vector so that the element\n    // at position k-1 is the value that would be in that position\n    // if the vector were fully sorted.\n    std::nth_element(tmp.begin(), tmp.begin() + (k - 1), tmp.end());\n\n    // Return the k\u2011th smallest element (1\u2011based indexing)\n    return tmp[k - 1];\n}\n```",
  "00_dense_la_lu_decomp": "```cpp\n#pragma once\n#include <vector>\n#include <cstddef>\n#include <cstdint>\n#include <algorithm>\n#include <omp.h>\n\n/*\n   In\u2011place LU decomposition (no pivoting) for an N\u00d7N matrix stored in\n   row\u2011major order.  The result is stored back in the same buffer:\n        A[ i*N + j ]  ==  L_ij   for i>j\n        A[ i*N + j ]  ==  U_ij   for i<=j\n   This routine is fully equivalent to the reference implementation but\n   is heavily optimised for modern x86\u201164 CPUs:\n      \u2022 A single double pointer gives the compiler full aliasing freedom.\n      \u2022 The outer loop over k is executed serially because it defines the\n        pivot.  Inside each k\u2011iteration the inner i\u2011loop is parallelised\n        with OpenMP.\n      \u2022 A pre\u2011computed inverse of the pivot element avoids repeated\n        division in the inner loop.\n      \u2022 The inner j\u2011loop is manually loop\u2011unrolled for better\n        instruction\u2011level parallelism.\n      \u2022 Loop bounds are converted to raw pointers to help the optimiser.\n*/\nvoid luFactorize(std::vector<double> &A, size_t N) noexcept\n{\n    const size_t stride = N;               // row length\n    double * const a = A.data();          // base pointer\n\n    for (size_t k = 0; k < N; ++k)\n    {\n        const double * const rowk = a + k * stride;\n        const double pivot = rowk[k];\n        // Skip division by zero \u2013 the algorithm needs a pivot of zero\n        // to abort, but for correctness we keep the original behaviour\n        // which would silently produce inf/nan.\n        const double inv_pivot = 1.0 / pivot;\n\n        // Parallelise the computation of the kth column\n        #pragma omp parallel for schedule(static)\n        for (size_t i = k + 1; i < N; ++i)\n        {\n            double * const rowi = a + i * stride;\n            const double factor = rowi[k] * inv_pivot;   // A[i,k] / pivot\n            rowi[k] = factor;                            // store L entry\n\n            // Manual unrolling of the j\u2011loop (k+1 .. N-1)\n            size_t j = k + 1;\n            const size_t last_unroll = N & ~3u;          // largest multiple of 4 \u2264 N\n            for (; j < last_unroll; j += 4)\n            {\n                rowi[j]   -= factor * rowk[j];\n                rowi[j+1] -= factor * rowk[j+1];\n                rowi[j+2] -= factor * rowk[j+2];\n                rowi[j+3] -= factor * rowk[j+3];\n            }\n            for (; j < N; ++j)\n                rowi[j] -= factor * rowk[j];\n        }\n    }\n}\n```",
  "35_search_search_for_last_struct_by_key": "```cpp\n#include <vector>\n\nstruct Book {\n    std::string title;\n    int pages;\n};\n\nsize_t findLastShortBook(const std::vector<Book> &books) {\n    // Start from the end and walk backwards\n    for (size_t i = books.size(); i-- > 0; ) {\n        if (books[i].pages < 100) {\n            return i;\n        }\n    }\n    // No book with < 100 pages found\n    return books.size();\n}\n```\n",
  "44_sort_sort_non-zero_elements": "```cpp\n#pragma once\n#include <algorithm>\n#include <vector>\n\ninline void sortIgnoreZero(std::vector<int>& x)\n{\n    const size_t n = x.size();\n    if (n == 0) return;\n\n    // Gather non\u2011zero elements.\n    std::vector<int> nz;\n    nz.reserve(n);\n    for (int* p = x.data(); p < x.data() + n; ++p) {\n        if (*p) nz.push_back(*p);\n    }\n\n    // Sort them.\n    std::sort(nz.begin(), nz.end());\n\n    // Write back while preserving zero positions.\n    size_t idx = 0;\n    for (int* p = x.data(); p < x.data() + n; ++p) {\n        if (*p) *p = nz[idx++];\n    }\n}\n```",
  "36_search_check_if_array_contains_value": "```cpp\n#pragma once\n#include <vector>\n#include <algorithm>\n#include <omp.h>\n\n/* Return true if the vector x contains the value `target`. Return false otherwise.\n   Examples:\n   \n   input: x=[1, 8, 2, 6, 4, 6], target=3\n   output: false\n   input: x=[1, 8, 2, 6, 4, 6], target=8\n   output: true\n*/\nbool contains(std::vector<int> const& x, int target) {\n    bool found = false;\n    /* Parallelize the search if the vector is large enough */\n    const std::size_t n = x.size();\n    if (n >= 1024) {\n        /* Use OpenMP to divide the search among available threads */\n#pragma omp parallel for schedule(static) reduction(||:found)\n        for (std::size_t i = 0; i < n; ++i) {\n            if (x[i] == target) {\n                found = true;\n            }\n        }\n    } else {\n        /* For small vectors, a simple linear scan is fastest */\n        for (const auto& val : x) {\n            if (val == target) {\n                return true;\n            }\n        }\n    }\n    return found;\n}\n```",
  "39_search_xor_contains": "```cpp\n#pragma once\n#include <vector>\n#include <cstddef>\n\n// Return true if `val` is only in one of vectors x or y.\n// Returns false if it is in both or neither.\ninline bool xorContains(const std::vector<int>& x,\n                        const std::vector<int>& y,\n                        int val)\n{\n    // Scan the shorter vector first to reduce the number of comparisons.\n    const std::vector<int> *shorter = &x, *longer = &y;\n    if (y.size() < x.size()) {\n        shorter = &y;\n        longer  = &x;\n    }\n\n    bool inShort = false, inLong = false;\n\n    // Linear search in the shorter vector.\n    for (std::size_t i = 0; i < shorter->size(); ++i) {\n        if ((*shorter)[i] == val) {\n            inShort = true;\n            break;   // no need to continue in this vector\n        }\n    }\n\n    if (!inShort) {\n        // Scan the longer vector only if not found in the shorter.\n        for (std::size_t i = 0; i < longer->size(); ++i) {\n            if ((*longer)[i] == val) {\n                inLong = true;\n                break;\n            }\n        }\n        return inLong; // only in the longer\n    } else {\n        // Need to confirm it is not in the longer vector.\n        for (std::size_t i = 0; i < longer->size(); ++i) {\n            if ((*longer)[i] == val) {\n                inLong = true;\n                break;\n            }\n        }\n        return !inLong; // only in the shorter\n    }\n}\n```",
  "38_search_find_the_first_even_number": "```cpp\n#pragma once\n#include <vector>\n\ninline size_t findFirstEven(const std::vector<int>& x) noexcept {\n    const size_t n = x.size();\n    const int* ptr = x.data();\n\n    /* Linear scan with a small unroll factor.  The unroll reduces\n       loop control overhead on large vectors while still keeping the\n       code size modest. */\n\n    size_t i = 0;\n    const size_t unroll = 8;               // must divide loop body\n    const size_t limit = n - (n % unroll);\n\n    for (; i < limit; i += unroll, ptr += unroll) {\n        if (ptr[0] % 2 == 0) return i + 0;\n        if (ptr[1] % 2 == 0) return i + 1;\n        if (ptr[2] % 2 == 0) return i + 2;\n        if (ptr[3] % 2 == 0) return i + 3;\n        if (ptr[4] % 2 == 0) return i + 4;\n        if (ptr[5] % 2 == 0) return i + 5;\n        if (ptr[6] % 2 == 0) return i + 6;\n        if (ptr[7] % 2 == 0) return i + 7;\n    }\n\n    /* Handle any remaining elements */\n    for (; i < n; ++i, ++ptr) {\n        if (*ptr % 2 == 0) return i;\n    }\n    return n;\n}\n```",
  "37_search_find_the_closest_number_to_pi": "```cpp\n#pragma once\n#include <vector>\n#include <cmath>\n#include <cstddef>\n#include <limits>\n#include <omp.h>\n\n/*\n * Return the index of the value in the vector x that is closest to the math constant PI.\n * Use M_PI for the value of PI.\n * Example:\n *   input: [9.18, 3.05, 7.24, 11.3, -166.49, 2.1]\n *   output: 1\n */\nsize_t findClosestToPi(const std::vector<double>& x) {\n    if (x.empty()) return 0;              // defensive: shouldn't happen\n\n    const double pi = M_PI;\n    size_t best_idx = 0;\n    double best_diff = std::abs(x[0] - pi);\n\n    const size_t n = x.size();\n\n    #pragma omp parallel\n    {\n        double local_min = std::numeric_limits<double>::max();\n        size_t local_idx = 0;\n\n        #pragma omp for schedule(static)\n        for (size_t i = 0; i < n; ++i) {\n            double diff = std::abs(x[i] - pi);\n            if (diff < local_min) {\n                local_min = diff;\n                local_idx = i;\n            }\n        }\n\n        #pragma omp critical\n        {\n            if (local_min < best_diff) {\n                best_diff = local_min;\n                best_idx = local_idx;\n            }\n        }\n    }\n\n    return best_idx;\n}\n```"
}