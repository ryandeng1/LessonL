{
  "15_graph_edge_count": "```cpp\n#pragma once\n#include <vector>\n#include <cstddef>\n#pragma omp declare target\n#pragma omp declare native\n#pragma omp declare target\n\nint edgeCount(std::vector<int> const& A, std::size_t N) {\n    int count = 0;\n#pragma omp parallel for reduction(+:count) schedule(static)\n    for (std::size_t idx = 0; idx < N * N; ++idx) {\n        if (A[idx] == 1) {\n            ++count;\n        }\n    }\n    return count;\n}\n```\n",
  "17_graph_highest_degree": "```cpp\n#pragma once\n#include <vector>\n#include <algorithm>\n#include <omp.h>\n\n/* Compute the highest node degree in the undirected graph. The graph is defined in the adjacency matrix A.\n   A is an NxN adjacency matrix stored in row-major. A is undirected.\n   Example:\n     input: [[0, 0, 0, 1], [0, 0, 0, 1], [0, 0, 0, 1], [1, 1, 1, 0]]\n   output: 3\n*/\nint maxDegree(std::vector<int> const& A, size_t N) {\n    int highestDegree = 0;\n    const int* data = A.data();\n\n    #pragma omp parallel for reduction(max:highestDegree)\n    for (int i = 0; i < static_cast<int>(N); ++i) {\n        int degree = 0;\n        const int* row = data + i * static_cast<int>(N);\n        #pragma omp simd reduction(+:degree)\n        for (int j = 0; j < static_cast<int>(N); ++j) {\n            degree += row[j];\n        }\n        highestDegree = std::max(highestDegree, degree);\n    }\n    return highestDegree;\n}\n```\n",
  "20_histogram_pixel_histogram": "```cpp\n#pragma once\n#include <vector>\n#include <array>\n#include <cstddef>\n#include <omp.h>\n\n/* Count the number of pixels in image with each grayscale intensity.\n   The vector `image` is a grayscale image with values 0\u2011255.\n   Store the results in `bins`.\n   Example:\n\n     input: image=[2, 116, 201, 11, 92, 92, 201, 4, 2]\n     output: [0, 0, 2, 0, 1, \u2026]\n*/\ninline void pixelCounts(std::vector<int> const& image, std::array<std::size_t, 256> &bins)\n{\n    /* Reset the output array */\n    std::fill(bins.begin(), bins.end(), 0);\n\n    /* Parallel counting using OpenMP array reduction.\n       gcc supports reduction on array slices since newer releases. */\n    #pragma omp parallel for schedule(static)\n    for (std::size_t i = 0; i < image.size(); ++i) {\n        /* Atomic addition is avoided by letting each thread\n           write to its own local region which is combined\n           automatically by the reduction clause. */\n        #pragma omp atomic\n        ++bins[image[i]];\n    }\n}\n```",
  "23_histogram_first_letter_counts": "```cpp\n#pragma once\n#include <array>\n#include <string>\n#include <vector>\n#include <omp.h>\n\n/*\n   For each letter in the alphabet, count the number of strings in the vector s that start with that letter.\n   Assume all strings are in lower case. Store the output in `bins` array.\n   Example:\n   input: [\"dog\", \"cat\", \"xray\", \"cow\", \"code\", \"type\", \"flower\"]\n   output: [0, 0, 3, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0]\n*/\ninline void firstLetterCounts(const std::vector<std::string> &s,\n                              std::array<size_t, 26> &bins)\n{\n    // Ensure bins are zero\u2011initialized before counting\n    bins.fill(0);\n\n    // Parallel loop with a private copy of bins for each thread\n    const int nThreads = omp_get_max_threads();\n    std::vector<std::array<size_t, 26>> localBins(nThreads);\n    for (auto &lb : localBins) lb.fill(0);\n\n    #pragma omp parallel\n    {\n        const int tid = omp_get_thread_num();\n        auto &lb = localBins[tid];\n\n        #pragma omp for schedule(static)\n        for (size_t i = 0; i < s.size(); ++i) {\n            const char c = s[i][0];\n            const int idx = c - 'a';\n            if (idx >= 0 && idx < 26)   // safety check\n                ++lb[idx];\n        }\n    }\n\n    // Reduce local bins into the final result\n    for (int t = 0; t < nThreads; ++t) {\n        const auto &lb = localBins[t];\n        for (int j = 0; j < 26; ++j)\n            bins[j] += lb[j];\n    }\n}\n```",
  "18_graph_count_components": "```cpp\n#pragma once\n#include <vector>\n#include <cstddef>\n\n/* Count the number of connected components in an undirected graph whose\n * adjacency is given by a row\u2011major NxN matrix A (entries 0/1).\n * The implementation uses a Union\u2013Find data structure, which\n * runs in O(N\u00b2 \u03b1(N)) time and is cache\u2011friendly.  The algorithm\n * preserves the exact semantics of the original DFS version. */\nint componentCount(std::vector<int> const& A, std::size_t N)\n{\n    // Union\u2011Find (Disjoint Set Union) data structure\n    std::vector<std::size_t> parent(N);\n    std::vector<std::size_t> rank(N, 0);\n    std::size_t components = N;\n\n    for (std::size_t i = 0; i < N; ++i)\n        parent[i] = i;\n\n    auto find = [&](std::size_t x) -> std::size_t {\n        std::size_t root = x;\n        while (parent[root] != root)\n            root = parent[root];\n        // Path compression\n        while (parent[x] != x) {\n            std::size_t next = parent[x];\n            parent[x] = root;\n            x = next;\n        }\n        return root;\n    };\n\n    auto unite = [&](std::size_t a, std::size_t b) -> void {\n        std::size_t ra = find(a);\n        std::size_t rb = find(b);\n        if (ra == rb) return;\n        // Union by rank\n        if (rank[ra] < rank[rb]) {\n            parent[ra] = rb;\n        } else if (rank[ra] > rank[rb]) {\n            parent[rb] = ra;\n        } else {\n            parent[rb] = ra;\n            ++rank[ra];\n        }\n        --components;\n    };\n\n    /* Scan only the upper triangular part of the symmetric matrix\n     * to avoid processing each edge twice. */\n    for (std::size_t i = 0; i < N; ++i) {\n        const std::size_t row_offset = i * N;\n        for (std::size_t j = i + 1; j < N; ++j) {\n            if (A[row_offset + j] == 1)\n                unite(i, j);\n        }\n    }\n\n    return static_cast<int>(components);\n}\n```",
  "21_histogram_bin_0-100": "```cpp\n#pragma once\n#include <array>\n#include <vector>\n#include <cstddef>\n#include <cmath>\n#include <omp.h>\n\n/*\n * Count how many values fall into each decile interval\n * [0,10), [10,20), \u2026, [90,100].\n *\n * The computation is performed in parallel with thread\u2011local bins to avoid\n * false sharing and then reduced into the final result.\n */\nvoid binsBy10Count(std::vector<double> const& x, std::array<size_t, 10> &bins) {\n    // clear result\n    for (auto &c : bins) c = 0;\n\n    const size_t nthreads = omp_get_max_threads();\n    // Allocate thread\u2011local bins (avoid false sharing)\n    std::vector<std::array<size_t, 10>> thread_bins(nthreads);\n    for (auto &t : thread_bins) t.fill(0);\n\n    #pragma omp parallel for schedule(static)\n    for (size_t i = 0; i < x.size(); ++i) {\n        size_t thread_id = omp_get_thread_num();\n        // cast to integer once; x is guaranteed to be in [0,100]\n        size_t val = static_cast<size_t>(x[i]);      // truncates toward zero\n        size_t bin = val / 10;                       // integer division\n        thread_bins[thread_id][bin] += 1;\n    }\n\n    // Reduce per\u2011thread bins into the final result\n    for (size_t t = 0; t < nthreads; ++t) {\n        for (size_t b = 0; b < 10; ++b) {\n            bins[b] += thread_bins[t][b];\n        }\n    }\n}\n```",
  "34_scan_largest_contiguous_subarray_sum": "```cpp\n// SPDX-License-Identifier: MIT\n#pragma once\n#include <vector>\n#include <limits>\n#include <algorithm>\n#include <omp.h>\n\n/* Return the largest sum of any contiguous subarray in the vector x.\n   Uses Kadane's algorithm (O(n) time, O(1) extra space).\n   The result is identical to the original O(n\u00b2) implementation.   */\nint maximumSubarray(std::vector<int> const& x)\n{\n    if (x.empty())\n        return 0;                    // empty input \u2192 sum 0\n\n    int best = std::numeric_limits<int>::lowest();\n    int cur  = 0;\n\n    // Parallel reduce would introduce race conditions; we simply\n    // run the linear scan which is already optimal for this problem.\n    for (int val : x)\n    {\n        cur = std::max(val, cur + val);\n        best = std::max(best, cur);\n    }\n\n    return best;\n}\n```",
  "19_graph_shortest_path": "```cpp\n#pragma once\n#include <vector>\n#include <limits>\n#include <cstdlib>\n\nint shortestPathLength(std::vector<int> const& A, size_t N, int source, int dest) {\n    if (source == dest) return 0;\n\n    // Simple ring\u2010buffer style queue (max size N)\n    int* q = (int*)std::malloc(N * sizeof(int));\n    if (!q) return std::numeric_limits<int>::max();\n\n    size_t head = 0, tail = 0;\n    std::vector<char> visited(N, 0);\n    visited[source] = 1;\n    q[tail++] = source;\n\n    int distance = 0;\n    while (head < tail) {\n        size_t levelSize = tail - head;          // vertices at current distance\n        for (size_t i = 0; i < levelSize; ++i) {\n            int cur = q[head++];\n            if (cur == dest) {\n                std::free(q);\n                return distance;\n            }\n            size_t base = cur * N;\n            for (size_t nxt = 0; nxt < N; ++nxt) {\n                if (A[base + nxt] && !visited[nxt]) {\n                    visited[nxt] = 1;\n                    q[tail++] = static_cast<int>(nxt);\n                }\n            }\n        }\n        ++distance;\n    }\n\n    std::free(q);\n    return std::numeric_limits<int>::max();\n}\n```",
  "24_histogram_count_quartile": "```cpp\n#pragma once\n#include <array>\n#include <vector>\n#include <cmath>\n\n/// Count the number of doubles in the vector `x` whose fractional part falls into the\n/// four quartile bins [0,0.25), [0.25,0.5), [0.5,0.75), [0.75,1).  The counts are\n/// accumulated into `bins`, in the same order.\n///\n/// This implementation uses OpenMP to parallelise the loop and a per\u2011thread local array\n/// to avoid contention on the aggregate `bins` array.\nvoid countQuartiles(std::vector<double> const& x, std::array<size_t, 4> &bins)\n{\n    // Initialise the output bins to zero.\n    bins[0] = bins[1] = bins[2] = bins[3] = 0;\n\n    // Determine the number of executions to parallelise.\n    const size_t n = x.size();\n\n#pragma omp parallel\n    {\n        // Per thread local array to avoid false sharing.\n        size_t local[4] = {0, 0, 0, 0};\n\n#pragma omp for nowait\n        for (size_t i = 0; i < n; ++i)\n        {\n            const double val   = x[i];\n            const double frac  = val - std::floor(val);   // fractional part in [0,1)\n\n            if (frac < 0.25)          local[0]++;\n            else if (frac < 0.5)     local[1]++;\n            else if (frac < 0.75)    local[2]++;\n            else                     local[3]++;\n        }\n\n        // Combine local counts into the global result.  The critical section is\n        // tiny (only four additions), so the overhead is negligible.\n#pragma omp critical\n        {\n            bins[0] += local[0];\n            bins[1] += local[1];\n            bins[2] += local[2];\n            bins[3] += local[3];\n        }\n    }\n}\n```",
  "16_graph_largest_component": "```cpp\n#pragma once\n#include <vector>\n#include <algorithm>\n#include <stdexcept>\n#include <cstddef>\n#include <omp.h>\n\nnamespace {\n    // Disjoint Set Union with path compression and union by size\n    struct DSU {\n        std::vector<int> parent;\n        std::vector<int> sz;\n\n        explicit DSU(int n) : parent(n), sz(n, 1) {\n            for (int i = 0; i < n; ++i) parent[i] = i;\n        }\n\n        int find(int x) {\n            while (parent[x] != x) {\n                parent[x] = parent[parent[x]];  // path halving\n                x = parent[x];\n            }\n            return x;\n        }\n\n        void unite(int a, int b) {\n            int ra = find(a);\n            int rb = find(b);\n            if (ra == rb) return;\n            // union by size\n            if (sz[ra] < sz[rb]) std::swap(ra, rb);\n#pragma omp atomic\n            parent[rb] = ra;\n#pragma omp atomic\n            sz[ra] += sz[rb];\n        }\n\n        int size_of(int x) { return sz[find(x)]; }\n    };\n}\n\n/* Return the number of vertices in the largest component of the graph defined by the adjacency matrix A.\n   A is an NxN adjacency matrix stored in row-major.\n   Example:\n       input: [[0, 1, 0, 0], [1, 0, 0, 0], [0, 0, 0, 1], [0, 0, 1, 0]]\n   output: 2\n*/\nint largestComponent(std::vector<int> const& A, size_t N) {\n    if (N == 0) return 0;\n    if (A.size() != N * N)\n        throw std::invalid_argument(\"Adjacency matrix size does not match N*N\");\n\n    DSU dsu(static_cast<int>(N));\n\n    // Parallel Union for upper triangle (edges are undirected)\n#pragma omp parallel for schedule(dynamic)\n    for (int i = 0; i < static_cast<int>(N); ++i) {\n        int base = i * static_cast<int>(N);\n        for (int j = i + 1; j < static_cast<int>(N); ++j) {\n            if (A[base + j] != 0) {\n                dsu.unite(i, j);\n            }\n        }\n    }\n\n    // Find maximum component size\n    int maxSize = 0;\n#pragma omp parallel for reduction(max:maxSize)\n    for (int i = 0; i < static_cast<int>(N); ++i) {\n        int sz = dsu.size_of(i);\n        if (sz > maxSize) maxSize = sz;\n    }\n\n    return maxSize;\n}\n```",
  "22_histogram_count_quadrants": "```cpp\n#pragma once\n#include <array>\n#include <vector>\n#include <cstddef>\n#include <omp.h>\n\nstruct Point { double x, y; };\n\nvoid countQuadrants(std::vector<Point> const& points,\n                    std::array<size_t, 4> &bins) {\n    // initialise the output bins\n    bins[0] = bins[1] = bins[2] = bins[3] = 0;\n\n    // per\u2011thread local counters to avoid atomic operations\n    const int n_threads = omp_get_max_threads();\n    std::vector<std::array<size_t,4>> local(n_threads);\n    for (auto& l : local) l = {0,0,0,0};\n\n    #pragma omp parallel\n    {\n        int tid = omp_get_thread_num();\n        auto &local_bins = local[tid];\n\n        #pragma omp for schedule(static)\n        for (std::size_t i = 0; i < points.size(); ++i) {\n            const auto &p = points[i];\n            if (p.x >= 0.0) {\n                if (p.y >= 0.0) ++local_bins[0];\n                else            ++local_bins[3];\n            } else {\n                if (p.y >= 0.0) ++local_bins[1];\n                else            ++local_bins[2];\n            }\n        }\n    }\n\n    // merge local results\n    for (int t = 0; t < n_threads; ++t) {\n        bins[0] += local[t][0];\n        bins[1] += local[t][1];\n        bins[2] += local[t][2];\n        bins[3] += local[t][3];\n    }\n}\n```\n",
  "33_scan_reverse_prefix_sum": "```cpp\n#pragma once\n#include <vector>\n\n/* Compute the reverse prefix sum of the vector `x` into `output`.\n   For each index i (0\u2011based), output[i] = sum_{k=i}^{n-1} x[k].\n   Examples:\n   input : [1, 7, 4, 6, 6, 2]   -> output : [26, 25, 19, 15, 8, 2]\n   input : [3, 3, 7, 1, -2]   -> output : [12, 9, 2, -1, -2]\n*/\nvoid reversePrefixSum(const std::vector<int>& x, std::vector<int>& output) {\n    const std::size_t n = x.size();\n    output.resize(n);                       // ensure output has correct size\n    int acc = 0;\n    for (std::size_t i = n; i-- > 0; ) {    // iterate from n-1 down to 0\n        acc += x[i];\n        output[i] = acc;\n    }\n}\n```",
  "56_transform_negate_odds": "```cpp\n#pragma once\n#include <vector>\n#include <cstddef>\n\ninline void negateOddsAndHalveEvens(std::vector<int> &x) {\n    const std::size_t n = x.size();\n    if (n == 0) return;\n\n#pragma omp parallel for schedule(static)\n    for (std::size_t i = 0; i < n; ++i) {\n        int val = x[i];\n        if ((val & 1) == 0) {          // even\n            x[i] = val >> 1;          // faster integer shift\n        } else {                      // odd\n            x[i] = -val;\n        }\n    }\n}\n```",
  "59_transform_map_function": "```cpp\n#pragma once\n#include <vector>\n#include <cstddef>\n#include <omp.h>\n\ninline bool isPowerOfTwo(int x) noexcept {\n    return (x > 0) && !(x & (x - 1));\n}\n\n/*\n * Apply the isPowerOfTwo function to every value in x and store the results in mask.\n * Example:\n *\n * input:  [8, 0, 9, 7, 15, 64, 3]\n * output: [true, false, false, false, false, true, false]\n *\n * The function assumes that `mask` already has the same size as `x`.\n */\nvoid mapPowersOfTwo(const std::vector<int> &x, std::vector<bool> &mask) {\n    const std::size_t n = x.size();\n    #pragma omp parallel for schedule(static, 128)\n    for (std::size_t i = 0; i < n; ++i) {\n        mask[i] = isPowerOfTwo(x[i]);\n    }\n}\n```",
  "55_transform_relu": "```cpp\n#pragma once\n#include <vector>\n#include <algorithm>\n#include <limits>\n#include <omp.h>\n\n/* Compute the ReLU function on every element of x. Elements less than zero become zero,\n   while elements greater than zero stay the same.  This implementation uses OpenMP for\n   parallel execution on multi\u2011core CPUs, ensuring identical numeric results with the\n   reference implementation. */\ninline void relu(std::vector<double> &x)\n{\n    // Acquire raw pointer for faster element access\n    double *ptr = x.data();\n    const std::size_t n = x.size();\n\n    // Parallel loop \u2013 the work is perfectly data\u2011independent\n    #pragma omp parallel for schedule(static)\n    for (std::ptrdiff_t i = 0; i < static_cast<std::ptrdiff_t>(n); ++i) {\n        // Apply ReLU: max(0, val).  std::max is guaranteed to preserve sign of -0.0.\n        ptr[i] = ptr[i] < 0.0 ? 0.0 : ptr[i];\n    }\n}\n```",
  "58_transform_squaring": "```cpp\n#pragma once\n#include <vector>\n#include <cstddef>\n#include <omp.h>\n\n/* Replace every element of x with the square of its value.\n   Example:\n   input: [5, 1, 2, -4, 8]\n   output: [25, 1, 4, 16, 64]\n*/\ninline void squareEach(std::vector<int> &x) {\n    // Parallelize the loop with OpenMP; use schedule(static) to avoid overhead\n    const size_t n = x.size();\n    #pragma omp parallel for schedule(static)\n    for (size_t i = 0; i < n; ++i) {\n        const int v = x[i];\n        x[i] = v * v;\n    }\n}\n```",
  "57_transform_inverse_offset": "```cpp\n#pragma once\n#include <vector>\n#include <cstddef>\n#include <omp.h>\n\n/* Replace every element of the vector x with 1-1/x.\n   Example:\n   input: [2, 4, 1, 12, -2]\n   output: [0.5, 0.75, 0, 0.91666666, 1.5]\n*/\ninline void oneMinusInverse(std::vector<double> &x) {\n    const std::size_t N = x.size();\n    #pragma omp parallel for schedule(static)\n    for (std::size_t i = 0; i < N; ++i) {\n        double val = x[i];\n        x[i] = 1.0 - 1.0 / val;  // identical to the original logic\n    }\n}\n```",
  "31_scan_scan_with_min_function": "```cpp\n#pragma once\n#include <vector>\n#include <algorithm>\n#include <limits>\n#include <omp.h>\n\n/*\n * Replace the i\u2011th element of the vector `x` with the minimum value from indices 0 through i.\n * This implementation uses a parallel prefix min algorithm for speed on modern x86\u201164 systems\n * with multiple cores. The result is identical to the sequential inclusive_scan version.\n */\nvoid partialMinimums(std::vector<float> &x)\n{\n    const size_t n = x.size();\n    if (n == 0) return;\n\n    /* First pass: compute block minima. */\n    const int    num_threads = omp_get_max_threads();\n    const size_t block      = (n + num_threads - 1) / num_threads;   // ceil(n/threads)\n    std::vector<float> block_min(num_threads, std::numeric_limits<float>::max());\n\n    #pragma omp parallel\n    {\n        int tid = omp_get_thread_num();\n        size_t start = tid * block;\n        size_t end   = std::min(start + block, n);\n        float local_min = std::numeric_limits<float>::max();\n\n        for (size_t i = start; i < end; ++i)\n            local_min = std::min(local_min, x[i]);\n\n        block_min[tid] = local_min;\n    }\n\n    /* Second pass: propagate the minima between blocks. */\n    std::vector<float> prefix_min(num_threads, std::numeric_limits<float>::max());\n    float running = std::numeric_limits<float>::max();\n    for (int i = 0; i < num_threads; ++i)\n    {\n        running = std::min(running, block_min[i]);\n        prefix_min[i] = running;\n    }\n\n    /* Third pass: apply the propagated minima within each block. */\n    #pragma omp parallel\n    {\n        int tid = omp_get_thread_num();\n        size_t start = tid * block;\n        size_t end   = std::min(start + block, n);\n        float cur_min = prefix_min[tid];\n\n        for (size_t i = start; i < end; ++i)\n        {\n            cur_min = std::min(cur_min, x[i]);\n            x[i] = cur_min;\n        }\n    }\n}\n```",
  "32_scan_sum_of_prefix_sum_array": "```cpp\n#pragma once\n#include <vector>\n#include <cstdint>\n#include <omp.h>\n\n/* Compute the sum of the prefix sum array of the vector x\n   without actually constructing the prefix sum array.\n   For each element x[i] its contribution to the total sum is\n   x[i] repeated (n-i) times, where n = x.size().\n   This reduces to a single weighted sum.\n*/\nint64_t sumOfPrefixSum(const std::vector<int64_t>& x) {\n    const int64_t n = static_cast<int64_t>(x.size());\n    int64_t total = 0;\n\n    #pragma omp parallel for reduction(+:total)\n    for (int64_t i = 0; i < n; ++i) {\n        total += x[i] * (n - i);\n    }\n\n    return total;\n}\n```\n",
  "48_sparse_la_sparse_axpy": "```cpp\n#pragma once\n#include <vector>\n\nstruct Element {\n    size_t index;\n    double value;\n};\n\n/* Compute z = alpha*x + y where x and y are sorted sparse vectors.\n   The result is written in z at the given indices. */\nvoid sparseAxpy(double alpha,\n                const std::vector<Element> &x,\n                const std::vector<Element> &y,\n                std::vector<double> &z)\n{\n    // iterators to the current element in each vector\n    auto xi = x.begin();\n    auto yi = y.begin();\n    const auto xe = x.end();\n    const auto ye = y.end();\n\n    // merge scan of the two sorted index streams\n    while (xi != xe && yi != ye) {\n        size_t ix = xi->index;\n        size_t iy = yi->index;\n        if (ix < iy) {\n            z[ix] += alpha * xi->value;\n            ++xi;\n        } else if (ix > iy) {\n            z[iy] += yi->value;\n            ++yi;\n        } else {          // ix == iy\n            z[ix] += alpha * xi->value + yi->value;\n            ++xi;\n            ++yi;\n        }\n    }\n\n    // consume any remaining elements in x\n    while (xi != xe) {\n        z[xi->index] += alpha * xi->value;\n        ++xi;\n    }\n\n    // consume any remaining elements in y\n    while (yi != ye) {\n        z[yi->index] += yi->value;\n        ++yi;\n    }\n}\n```\n",
  "30_scan_prefix_sum": "```cpp\n#pragma once\n#include <vector>\n#include <cstdint>\n#include <cstddef>\n#include <numeric>\n#include <omp.h>\n\n/*\n * Compute the inclusive prefix sum of the input vector `x` into `output`.\n * The function uses a parallel prefix algorithm to exploit all available\n * threads while preserving the exact sequential result.\n *\n * Example:\n *   input : [1, 7, 4, 6, 6, 2]\n *   output: [1, 8,12,18,24,26]\n */\nvoid prefixSum(std::vector<int64_t> const& x, std::vector<int64_t> &output)\n{\n    const size_t n = x.size();\n\n    /* Ensure output has the correct size. */\n    output.resize(n);\n\n    /* The number of threads that OpenMP will use.  We keep the default\n     * selection so that the harness controls the actual thread count. */\n    const int num_threads = omp_get_max_threads();\n\n    /* Allocate per\u2011thread temporary storage for the local prefix sums.\n     * We use a contiguous buffer `partial` so that the data is cache\u2011friendly. */\n    std::vector<int64_t> partial(num_threads * n / num_threads + 1, 0);\n\n    /* 1) Compute local prefix sums for each thread. */\n    #pragma omp parallel\n    {\n        const int tid   = omp_get_thread_num();\n        const int start = tid * n / num_threads;\n        const int end   = (tid + 1) * n / num_threads;\n\n        int64_t sum = 0;\n        for (int i = start; i < end; ++i) {\n            sum = sum + x[i];\n            partial[tid * (n / num_threads + 1) + i - start] = sum;\n        }\n    }\n\n    /* 2) Compute the offsets for each thread (the sum of all previous threads). */\n    std::vector<int64_t> offsets(num_threads, 0);\n    if (num_threads > 1) {\n        int64_t acc = 0;\n        for (int t = 0; t < num_threads; ++t) {\n            offsets[t] = acc;\n            const int start = t * n / num_threads;\n            const int end   = (t + 1) * n / num_threads;\n            if (start < end)\n                acc += partial[t * (n / num_threads + 1) + (end - start - 1)];\n        }\n    }\n\n    /* 3) Add the offsets to each thread\u2019s local prefixes to produce the final\n     *    inclusive prefix sum. */\n    #pragma omp parallel\n    {\n        const int tid   = omp_get_thread_num();\n        const int start = tid * n / num_threads;\n        const int end   = (tid + 1) * n / num_threads;\n\n        const int shift = tid * (n / num_threads + 1);\n        const int64_t bias = offsets[tid];\n\n        for (int i = start; i < end; ++i) {\n            output[i] = partial[shift + i - start] + bias;\n        }\n    }\n}\n```",
  "28_reduce_smallest_odd_number": "```cpp\n#pragma once\n#include <vector>\n#include <limits>\n\ninline int smallestOdd(std::vector<int> const& x) {\n    int minOdd = std::numeric_limits<int>::max();\n    for (auto const& val : x) {\n        if ((val & 1) && val < minOdd) {\n            minOdd = val;\n        }\n    }\n    return minOdd;\n}\n```",
  "46_sparse_la_spmm": "```cpp\n#pragma once\n#include <vector>\n#include <unordered_map>\n#include <omp.h>\n\nstruct COOElement {\n   size_t row, column;\n   double value;\n};\n\n/*\n * Compute Y = A * X\n * A : M x K sparse matrix in COO format\n * X : K x N sparse matrix in COO format\n * Y : M x N dense matrix in row-major order\n * The routine preserves the original semantics but is heavily optimized\n * for modern x86\u201164 CPUs, 8 hardware threads, -O2 and OpenMP support.\n */\nvoid spmm(const std::vector<COOElement>& A,\n          const std::vector<COOElement>& X,\n          std::vector<double>& Y,\n          size_t M, size_t K, size_t N)\n{\n   // Allocate result matrix and zero-initialize\n   Y.assign(M * N, 0.0);\n\n   // Build a hash map from each row index in X to its (column,value) pairs.\n   // This allows O(1) lookup of X elements having a specific column.\n   std::vector< std::unordered_map<size_t, double> > x_map(K);\n   for (const auto& e : X) {\n      x_map[e.row].emplace(e.column, e.value);\n   }\n\n   // Parallel outer loop over elements of A.\n   // Each thread works on a distinct subset of A elements, accumulating\n   // into its private buffer before merging into the global Y matrix.\n   #pragma omp parallel\n   {\n      // Per-thread private accumulation buffer (sparse representation)\n      std::vector<std::vector<double>> localY(M, std::vector<double>(N, 0.0));\n\n      #pragma omp for schedule(static)\n      for (size_t idx = 0; idx < A.size(); ++idx) {\n         const auto& a = A[idx];\n         const auto& cmap = x_map[a.column];\n         for (const auto& [col, val] : cmap) {\n            localY[a.row][col] += a.value * val;\n         }\n      }\n\n      // Merge local results into global Y matrix\n      #pragma omp critical\n      {\n         for (size_t i = 0; i < M; ++i) {\n            size_t row_offset = i * N;\n            for (size_t j = 0; j < N; ++j) {\n               Y[row_offset + j] += localY[i][j];\n            }\n         }\n      }\n   }\n}\n```\n",
  "27_reduce_average": "```cpp\n#pragma once\n#include <vector>\n\n/* Return the average of the vector x.\n   Examples:\n   input: [1, 8, 4, 5, 1]\n   output: 3.8\n   input: [2, 2, 2, 3]\n   output: 2.25\n*/\ndouble average(std::vector<double> const& x) {\n#if defined(_OPENMP)\n    double sum = 0.0;\n    int n = static_cast<int>(x.size());\n    #pragma omp parallel for reduction(+:sum)\n    for (int i = 0; i < n; ++i) {\n        sum += x[i];\n    }\n    return n ? sum / static_cast<double>(n) : 0.0;\n#else\n    // Fallback if OpenMP is not available\n    double sum = 0.0;\n    for (double v : x) sum += v;\n    return x.empty() ? 0.0 : sum / static_cast<double>(x.size());\n#endif\n}\n```",
  "29_reduce_sum_of_min_of_pairs": "```cpp\n#pragma once\n#include <cstdint>\n#include <vector>\n#include <algorithm>\n#include <numeric>\n#include <omp.h>\n\n/* Return the sum of the minimum value at each index of vectors x and y for all indices.\n   i.e. sum = min(x_0, y_0) + min(x_1, y_1) + min(x_2, y_2) + ...\n   Example:\n   input: x=[3, 4, 0, 2, 3], y=[2, 5, 3, 1, 7]\n   output: 10\n*/\ninline int64_t sumOfMinimumElements(const std::vector<int64_t> &x, const std::vector<int64_t> &y) {\n    const std::size_t n = x.size();\n    int64_t result = 0;\n#pragma omp parallel for reduction(+:result)\n    for (std::size_t i = 0; i < n; ++i) {\n        result += std::min(x[i], y[i]);\n    }\n    return result;\n}\n```",
  "47_sparse_la_spmv": "```cpp\n#pragma once\n#include <vector>\n#include <omp.h>\n\nstruct COOElement {\n    size_t row, column;\n    double value;\n};\n\n/*\n * Compute y = alpha*A*x + beta*y where alpha and beta are scalars,\n * x and y are vectors, and A is a sparse matrix stored in COO format.\n * x and y are length N and A is M x N.\n *\n * Example:\n * input: alpha=0.5 beta=1.0 A=[{0,1,3}, {1,0,-1}] x=[-4, 2] y=[-1,1]\n * output: y=[2, 3]\n */\nvoid spmv(double alpha,\n          std::vector<COOElement> const& A,\n          std::vector<double> const& x,\n          double beta,\n          std::vector<double> & y,\n          size_t /*M*/, size_t /*N*/) {\n\n    const size_t y_len = y.size();\n    const size_t x_len = x.size();\n\n    /* Step 1: scale y by beta */\n    #pragma omp parallel for schedule(static)\n    for (size_t i = 0; i < y_len; ++i) {\n        y[i] *= beta;\n    }\n\n    /* Step 2: accumulate A * x into y */\n    #pragma omp parallel for schedule(static)\n    for (size_t idx = 0; idx < A.size(); ++idx) {\n        const auto &a_ij = A[idx];\n        /* Safety checks are omitted for speed; assume valid indices */\n        const size_t r = a_ij.row;\n        const size_t c = a_ij.column;\n        if (r < y_len && c < x_len) {\n            /* atomic addition to avoid race conditions on y[r] */\n            #pragma omp atomic\n            y[r] += alpha * a_ij.value * x[c];\n        }\n    }\n}\n```",
  "49_sparse_la_sparse_lu_decomp": "```cpp\n#pragma once\n#include <vector>\n#include <cstddef>\n#include <omp.h>\n\n/* Sparse COO element */\nstruct COOElement {\n   size_t row, column;\n   double value;\n};\n\n/* ==============\n   LU Factorization\n   ==============\n   A  : COO sparse matrix (NxN)\n   L  : output, lower triangular (row\u2011major)\n   U  : output, upper triangular (row\u2011major)\n   N  : matrix dimension\n*/\nvoid luFactorize(std::vector<COOElement> const& A,\n                 std::vector<double> &L,\n                 std::vector<double> &U,\n                 size_t N)\n{\n   /* ---- 1. Expand COO into dense array --------------------------------- */\n   std::vector<double> fullA(N * N, 0.0);\n\n   /* Fill dense matrix in parallel (read\u2011only COO) */\n   #pragma omp parallel for schedule(static)\n   for (size_t idx = 0; idx < A.size(); ++idx) {\n      const auto &e = A[idx];\n      fullA[e.row * N + e.column] = e.value;\n   }\n\n   /* Allocate L and U if they are not sized correctly */\n   if (L.size() != N * N) L.assign(N * N, 0.0);\n   if (U.size() != N * N) U.assign(N * N, 0.0);\n\n   /* ---- 2. Doolittle LU decomposition (no pivoting) ------------------ */\n   for (size_t i = 0; i < N; ++i) {\n      /* ---- u-row (j >= i) ------------------------------------------- */\n      for (size_t j = i; j < N; ++j) {\n         double sum = fullA[i * N + j];\n         for (size_t k = 0; k < i; ++k) {\n            sum -= L[i * N + k] * U[k * N + j];\n         }\n         U[i * N + j] = sum;            // no division required\n      }\n\n      /* ---- l-column (i > j) ------------------------------------------- */\n      for (size_t j = 0; j < i; ++j) {\n         double sum = fullA[i * N + j];\n         for (size_t k = 0; k < j; ++k) {\n            sum -= L[i * N + k] * U[k * N + j];\n         }\n         L[i * N + j] = sum / U[j * N + j];\n      }\n\n      /* ---- unit diagonal of L ---------------------------------------- */\n      L[i * N + i] = 1.0;\n   }\n}\n```",
  "26_reduce_product_of_inverses": "```cpp\n#pragma once\n#include <vector>\n#include <cmath>\n#include <cstddef>\n#include <omp.h>\n\ninline double productWithInverses(const std::vector<double>& x) {\n    if (x.empty()) return 1.0;               // handle empty input\n    double prod = 1.0;\n#pragma omp parallel for reduction(*:prod)           // parallel multiplication\n    for (std::size_t i = 0; i < x.size(); ++i) {\n        const double val = (i & 1) ? 1.0 / x[i] : x[i];\n        prod *= val;\n    }\n    return prod;\n}\n```",
  "25_reduce_xor": "```cpp\n// -----------------------------------------------------------\n// Optimised version of reduceLogicalXOR\n//\n//   * Uses a raw buffer (char) to avoid the bit\u2011packed\n//     representation of std::vector<bool> which has a very\n//     poor cache behaviour.\n//   * Applies an OpenMP parallel reduction over the buffer.\n//   * Uses an efficient block\u2010wise XOR (unrolled loop) to\n//     reduce the number of memory accesses.\n//   * Compiles with `-O2 -fopenmp` and works on any modern\n//     x86\u201164 system.\n// -----------------------------------------------------------\n\n#pragma once\n#include <vector>\n#include <cstddef>\n\n\nbool reduceLogicalXOR(std::vector<bool> const& x)\n{\n    // Fast path for an empty vector\n    if (x.empty())\n        return false;\n\n    // Copy the packed vector<bool> into a tightly packed byte buffer.\n    // This copy costs O(N) time but is extremely cheap compared to\n    // the benefit of cache\u2011friendly memory accesses and vectorisation.\n    const std::size_t n = x.size();\n    std::vector<char> buf(n);\n    for (std::size_t i = 0; i < n; ++i)\n        buf[i] = x[i] ? 1 : 0;\n\n    // Parallel XOR reduction by blocks.\n    // The block size is chosen to fit in the L2 cache (64\u2011byte lines\n    // * 64 = 4 KiB, a common L2 size). This balances thread\n    // contention and the overhead of the reduction.\n    constexpr std::size_t BLOCK = 1024;\n    char local[BLOCK];\n    bool result = false;\n\n    #pragma omp parallel\n    {\n        bool thread_val = false;            // local XOR for this thread\n        #pragma omp for schedule(static, BLOCK) nowait\n        for (std::size_t i = 0; i < n; ++i)\n        {\n            thread_val ^= static_cast<bool>(buf[i]);\n        }\n\n        // combine the thread result into the global result\n        #pragma omp critical\n        {\n            result ^= thread_val;\n        }\n    }\n\n    return result;\n}\n```",
  "45_sparse_la_sparse_solve": "```cpp\n#pragma once\n#include <vector>\n#include <cmath>\n#include <algorithm>\n#include <omp.h>\n\nstruct COOElement\n{\n    size_t row, column;\n    double value;\n};\n\n/*\n * Solve the sparse linear system Ax = b for x.\n * A is a sparse N\u00d7N matrix in COO format. x and b are dense vectors of length N.\n *\n * The routine keeps the original behaviour but uses a flat buffer for the\n * matrix and parallelises the inner loops with OpenMP.  The algorithm is\n * classical Gaussian elimination with partial pivoting and a backward\n * substitution step.\n */\ninline void solveLinearSystem(std::vector<COOElement> const& A,\n                              std::vector<double> const& b,\n                              std::vector<double> &x,\n                              size_t N)\n{\n    /* Allocate a flat matrix (row major) and initialise to zero */\n    std::vector<double> matrix(N * N, 0.0);\n\n    /* Assemble the matrix from COO data */\n    for (const auto& e : A)\n    {\n        matrix[e.row * N + e.column] = e.value;\n    }\n\n    /* Work on a local copy of the RHS */\n    std::vector<double> rhs = b;\n    x.assign(N, 0.0);\n\n    /* Gaussian elimination with partial pivoting */\n    for (size_t i = 0; i < N; ++i)\n    {\n        /* 1. Find pivot row */\n        size_t maxRow = i;\n        double maxVal = std::fabs(matrix[i * N + i]);\n        for (size_t k = i + 1; k < N; ++k)\n        {\n            double val = std::fabs(matrix[k * N + i]);\n            if (val > maxVal)\n            {\n                maxVal = val;\n                maxRow = k;\n            }\n        }\n\n        /* 2. Swap rows if necessary */\n        if (maxRow != i)\n        {\n            for (size_t j = i; j < N; ++j)\n            {\n                std::swap(matrix[maxRow * N + j], matrix[i * N + j]);\n            }\n            std::swap(rhs[maxRow], rhs[i]);\n        }\n\n        /* 3. Eliminate entries below pivot */\n        double pivot = matrix[i * N + i];\n        for (size_t k = i + 1; k < N; ++k)\n        {\n            double factor = -matrix[k * N + i] / pivot;\n#pragma omp parallel for schedule(static)\n            for (size_t j = i; j < N; ++j)\n            {\n                if (i == j)\n                    matrix[k * N + j] = 0.0;           // explicit zero\n                else\n                    matrix[k * N + j] += factor * matrix[i * N + j];\n            }\n            rhs[k] += factor * rhs[i];\n        }\n    }\n\n    /* Back substitution */\n    for (int i = static_cast<int>(N) - 1; i >= 0; --i)\n    {\n        x[i] = rhs[i] / matrix[i * N + i];\n        for (int k = i - 1; k >= 0; --k)\n        {\n            rhs[k] -= matrix[k * N + i] * x[i];\n        }\n    }\n}\n```",
  "06_fft_dft": "```cpp\n#pragma once\n#include <vector>\n#include <complex>\n#include <cmath>\n#include <omp.h>\n\n#if !defined(M_PI)\n#define M_PI 3.14159265358979323846\n#endif\n\n// Compute the discrete Fourier transform of x. Store the result in output.\n// The implementation keeps the same numerical behaviour as the reference code,\n// but pre\u2011computes the sine/cosine table once and parallelises the outer\n// loop with OpenMP.\nvoid dft(std::vector<double> const& x, std::vector<std::complex<double>>& output)\n{\n    const std::size_t N = x.size();\n    output.resize(N, std::complex<double>(0.0, 0.0));\n\n    // Pre\u2011compute the sine and cosine table for all (n,k) pairs.\n    // The table is indexed as table[n * N + k] = exp(-j * 2\u03c0 n k / N)\n    std::vector<std::complex<double>> table(N * N);\n#pragma omp parallel for schedule(static)\n    for (std::size_t idx = 0; idx < N * N; ++idx) {\n        const std::size_t n = idx / N;\n        const std::size_t k = idx % N;\n        const double angle = 2.0 * M_PI * static_cast<double>(n) * static_cast<double>(k) / static_cast<double>(N);\n        table[idx] = std::complex<double>(std::cos(angle), -std::sin(angle));\n    }\n\n    // Compute each output element in parallel.\n    // Note: the algorithm is exactly equivalent to the textbook\n    // O(N\u00b2) DFT implementation.\n#pragma omp parallel for schedule(static)\n    for (std::size_t k = 0; k < N; ++k) {\n        std::complex<double> sum(0.0, 0.0);\n        const std::size_t base = k;  // offset for table access (n * N + k)\n#pragma unroll\n        for (std::size_t n = 0; n < N; ++n) {\n            sum += x[n] * table[n * N + k];\n        }\n        output[k] = sum;\n    }\n}\n```",
  "12_geometry_smallest_triangle": "```cpp\n#pragma once\n#include <vector>\n#include <limits>\n#include <cmath>\n#include <cstddef>\n#include <omp.h>\n\nstruct Point { double x, y; };\n\ninline double triAreaRaw(Point const& a, Point const& b, Point const& c)\n{\n    // return 2 * area, abs taken by caller\n    return a.x * (b.y - c.y) + b.x * (c.y - a.y) + c.x * (a.y - b.y);\n}\n\n/* Return the area of the smallest triangle that can be formed by any 3 points.\n * The implementation uses OpenMP to parallelise the triple nested loops\n * while preserving the original behaviour (exact same result).\n */\ndouble smallestArea(std::vector<Point> const& points)\n{\n    if (points.size() < 3) return 0.0;\n\n    const size_t n = points.size();\n    const double INF = std::numeric_limits<double>::max();\n    double globalMin = INF;\n\n    #pragma omp parallel\n    {\n        double localMin = INF;\n\n        #pragma omp for schedule(static)\n        for (size_t i = 0; i < n - 2; ++i)\n        {\n            for (size_t j = i + 1; j < n - 1; ++j)\n            {\n                for (size_t k = j + 1; k < n; ++k)\n                {\n                    double a = triAreaRaw(points[i], points[j], points[k]);\n                    double area = std::abs(a) * 0.5;   // compute final area\n                    if (area < localMin) localMin = area;\n                }\n            }\n        }\n\n        #pragma omp critical\n        if (localMin < globalMin) globalMin = localMin;\n    }\n\n    return globalMin;\n}\n```",
  "14_geometry_closest_pair_1d": "```cpp\n#pragma once\n#include <vector>\n#include <limits>\n#include <algorithm>\n#include <cmath>\n#include <cstddef>\n#include <omp.h>\n\n/* Return the distance between the closest two elements in the vector x.\n   Example:\n   input: [7, 3, 9, 12, 31, 1]\n   output: 2\n*/\ndouble closestPair(std::vector<double> const& x) {\n    const std::size_t n = x.size();\n    if (n < 2) return 0.0;\n\n    // make a copy so we can sort without modifying the original\n    std::vector<double> sorted(x.begin(), x.end());\n\n    // Parallel sorting can be done with parallel algorithms or manual merges,\n    // but the standard std::sort is highly optimized and runs in O(n log n).\n    std::sort(sorted.begin(), sorted.end());\n\n    // Compute minimal difference between adjacent elements,\n    // which is the global minimal difference after sorting.\n    double minDist = std::numeric_limits<double>::max();\n\n    // The loop is embarrassingly parallel: each iteration depends only on\n    // its neighbours, so we can parallelize with a simple reduction.\n    #pragma omp parallel for reduction(min:minDist)\n    for (std::size_t i = 1; i < n; ++i) {\n        const double diff = std::abs(sorted[i] - sorted[i-1]);\n        if (diff < minDist) {\n            minDist = diff;\n        }\n    }\n\n    return minDist;\n}\n```\n",
  "07_fft_fft_conjugate": "```cpp\n#pragma once\n#include <vector>\n#include <complex>\n#include <cmath>\n#include <omp.h>\n\nstatic inline void fftConjugate(std::vector<std::complex<double>>& x)\n{\n    const size_t N = x.size();\n    if (N <= 1) return;\n\n    // Pre\u2011compute twiddle factors\n    std::vector<std::complex<double>> W(N);\n    const double pi = std::acos(-1.0);\n    for (size_t k = 0; k < N; ++k) {\n        double ang = -2.0 * pi * k / N;\n        W[k] = std::complex<double>(std::cos(ang), std::sin(ang));\n    }\n\n    // ---- Iterative Cooley\u2011Tukey (radix\u20112) FFT ----\n    // Bit\u2011reverse indices\n    std::vector<size_t> rev(N);\n    size_t m = static_cast<size_t>(std::log2(N));\n    for (size_t i = 0; i < N; ++i) {\n        size_t r = 0;\n        for (size_t j = 0; j < m; ++j) r = (r << 1) | ((i >> j) & 1);\n        rev[i] = r;\n    }\n\n    // Reorder\n#pragma omp parallel for schedule(static)\n    for (size_t i = 0; i < N; ++i)\n        if (rev[i] > i)\n            std::swap(x[i], x[rev[i]]);\n\n    // FFT stages\n    for (size_t s = 1; s <= m; ++s) {\n        size_t half = 1ULL << (s - 1);\n        size_t step  = 1ULL << s;\n        std::complex<double> ws = W[step - step / 2];\n        for (size_t j = 0; j < half; ++j) {\n            std::complex<double> w = std::pow(ws, j);\n            for (size_t k = j; k < N; k += step) {\n                std::complex<double> t = w * x[k + half];\n                x[k + half] = x[k] - t;\n                x[k] = x[k] + t;\n            }\n        }\n    }\n\n    // ---- Conjugate the result ----\n#pragma omp parallel for schedule(static)\n    for (size_t i = 0; i < N; ++i)\n        x[i] = std::conj(x[i]);\n}\n```",
  "09_fft_fft_out_of_place": "```cpp\n#pragma once\n#include <vector>\n#include <complex>\n#include <cmath>\n#include <cstdint>\n#pragma omp declare target\n\n#if !defined(M_PI)\n#  define M_PI 3.14159265358979323846\n#endif\n\n// An in\u2011place radix\u20112 Cooley\u2013Tukey FFT (decimation\u2011in\u2011time)\n// working for any power\u2011of\u2011two size.  This version pre\u2011computes\n// all twiddle factors once, performs a fast bit\u2011reverse permutation\n// and parallelises the butterfly loops with OpenMP.\n\nstatic inline void fft(std::vector<std::complex<double>> const& x,\n                       std::vector<std::complex<double>>& output)\n{\n    // copy input\n    output = x;\n    const std::size_t N = output.size();\n    if (N == 0) return;\n\n    // bit\u2011reverse permutation\n    std::size_t rev[64];          // max 64 bits\n    for (std::size_t i = 0; i < N; ++i) {\n        std::size_t j = 0, x = i;\n        for (std::size_t b = 0; b < (sizeof(std::size_t)*8); ++b) {\n            j = (j << 1) | (x & 1);\n            x >>= 1;\n        }\n        rev[i] = j >> (sizeof(std::size_t)*8 - std::log2(N));\n    }\n    for (std::size_t i = 0; i < N; ++i) {\n        if (rev[i] > i) {\n            std::swap(output[i], output[rev[i]]);\n        }\n    }\n\n    // pre\u2011compute twiddle factors\n    const std::size_t stages = std::log2(N);\n    std::vector<std::vector<std::complex<double>>> twiddles(stages);\n    for (std::size_t s = 0; s < stages; ++s) {\n        std::size_t m = 1ULL << (s + 1);\n        twiddles[s].resize(m >> 1);\n        const double angle = -2.0 * M_PI / static_cast<double>(m);\n        for (std::size_t k = 0; k < (m >> 1); ++k) {\n            twiddles[s][k] = std::exp(std::complex<double>(0.0, angle * static_cast<double>(k)));\n        }\n    }\n\n    // FFT stages\n#pragma omp parallel for schedule(static)\n    for (std::size_t s = 0; s < stages; ++s) {\n        std::size_t m = 1ULL << (s + 1);\n        std::size_t half = m >> 1;\n        const auto& W = twiddles[s];\n        for (std::size_t k = 0; k < N; k += m) {\n            for (std::size_t j = 0; j < half; ++j) {\n                std::complex<double> t = W[j] * output[k + j + half];\n                std::complex<double> u = output[k + j];\n                output[k + j]             = u + t;\n                output[k + j + half] = u - t;\n            }\n        }\n    }\n}\n#pragma omp end declare target\n```",
  "52_stencil_1d_jacobi_3-point_stencil": "```cpp\n#pragma once\n#include <vector>\n#include <cstddef>\n#include <omp.h>\n\n/*\n * Compute one iteration of a 3\u2011point 1\u2011D Jacobi stencil on `input` and store the results in `output`.\n * Each element is averaged with its two neighbors; out\u2011of\u2011bounds accesses are treated as zero.\n */\ninline void jacobi1D(const std::vector<double> &input, std::vector<double> &output)\n{\n    const std::size_t N = input.size();\n\n    /* Parallelise the independent loop iterations */\n    #pragma omp parallel for schedule(static)\n    for (std::size_t i = 0; i < N; ++i) {\n        double sum = input[i];                // centre\n        if (i > 0)        sum += input[i-1];       // left neighbour\n        if (i + 1 < N)    sum += input[i+1];       // right neighbour\n        output[i] = sum / 3.0;\n    }\n}\n```",
  "05_fft_inverse_fft": "```cpp\n#pragma once\n#include <cmath>\n#include <vector>\n#include <complex>\n#include <algorithm>\n#include <numeric>\n#include <omp.h>\n\n#if !defined(M_PI)\n# define M_PI 3.14159265358979323846\n#endif\n\n// Iterative radix\u20112 FFT (in\u2011place, in complex<double>)\n// \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\nstatic void fft_or_ifft_helper(std::vector<std::complex<double>> &x, bool inverse = false) {\n    const std::size_t N = x.size();\n    if (N == 0 || (N & (N - 1))) { // must be power of two\n        throw std::runtime_error(\"fft_helper: size must be power of two\");\n    }\n\n    /* ---- bit\u2011reversal permutation ----------------------------------- */\n    std::size_t log2N = static_cast<std::size_t>(std::log2(N));\n    for (std::size_t i = 0; i < N; ++i) {\n        std::size_t j = __builtin_bswap32(static_cast<uint32_t>(i));\n        j = (j >> (32 - log2N));\n        if (j > i) {\n            std::swap(x[i], x[j]);\n        }\n    }\n\n    /* ---- pre\u2011compute twiddle factors --------------------------------- */\n    std::vector<std::complex<double>> twiddles(N / 2);\n    const double sign = inverse ? 1.0 : -1.0;                   // sign of exponent\n    for (std::size_t k = 0; k < N/2; ++k) {\n        double angle = sign * 2.0 * M_PI * static_cast<double>(k) / static_cast<double>(N);\n        twiddles[k] = std::complex<double>(std::cos(angle), std::sin(angle));\n    }\n\n    /* ---- Cooley\u2011Tukey iterative butterflies --------------------------- */\n    for (std::size_t s = 1; s <= log2N; ++s) {\n        std::size_t m = 1ULL << s;      // block size\n        std::size_t m2 = m >> 1;        // half block\n\n        std::size_t numBlocks = N / m;\n        std::size_t stride = N / m;             // spacing between start indices\n\n        // Parallelise over blocks: each block can be processed independently\n#pragma omp parallel for schedule(static)\n        for (std::size_t blk = 0; blk < numBlocks; ++blk) {\n            std::size_t base = blk * m;\n            for (std::size_t k = 0; k < m2; ++k) {\n                std::size_t idx  = base + k;\n                std::size_t idx2 = idx + m2;\n\n                std::complex<double> t = x[idx2] * twiddles[(k * stride)];\n                x[idx2] = x[idx] - t;\n                x[idx]  += t;\n            }\n        }\n    }\n\n    /* ---- scaling for inverse FFT ------------------------------------- */\n    if (inverse) {\n        double invN = 1.0 / static_cast<double>(N);\n#pragma omp parallel for schedule(static)\n        for (std::size_t i = 0; i < N; ++i) {\n            x[i] *= invN;\n        }\n    }\n}\n\n/* --------------------------------------------------------------\n   Public interface \u2013 keeps original semantics\n   -------------------------------------------------------------- */\n\n// forward FFT \u2013 used by ifft via conjugation\ninline void fft_helper(std::vector<std::complex<double>> &x) {\n    fft_or_ifft_helper(x, /*inverse=*/false);\n}\n\n// inverse FFT \u2013 directly compute the inverse\ninline void ifft(std::vector<std::complex<double>> &x) {\n    fft_or_ifft_helper(x, /*inverse=*/true);\n}\n```",
  "08_fft_split_fft": "```cpp\n#pragma once\n#include <vector>\n#include <complex>\n#include <cmath>\n#include <cstdint>\n#include <omp.h>\n\n/* Compute the fourier transform of x. Store real part of results in r and imaginary in i.\n   Example:\n   input: [1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0]\n   output: r: [4, 1, 0, 1, 0, 1, 0, 1] i: [0, -2.41421, 0, -0.414214, 0, 0.414214, 0, 2.41421]\n*/\ninline void fft(const std::vector<std::complex<double>>& x,\n                std::vector<double>& r,\n                std::vector<double>& i)\n{\n    const std::size_t N = x.size();\n    std::vector<std::complex<double>> data(x);          // working copy\n\n    // Pre\u2011compute twiddle factors for all stages\n    const double two_pi = 2.0 * M_PI;\n    std::vector<std::complex<double>> twiddles(N / 2);\n    for (std::size_t k = 0; k < N / 2; ++k)\n        twiddles[k] = std::exp(std::complex<double>(0.0, -two_pi * k / N));\n\n    // Iterative Cooley\u2013Tukey FFT (in\u2011place, radix\u20112)\n    for (std::size_t len = 2; len <= N; len <<= 1) {\n        std::size_t half = len >> 1;\n        std::size_t step  = N / len;          // index into twiddles\n        for (std::size_t i0 = 0; i0 < N; i0 += len) {\n            std::size_t j = 0;\n            for (std::size_t j0 = i0; j0 < i0 + half; ++j0, ++j) {\n                const std::complex<double> t = twiddles[j * step] * data[j0 + half];\n                const std::complex<double> u = data[j0];\n                data[j0]         = u + t;\n                data[j0 + half] = u - t;\n            }\n        }\n    }\n\n    // Bit reversal (parallel)\n    constexpr std::uint32_t rev16[65536] = []{\n        std::uint32_t table[65536];\n        for (std::uint32_t i = 0; i < 65536; ++i) {\n            std::uint32_t x = i;\n            x = (x >> 1) & 0x55555555 | (x & 0x55555555) << 1;\n            x = (x >> 2) & 0x33333333 | (x & 0x33333333) << 2;\n            x = (x >> 4) & 0x0f0f0f0f | (x & 0x0f0f0f0f) << 4;\n            x = (x >> 8) & 0x00ff00ff | (x & 0x00ff00ff) << 8;\n            x = (x >> 16) | (x << 16);\n            table[i] = x;\n        }\n        return table;\n    }();\n\n    const unsigned int m = static_cast<unsigned int>(std::ceil(std::log2(static_cast<double>(N))));\n    #pragma omp parallel for schedule(static)\n    for (std::size_t a = 0; a < N; ++a) {\n        std::size_t b = rev16[a & 0xFFFF] >> (16 - m);      // 32\u2011bit reverse truncated to log2(N)\n        b <<= (32 - m);                                   // shift to correct width\n        if (b > a) {\n            std::swap(data[a], data[b]);\n        }\n    }\n\n    // Split into real and imaginary parts\n    for (std::size_t j = 0; j < N; ++j) {\n        r[j] = data[j].real();\n        i[j] = data[j].imag();\n    }\n}\n```",
  "11_geometry_convex_hull_perimeter": "```cpp\n#pragma once\n#include <vector>\n#include <algorithm>\n#include <cmath>\n\nstruct Point\n{\n    double x, y;\n};\n\n/* Compute Euclidean distance between two points without calling pow or sqrt\n   repeatedly. Uses hypot which is usually implemented as a hardware\u2011level sqrt. */\ninline double dist2(const Point& a, const Point& b)\n{\n    double dx = b.x - a.x;\n    double dy = b.y - a.y;\n    return std::hypot(dx, dy);            // sqrt(dx*dx + dy*dy) without overflow\n}\n\n/* Cross product (b-a) x (c-a) */\ninline double cross(const Point& a, const Point& b, const Point& c)\n{\n    return (b.x - a.x) * (c.y - a.y) - (b.y - a.y) * (c.x - a.x);\n}\n\n/* Return the perimeter of the smallest convex polygon that contains all the points\n   in the vector. Uses a standard O(n log n) monotone chain convex hull. */\ndouble convexHullPerimeter(const std::vector<Point>& points)\n{\n    const std::size_t n = points.size();\n    if (n < 3)\n        return 0.0;\n\n    std::vector<Point> tmp = points;\n    std::sort(tmp.begin(), tmp.end(),\n              [](const Point& a, const Point& b)\n              {\n                  return a.x < b.x || (a.x == b.x && a.y < b.y);\n              });\n\n    std::vector<Point> hull(2 * n);\n    std::size_t k = 0;\n\n    /* Build lower hull */\n    for (std::size_t i = 0; i < n; ++i)\n    {\n        while (k >= 2 && cross(hull[k - 2], hull[k - 1], tmp[i]) <= 0)\n            --k;\n        hull[k++] = tmp[i];\n    }\n\n    /* Build upper hull */\n    std::size_t t = k + 1;\n    for (std::size_t i = n - 1; i > 0; --i)\n    {\n        while (k >= t && cross(hull[k - 2], hull[k - 1], tmp[i - 1]) <= 0)\n            --k;\n        hull[k++] = tmp[i - 1];\n    }\n\n    hull.resize(k - 1);                 // remove duplicate of first point\n\n    double perimeter = 0.0;\n    for (std::size_t i = 0; i + 1 < hull.size(); ++i)\n        perimeter += dist2(hull[i], hull[i + 1]);\n\n    perimeter += dist2(hull.front(), hull.back());  // close the polygon\n\n    return perimeter;\n}\n```\n",
  "13_geometry_closest_pair_2d": "```cpp\n#pragma once\n#include <vector>\n#include <algorithm>\n#include <cmath>\n#include <limits>\n\nstruct Point {\n    double x, y;\n};\n\ndouble closestPair(const std::vector<Point>& points_input) {\n    if (points_input.size() < 2) return 0.0;\n\n    // Copy and sort by x coordinate\n    std::vector<Point> pts = points_input;\n    std::sort(pts.begin(), pts.end(),\n              [](const Point& a, const Point& b) { return a.x < b.x; });\n\n    // Temporary vector for merging by y\n    std::vector<Point> tmp(pts.size());\n\n    auto cmpY = [](const Point& a, const Point& b) { return a.y < b.y; };\n\n    std::function<double(int, int)> rec = [&](int l, int r) -> double {\n        int n = r - l + 1;\n        if (n <= 3) {\n            double minSq = std::numeric_limits<double>::infinity();\n            for (int i = l; i <= r; ++i) {\n                for (int j = i + 1; j <= r; ++j) {\n                    double dx = pts[i].x - pts[j].x;\n                    double dy = pts[i].y - pts[j].y;\n                    double dSq = dx * dx + dy * dy;\n                    if (dSq < minSq) minSq = dSq;\n                }\n            }\n            std::sort(pts.begin() + l, pts.begin() + r + 1, cmpY);\n            return minSq;\n        }\n\n        int m = (l + r) / 2;\n        double midx = pts[m].x;\n\n        double dSq = std::min(rec(l, m), rec(m + 1, r));\n\n        // Merge by y\n        std::inplace_merge(pts.begin() + l, pts.begin() + m + 1, pts.begin() + r + 1, cmpY);\n\n        // Build strip of points within sqrt(dSq) of the vertical line\n        std::vector<Point> strip;\n        double d = std::sqrt(dSq);\n        for (int i = l; i <= r; ++i) {\n            if (std::abs(pts[i].x - midx) < d) {\n                strip.push_back(pts[i]);\n            }\n        }\n\n        // Compare each point with next up to 7 points\n        for (size_t i = 0; i < strip.size(); ++i) {\n            for (size_t j = i + 1; j < strip.size() && (strip[j].y - strip[i].y) < d; ++j) {\n                double dx = strip[i].x - strip[j].x;\n                double dy = strip[i].y - strip[j].y;\n                double dz = dx * dx + dy * dy;\n                if (dz < dSq) {\n                    dSq = dz;\n                    d = std::sqrt(dSq);\n                }\n            }\n        }\n        return dSq;\n    };\n\n    double minSq = rec(0, static_cast<int>(pts.size()) - 1);\n    return std::sqrt(minSq);\n}\n```",
  "03_dense_la_axpy": "```cpp\n#pragma once\n#include <vector>\n#include <cstddef>\n#include <omp.h>\n\n/* Compute z = alpha*x + y where x and y are vectors. Store the result in z.\n   Example:\n\n   input: x=[1, -5, 2, 9] y=[0, 4, 1, -1] alpha=2\n   output: z=[2, -6, 5, 17]\n*/\ninline void axpy(double alpha, std::vector<double> const& x,\n                 std::vector<double> const& y,\n                 std::vector<double> &z) {\n    // Assume z has the same size as x and y (as per original behaviour)\n    #pragma omp parallel for schedule(static)\n    for (std::size_t i = 0; i < x.size(); ++i) {\n        z[i] = alpha * x[i] + y[i];\n    }\n}\n```\n",
  "50_stencil_xor_kernel": "```cpp\n#pragma once\n#include <vector>\n#include <cstddef>\n#include <omp.h>\n\n/* Set every cell's value to 1 if it has exactly one neighbor that's a 1.\n   Neighborhood is the four 4\u2011connect neighbors (up, down, left, right).\n   The function works on flattened row\u2011major NxN grids.\n   It preserves the original deterministic behaviour while applying OpenMP\n   parallelism and a small unroll to maximize throughput.\n*/\ninline void cellsXOR(std::vector<int> const& input,\n                     std::vector<int>& output,\n                     size_t N)\n{\n    // Ensure output has the same size as input\n    output.resize(N * N);\n\n    const int* in  = input.data();\n    int*       out = output.data();\n\n    #pragma omp parallel for schedule(static)\n    for (size_t i = 0; i < N; ++i)\n    {\n        size_t row_offset = i * N;\n        size_t row_above  = (i > 0)     ? (i - 1) * N : SIZE_MAX;\n        size_t row_below  = (i < N - 1) ? (i + 1) * N : SIZE_MAX;\n\n        for (size_t j = 0; j < N; ++j)\n        {\n            int count = 0;\n\n            // Count up\n            if (row_above != SIZE_MAX && in[row_above + j] == 1)\n                ++count;\n            // Count down\n            if (row_below != SIZE_MAX && in[row_below + j] == 1)\n                ++count;\n            // Count left\n            if (j > 0 && in[row_offset + j - 1] == 1)\n                ++count;\n            // Count right\n            if (j + 1 < N && in[row_offset + j + 1] == 1)\n                ++count;\n\n            out[row_offset + j] = (count == 1) ? 1 : 0;\n        }\n    }\n}\n```",
  "53_stencil_2d_jacobi_5-point_stencil": "```cpp\n#pragma once\n#include <vector>\n#include <omp.h>\n\n/* Compute one iteration of a 5\u2011point 2D Jacobi stencil.\n   input and output are NxN row\u2011major grids. 0 is assumed outside the\n   boundaries so no boundary checks are required. */\ninline void jacobi2D(std::vector<double> const& input, std::vector<double>& output, size_t N)\n{\n    const size_t stride = N;\n    const double inv5 = 0.2;                // 1/5\n    const size_t total = N * N;\n\n    /* Parallelise over rows.  The inner loop is fully vectorisable and\n       the data dependencies are eliminated because we only read from\n       `input` and write to `output`. */\n    #pragma omp parallel for\n    for (size_t i = 0; i < N; ++i)\n    {\n        const size_t idx     = i * stride;\n        const size_t prevIdx = (i > 0)     ? (i - 1) * stride : SIZE_MAX;\n        const size_t nextIdx = (i + 1 < N) ? (i + 1) * stride : SIZE_MAX;\n\n        for (size_t j = 0; j < N; ++j)\n        {\n            /* Gather the 5 neighbours. The boundary elements\n               automatically contribute 0 because the indices are out\n               of bounds and skipped. */\n            double sum = input[idx + j];            // centre\n\n            if (prevIdx != SIZE_MAX) sum += input[prevIdx + j];   // north\n            if (nextIdx != SIZE_MAX) sum += input[nextIdx + j];   // south\n\n            if (j > 0)     sum += input[idx + j - 1];             // west\n            if (j + 1 < N) sum += input[idx + j + 1];             // east\n\n            output[idx + j] = sum * inv5;\n        }\n    }\n}\n```",
  "10_geometry_convex_hull": "```cpp\n#pragma once\n#include <vector>\n#include <algorithm>\n#include <cstdio>\n#include <cstddef>\n\nstruct Point {\n    double x, y;\n};\n\ninline double cross(const Point& a, const Point& b, const Point& c) noexcept {\n    return (b.x - a.x) * (c.y - a.y) - (b.y - a.y) * (c.x - a.x);\n}\n\n/*\n * Compute the convex hull of a set of points (Monotone chain algorithm).\n * The resulting hull is stored in `hull`. The algorithm runs in O(n log n)\n * because of the sorting step and uses only a single temporary array of size\n * 2*n. It preserves the exact behaviour of the original implementation.\n */\nstatic void convexHull(const std::vector<Point>& points, std::vector<Point>& hull) noexcept\n{\n    const std::size_t n = points.size();\n    if (n < 3) {           // degenerate case\n        hull = points;\n        return;\n    }\n\n    // Sort points lexicographically (by x, then y).\n    std::vector<Point> sorted(points);   // copy once\n    std::sort(sorted.begin(), sorted.end(), [](const Point& a, const Point& b) noexcept {\n        return a.x < b.x || (a.x == b.x && a.y < b.y);\n    });\n\n    // Reserve space for the lower and upper hulls (max 2*n points).\n    std::vector<Point> hullTmp;\n    hullTmp.reserve(2 * n);\n\n    // Build lower hull.\n    for (const Point& p : sorted) {\n        while (hullTmp.size() >= 2 &&\n               cross(hullTmp[hullTmp.size() - 2], hullTmp.back(), p) <= 0.0) {\n            hullTmp.pop_back();\n        }\n        hullTmp.push_back(p);\n    }\n\n    // Build upper hull.\n    std::size_t lowerSize = hullTmp.size();          // number of points in lower hull\n    for (auto it = sorted.rbegin(); it != sorted.rend(); ++it) {\n        const Point& p = *it;\n        // Skip the first point of the upper hull if it is the same as the last\n        // point of the lower hull to avoid duplication.\n        if (p.x == hullTmp.back().x && p.y == hullTmp.back().y) continue;\n\n        while (hullTmp.size() >= lowerSize + 1 &&\n               cross(hullTmp[hullTmp.size() - 2], hullTmp.back(), p) <= 0.0) {\n            hullTmp.pop_back();\n        }\n        hullTmp.push_back(p);\n    }\n\n    // Remove the duplicated start point at the end.\n    if (!hullTmp.empty() && hullTmp.front().x == hullTmp.back().x &&\n        hullTmp.front().y == hullTmp.back().y) {\n        hullTmp.pop_back();\n    }\n\n    hull.swap(hullTmp);          // transfer ownership to the caller\n}\n```",
  "51_stencil_edge_kernel": "```cpp\n#pragma once\n#include <vector>\n#include <cstddef>\n#include <algorithm>\n#include <omp.h>\n\nstatic constexpr int8_t edgeKernel[3][3] = {\n    {-1, -1, -1},\n    {-1,  8, -1},\n    {-1, -1, -1}\n};\n\n/*\n * Convolve the edge kernel with a grayscale image. Each pixel is replaced with\n * the dot product of itself and its 3\u00d73 neighborhood weighted by the edge kernel.\n * Outside image boundaries are treated as zero.  Results are clipped to [0,255].\n * imageIn and imageOut are N\u00d7N row\u2011major grayscale images.\n */\nvoid convolveKernel(const std::vector<int> &imageIn,\n                    std::vector<int> &imageOut,\n                    const size_t N) noexcept\n{\n    const size_t stride = N;\n    const size_t total  = N * N;\n    if (imageOut.size() < total) imageOut.resize(total);\n\n    /* We avoid boundary checks by first copying the input into a padded\n     * buffer (double width).  The padded buffer has an extra row and column\n     * on each side filled with zeros.  This means every 3\u00d73 kernel accesses\n     * valid memory without conditional tests. */\n    const size_t paddedSize = (N + 2) * (N + 2);\n    std::vector<int> padded(paddedSize, 0);\n\n    // Copy the original image into the padded buffer\n    for (size_t i = 0; i < N; ++i) {\n        const int srcOffset = i * N;\n        const int dstOffset = (i + 1) * (N + 2) + 1;\n        std::copy_n(&imageIn[srcOffset], N, &padded[dstOffset]);\n    }\n\n    #pragma omp parallel for schedule(static)\n    for (intptr_t idx = 0; idx < static_cast<intptr_t>(total); ++idx) {\n        const size_t i = idx / N;\n        const size_t j = idx % N;\n\n        // Index of the top-left corner of the 3\u00d73 neighborhood in padded\n        const int base = (i + 1) * (N + 2) + 1;\n\n        // Manual unrolling of the 3\u00d73 loop for speed\n        int sum  = 0;\n        sum += edgeKernel[0][0] * padded[base - (N + 2) - 1];\n        sum += edgeKernel[0][1] * padded[base - (N + 2)];\n        sum += edgeKernel[0][2] * padded[base - (N + 2) + 1];\n        sum += edgeKernel[1][0] * padded[base - 1];\n        sum += edgeKernel[1][1] * padded[base];\n        sum += edgeKernel[1][2] * padded[base + 1];\n        sum += edgeKernel[2][0] * padded[base + (N + 2) - 1];\n        sum += edgeKernel[2][1] * padded[base + (N + 2)];\n        sum += edgeKernel[2][2] * padded[base + (N + 2) + 1];\n\n        // Clamp to [0, 255]\n        if (sum < 0)       imageOut[idx] = 0;\n        else if (sum > 255) imageOut[idx] = 255;\n        else                imageOut[idx] = sum;\n    }\n}\n```",
  "04_dense_la_gemv": "```cpp\n#pragma once\n#include <vector>\n#include <omp.h>\n\n/* Multiply the matrix A by the vector x. Store the results in the vector y.\n   A is an MxN matrix stored in row-major, x has N elements, and y has M elements.\n   Example:\n   input: A=[[1, -1, 2], [0, -3, 1]] x=[2, 1, 0]\n   output: y=[1, -3]\n*/\nvoid gemv(std::vector<double> const& A,\n          std::vector<double> const& x,\n          std::vector<double> &y,\n          size_t M,\n          size_t N) {\n    // Basic safety: all vectors must be the correct size\n    if (A.size() < M * N || x.size() < N || y.size() < M) return;\n\n    // Parallelize over rows. Work is embarrassingly parallel.\n    #pragma omp parallel for schedule(static)\n    for (long long i = 0; i < static_cast<long long>(M); ++i) {\n        double sum = 0.0;\n        const double *a_row = &A[i * N];\n        for (size_t j = 0; j < N; ++j) {\n            sum += a_row[j] * x[j];\n        }\n        y[i] = sum;\n    }\n}\n```",
  "54_stencil_game_of_life": "```cpp\n#pragma once\n#include <vector>\n#include <cstddef>\n#include <omp.h>\n\n/*  Game of Life \u2013 single generation\n    Parameters:\n      - input  : N\u00d7N grid (row\u2011major) of 0/1 cells\n      - output : N\u00d7N grid to be written\n      - N      : dimension of the grid\n    The function is deterministic and thread\u2011safe. */\ninline void gameOfLife(const std::vector<int> &input,\n                       std::vector<int> &output,\n                       size_t N)\n{\n    // assume input and output are already sized to N*N\n    const int *in  = input.data();\n    int       *out = output.data();\n    /* Parallelise the outer loop \u2013 each row can be processed independently.\n       Schedule static to keep work evenly distributed on 8 threads. */\n    #pragma omp parallel for schedule(static)\n    for (size_t i = 0; i < N; ++i) {\n        const size_t idx = i * N;                 // index of first element in the row\n        const int *row_above   = (i > 0)     ? &in[(i-1)*N] : nullptr;\n        const int *row_below   = (i < N-1)   ? &in[(i+1)*N] : nullptr;\n        const int *row_current = &in[idx];\n\n        for (size_t j = 0; j < N; ++j) {\n            // neighbour sum\n            int sum = 0;\n            // vertical neighbours\n            if (row_above)  sum += row_above[j];\n            if (row_below)  sum += row_below[j];\n            // horizontal neighbours\n            if (j > 0)      sum += row_current[j-1];\n            if (j < N-1)    sum += row_current[j+1];\n            // diagonal neighbours\n            if (row_above && j>0)       sum += row_above[j-1];\n            if (row_above && j<N-1)     sum += row_above[j+1];\n            if (row_below && j>0)       sum += row_below[j-1];\n            if (row_below && j<N-1)     sum += row_below[j+1];\n\n            // apply rules \u2013 all branches removed via ternary operators\n            const int cur = row_current[j];\n            out[idx + j] = (cur == 1 ?\n                            ((sum==2||sum==3) ? 1 : 0) :\n                            (sum==3 ? 1 : 0));\n        }\n    }\n}\n```",
  "43_sort_sort_an_array_of_structs_by_key": "```cpp\n#pragma once\n#include <vector>\n#include <parallel/algorithm>   // OpenMP-enabled parallel sort\n\nstruct Result {\n    int startTime, duration;\n    float value;\n};\n\n// Sort vector of Result structs by start time in ascending order.\n// Keeps the exact semantics of std::sort but runs in parallel on the\n// available 8 threads thanks to GCC's Intel Parallel STL.\nvoid sortByStartTime(std::vector<Result> &results) {\n    __gnu_parallel::sort(results.begin(), results.end(),\n        [](const Result &a, const Result &b) noexcept {\n            return a.startTime < b.startTime;\n        });\n}\n```",
  "00_dense_la_lu_decomp": "```cpp\n#pragma once\n#include <vector>\n#include <omp.h>\n\n/* Factorize the matrix A into A = LU where L is a lower triangular matrix\n   and U is an upper triangular matrix.\n   The result is stored in place: the elements below the diagonal contain L\n   (excluding the unit diagonal which is assumed to be 1) and the diagonal\n   and above contain U.\n   A is a flattened NxN matrix in row-major order.\n*/\nvoid luFactorize(std::vector<double> &A, size_t N)\n{\n    /* The outer loop over k has to remain serial because each iteration\n       depends on all previous ones.  For every fixed k we can update all\n       rows i > k independently: this is where we gain parallelism. */\n    for (size_t k = 0; k < N; ++k)\n    {\n        const double akk = A[k * N + k];\n        /* Scale the lower part of column k and update the remaining rows.\n           Parallelise over the rows i > k. */\n#pragma omp parallel for schedule(static)\n        for (size_t i = k + 1; i < N; ++i)\n        {\n            double factor = A[i * N + k] / akk;\n            A[i * N + k] = factor;                // store L[i][k]\n\n            /* Update the rest of row i (columns j > k).  The inner loop\n               is very short so keeping it scalar is optimal. */\n            for (size_t j = k + 1; j < N; ++j)\n            {\n                A[i * N + j] -= factor * A[k * N + j];  // U part\n            }\n        }\n    }\n}\n```",
  "01_dense_la_solve": "```cpp\n#pragma once\n#include <vector>\n#include <cstddef>\n#include <omp.h>\n\n/* Solve the linear system Ax=b for x.\n   A is an NxN matrix in row-major. x and b have N elements.\n   The function preserves the exact behavior of the original implementation,\n   i.e. it aborts with no changes if a pivot element becomes zero.\n*/\nvoid solveLinearSystem(std::vector<double> const& A,\n                       std::vector<double> const& b,\n                       std::vector<double> &x, size_t N) {\n    // Create local copies of A and b (to keep the original data unchanged)\n    std::vector<double> A_copy = A;\n    std::vector<double> b_copy = b;\n\n    // Forward elimination\n    for (size_t i = 0; i < N - 1; ++i) {\n        double pivot = A_copy[i * N + i];\n        if (pivot == 0.0)                     // Preserve original zero-pivot behaviour\n            return;\n\n        // Eliminate rows below the current pivot row\n        #pragma omp parallel for schedule(static)\n        for (size_t j = i + 1; j < N; ++j) {\n            double factor = A_copy[j * N + i] / pivot;\n            // Update the remaining elements in the row\n            for (size_t k = i; k < N; ++k)\n                A_copy[j * N + k] -= factor * A_copy[i * N + k];\n            b_copy[j] -= factor * b_copy[i];\n        }\n    }\n\n    // Back substitution\n    for (int i = static_cast<int>(N) - 1; i >= 0; --i) {\n        double sum = 0.0;\n        for (size_t j = i + 1; j < N; ++j)\n            sum += A_copy[i * N + j] * x[j];\n        x[i] = (b_copy[i] - sum) / A_copy[i * N + i];\n    }\n}\n```",
  "42_sort_sorted_ranks": "```cpp\n#pragma once\n#include <algorithm>\n#include <numeric>\n#include <vector>\n#include <cstddef>\n#ifdef _OPENMP\n#include <omp.h>\n#endif\n\n/* For each value in the vector x compute its index in the sorted vector.\n   Store the results in `ranks_`. */\ninline void ranks(std::vector<float> const& x, std::vector<std::size_t> &ranks_) {\n    const std::size_t n = x.size();\n\n    // Build array of indices [0..n-1]\n    std::vector<std::size_t> indices(n);\n    std::iota(indices.begin(), indices.end(), 0);\n\n    // Sort indices by corresponding values in x\n    std::sort(indices.begin(), indices.end(),\n              [&x](std::size_t i1, std::size_t i2) { return x[i1] < x[i2]; });\n\n    // Reverse mapping: rank of each original element\n#ifdef _OPENMP\n#pragma omp parallel for schedule(static)\n#endif\n    for (std::size_t i = 0; i < n; ++i) {\n        ranks_[indices[i]] = i;\n    }\n}\n```",
  "02_dense_la_gemm": "```cpp\n#pragma once\n#include <vector>\n#include <cstddef>\n#include <omp.h>\n\n/*\n * Multiply the matrix A by the matrix B. Store the results in the matrix C.\n * A is an MxK matrix, B is a KxN matrix, and C is a MxN matrix. The matrices are\n * stored in row-major.\n *\n * Example:\n *   input: A = [[1, -1, 2], [0, -2, 1]]\n *          B = [[4, 1], [-1, 0], [2, 2]]\n *   output: C = [[9, 5], [4, 2]]\n *\n * This implementation is fully cache\u2011friendly and parallelises the outer\n * loop over `i` with OpenMP. The inner loops are unrolled for vector\n * friendliness and the accumulator is kept in a register. The code\n * preserves the exact semantics of the original implementation.\n */\nvoid gemm(std::vector<double> const& A,\n          std::vector<double> const& B,\n          std::vector<double> &C,\n          size_t M,\n          size_t K,\n          size_t N)\n{\n    /* Ensure C is zeroed or already contains the correct base values.\n       The kernel adds to the existing C values just like the original\n       implementation. */\n    #pragma omp parallel for schedule(static)\n    for (size_t i = 0; i < M; ++i) {\n        size_t iN = i * N;        // base index of row i in C\n        size_t iK = i * K;        // base index of row i in A\n        for (size_t k = 0; k < K; ++k) {\n            double aik = A[iK + k];\n            size_t kN = k * N;    // base index of row k in B\n            /* Unroll the inner j loop for a few iterations to reduce\n               loop overhead and improve instruction\u2011level parallelism. */\n            size_t j = 0;\n            for (; j + 7 < N; j += 8) {\n                C[iN + j    ] += aik * B[kN + j    ];\n                C[iN + j + 1] += aik * B[kN + j + 1];\n                C[iN + j + 2] += aik * B[kN + j + 2];\n                C[iN + j + 3] += aik * B[kN + j + 3];\n                C[iN + j + 4] += aik * B[kN + j + 4];\n                C[iN + j + 5] += aik * B[kN + j + 5];\n                C[iN + j + 6] += aik * B[kN + j + 6];\n                C[iN + j + 7] += aik * B[kN + j + 7];\n            }\n            /* Handle any remaining elements */\n            for (; j < N; ++j) {\n                C[iN + j] += aik * B[kN + j];\n            }\n        }\n    }\n}\n```",
  "41_sort_k-th_smallest_element": "```cpp\n#pragma once\n#include <utility>\n#include <vector>\n#include <algorithm>\n\n/* Find the k\u2011th smallest element of the vector x.\n   Example:\n   input: x=[1, 7, 6, 0, 2, 2, 10, 6], k=4\n   output: 6\n*/\nint findKthSmallest(const std::vector<int>& x, int k) {\n    // Validate k (indices are 1\u2011based per the original contract)\n    if (k < 1 || k > static_cast<int>(x.size())) {\n        throw std::out_of_range(\"k is out of bounds\");\n    }\n\n    // Make a mutable copy to perform nth_element\n    std::vector<int> tmp = x;\n    std::nth_element(tmp.begin(), tmp.begin() + (k - 1), tmp.end());\n    return tmp[k - 1];\n}\n```",
  "40_sort_sort_an_array_of_complex_numbers_by_magnitude": "```cpp\n#pragma once\n#include <complex>\n#include <vector>\n#include <algorithm>\n#include <execution>\n\n/*\n * Sort the vector `x` of complex numbers by their magnitude in ascending order.\n *\n * The function uses the parallel execution policy provided by the\n * standard library to speed up the sort on systems with multiple cores.\n * The behavior (ordering and stability of equal magnitudes) is identical\n * to the serial implementation.\n *\n * Example:\n *   input: [3.0-1.0i, 4.5+2.1i, 0.0-1.0i, 1.0-0.0i, 0.5+0.5i]\n *   output: [0.5+0.5i, 0.0-1.0i, 1.0-0.0i, 3.0-1.0i, 4.5+2.1i]\n */\ninline void sortComplexByMagnitude(std::vector<std::complex<double>> &x)\n{\n    std::sort(std::execution::par_unseq,\n              x.begin(),\n              x.end(),\n              [](const std::complex<double> &a,\n                 const std::complex<double> &b) noexcept\n              {\n                  return std::abs(a) < std::abs(b);\n              });\n}\n```",
  "44_sort_sort_non-zero_elements": "```cpp\n#pragma once\n#include <algorithm>\n#include <vector>\n\ninline void sortIgnoreZero(std::vector<int>& x) {\n    const std::size_t n = x.size();\n\n    // Collect non\u2011zero values and the positions where they appear\n    std::vector<int> values;\n    std::vector<std::size_t> indices;\n    values.reserve(n);\n    indices.reserve(n);\n\n    for (std::size_t i = 0; i < n; ++i) {\n        if (x[i] != 0) {\n            values.push_back(x[i]);\n            indices.push_back(i);\n        }\n    }\n\n    // Sort the non\u2011zero values\n    std::sort(values.begin(), values.end());\n\n    // Place the sorted values back into their original positions\n    for (std::size_t k = 0; k < indices.size(); ++k) {\n        x[indices[k]] = values[k];\n    }\n}\n```",
  "39_search_xor_contains": "```cpp\n#pragma once\n#include <vector>\n\n/* Return true if `val` is only in one of vectors x or y.\n   Return false if it is in both or neither. */\ninline bool xorContains(std::vector<int> const& x,\n                        std::vector<int> const& y,\n                        int val)\n{\n    bool foundInX = false, foundInY = false;\n\n    // Search in x\n    for (int v : x) {\n        if (v == val) {\n            foundInX = true;\n            break;\n        }\n    }\n\n    // If already found in both, we can return false immediately\n    if (foundInX) {\n        for (int v : y) {\n            if (v == val) {\n                foundInY = true;\n                break;\n            }\n        }\n        return foundInX ^ foundInY;\n    }\n\n    // Not found in x, search in y\n    for (int v : y) {\n        if (v == val) {\n            foundInY = true;\n            break;\n        }\n    }\n\n    return foundInX ^ foundInY;\n}\n```",
  "35_search_search_for_last_struct_by_key": "```cpp\n#pragma once\n#include <vector>\n\nstruct Book {\n    std::string title;\n    int pages;\n};\n\n// Return the index of the last Book item in the vector books where Book.pages is less than 100.\n// Example:\n//    input: [{title=\"Green Eggs and Ham\", pages=72},\n//            {title=\"gulliver's travels\", pages=362},\n//            {title=\"Stories of Your Life\", pages=54},\n//            {title=\"Hamilton\", pages=818}]\n//\n//    output: 2\nsize_t findLastShortBook(std::vector<Book> const& books) {\n    // Traverse backwards; use unsigned size_t to avoid signed overflow.\n    for (size_t i = books.size(); i-- > 0; ) {\n        if (books[i].pages < 100) {\n            return i;\n        }\n    }\n    return books.size();  // no match found\n}\n```",
  "36_search_check_if_array_contains_value": "```cpp\n#pragma once\n#include <vector>\n#include <cstddef>\n#include <omp.h>\n\n/* Return true if the vector x contains the value `target`. Return false otherwise.\n   Examples:\n   \n   input: x=[1, 8, 2, 6, 4, 6], target=3\n   output: false\n   input: x=[1, 8, 2, 6, 4, 6], target=8\n   output: true\n*/\ninline bool contains(const std::vector<int>& x, int target) {\n    // Quick return for empty vector\n    if (x.empty()) return false;\n\n    const int* data = x.data();\n    const std::size_t n   = x.size();\n    bool found = false;\n\n    // Parallel search with early exit\n#pragma omp parallel for shared(found) schedule(static)\n    for (std::size_t i = 0; i < n; ++i) {\n        if (found) continue;          // already found\n        if (data[i] == target) {\n            found = true;\n        }\n    }\n\n    return found;\n}\n```",
  "37_search_find_the_closest_number_to_pi": "```cpp\n#pragma once\n#include <vector>\n#include <cmath>\n#include <cstddef>\n#include <algorithm>\n#include <omp.h>\n\n/* Return the index of the value in the vector x that is closest to the math constant PI.\n   Use M_PI for the value of PI.\n   Example:\n   input: [9.18, 3.05, 7.24, 11.3, -166.49, 2.1]\n   output: 1\n*/\nsize_t findClosestToPi(const std::vector<double>& x)\n{\n    if (x.empty()) return 0;                     // safety guard\n\n    double global_min = std::abs(x[0] - M_PI);    // initialise with first element\n    size_t global_index = 0;\n\n    // parallel scan \u2013 each thread keeps its own best; the global values\n    // are updated atomically via a critical section.\n    #pragma omp parallel\n    {\n        double local_min = global_min;\n        size_t local_index = global_index;\n        #pragma omp for nowait\n        for (size_t i = 1; i < x.size(); ++i) {\n            double diff = std::abs(x[i] - M_PI);\n            if (diff < local_min) {\n                local_min   = diff;\n                local_index = i;\n            }\n        }\n        // Merge local minima into globals\n        #pragma omp critical\n        {\n            if (local_min < global_min) {\n                global_min   = local_min;\n                global_index = local_index;\n            }\n        }\n    }\n\n    return global_index;\n}\n```",
  "38_search_find_the_first_even_number": "```cpp\n#pragma once\n#include <vector>\n#include <cstddef>\n#include <cstdint>\n#include <algorithm>\n\n/* Return the index of the first even number in the vector `x`.\n *\n * Examples:\n *   input: [7, 3, 9, 5, 5, 7, 2, 9, 12, 11] -> 6\n *   input: [3, 8, 9, 9, 3, 4, 8, 6]           -> 1\n *\n * The implementation is fully vectorised, relies only on\n * standard C++ and is layout\u2011friendly for modern x86\u201164 CPUs.\n */\ninline std::size_t findFirstEven(std::vector<int> const& x)\n{\n    const std::size_t n = x.size();\n    // Common case: small vectors are cheap to scan sequentially.\n    if (n <= 64) {\n        for (std::size_t i = 0; i < n; ++i) {\n            // test evenness using bitwise AND (faster than % on many CPUs)\n            if ((x[i] & 1) == 0) {\n                return i;\n            }\n        }\n        return n;\n    }\n\n    // For larger vectors, process in blocks to keep the loop in lock\u2011step\n    // with the CPU pipeline and to encourage vectorisation.\n    std::size_t i = 0;\n    const std::size_t stride = 16;              // 16 elements = 64 bytes (cache line)\n    const std::size_t limit = n - (n % stride);\n\n    // Vectorised check over 16 elements with branchless logic\n    for (; i < limit; i += stride) {\n        // Load 16 ints into an array for SIMD friendliness\n        const int* ptr = &x[i];\n        bool found[16];\n        // Unrolled check to avoid a branch on % (use bit test)\n        found[0]  = (ptr[0]  & 1) == 0;\n        found[1]  = (ptr[1]  & 1) == 0;\n        found[2]  = (ptr[2]  & 1) == 0;\n        found[3]  = (ptr[3]  & 1) == 0;\n        found[4]  = (ptr[4]  & 1) == 0;\n        found[5]  = (ptr[5]  & 1) == 0;\n        found[6]  = (ptr[6]  & 1) == 0;\n        found[7]  = (ptr[7]  & 1) == 0;\n        found[8]  = (ptr[8]  & 1) == 0;\n        found[9]  = (ptr[9]  & 1) == 0;\n        found[10] = (ptr[10] & 1) == 0;\n        found[11] = (ptr[11] & 1) == 0;\n        found[12] = (ptr[12] & 1) == 0;\n        found[13] = (ptr[13] & 1) == 0;\n        found[14] = (ptr[14] & 1) == 0;\n        found[15] = (ptr[15] & 1) == 0;\n\n        // If any even number is found, locate it and return\n        for (int j = 0; j < 16; ++j) {\n            if (found[j]) {\n                return i + j;\n            }\n        }\n    }\n\n    // Handle the tail (less than 16 elements)\n    for (; i < n; ++i) {\n        if ((x[i] & 1) == 0) {\n            return i;\n        }\n    }\n    return n;\n}\n```"
}