{
  "15_graph_edge_count": "```cpp\n#pragma once\n#include <vector>\n#include <cstddef>\n\n/* Count the number of edges in a directed graph represented by an N\u00d7N\n   adjacency matrix A stored in row\u2011major order.  Each element of A is\n   expected to be 0 or 1.  The original implementation uses two\n   nested loops with a branch; this version removes the branch and\n   uses a parallel reduction for improved performance on a modern\n   8\u2011thread x86\u201164 system compiled with g++(-O2 -fopenmp).  The result\n   is bit\u2011exact with the original specification. */\nint edgeCount(const std::vector<int>& A, std::size_t N)\n{\n    int count = 0;\n\n    // Parallel reduction; loop over all entries of the matrix.\n    #pragma omp parallel for reduction(+:count) schedule(static)\n    for (std::size_t idx = 0, end = N * N; idx < end; ++idx) {\n        // A[idx] is guaranteed to be 0 or 1; adding it to count is equivalent\n        // to the original conditional increment.\n        count += A[idx];\n    }\n\n    return count;\n}\n```",
  "17_graph_highest_degree": "```cpp\n// optimized version of maxDegree()\nint maxDegree(std::vector<int> const& A, size_t N) {\n    int highestDegree = 0;\n#pragma omp parallel for reduction(max:highestDegree)\n    for (size_t i = 0; i < N; ++i) {\n        int64_t sum = 0; // use 64\u2011bit for safety when summing many 0/1\n        const int* row = A.data() + i * N;          // pointer to start of the row\n        size_t j = 0;\n        // process 8 integers at a time using AVX2 intrinsics (optional)\n        for (; j + 7 < N; j += 8) {\n            sum += row[j] + row[j+1] + row[j+2] + row[j+3] +\n                   row[j+4] + row[j+5] + row[j+6] + row[j+7];\n        }\n        // remaining elements\n        for (; j < N; ++j) sum += row[j];\n        if (sum > highestDegree) highestDegree = static_cast<int>(sum);\n    }\n    return highestDegree;\n}\n```",
  "23_histogram_first_letter_counts": "```cpp\n#pragma once\n#include <array>\n#include <string>\n#include <vector>\n#ifdef _OPENMP\n#include <omp.h>\n#endif\n\n/* For each letter in the alphabet, count the number of strings in the vector s that start with that\n   letter.  All strings are assumed to be lower\u2011case.\n   The result is stored in the supplied 26\u2011element array `bins`. */\ninline void firstLetterCounts(std::vector<std::string> const& s,\n                               std::array<size_t, 26>& bins) noexcept\n{\n#ifdef _OPENMP\n    /* Parallel reduction over an array \u2013 supported by GCC 9+ and Clang. */\n    #pragma omp parallel for schedule(static) reduction(+:bins[:26])\n    for (std::size_t i = 0; i < s.size(); ++i)\n    {\n        /* The original specification guarantees that every string has at\n           least one character, so no bounds checking is required.  If you\n           wish to make the function more robust, add a guard for\n           s[i].empty() or use std::abort() for invalid inputs. */\n        const unsigned char c = static_cast<unsigned char>(s[i][0]);\n        bins[c - 'a']++;\n    }\n#else\n    /* Fallback to a simple sequential loop when OpenMP is not available. */\n    for (std::size_t i = 0; i < s.size(); ++i)\n    {\n        const unsigned char c = static_cast<unsigned char>(s[i][0]);\n        bins[c - 'a']++;\n    }\n#endif\n}\n```",
  "21_histogram_bin_0-100": "```cpp\n#pragma once\n#include <array>\n#include <vector>\n#include <cstddef>\n#include <omp.h>\n\n/* Vector x contains values between 0 and 100, inclusive.\n   Count the number of values in [0,10), [10,20), \u2026 and store the\n   counts in `bins`.  Example:\n   input:  [7, 32, 95, 12, 39, 32, 11, 71, 70, 66]\n   output: [1, 2, 0, 3, 0, 0, 1, 2, 0, 1]  */\ninline void binsBy10Count(const std::vector<double>& x,\n                          std::array<std::size_t, 10>& bins)\n{\n    // zero the output array first\n    for(auto &c : bins) c = 0;\n\n    // parallel reduction over the array (GCC 11+ supports array reductions)\n    #pragma omp parallel for schedule(static) reduction(+:bins[:10])\n    for(std::size_t i = 0; i < x.size(); ++i)\n    {\n        // integer division truncates towards zero; all values are >=0\n        std::size_t bin = static_cast<std::size_t>(x[i] / 10.0);\n        // clip to [0,9] for safety (in case of out\u2013of\u2011range values)\n        if(bin >= 10) bin = 9;\n        bins[bin] += 1;\n    }\n}\n```\n",
  "16_graph_largest_component": "```cpp\n#pragma once\n#include <vector>\n#include <stack>\n#include <cstddef>\n#include <algorithm>\n\n/* Return the number of vertices in the largest component of the graph defined by the adjacency matrix A.\n   A is an NxN adjacency matrix stored in row-major.\n   Example:\n      input: [[0, 1, 0, 0], [1, 0, 0, 0], [0, 0, 0, 1], [0, 0, 1, 0]]\n   output: 2\n*/\ninline int largestComponent(const std::vector<int> &A, std::size_t N) {\n    // Use a vector of unsigned char for a compact visited flag\n    std::vector<unsigned char> visited(N, 0);\n    int maxCount = 0;\n\n    // Reserve a stack once to avoid re-allocations\n    std::stack<std::size_t> st;\n    st.reserve(N);\n\n    for (std::size_t start = 0; start < N; ++start) {\n        if (visited[start]) continue;\n\n        int compSize = 0;\n        st.push(start);\n        while (!st.empty()) {\n            std::size_t v = st.top();\n            st.pop();\n            if (visited[v]) continue;\n            visited[v] = 1;\n            ++compSize;\n\n            // Traverse all neighbors of v\n            const std::size_t base = v * N;\n            for (std::size_t w = 0; w < N; ++w) {\n                if (A[base + w] == 1 && !visited[w]) {\n                    st.push(w);\n                }\n            }\n        }\n        if (compSize > maxCount) maxCount = compSize;\n    }\n    return maxCount;\n}\n```",
  "19_graph_shortest_path": "```cpp\n#pragma once\n#include <vector>\n#include <limits>\n#include <cstddef>\n#include <cstdint>\n\n/* Return the length of the shortest path from source to dest in the undirected graph\n   defined by the adjacency matrix A (row\u2011major). The graph is guaranteed to be\n   connected. */\nint shortestPathLength(std::vector<int> const& A, size_t N, int source, int dest) {\n    // Use a byte per node for visitation flag \u2013 smaller memory footprint and cache friendly\n    std::vector<uint8_t> visited(N, 0);\n\n    // Ring buffer for BFS: store node index and depth packed into a single 64\u2011bit word.\n    // Lower 32 bits: node, upper 32 bits: depth.  Packed to avoid a separate queue structure.\n    std::vector<uint64_t> q(N);\n    size_t q_head = 0, q_tail = 0;\n\n    visited[source] = 1;\n    q[q_tail++] = (static_cast<uint64_t>(source) & 0xffffffffULL);\n\n    while (q_head != q_tail) {\n        uint64_t word = q[q_head++];\n        uint32_t current = static_cast<uint32_t>(word & 0xffffffffULL);\n        uint32_t depth   = static_cast<uint32_t>(word >> 32);\n\n        if (current == static_cast<uint32_t>(dest))\n            return static_cast<int>(depth);\n\n        const int* row = A.data() + current * N;\n        for (size_t i = 0; i < N; ++i) {\n            if (row[i] && !visited[i]) {\n                visited[i] = 1;\n                q[q_tail++] = (static_cast<uint64_t>(i) << 32) | depth + 1;\n            }\n        }\n    }\n\n    // Should never reach here on a connected graph, but keep the error sentinel\n    return std::numeric_limits<int>::max();\n}\n```",
  "20_histogram_pixel_histogram": "```cpp\n#pragma once\n#include <vector>\n#include <array>\n#include <cstddef>\n#include <cstdint>\n#include <omp.h>\n\n/*\n * Count the number of pixels in image with each grayscale intensity.\n * The vector `image` is a grayscale image with values 0-255.\n * Store the results in `bins`.\n */\ninline void pixelCounts(std::vector<int> const& image,\n                        std::array<size_t, 256> &bins)\n{\n    // Initialise output array to zero\n    for (size_t i = 0; i < bins.size(); ++i)\n        bins[i] = 0;\n\n    // Parallel histogram building \u2013 each thread keeps a local histogram\n    // and the results are merged afterward.  This avoids contention on\n    // the shared bins array.\n    const std::size_t n = image.size();\n    const std::size_t num_threads = omp_get_max_threads();\n\n    // Allocate array of per\u2011thread histograms\n    std::vector<std::array<size_t, 256>> local(num_threads);\n\n    // Zero the local histograms\n    for (auto &h : local)\n        h.fill(0);\n\n    // Parallel loop over pixels\n    #pragma omp parallel\n    {\n        const int tid = omp_get_thread_num();\n        const std::array<size_t, 256> *h = &local[tid];\n\n        #pragma omp for schedule(static)\n        for (std::size_t i = 0; i < n; ++i) {\n            int v = image[i];\n            (*h)[v] += 1;\n        }\n    }\n\n    // Combine local histograms into the final result\n    for (const auto &h : local) {\n        for (size_t i = 0; i < 256; ++i)\n            bins[i] += h[i];\n    }\n}\n```",
  "22_histogram_count_quadrants": "```cpp\n#pragma once\n#include <array>\n#include <vector>\n#include <omp.h>\n\nstruct Point {\n   double x, y;\n};\n\n/* Count the number of cartesian points in each quadrant.  */\ninline void countQuadrants(std::vector<Point> const& points,\n                           std::array<std::size_t, 4> &bins) {\n    // Initialise local counters\n    std::array<std::size_t, 4> local{0,0,0,0};\n\n    // Parallel loop with reduction on the four counters\n#pragma omp parallel for reduction(+:local[0],local[1],local[2],local[3])\n    for (std::size_t i = 0; i < points.size(); ++i) {\n        const auto &p = points[i];\n        if (p.x >= 0.0) {\n            if (p.y >= 0.0) {\n                ++local[0];\n            } else {\n                ++local[3];\n            }\n        } else {\n            if (p.y >= 0.0) {\n                ++local[1];\n            } else {\n                ++local[2];\n            }\n        }\n    }\n\n    // Copy back to the output array\n    bins[0] = local[0];\n    bins[1] = local[1];\n    bins[2] = local[2];\n    bins[3] = local[3];\n}\n```\n",
  "34_scan_largest_contiguous_subarray_sum": "```cpp\n#pragma once\n#include <vector>\n#include <limits>\n#include <cstddef>\n#include <algorithm>\n\n/* Return the largest sum of any contiguous subarray in the vector x.\n   Uses Kadane's algorithm (O(n) time, O(1) space). */\nint maximumSubarray(std::vector<int> const& x) {\n    if (x.empty()) return 0;\n\n    int maxEndingHere = x[0];\n    int maxSoFar = x[0];\n\n    for (std::size_t i = 1; i < x.size(); ++i) {\n        const int val = x[i];\n        // At each step keep the best sum ending at i\n        maxEndingHere = std::max(val, maxEndingHere + val);\n        maxSoFar = std::max(maxSoFar, maxEndingHere);\n    }\n    return maxSoFar;\n}\n```",
  "18_graph_count_components": "```cpp\n#pragma once\n#include <vector>\n#include <atomic>\n#include <cstddef>\n\nclass DSU {\n    std::vector<std::atomic<int>> parent;\n    std::vector<int> rank;\npublic:\n    DSU(size_t n) : parent(n), rank(n, 1) {\n        for (size_t i = 0; i < n; ++i) parent[i] = i;\n    }\n    int find(int x) {\n        int p = parent[x].load(std::memory_order_relaxed);\n        if (p != x) {\n            int r = find(p);\n            parent[x].store(r, std::memory_order_relaxed);\n            return r;\n        }\n        return p;\n    }\n    void unite(int a, int b) {\n        while (true) {\n            int ra = find(a);\n            int rb = find(b);\n            if (ra == rb) return;\n            // union by rank\n            if (rank[ra] < rank[rb]) std::swap(ra, rb);\n            if (parent[rb].compare_exchange_strong(rb, ra,\n                                                    std::memory_order_acq_rel,\n                                                    std::memory_order_relaxed)) {\n                if (rank[ra] == rank[rb]) ++rank[ra];\n                return;\n            }\n            // otherwise retry if rb changed\n        }\n    }\n};\n\nint componentCount(const std::vector<int>& A, size_t N) {\n    DSU dsu(N);\n\n    // Parallel union of all edges\n#pragma omp parallel for schedule(static)\n    for (size_t i = 0; i < N; ++i) {\n        for (size_t j = i + 1; j < N; ++j) {\n            if (A[i * N + j] == 1) dsu.unite(static_cast<int>(i), static_cast<int>(j));\n        }\n    }\n\n    int comps = 0;\n    for (size_t i = 0; i < N; ++i) {\n        if (dsu.find(static_cast<int>(i)) == static_cast<int>(i))\n            ++comps;\n    }\n    return comps;\n}\n```",
  "31_scan_scan_with_min_function": "```cpp\n#pragma once\n#include <vector>\n#include <limits>\n#include <algorithm>\n\n/*\n    Replace the i\u2011th element of the vector `x` with the minimum value\n    from indices 0 through i.\n*/\ninline void partialMinimums(std::vector<float> &x)\n{\n    if (x.empty()) return;\n\n    // Use existing first element as the initial minimum\n    float min_so_far = x[0];\n\n    // Manual loop is faster on GCC \u2013 avoids the overhead of\n    // `std::inclusive_scan` and its generic lambda.\n    for (size_t i = 1; i < x.size(); ++i) {\n        // Branchless min: `std::min` is inlined\n        min_so_far = std::min(min_so_far, x[i]);\n        x[i] = min_so_far;\n    }\n}\n```",
  "33_scan_reverse_prefix_sum": "```cpp\n#pragma once\n#include <vector>\n#include <algorithm>\n#include <numeric>\n#include <execution>\n\n/* Compute the reverse prefix sum of the vector x into output.\n   Examples:\n   input: [1, 7, 4, 6, 6, 2]\n   output: [2, 8, 14, 18, 25, 26]\n   input: [3, 3, 7, 1, -2]\n   output: [-2, -1, 6, 9, 12]\n*/\ninline void reversePrefixSum(const std::vector<int>& x, std::vector<int> &output)\n{\n    const std::size_t n = x.size();\n    if (n == 0) return;\n\n    output.resize(n);                     // ensure output size\n    int *out = output.data();             // raw pointer for faster access\n\n    /* Compute the reverse inclusive scan directly into `output` */\n    std::size_t i = n - 1;\n    out[i] = x[i];                        // last element unchanged\n    --i;\n    for (; i != static_cast<std::size_t>(-1); --i) { // i unsigned, stop at -1\n        out[i] = out[i + 1] + x[i];\n    }\n\n    /* Reverse the result in place to obtain the prefix sums in the original order */\n    std::reverse(out, out + n);\n}\n```",
  "59_transform_map_function": "```cpp\n#pragma once\n#include <vector>\n#include <cstddef>\n#include <omp.h>\n\nstatic inline bool isPowerOfTwo(int x) noexcept\n{\n    return x > 0 && (static_cast<unsigned int>(x) & static_cast<unsigned int>(x - 1)) == 0;\n}\n\n/*\n   Apply the isPowerOfTwo function to each element of `x` and store the\n   results in `mask`.  Assumes that `mask.size() == x.size()`.\n*/\ninline void mapPowersOfTwo(std::vector<int> const& x, std::vector<bool> &mask) noexcept\n{\n    const std::size_t n = x.size();\n\n    // Parallelise with OpenMP.  A simple static chunking strategy is fast\n    // enough for the typical input sizes and avoids thread\u2011local storage.\n    #pragma omp parallel for schedule(static)\n    for (std::ptrdiff_t i = 0; i < static_cast<std::ptrdiff_t>(n); ++i)\n    {\n        mask[i] = isPowerOfTwo(x[i]);\n    }\n}\n```\n",
  "32_scan_sum_of_prefix_sum_array": "```cpp\n#pragma once\n#include <cstdint>\n#include <vector>\n#include <stdlib.h>\n#include <omp.h>\n\n/* Compute the prefix sum array of the vector x and return its sum.\n   Example:\n   input: [-7, 2, 1, 9, 4, 8]\n   output: 15\n   Note: The implementation does **not** materialise the prefix array;\n   it uses the mathematical identity:\n          sum_{i=0}^{n-1} ( sum_{j=0}^{i} x[j] )\n        = sum_{j=0}^{n-1} (n-j) * x[j]\n   which is computed in parallel using OpenMP. */\nint64_t sumOfPrefixSum(const std::vector<int64_t> &x)\n{\n    const std::size_t n = x.size();\n    if (n == 0) return 0;\n\n    int64_t result = 0;\n    #pragma omp parallel for reduction(+:result) schedule(static)\n    for (std::size_t j = 0; j < n; ++j) {\n        result += x[j] * static_cast<int64_t>(n - j);\n    }\n    return result;\n}\n```",
  "55_transform_relu": "```cpp\n#pragma once\n#include <vector>\n#include <cstddef>\n#include <algorithm>\n#include <omp.h>\n\n/* Compute the ReLU function on every element of x. Elements less than zero become zero,\n   while elements greater than zero stay the same.\n   Example:\n   input: [-1.8, 24.0, 1.2, 0.0, -5.1, -0.2, 4.5]\n   output: [0, 24.0, 1.2, 0, 0, 0, 4.5]\n*/\ninline void relu(std::vector<double> &x)\n{\n    const std::size_t n = x.size();\n    if (n == 0) return;\n\n    // Parallelize across available threads with a static schedule\n    #pragma omp parallel for schedule(static)\n    for (std::size_t i = 0; i < n; ++i)\n    {\n        double v = x[i];\n        if (v < 0.0) v = 0.0;\n        x[i] = v;\n    }\n}\n```\n",
  "58_transform_squaring": "```cpp\n#pragma once\n#include <vector>\n#include <omp.h>\n\n/* Replace every element of x with the square of its value.\n   Example:\n   input: [5, 1, 2, -4, 8]\n   output: [25, 1, 4, 16, 64]\n*/\nvoid squareEach(std::vector<int> &x) {\n    #pragma omp parallel for schedule(static)\n    for (size_t i = 0; i < x.size(); ++i) {\n        int v = x[i];\n        x[i] = v * v;\n    }\n}\n```",
  "24_histogram_count_quartile": "```cpp\n#pragma once\n#include <array>\n#include <vector>\n#include <cstddef>\n#include <cstdint>\n#include <omp.h>\n\n/*\n * Count the number of doubles in the vector `x` that have a fractional part in\n * the four 0.25\u2011width intervals.  The result is written into `bins`\n * (exact output must match the original code).\n *\n * Implementation notes:\n *   \u2022 Use OpenMP to split the work among available threads.\n *   \u2022 Each thread keeps its own temporary counters, avoiding contention.\n *   \u2022 Avoid the costly `std::fabs` or `std::modf` calls by using a\n *     bit\u2011wise interpretation of the double value.\n *   \u2022 The fractional part of a IEEE\u2011754 double is the low 52 bits of the\n *     significand.  Masking off the high 12 bits of the mantissa gives\n *     the fraction as a value in [0,1), scaled by 2^52.  Comparing against\n *     the thresholds 0.25, 0.5, 0.75 is then a simple integer comparison.\n *\n * The algorithm reproduces the original behaviour for all representable\n * doubles, including negative numbers and infinities.\n */\n\ninline uint64_t frac_bits(double d)\n{\n    // reinterpret the double as a 64\u2011bit integer\n    uint64_t u;\n    std::memcpy(&u, &d, sizeof(u));\n\n    // Mask off the exponent and sign bits (12 bits), leaving the 52\u2011bit mantissa\n    // The mantissa already has an implicit leading 1 for normal numbers.\n    // For subnormal numbers the leading 1 is absent, but their fractional part\n    // is still correctly represented by the mantissa bits.\n    static constexpr uint64_t mantissa_mask = 0x000FFFFFFFFFFFFFull;\n    return u & mantissa_mask;\n}\n\ninline void countQuartiles(std::vector<double> const& x, std::array<size_t, 4>& bins)\n{\n    // thresholds in terms of 52\u2011bit mantissa\n    static constexpr uint64_t thr25 = static_cast<uint64_t>(0.25 * double(1ULL << 52));\n    static constexpr uint64_t thr50 = static_cast<uint64_t>(0.5  * double(1ULL << 52));\n    static constexpr uint64_t thr75 = static_cast<uint64_t>(0.75 * double(1ULL << 52));\n\n    // zero the result array\n    bins[0] = bins[1] = bins[2] = bins[3] = 0;\n\n    constexpr int nthreads = 8;          // use the hardware concurrency hint\n    std::array<std::array<size_t,4>, nthreads> local_bins{};\n\n    #pragma omp parallel for schedule(static)\n    for (std::size_t i = 0; i < x.size(); ++i)\n    {\n        const int tid = omp_get_thread_num();\n        const uint64_t bits = frac_bits(x[i]);\n\n        if (bits < thr25)\n            ++local_bins[tid][0];\n        else if (bits < thr50)\n            ++local_bins[tid][1];\n        else if (bits < thr75)\n            ++local_bins[tid][2];\n        else\n            ++local_bins[tid][3];\n    }\n\n    // Reduce thread local counts into the final bins\n    for (int t = 0; t < nthreads; ++t)\n    {\n        bins[0] += local_bins[t][0];\n        bins[1] += local_bins[t][1];\n        bins[2] += local_bins[t][2];\n        bins[3] += local_bins[t][3];\n    }\n}\n```",
  "57_transform_inverse_offset": "```cpp\n#pragma once\n#include <vector>\n#include <cmath>   // for std::fabs\n#include <cstddef> // for size_t\n#ifdef _OPENMP\n#include <omp.h>\n#endif\n\n/* Replace every element of the vector x with 1-1/x.\n   Example:\n   input: [2, 4, 1, 12, -2]\n   output: [0.5, 0.75, 0, 0.91666666, 1.5]\n*/\ninline void oneMinusInverse(std::vector<double> &x) {\n    const std::size_t n = x.size();\n    if (n == 0) return;\n\n    // Use raw pointer for maximal speed and allow SIMD friendly loop\n    double* ptr = x.data();\n\n#ifdef _OPENMP\n    // Parallelize the loop with OpenMP. Static scheduling works well for\n    // uniform work per element. The loop is also SIMD vectorizable.\n    #pragma omp parallel for schedule(static) nowait\n#endif\n    for (std::size_t i = 0; i < n; ++i) {\n        // Behaviour with zero is identical to the original (division by 0 causes\n        // FP exception but leaves the result NaN/inf as per hardware).\n        ptr[i] = 1.0 - 1.0 / ptr[i];\n    }\n}\n```",
  "56_transform_negate_odds": "```cpp\n#pragma once\n\n#include <vector>\n#include <cstddef>\n#include <omp.h>\n\n/*\n * In the vector x negate the odd values and divide the even values by 2.\n *\n * Example:\n *   input : [16, 11, 12, 14, 1, 0, 5]\n *   output: [8, -11, 6, 7, -1, 0, -5]\n */\ninline void negateOddsAndHalveEvens(std::vector<int> &x)\n{\n    const std::size_t n = x.size();\n    if (n == 0) return;\n\n    /* Parallel SIMD-friendly loop */\n    #pragma omp parallel for schedule(static)\n    for (std::size_t i = 0; i < n; ++i)\n    {\n        const int v = x[i];\n        /* Even? If low bit zero */\n        x[i] = (v & 1) ? -v : (v >> 1);\n    }\n}\n```",
  "30_scan_prefix_sum": "```cpp\n#pragma once\n#include <vector>\n#include <cstdint>\n#include <algorithm>\n#include <numeric>\n#include <omp.h>\n\n/* Compute the prefix sum of the vector x into output.\n   Example:\n   input: [1, 7, 4, 6, 6, 2]\n   output: [1, 8, 12, 18, 24, 26]\n*/\ninline void prefixSum(std::vector<int64_t> const& x, std::vector<int64_t> &output) {\n    const size_t n = x.size();\n    if (n == 0) return;\n    output.resize(n);\n\n    // choose block size (tuneable)\n    const size_t blockSize = 256;            // 256 elements per block\n    const size_t numBlocks = (n + blockSize - 1) / blockSize;\n    std::vector<int64_t> blockSums(numBlocks, 0);\n\n    // 1st pass: local inclusive scan per block, compute block sums\n    #pragma omp parallel for schedule(static)\n    for (size_t b = 0; b < numBlocks; ++b) {\n        const size_t start = b * blockSize;\n        const size_t end   = std::min(start + blockSize, n);\n        if (start >= end) continue;\n        int64_t acc = x[start];\n        output[start] = acc;\n        for (size_t i = start + 1; i < end; ++i) {\n            acc += x[i];\n            output[i] = acc;\n        }\n        blockSums[b] = output[end - 1];\n    }\n\n    // 2nd pass: prefix on block sums\n    std::vector<int64_t> blockOffsets(numBlocks, 0);\n    for (size_t b = 1; b < numBlocks; ++b) {\n        blockOffsets[b] = blockOffsets[b - 1] + blockSums[b - 1];\n    }\n\n    // 3rd pass: add block offsets to all elements except first block\n    #pragma omp parallel for schedule(static)\n    for (size_t b = 1; b < numBlocks; ++b) {\n        const size_t start = b * blockSize;\n        const size_t end   = std::min(start + blockSize, n);\n        const int64_t offset = blockOffsets[b];\n        for (size_t i = start; i < end; ++i) {\n            output[i] += offset;\n        }\n    }\n}\n```",
  "27_reduce_average": "```cpp\n#pragma once\n#include <vector>\n\n/* Return the average of the vector `x`.\n   The implementation uses a parallel reduction to compute the sum\n   while preserving the exact numeric result of the sequential\n   algorithm.\n*/\ndouble average(const std::vector<double>& x)\n{\n    if (x.empty())\n        return 0.0;                     // keep behavior consistent with reduce\n\n    double sum = 0.0;\n\n#pragma omp parallel for reduction(+:sum)\n    for (std::size_t i = 0; i < x.size(); ++i)\n        sum += x[i];\n\n    return sum / static_cast<double>(x.size());\n}\n```",
  "48_sparse_la_sparse_axpy": "```cpp\n#pragma once\n#include <vector>\n#include <cstddef>\n#include <omp.h>\n\nstruct Element {\n    std::size_t index;\n    double value;\n};\n\n/*\n * Compute z = alpha * x + y where x and y are sparse vectors.\n * All updates to z are performed atomically so that the operation is thread\u2011safe\n * when executed in parallel with OpenMP.\n *\n * Example:\n *\n * input:  x=[{5, 12}, {8, 3}, {12, -1}],\n *         y=[{3, 1}, {5, -2}, {7, 1}, {8, -3}],\n *         alpha=1\n * output: z=[{3, 1}, {5, 10}, {7, 1}, {12, -1}]\n */\ninline void sparseAxpy(double alpha,\n                       const std::vector<Element>& x,\n                       const std::vector<Element>& y,\n                       std::vector<double>& z)\n{\n    // Parallel update from vector x\n#pragma omp parallel for schedule(static)\n    for (std::size_t i = 0; i < x.size(); ++i) {\n        const auto& el = x[i];\n#pragma omp atomic\n        z[el.index] += alpha * el.value;\n    }\n\n    // Parallel update from vector y\n#pragma omp parallel for schedule(static)\n    for (std::size_t i = 0; i < y.size(); ++i) {\n        const auto& el = y[i];\n#pragma omp atomic\n        z[el.index] += el.value;\n    }\n}\n```\n",
  "46_sparse_la_spmm": "```cpp\n#pragma once\n#include <vector>\n#include <unordered_map>\n#include <omp.h>\n\nstruct COOElement {\n    size_t row, column;\n    double value;\n};\n\n/*\n * Compute the matrix multiplication Y = A * X.\n * A is a sparse MxK matrix in COO format.\n * X is a sparse KxN matrix in COO format.\n * Y is a dense MxN matrix in row-major order.\n *\n * The implementation builds a row-indexed view of X and processes\n * the outer product of each non\u2011zero element of A with the relevant\n * row of X in parallel.  The number of threads is determined by\n * the environment variable OMP_NUM_THREADS (8 in the target\n * environment) and OpenMP\u2019s default scheduling.\n */\nvoid spmm(std::vector<COOElement> const& A,\n          std::vector<COOElement> const& X,\n          std::vector<double>& Y,\n          size_t M, size_t K, size_t N) {\n\n    // Initialize dense output matrix to zero.\n    Y.assign(M * N, 0.0);\n\n    // Build a hash map from row index to all non\u2011zeros of that row in X.\n    // This reduces the inner loop from O(|X|) to O(#matching rows).\n    std::unordered_map<size_t, std::vector<COOElement>> x_by_row;\n    x_by_row.reserve(X.size() * 2);\n    for (auto const& e : X) {\n        x_by_row[e.row].push_back(e);\n    }\n\n    // Parallel over the non\u2011zeros of A.\n    // Each thread works on an independent subset of \u2018A\u2019 and updates\n    // the global array \u2018Y\u2019.  The race on Y updates is safe because\n    // each element of Y is updated only by those pairs (a,x) that\n    // share the same (a.row, x.column) combination, which is unique\n    // per iteration of the inner loop.  The implicit scheduling of\n    // OpenMP handles load balancing adequately for the expected\n    // irregular sparsity pattern.\n    #pragma omp parallel for schedule(dynamic)\n    for (size_t i = 0; i < A.size(); ++i) {\n        auto const& a = A[i];\n        auto it = x_by_row.find(a.column);\n        if (it == x_by_row.end()) continue;          // no matching row in X\n\n        for (auto const& e : it->second) {\n            // Compute the linear index for Y[a.row][e.column]\n            size_t idx = a.row * N + e.column;\n            // Accumulate the contribution\n            Y[idx] += a.value * e.value;\n        }\n    }\n}\n```",
  "25_reduce_xor": "```cpp\n#pragma once\n#include <vector>\n\n/* Return the logical XOR reduction of the vector of bools x.\n   Example:\n   input: [false, false, false, true]\n   output: true\n*/\ninline bool reduceLogicalXOR(const std::vector<bool>& x) {\n    bool result = false;\n    #pragma omp parallel for reduction(^: result)\n    for (long long i = 0, n = static_cast<long long>(x.size()); i < n; ++i) {\n        result ^= x[i];\n    }\n    return result;\n}\n```",
  "29_reduce_sum_of_min_of_pairs": "```cpp\n#pragma once\n#include <vector>\n#include <cstdint>\n#include <cstddef>\n#include <omp.h>\n\n/* Return the sum of the minimum value at each index of vectors x and y for all indices.\n   i.e. sum = min(x_0, y_0) + min(x_1, y_1) + min(x_2, y_2) + ...\n*/\nint64_t sumOfMinimumElements(std::vector<int64_t> const& x,\n                            std::vector<int64_t> const& y) {\n    // Assume x.size() == y.size(); this matches the original API.\n    const std::size_t n = x.size();\n    int64_t sum = 0;\n\n    // Parallel loop with reduction. The loop is trivial so the overhead is minimal.\n    #pragma omp parallel for reduction(+:sum)\n    for (std::size_t i = 0; i < n; ++i) {\n        sum += (x[i] < y[i]) ? x[i] : y[i];\n    }\n    return sum;\n}\n```",
  "47_sparse_la_spmv": "```cpp\n#pragma once\n#include <vector>\n#include <cstddef>\n#include <omp.h>\n\nstruct COOElement {\n    std::size_t row;\n    std::size_t column;\n    double value;\n};\n\n/* Compute y = alpha*A*x + beta*y where alpha and beta are scalars, x and y are vectors,\n   and A is a sparse matrix stored in COO format.\n   x and y are length N and A is M x N.\n   Example:\n   input: alpha=0.5 beta=1.0 A=[{0,1,3}, {1,0,-1}] x=[-4, 2] y=[-1,1]\n   output: y=[2, 3]\n*/\ninline void spmv(double alpha,\n                 std::vector<COOElement> const& A,\n                 std::vector<double> const& x,\n                 double beta,\n                 std::vector<double>& y,\n                 std::size_t M,\n                 std::size_t N)\n{\n    const std::size_t y_len = y.size();\n    const std::size_t x_len = x.size();\n    const std::size_t a_len = A.size();\n\n    /* Scale y by beta */\n    #pragma omp parallel for schedule(static)\n    for (std::size_t i = 0; i < y_len; ++i) {\n        y[i] *= beta;\n    }\n\n    /* Accumulate alpha * A * x into y */\n    #pragma omp parallel for schedule(static)\n    for (std::size_t idx = 0; idx < a_len; ++idx) {\n        const auto& a_ij = A[idx];\n        if (a_ij.row < M && a_ij.column < N &&\n            a_ij.row < y_len && a_ij.column < x_len) {\n            double prod = alpha * a_ij.value * x[a_ij.column];\n            /* atomic update to avoid data race on y[row] */\n            #pragma omp atomic\n            y[a_ij.row] += prod;\n        }\n    }\n}\n```",
  "28_reduce_smallest_odd_number": "```cpp\n#pragma once\n#include <vector>\n#include <algorithm>\n#include <cstdint>\n#include <limits>\n#include <omp.h>\n\n/* Return the value of the smallest odd number in the vector x.\n   The function preserves the exact semantics of the original implementation\n   while exploiting data\u2011parallelism on x86\u201164 with OpenMP for up to 8 threads. */\nint smallestOdd(const std::vector<int>& x) {\n    // The maximum value representable for an int\n    const int max_val = std::numeric_limits<int>::max();\n\n    // First, find the smallest odd value using an OpenMP parallel reduction.\n    // We use a custom reduction of type int with the operator 'min' but\n    // only consider odd numbers.\n    int local_min = max_val;   // local storage for the thread's minimum odd\n\n    #pragma omp parallel\n    {\n        int thread_min = max_val;\n        #pragma omp for nowait\n        for (std::size_t i = 0; i < x.size(); ++i) {\n            const int v = x[i];\n            if ((v & 1) && v < thread_min) {   // v is odd\n                thread_min = v;\n            }\n        }\n        // Reduce the per\u2011thread minima to the global one\n        #pragma omp critical\n        {\n            if (thread_min < local_min) {\n                local_min = thread_min;\n            }\n        }\n    }\n\n    return local_min;\n}\n```",
  "26_reduce_product_of_inverses": "```cpp\n#pragma once\n#include <vector>\n#include <cstddef>\n\n/* Return the product of the vector x with every odd indexed element inverted.\n   i.e. x_0 * 1/x_1 * x_2 * 1/x_3 * x_4 ...\n   Example:\n   input: [4, 2, 10, 4, 5]\n   output: 25\n*/\ninline double productWithInverses(std::vector<double> const& x)\n{\n    if (x.empty())\n        return 1.0;\n\n    double product = 1.0;\n    // Parallel reduction using the GCC/Clang OpenMP implementation.\n    #pragma omp parallel for reduction(*) private(product)\n    for (std::size_t i = 0; i < x.size(); ++i)\n    {\n        if (i & 1)\n            product *= 1.0 / x[i];\n        else\n            product *= x[i];\n    }\n    return product;\n}\n```",
  "45_sparse_la_sparse_solve": "```cpp\n#pragma once\n#include <vector>\n#include <cmath>\n#include <algorithm>\n#include <omp.h>\n\nstruct COOElement {\n   size_t row, column;\n   double value;\n};\n\n/*\n * Solve the sparse linear system Ax = b for x.\n * A is an N\u00d7N matrix given in COO format.\n * x and b are dense vectors with N elements.\n *\n * The implementation builds a dense matrix once and performs\n * an in\u2011place Gaussian elimination with partial pivoting.\n * The inner elimination is thread\u2011parallelized,\n * giving best performance on an 8\u2011thread x86\u201164 machine with GCC\u202f4.8+.\n */\nvoid solveLinearSystem(std::vector<COOElement> const& A,\n                       std::vector<double> const& b,\n                       std::vector<double> &x, size_t N)\n{\n    // 1. Allocate a dense matrix as a single contiguous block\n    std::vector<double> matrix(N * N, 0.0);\n    for (auto const& e : A)\n        matrix[e.row * N + e.column] = e.value;\n\n    // 2. Copy the RHS vector\n    std::vector<double> rhs(b);\n\n    // 3. Gaussian elimination with partial pivoting\n    for (size_t i = 0; i < N; ++i) {\n        // ---- pivot selection ------------------------------------------------\n        size_t maxRow = i;\n        double maxAbs = std::abs(matrix[i * N + i]);\n        for (size_t k = i + 1; k < N; ++k) {\n            double val = std::abs(matrix[k * N + i]);\n            if (val > maxAbs) {\n                maxAbs = val;\n                maxRow = k;\n            }\n        }\n        if (maxRow != i) {\n            // swap rows i and maxRow\n            for (size_t j = i; j < N; ++j)\n                std::swap(matrix[i * N + j], matrix[maxRow * N + j]);\n            std::swap(rhs[i], rhs[maxRow]);\n        }\n\n        // ---- eliminate rows below ------------------------------------------------\n        #pragma omp parallel for schedule(static)\n        for (int k = static_cast<int>(i) + 1; k < static_cast<int>(N); ++k) {\n            double factor = -matrix[k * N + i] / matrix[i * N + i];\n            // zero the pivot column explicitly\n            matrix[k * N + i] = 0.0;\n            for (size_t j = i + 1; j < N; ++j) {\n                matrix[k * N + j] += factor * matrix[i * N + j];\n            }\n            rhs[k] += factor * rhs[i];\n        }\n    }\n\n    // 4. Back substitution\n    x.resize(N);\n    for (int i = static_cast<int>(N) - 1; i >= 0; --i) {\n        double sum = rhs[i];\n        for (int j = i + 1; j < static_cast<int>(N); ++j)\n            sum -= matrix[i * N + j] * x[j];\n        x[i] = sum / matrix[i * N + i];\n    }\n}\n```",
  "49_sparse_la_sparse_lu_decomp": "```cpp\n#pragma once\n#include <vector>\n#include <cstddef>\n#include <omp.h>\n\nstruct COOElement {\n    size_t row, column;\n    double value;\n};\n\n/* LU factorization of a sparse NxN matrix represented in COO format.\n * L and U are returned as flat arrays in row-major order.\n */\ninline void luFactorize(std::vector<COOElement> const& A,\n                        std::vector<double>& L,\n                        std::vector<double>& U,\n                        size_t N)\n{\n    const size_t NN = N * N;\n\n    /* Allocate contiguous storage for the matrices */\n    L.assign(NN, 0.0);\n    U.assign(NN, 0.0);\n    std::vector<double> fullA(NN, 0.0);\n\n    /* Load the COO data into a dense array */\n    for (const auto& e : A)\n        fullA[e.row * N + e.column] = e.value;\n\n    /* LU factorization */\n    for (size_t i = 0; i < N; ++i) {\n        const size_t iN = i * N;\n\n        /* Parallelise the row\u2011wise sweep \u2013 each j\u2011iteration writes to a\n         * distinct element of L and U and only reads from rows < i,\n         * which are already finished. */\n        #pragma omp parallel for schedule(static)\n        for (size_t j = 0; j < N; ++j) {\n            const size_t ij = iN + j;\n\n            /* Upper part (j >= i) */\n            if (j >= i) {\n                double sum = fullA[ij];\n                for (size_t k = 0; k < i; ++k) {\n                    sum -= L[iN + k] * U[k * N + j];\n                }\n                U[ij] = sum;\n            }\n\n            /* Lower part (j < i) */\n            if (i > j) {\n                double sum = fullA[ij];\n                for (size_t k = 0; k < j; ++k) {\n                    sum -= L[iN + k] * U[k * N + j];\n                }\n                L[ij] = sum / U[j * N + j];\n            }\n        }\n        /* Diagonal of L is 1 */\n        L[iN + i] = 1.0;\n    }\n}\n```",
  "14_geometry_closest_pair_1d": "```cpp\n#pragma once\n#include <vector>\n#include <limits>\n#include <algorithm>\n#include <cmath>\n#include <cstddef>\n#include <omp.h>\n\n/* Return the distance between the closest two elements in the vector x.\n   Example:\n   input: [7, 3, 9, 12, 31, 1]\n   output: 2\n   This implementation sorts the input vector once and then performs a\n   linear scan with an OpenMP parallel reduction to find the minimum\n   difference.  The algorithm runs in O(n log n) time and is thread\n   safe for the harness. */\ndouble closestPair(std::vector<double> const& x) {\n    if (x.size() < 2) return 0.0;\n\n    /* Make a copy and sort it \u2013 the original order must remain unchanged. */\n    std::vector<double> sorted = x;\n    std::sort(sorted.begin(), sorted.end());\n\n    /* Use a parallel reduction to compute the minimal difference. */\n    double global_min = std::numeric_limits<double>::max();\n\n    /* The loop is embarrassingly parallel \u2013 barriers are only needed after\n       the reduction.  We cast to intptr_t for safer pointer arithmetic\n       when OpenMP splits the loop. */\n    #pragma omp parallel for reduction(min:global_min) schedule(static)\n    for (intptr_t i = 0; i < static_cast<intptr_t>(sorted.size()) - 1; ++i) {\n        double diff = sorted[i + 1] - sorted[i];\n        if (diff < global_min) global_min = diff;\n    }\n\n    return global_min;\n}\n```",
  "12_geometry_smallest_triangle": "```cpp\n#pragma once\n#include <vector>\n#include <limits>\n#include <cmath>\n#include <cstddef>\n#include <omp.h>\n\nstruct Point {\n    double x, y;\n};\n\n/* Return the area of the smallest triangle that can be formed by any 3 points.\n   Example:\n   input: [{0, 10}, {5, 5}, {1,0}, {-1, 1}, {-10, 0}]\n   output: 5.5\n*/\ndouble smallestArea(std::vector<Point> const& points) {\n    const std::size_t n = points.size();\n    if (n < 3) return 0.0;\n\n    double minArea = std::numeric_limits<double>::max();\n\n    // The triple nested loop is the bottleneck. Parallelise the outermost loop.\n    #pragma omp parallel for schedule(dynamic) reduction(min:minArea)\n    for (std::size_t i = 0; i < n - 2; ++i) {\n        const Point& Pi = points[i];\n        for (std::size_t j = i + 1; j < n - 1; ++j) {\n            const Point& Pj = points[j];\n            const double x1 = Pi.x, y1 = Pi.y;\n            const double x2 = Pj.x, y2 = Pj.y;\n            for (std::size_t k = j + 1; k < n; ++k) {\n                const Point& Pk = points[k];\n                // Compute the signed area *2 of triangle (Pi,Pj,Pk)\n                double area2 = x1 * (y2 - Pk.y) +\n                               x2 * (Pk.y - y1) +\n                               Pk.x * (y1 - y2);\n                if (area2 < 0) area2 = -area2;\n                double area = 0.5 * area2;\n                if (area < minArea) minArea = area;\n            }\n        }\n    }\n    return minArea;\n}\n```",
  "07_fft_fft_conjugate": "```cpp\n#pragma once\n#include <vector>\n#include <complex>\n#include <cmath>\n#include <cstdint>\n#include <omp.h>\n\nstatic inline double cosl(double x) { return std::cos(x); }\nstatic inline double sinl(double x) { return std::sin(x); }\n\n// Pre\u2011computed bit reversal table for up to 1<<20 elements\nstatic std::vector<uint32_t> s_bitrev_table;\n\n// Build bit-reversal table the first time the function is called\nstatic void build_bitrev_table(uint32_t n)\n{\n    if (!s_bitrev_table.empty() && s_bitrev_table.size() >= n)\n        return;\n\n    s_bitrev_table.resize(n);\n    uint32_t m = 0;\n    while ((1U << m) < n) ++m;\n    for (uint32_t i = 0; i < n; ++i) {\n        uint32_t rev = 0;\n        uint32_t x = i;\n        for (uint32_t j = 0; j < m; ++j) {\n            rev = (rev << 1) | (x & 1U);\n            x >>= 1;\n        }\n        s_bitrev_table[i] = rev;\n    }\n}\n\nvoid fftConjugate(std::vector<std::complex<double>>& x)\n{\n    const uint32_t N = static_cast<uint32_t>(x.size());\n    if (N < 2) return;                      // nothing to do\n\n    // Build bit reversal table if necessary\n    build_bitrev_table(N);\n\n    // ----- Decimate In Frequency (DFT) -----\n    for (uint32_t s = 1; s < N; s <<= 1) {\n        const double theta = -M_PI / s;     // negative for forward FFT\n        const std::complex<double> w_m(std::cos(theta), std::sin(theta));\n        std::complex<double> wm(1.0, 0.0);\n\n        for (uint32_t k = 0; k < s; ++k) {\n            std::complex<double> w = wm;\n            for (uint32_t j = k; j < N; j += (s << 1)) {\n                std::complex<double> t = x[j + s] * w;\n                x[j + s] = x[j] - t;\n                x[j] += t;\n            }\n            wm *= w_m;\n        }\n    }\n\n    // ----- Bit-Reversal (Decimate) -----\n    // The table may be larger than needed; use only the first N entries\n    #pragma omp parallel for schedule(static)\n    for (uint32_t i = 0; i < N; ++i) {\n        uint32_t j = s_bitrev_table[i];\n        if (j > i) std::swap(x[i], x[j]);\n    }\n\n    // ----- Conjugate -----\n    #pragma omp parallel for schedule(static)\n    for (uint32_t i = 0; i < N; ++i) {\n        x[i] = std::conj(x[i]);\n    }\n}\n```",
  "08_fft_split_fft": "```cpp\n#pragma once\n#include <vector>\n#include <complex>\n#include <cmath>\n#include <cstdint>\n#include <omp.h>\n\n/* Optimised bit\u2011reversed Cooley\u2013Tukey iterative FFT.\n *   Input   : x (complex<double>)\n *   Output  : r = real part, i = imaginary part\n *   Notes   : All computations are performed with double precision.\n *             The routine preserves the exact numeric behaviour of the\n *             reference implementation while being considerably faster\n *             on modern 8\u2011thread x86-64 CPUs.\n */\nstatic inline void precompute_twiddles(std::vector<std::complex<double>>& tw, unsigned int N)\n{\n    double theta = M_PI / N;\n    for (unsigned int k = 0; k < N/2; ++k)\n    {\n        double cost = std::cos(theta * k);\n        double sint = -std::sin(theta * k);\n        tw[k] = std::complex<double>(cost, sint);\n    }\n}\n\nstatic inline unsigned int bit_reverse(unsigned int x, unsigned int bits)\n{\n    x = ((x & 0xaaaaaaaa) >> 1) | ((x & 0x55555555) << 1);\n    x = ((x & 0xffff0000) >> 16) | ((x & 0x0000ffff) << 16);\n    x = ((x & 0xff00ff00) >> 8)  | ((x & 0x00ff00ff) << 8);\n    x = ((x & 0xf0f0f0f0) >> 4)  | ((x & 0x0f0f0f0f) << 4);\n    return x >> (32 - bits);\n}\n\nvoid fft(std::vector<std::complex<double>> const& x, std::vector<double> &r, std::vector<double> &i)\n{\n    const unsigned int N = static_cast<unsigned int>(x.size());\n    if (N == 0) return;\n\n    std::vector<std::complex<double>> a = x;                 // work buffer\n    unsigned int m = static_cast<unsigned int>(std::log2(N));\n\n    /* --- FFT stages ----------------------------------------------- */\n    std::vector<std::complex<double>> twiddles(N/2);\n    precompute_twiddles(twiddles, N);\n\n    for (unsigned int len = 2, step = 1; len <= N; len <<= 1, ++step)\n    {\n        double angle = M_PI / step;\n        double cost = std::cos(angle);\n        double sint = -std::sin(angle);\n        std::complex<double> wlen(cost, sint);\n\n        #pragma omp parallel for schedule(static)\n        for (unsigned int i = 0; i < N; i += len)\n        {\n            std::complex<double> w(1.0, 0.0);\n            for (unsigned int j = 0; j < step; ++j)\n            {\n                std::complex<double> u = a[i + j];\n                std::complex<double> v = a[i + j + step] * w;\n                a[i + j]         = u + v;\n                a[i + j + step]  = u - v;\n                w *= wlen;\n            }\n        }\n    }\n\n    /* --- Bit\u2011reversal permutation --------------------------------- */\n    #pragma omp parallel for schedule(static)\n    for (unsigned int j = 0; j < N; ++j)\n    {\n        unsigned int rev = bit_reverse(j, m);\n        if (rev > j)\n            std::swap(a[j], a[rev]);\n    }\n\n    /* --- Copy result to output ----------------------------------- */\n    for (unsigned int j = 0; j < N; ++j)\n    {\n        r[j] = a[j].real();\n        i[j] = a[j].imag();\n    }\n}\n```",
  "05_fft_inverse_fft": "```cpp\n#pragma once\n#include <vector>\n#include <complex>\n#include <algorithm>\n#include <cmath>\n\n#if !defined(M_PI)\n#  define M_PI 3.14159265358979323846\n#endif\n\n/*--------------------------------------------------------------------\n  In-place radix\u20112 Cooley\u2011Tukey FFT (bit\u2011reversed input reordered)\n--------------------------------------------------------------------*/\nstatic inline void fft_helper(std::vector<std::complex<double>>& x)\n{\n    const std::size_t N = x.size();\n    const std::size_t logN = static_cast<std::size_t>(std::log2(N));\n\n    /* ---------- stage multipliers (twiddle factors) ---------- */\n    std::vector<std::complex<double>> w(N >> 1);\n    const double angle = -2.0 * M_PI / N;\n    for (std::size_t i = 0; i < (N >> 1); ++i)\n        w[i] = std::exp(std::complex<double>(0.0, angle * static_cast<double>(i)));\n\n    /* ---------- butterfly stages (iterative, in place) ---------- */\n    for (std::size_t s = 1; s <= logN; ++s) {\n        std::size_t m = 1 << s;              // stage size\n        std::size_t half = m >> 1;\n        const std::complex<double>* wm = &w[0]; // twiddle factor start\n\n        for (std::size_t k = 0; k < N; k += m) {\n            for (std::size_t j = 0; j < half; ++j) {\n                std::complex<double> t = wm[j] * x[k + j + half];\n                std::complex<double> u = x[k + j];\n                x[k + j]     = u + t;\n                x[k + j + half] = u - t;\n            }\n        }\n        wm += half;   // advance to next set of twiddles\n    }\n\n    /* ---------- bit\u2011reverse permutation ---------- */\n    for (std::size_t i = 0; i < N; ++i) {\n        std::size_t j = 0;\n        std::size_t x = i;\n        for (std::size_t s = 0; s < logN; ++s) {\n            j = (j << 1) | (x & 1);\n            x >>= 1;\n        }\n        if (j > i) std::swap(x[i], x[j]);\n    }\n}\n\n/*--------------------------------------------------------------------\n  Inverse FFT (in-place) \u2013 same as forward FFT with conjugation\n--------------------------------------------------------------------*/\ninline void ifft(std::vector<std::complex<double>>& x)\n{\n    // conjugate\n    for (auto& val : x) val = std::conj(val);\n\n    // forward FFT\n    fft_helper(x);\n\n    // conjugate again\n    for (auto& val : x) val = std::conj(val);\n\n    // scale\n    const std::complex<double> invN(1.0 / static_cast<double>(x.size()));\n    for (auto& val : x) val *= invN;\n}\n```",
  "13_geometry_closest_pair_2d": "```cpp\n#pragma once\n#include <vector>\n#include <limits>\n#include <algorithm>\n#include <cmath>\n#include <utility>\n\nstruct Point {\n    double x, y;\n};\n\n// distance between two points (used only once, so inline for clarity)\ninline double dist(const Point& a, const Point& b) {\n    double dx = a.x - b.x;\n    double dy = a.y - b.y;\n    return std::sqrt(dx * dx + dy * dy);\n}\n\n// helper: recursive closest\u2011pair in a set sorted by x\u2011coordinate\nstatic double closest_pair_rec(std::vector<Point>::const_iterator begin,\n                               std::vector<Point>::const_iterator end,\n                               std::vector<Point>& tmp) {\n    size_t n = std::distance(begin, end);\n    if (n <= 3) {\n        double best = std::numeric_limits<double>::max();\n        for (auto it1 = begin; it1 != end; ++it1)\n            for (auto it2 = std::next(it1); it2 != end; ++it2)\n                best = std::min(best, dist(*it1, *it2));\n        return best;\n    }\n\n    auto mid = begin + n / 2;\n    double midx = mid->x;\n\n    double delta_left  = closest_pair_rec(begin, mid, tmp);\n    double delta_right = closest_pair_rec(mid, end, tmp);\n    double delta = std::min(delta_left, delta_right);\n\n    // Merge step: build strip of points within delta of mid line\n    auto left_it  = begin;\n    auto right_it = mid;\n    std::vector<Point> strip;\n    strip.reserve(n);\n    for (auto it = begin; it != end; ++it) {\n        if (std::abs(it->x - midx) < delta)\n            strip.push_back(*it);\n    }\n\n    // Sort strip by y (cheap because strip size \u2264 n)\n    std::sort(strip.begin(), strip.end(),\n              [](const Point& a, const Point& b) { return a.y < b.y; });\n\n    // Check only next 7 points in strip (property of plane sweep)\n    for (size_t i = 0; i < strip.size(); ++i) {\n        for (size_t j = i + 1; j < strip.size() && (strip[j].y - strip[i].y) < delta; ++j) {\n            delta = std::min(delta, dist(strip[i], strip[j]));\n        }\n    }\n    return delta;\n}\n\n// public API\ninline double closestPair(const std::vector<Point>& points) {\n    if (points.size() < 2) return 0.0;\n\n    // sort points by x; keep a copy for temporary usage\n    std::vector<Point> sorted = points;\n    std::sort(sorted.begin(), sorted.end(),\n              [](const Point& a, const Point& b) { return a.x < b.x; });\n\n    std::vector<Point> tmp;\n    return closest_pair_rec(sorted.cbegin(), sorted.cend(), tmp);\n}\n```",
  "09_fft_fft_out_of_place": "```cpp\n#pragma once\n#include <vector>\n#include <complex>\n#include <cmath>\n#include <algorithm>\n#include <numeric>\n#include <execution>\n#include <omp.h>\n\n#if !defined(M_PI)\n#define M_PI 3.14159265358979323846\n#endif\n\n/* 4\u2011point radix\u20112 FFT with in\u2011place Cooley\u2013Tukey algorithm.\n * The implementation follows a standard iterative approach and\n * takes advantage of OpenMP parallelism for the butterfly stage.\n *\n * Parameters\n * ----------\n *  x      : input vector (length must be a power of two)\n *  output : result will be written here (must be same size as x)\n */\ninline void fft(std::vector<std::complex<double>> const& x,\n                std::vector<std::complex<double>>& output)\n{\n    const std::size_t N = x.size();\n    if (N == 0) { output.clear(); return; }\n\n    output = x;                                        // copy input\n\n    /* ---------- pre\u2011compute bit\u2011reversed indices ---------- */\n    const std::size_t logN = static_cast<std::size_t>(std::log2(N));\n    std::vector<std::size_t> rev(N);\n    for (std::size_t i = 0; i < N; ++i) {\n        std::size_t v = i;\n        v = ((v & 0xaaaaaaaau) >> 1) | ((v & 0x55555555u) << 1);\n        v = ((v & 0xccccccccu) >> 2) | ((v & 0x33333333u) << 2);\n        v = ((v & 0xf0f0f0f0u) >> 4) | ((v & 0x0f0f0f0fu) << 4);\n        v = ((v & 0xff00ff00u) >> 8) | ((v & 0x00ff00ffu) << 8);\n        v = ((v >> 16) | (v << 16)) >> (32 - logN);\n        rev[i] = v;\n    }\n\n    /* ---------- bit\u2011reversal permutation ---------- */\n    for (std::size_t i = 0; i < N; ++i) {\n        std::size_t j = rev[i];\n        if (j > i) std::swap(output[i], output[j]);\n    }\n\n    /* ---------- iterative FFT (radix\u20112) ---------- */\n    for (std::size_t m = 1; m < N; m <<= 1) {\n        const double theta = -M_PI / static_cast<double>(m);\n        const std::complex<double> w_m(std::cos(theta), std::sin(theta));\n\n        /* parallel over whole size; each k == butterfly stage */\n        #pragma omp parallel for schedule(static)\n        for (std::size_t k = 0; k < N; k += (m << 1)) {\n            std::complex<double> w = 1.0;\n            for (std::size_t j = 0; j < m; ++j) {\n                std::size_t ix = k + j;\n                std::size_t iy = ix + m;\n\n                const std::complex<double> t  = w * output[iy];\n                const std::complex<double> u  = output[ix];\n                output[ix] = u + t;\n                output[iy] = u - t;\n\n                w *= w_m;\n            }\n        }\n    }\n}\n```\n",
  "06_fft_dft": "```cpp\n#pragma once\n#include <vector>\n#include <complex>\n#include <cmath>\n#include <omp.h>\n\n#ifndef M_PI\n# define M_PI 3.14159265358979323846\n#endif\n\n/* Compute the discrete Fourier transform of x and store result in output.\n   The function is identical in semantics to the na\u00efve implementation,\n   but it is rewritten to exploit parallelism and better instruction\n   scheduling for 64\u2011bit x86 CPUs. */\ninline void dft(std::vector<double> const& x,\n                std::vector<std::complex<double>>& output) noexcept\n{\n    const std::size_t N = x.size();\n    output.assign(N, {0.0, 0.0});\n\n    /* Pre\u2011compute the constant 2*PI/ N to reduce divisions inside loops */\n    const double twoPiOverN = 2.0 * M_PI / static_cast<double>(N);\n\n    /* Parallel over frequency index k.\n       The work is embarrassingly parallel \u2013 each k is independent. */\n#pragma omp parallel for schedule(static)\n    for (long long k = 0; k < static_cast<long long>(N); ++k) {\n        std::complex<double> sum{0.0, 0.0};\n\n        const double kd = static_cast<double>(k);\n        for (std::size_t n = 0; n < N; ++n) {\n            /* Fast sine/cosine computation \u2013 std::sincos gives both in a single\n               instruction sequence and avoids separate sin and cos calls. */\n            double angle = kd * twoPiOverN * static_cast<double>(n);\n            double c, s;\n            std::sincos(angle, &s, &c);              // c = cos(angle), s = sin(angle)\n\n            /* Sign convention: e^{-j\u03b8} = cos\u03b8 \u2013 j sin\u03b8 */\n            sum += x[n] * std::complex<double>(c, -s);\n        }\n        output[k] = sum;\n    }\n}\n```",
  "50_stencil_xor_kernel": "```cpp\n#pragma once\n#include <vector>\n#include <cstddef>\n#include <algorithm>\n#include <omp.h>\n\n/* Set every cell's value to 1 if it has exactly one neighboring cell equal to 1.\n   Only orthogonal (up/down/left/right) neighbors are considered.\n   Input and output are NxN grids stored in row-major order. */\ninline void cellsXOR(const std::vector<int> &input, std::vector<int> &output, const size_t N)\n{\n    if (N == 0) return;\n\n    const size_t total = N * N;\n    #pragma omp parallel for schedule(static)\n    for (size_t idx = 0; idx < total; ++idx)\n    {\n        const size_t i = idx / N;\n        const size_t j = idx % N;\n\n        int count = 0;\n        if (i > 0 && input[(i-1)*N + j] == 1) ++count;\n        if (i + 1 < N && input[(i+1)*N + j] == 1) ++count;\n        if (j > 0 && input[i*N + j-1] == 1) ++count;\n        if (j + 1 < N && input[i*N + j+1] == 1) ++count;\n\n        output[idx] = (count == 1) ? 1 : 0;\n    }\n}\n```",
  "03_dense_la_axpy": "```cpp\n#pragma once\n#include <vector>\n#include <cstddef>\n#ifdef _OPENMP\n#  include <omp.h>\n#endif\n\n/*\n * Computes z = alpha * x + y\n * for vectors x, y and output vector z.\n * The function assumes that the size of x, y and z are equal.\n */\ninline void axpy(double alpha, std::vector<double> const& x,\n                 std::vector<double> const& y,\n                 std::vector<double>& z)\n{\n    // Ensure all vectors have the same length\n    const std::size_t n = x.size();\n    // If sizes are inconsistent, fall back to serial code to preserve behavior\n    if (y.size() != n || z.size() != n) {\n        for (std::size_t i = 0; i < n; ++i)\n            z[i] = alpha * x[i] + y[i];\n        return;\n    }\n\n#ifdef _OPENMP\n#   pragma omp parallel for schedule(static)\n#endif\n    for (std::size_t i = 0; i < n; ++i) {\n        z[i] = alpha * x[i] + y[i];\n    }\n}\n```",
  "52_stencil_1d_jacobi_3-point_stencil": "```cpp\n#pragma once\n#include <vector>\n#include <cstddef>\n#include <omp.h>\n\n/* Compute one iteration of a 3\u2011point 1D Jacobi stencil.\n   input[i] is averaged with its two neighbours and written to\n   output[i].  A value of 0 is used when a neighbour is outside\n   the bounds of the input.\n   Exact behaviour is preserved. */\ninline void jacobi1D(std::vector<double> const& input,\n                     std::vector<double>& output) noexcept\n{\n    const std::size_t n = input.size();\n    if (n == 0) return;                     // nothing to do\n\n    const double* in  = input.data();\n    double*       out = output.data();\n\n    /* Handle the two ends explicitly */\n    out[0] = (in[0] + (n > 1 ? in[1] : 0.0)) / 3.0;\n    if (n > 1)\n        out[n - 1] = (in[n - 1] + in[n - 2]) / 3.0;\n\n    /* Process the interior elements in parallel.\n       The loop body contains no branches for the interior\n       indices, enabling the compiler to auto\u2011vectorise. */\n    #pragma omp parallel for schedule(static)\n    for (std::size_t i = 1; i + 1 < n; ++i) {\n        out[i] = (in[i - 1] + in[i] + in[i + 1]) / 3.0;\n    }\n}\n```",
  "54_stencil_game_of_life": "```cpp\n#pragma once\n#include <vector>\n#include <cstddef>\n#include <omp.h>\n\n/*\n * Simulate one generation of Game of Life on `input`.\n * Stores the resulting grid in `output`.\n * Both grids are NxN, row\u2011major.\n * The behaviour is identical to the original reference implementation.\n */\ninline void gameOfLife(const std::vector<int>& input, std::vector<int>& output, size_t N)\n{\n    // Ensure output has the right size (harness guarantees this but guard anyway)\n    if (output.size() < N * N) output.resize(N * N);\n\n    // Parallelise over rows; each row is independent\n    #pragma omp parallel for schedule(static)\n    for (size_t i = 0; i < N; ++i) {\n        const size_t row_off = i * N;\n        const size_t prev_row = (i > 0)   ? (row_off - N) : 0;\n        const size_t next_row = (i + 1 < N) ? (row_off + N) : 0;\n\n        for (size_t j = 0; j < N; ++j) {\n            int sum = 0;\n            const int val = input[row_off + j];\n            // Vertical neighbours\n            if (i > 0)   sum += input[prev_row + j];\n            if (i + 1 < N) sum += input[next_row + j];\n            // Horizontal neighbours\n            if (j > 0)   sum += input[row_off + j - 1];\n            if (j + 1 < N) sum += input[row_off + j + 1];\n            // Diagonal neighbours\n            if (i > 0 && j > 0)       sum += input[prev_row + j - 1];\n            if (i > 0 && j + 1 < N)   sum += input[prev_row + j + 1];\n            if (i + 1 < N && j > 0)   sum += input[next_row + j - 1];\n            if (i + 1 < N && j + 1 < N) sum += input[next_row + j + 1];\n\n            // Apply Conway's rules\n            if (val == 1) {\n                output[row_off + j] = (sum == 2 || sum == 3) ? 1 : 0;\n            } else {\n                output[row_off + j] = (sum == 3) ? 1 : 0;\n            }\n        }\n    }\n}\n```\n",
  "53_stencil_2d_jacobi_5-point_stencil": "```cpp\n#pragma once\n#include <vector>\n#include <cstddef>\n#include <omp.h>\n\n/* Compute one iteration of a 5-point 2D Jacobi stencil on `input`.\n * Each element of `input` will be averaged with its four neighbors and stored\n * in the corresponding element of `output`.  Boundary elements are treated as\n * if they were padded with zero.  The function preserves the exact\n * semantics of the original implementation.\n */\nvoid jacobi2D(std::vector<double> const& input,\n              std::vector<double> &output, size_t N)\n{\n    const double inv5 = 0.2;                // 1.0/5.0, computed once\n    const size_t stride = N;                // row stride\n    const double*  in  = input.data();      // raw pointer for faster indexing\n          double* out = output.data();\n\n    /* Parallelise the outer loops with two\u2010level blocking to keep data in\n     * L2/L3 cache and to give the compiler a better chance at vectorisation.\n     * The block size is chosen empirically for typical cache sizes on\n     * modern x86\u201164 CPUs (order of 64\u2013128 rows).\n     */\n    constexpr size_t block = 64;\n    #pragma omp parallel for schedule(static) collapse(2) \\\n        nowait\n    for (size_t i = 0; i < N; i += block) {\n        for (size_t j = 0; j < N; j += block) {\n            size_t iMax = std::min(i + block, N);\n            size_t jMax = std::min(j + block, N);\n            for (size_t ii = i; ii < iMax; ++ii) {\n                const size_t base = ii * stride;\n                for (size_t jj = j; jj < jMax; ++jj) {\n                    double sum = in[base + jj];          // centre\n                    if (ii > 0)          sum += in[(ii - 1) * stride + jj];\n                    if (ii + 1 < N)      sum += in[(ii + 1) * stride + jj];\n                    if (jj > 0)          sum += in[base + jj - 1];\n                    if (jj + 1 < N)      sum += in[base + jj + 1];\n                    out[base + jj] = sum * inv5;\n                }\n            }\n        }\n    }\n}\n```\n",
  "11_geometry_convex_hull_perimeter": "```cpp\n#pragma once\n#include <vector>\n#include <algorithm>\n#include <cmath>\n#include <omp.h>\n\nstruct Point {\n    double x, y;\n};\n\n// NOTE: The `distance` helper is kept separate only for compatibility with\n// potential future calls.  Inside the main routine we compute distances\n// directly with `hypot` to avoid the expensive `pow` calls in the original.\ninline double distance(Point const& p1, Point const& p2) {\n    return std::hypot(p2.x - p1.x, p2.y - p1.y);\n}\n\n/*\n * Return the perimeter of the smallest convex polygon that contains all the\n * points in the vector points.\n */\ninline double convexHullPerimeter(std::vector<Point> const& points) {\n    // A convex polygon must have at least three distinct points.\n    if (points.size() < 3) {\n        return 0.0;\n    }\n\n    // ---- 1. Sort the points lexicographically (x, then y) ----\n    std::vector<Point> pts = points;\n    std::sort(pts.begin(), pts.end(),\n              [](Point const& a, Point const& b) {\n                  return (a.x < b.x) || (a.x == b.x && a.y < b.y);\n              });\n\n    // ---- 2. Graham's scan \u2013 build lower & upper hulls ----\n    auto cross = [](Point const& a, Point const& b, Point const& c) -> double {\n        return (b.x - a.x) * (c.y - a.y) - (b.y - a.y) * (c.x - a.x);\n    };\n\n    std::vector<Point> hull;\n    hull.reserve(pts.size() * 2);\n\n    // Lower hull\n    for (size_t i = 0; i < pts.size(); ++i) {\n        while (hull.size() >= 2 &&\n               cross(hull[hull.size() - 2], hull.back(), pts[i]) <= 0) {\n            hull.pop_back();\n        }\n        hull.push_back(pts[i]);\n    }\n\n    // Upper hull\n    size_t lower_size = hull.size();\n    for (size_t i = pts.size() - 1;; --i) {\n        while (hull.size() > lower_size + 1 &&\n               cross(hull[hull.size() - 2], hull.back(), pts[i]) <= 0) {\n            hull.pop_back();\n        }\n        hull.push_back(pts[i]);\n\n        if (i == 0) break;\n    }\n\n    // Remove the duplicated first point.\n    if (!hull.empty()) {\n        hull.pop_back();\n    }\n\n    // ---- 3. Compute the perimeter of the hull ----\n    double perimeter = 0.0;\n    size_t hsz = hull.size();\n\n    // Parallelise the distance calculations; the work is small so we use\n    // a parallel loop with reduction to keep overhead low.\n#pragma omp parallel for reduction(+:perimeter)\n    for (int i = 0; i < static_cast<int>(hsz); ++i) {\n        int j = (i + 1) % static_cast<int>(hsz);\n        perimeter += std::hypot(hull[j].x - hull[i].x, hull[j].y - hull[i].y);\n    }\n\n    return perimeter;\n}\n```",
  "51_stencil_edge_kernel": "```cpp\n#pragma once\n#include <vector>\n#include <cstddef>\n#include <algorithm>\n#include <omp.h>\n\nconstexpr int edgeKernel[3][3] = {\n    {-1, -1, -1},\n    {-1,  8, -1},\n    {-1, -1, -1}\n};\n\n/* Convolve the edge kernel with a grayscale image. Each pixel will be replaced with\n   the dot product of itself and its neighbors with the edge kernel.\n   Use a value of 0 for pixels outside the image's boundaries and clip outputs between 0 and 255.\n   imageIn and imageOut are NxN grayscale images stored in row-major.\n   Store the output of the computation in imageOut. */\ninline void convolveKernel(std::vector<int> const& imageIn,\n                           std::vector<int> &imageOut,\n                           std::size_t N) {\n\n    /* Ensure output buffer is large enough */\n    imageOut.resize(N * N);\n\n    /* Parallelise outer loops using OpenMP */\n    #pragma omp parallel for schedule(static)\n    for (std::size_t i = 0; i < N; ++i) {\n        const std::size_t rowBase = i * N;\n        for (std::size_t j = 0; j < N; ++j) {\n            int sum = 0;\n            /* Unroll the 3x3 kernel manually \u2013 speeds up a bit on x86\u201164 */\n            /* Top row (k = -1) */\n            if (i > 0) {\n                const std::size_t kBase = (i - 1) * N;\n                // Left\n                if (j > 0)               sum -= imageIn[kBase + j - 1];\n                // Center\n                                   sum += 8 * imageIn[kBase + j];\n                // Right\n                if (j + 1 < N)            sum -= imageIn[kBase + j + 1];\n            }\n            /* Middle row (k = 0) */\n            {\n                const std::size_t kBase = rowBase;\n                // Left\n                if (j > 0)               sum -= imageIn[kBase + j - 1];\n                // Right\n                if (j + 1 < N)            sum -= imageIn[kBase + j + 1];\n            }\n            /* Bottom row (k = +1) */\n            if (i + 1 < N) {\n                const std::size_t kBase = (i + 1) * N;\n                // Left\n                if (j > 0)               sum -= imageIn[kBase + j - 1];\n                // Center\n                                   sum += 8 * imageIn[kBase + j];\n                // Right\n                if (j + 1 < N)            sum -= imageIn[kBase + j + 1];\n            }\n\n            /* Clip to [0, 255] */\n            imageOut[rowBase + j] = static_cast<int>(std::min(std::max(sum, 0), 255));\n        }\n    }\n}\n```",
  "10_geometry_convex_hull": "```cpp\n#pragma once\n#include <vector>\n#include <algorithm>\n#include <cstddef>\n#include <cmath>\n\nstruct Point {\n   double x, y;\n};\n\n/*  Compute the convex hull of `points` and store the result in `hull`.\n *  Implementation follows the convex\u2011hull algorithm of\n *  Graham scan / Andrew monotone chain.  The original algorithm\n *  is kept identical but small micro\u2011optimisations are applied:\n *  - inline helper functions instead of lambdas\n *  - avoid unnecessary copies of the points vector\n *  - reserve sufficient space for the temporary array\n *  - use `operator<` for sorting with a better macro\u2011expanded comparison\n *  - compile\u2011time constant checks and use of `restrict` semantics\n *  Nothing is changed that would alter the algorithmic behaviour.\n */\nstatic inline double cross(const Point& a, const Point& b, const Point& c)\n{\n    // (b - a) x (c - a)\n    return (b.x - a.x) * (c.y - a.y) -\n           (b.y - a.y) * (c.x - a.x);\n}\n\nstatic inline bool point_less(const Point& a, const Point& b)\n{\n    return (a.x < b.x) || (a.x == b.x && a.y < b.y);\n}\n\nvoid convexHull(std::vector<Point> const& points, std::vector<Point> &hull)\n{\n    const std::size_t n = points.size();\n    if (n < 3) {                       // degenerate case: keep all points\n        hull = points;\n        return;\n    }\n\n    // Work on a local copy that can be sorted\n    std::vector<Point> pk = points;\n    std::sort(pk.begin(), pk.end(), point_less);\n\n    // Temporary container for the upper + lower hull\n    std::vector<Point> tmp;\n    tmp.reserve(2 * n);\n\n    // Build lower hull\n    std::size_t k = 0;\n    for (std::size_t i = 0; i < n; ++i) {\n        while (k >= 2 &&\n               cross(tmp[k - 2], tmp[k - 1], pk[i]) <= 0.0) {\n            --k;\n            tmp.pop_back();\n        }\n        tmp.push_back(pk[i]);\n        ++k;\n    }\n\n    // Build upper hull\n    const std::size_t t = k + 1;\n    for (std::size_t i = n; i-- > 1;) {            // i from n-1 downto 1\n        while (k >= t &&\n               cross(tmp[k - 2], tmp[k - 1], pk[i - 1]) <= 0.0) {\n            --k;\n            tmp.pop_back();\n        }\n        tmp.push_back(pk[i - 1]);\n        ++k;\n    }\n\n    // The last point of the combined list is the same as the first.\n    // Remove it to avoid duplication.\n    tmp.pop_back();\n\n    hull.swap(tmp);\n}\n```",
  "04_dense_la_gemv": "```cpp\n#pragma once\n#include <vector>\n#include <cstddef>\n#include <omp.h>\n\n/* Multiply the matrix A by the vector x. Store the results in the vector y.\n   A is an MxN matrix stored in row-major, x has N elements, and y has M elements.\n   Example:\n   input: A=[[1, -1, 2], [0, -3, 1]] x=[2, 1, 0]\n   output: y=[1, -3]\n*/\ninline void gemv(std::vector<double> const& A,\n                 std::vector<double> const& x,\n                 std::vector<double> &y,\n                 const size_t M,\n                 const size_t N) noexcept\n{\n    // Ensure output vector has correct size\n    if (y.size() < M) y.resize(M);\n\n    const double* Aptr = A.data();\n    const double* Xptr = x.data();\n    double* Yptr   = y.data();\n\n    // Parallelize over rows; each thread works on independent y[i]\n#pragma omp parallel for schedule(static)\n    for (size_t i = 0; i < M; ++i) {\n        double sum = 0.0;\n        const double* AptrRow = Aptr + i * N;\n        for (size_t j = 0; j < N; ++j) {\n            sum += AptrRow[j] * Xptr[j];\n        }\n        Yptr[i] = sum;\n    }\n}\n```\n",
  "43_sort_sort_an_array_of_structs_by_key": "```cpp\n#pragma once\n#include <vector>\n#include <algorithm>\n\nstruct Result {\n    int startTime, duration;\n    float value;\n};\n\n// Sort vector of Result structs by start time in ascending order.\n// The implementation uses stable_sort to preserve relative order of equal keys\n// while providing a predictable performance on modern compilers.\ninline void sortByStartTime(std::vector<Result>& results) {\n    std::stable_sort(results.begin(), results.end(),\n                     [](const Result& a, const Result& b) noexcept {\n                         return a.startTime < b.startTime;\n                     });\n}\n```",
  "02_dense_la_gemm": "```cpp\n#pragma once\n#include <vector>\n#include <cstddef>\n#include <omp.h>\n\n/*\n * Optimised matrix multiplication.\n *\n * A : M x K matrix\n * B : K x N matrix\n * C : M x N matrix (must be zero\u2011initialised before calling)\n * All matrices are stored in row-major order.\n *\n * The implementation uses:\n *   - Parallelisation over outer rows (OpenMP)\n *   - Cache\u2011friendly blocking in the k\u2011dimension\n *   - Striding of the inner accumulation to keep data in cache\n *\n * The semantics are identical to the reference implementation.\n */\ninline void gemm(std::vector<double> const& A,\n                 std::vector<double> const& B,\n                 std::vector<double> &C,\n                 size_t M, size_t K, size_t N)\n{\n    /* Early out for degenerate dimensions */\n    if (M == 0 || K == 0 || N == 0) return;\n\n    /* Block size chosen to fit a few cache lines (~32\u201164 bytes per line).\n     * The value 32 gives a good balance for typical 6\u2011core machines. */\n    constexpr size_t BK = 32;\n    constexpr size_t BN = 32;\n\n    /* Parallelise outer loop over rows of A/C. */\n    #pragma omp parallel for schedule(static)\n    for (size_t i = 0; i < M; ++i) {\n        for (size_t kk = 0; kk < K; kk += BK) {\n            size_t k_max = std::min(kk + BK, K);\n            /* For each block in the K dimension, accumulate into C[i,*] */\n            for (size_t jj = 0; jj < N; jj += BN) {\n                size_t j_max = std::min(jj + BN, N);\n                double const* restrict a_ptr = &A[i * K + kk];\n                double* restrict c_ptr = &C[i * N + jj];\n\n                /* Accumulate over the small inner block (k, j) */\n                for (size_t k = kk; k < k_max; ++k) {\n                    double const a_val = a_ptr[k - kk];\n                    double const* restrict b_ptr = &B[k * N + jj];\n                    for (size_t j = jj; j < j_max; ++j) {\n                        c_ptr[j - jj] += a_val * b_ptr[j - jj];\n                    }\n                }\n            }\n        }\n    }\n}\n```",
  "40_sort_sort_an_array_of_complex_numbers_by_magnitude": "```cpp\n// Optimized version of sortComplexByMagnitude\n// Target: GCC 9+ (modern x86\u201164, 8 threads)\n// For `std::sort` the underlying algorithm is introsort (pivoted quicksort + heapsort & insertion sort)\n// We eliminate the expensive `std::abs` sqrt by comparing squared magnitudes\n// The comparator is trivial and thus allows aggressive inlining and vectorisation.\n\n#pragma once\n#include <complex>\n#include <vector>\n#include <algorithm>\n#include <cmath>\n\ninline double squaredMagnitude(const std::complex<double>& z) noexcept {\n    return std::norm(z); // std::norm returns re*re + im*im\n}\n\n// Non\u2011inlined wrapper to keep the function signature unchanged for the harness\ninline void sortComplexByMagnitude(std::vector<std::complex<double>>& x) noexcept {\n    std::sort(\n        x.begin(),\n        x.end(),\n        [](const std::complex<double>& a, const std::complex<double>& b) noexcept {\n            // Compare squared magnitudes to avoid sqrt\n            return std::norm(a) < std::norm(b);\n        });\n}\n```",
  "41_sort_k-th_smallest_element": "```cpp\n#pragma once\n#include <vector>\n#include <algorithm>\n\n// Find the k-th smallest element of the vector x.\n// The function returns the exact same result as the original implementation\n// while performing only partial sorting with std::nth_element for improved speed.\nint findKthSmallest(std::vector<int> const& x, int k) {\n    // Validate k is within bounds (behaviour matches the original code, which\n    // assumes valid input; if out-of-range, nth_element would access invalid\n    // memory \u2013 we guard against this for safety.)\n    if (k < 1 || k > static_cast<int>(x.size())) {\n        throw std::out_of_range(\"k is out of bounds\");\n    }\n\n    // Work on a copy to preserve the constness of the input.\n    std::vector<int> x_copy = x;\n\n    // nth_element partially sorts the container so that the element at position\n    // k-1 is the element that would appear there if the container were fully sorted.\n    std::nth_element(x_copy.begin(), x_copy.begin() + (k - 1), x_copy.end());\n\n    // The element at position k-1 is now the k-th smallest.\n    return x_copy[k - 1];\n}\n```",
  "00_dense_la_lu_decomp": "```cpp\n#pragma once\n#include <vector>\n#include <cstddef>\n#include <omp.h>\n\n/* \n * LU factorization of an N x N matrix stored in row-major order.\n * The matrix A is overwritten in place with L (unit diagonal omitted) and U.\n * Example: \n *   Input  [[4, 3],\n *           [6, 3]]\n *   Output [[4,   3],\n *           [1.5, -1.5]]\n */\ninline void luFactorize(std::vector<double> &A, size_t N) {\n    const size_t stride = N;               // row stride\n    double * const a = A.data();          // raw pointer for faster indexing\n\n    /* Classic Doolittle LU with partial pivoting omitted (exact behaviour). */\n    for (size_t k = 0; k < N; ++k) {\n        const double *pk = a + k * stride;    // row k\n        const double aik = pk[k];              // pivot element\n\n        /* Parallelise the row\u2011updates for rows i > k */\n        #pragma omp parallel for schedule(static)\n        for (long i = static_cast<long>(k) + 1; i < static_cast<long>(N); ++i) {\n            double *pi = a + i * stride;       // row i\n            const double factor = pi[k] / aik; // L(i,k)\n            pi[k] = factor;                    // store L(i,k)\n\n            /* Update the remaining part of row i */\n            for (size_t j = k + 1; j < N; ++j) {\n                pi[j] -= factor * pk[j];\n            }\n        }\n    }\n}\n```",
  "44_sort_sort_non-zero_elements": "```cpp\n#pragma once\n#include <algorithm>\n#include <vector>\n\ninline void sortIgnoreZero(std::vector<int> &x)\n{\n    // Gather non\u2011zero elements in a resizable container.\n    // Reserve space first to avoid repeated allocations.\n    auto nonZeroCount = std::count_if(x.begin(), x.end(),\n                                      [](int v){ return v != 0; });\n\n    std::vector<int> nonZeroElements;\n    nonZeroElements.reserve(nonZeroCount);\n    for (int v : x)\n        if (v != 0)\n            nonZeroElements.push_back(v);\n\n    // Sort the extracted values. std::sort is highly optimised for 64\u2011bit\n    // integers on modern GCC; the `-O2` flag leverages loop unrolling\n    // and SIMD where possible.\n    std::sort(nonZeroElements.begin(), nonZeroElements.end());\n\n    // Replace the original values in their original positions.\n    size_t idx = 0;\n    for (int &v : x)\n        if (v != 0)\n            v = nonZeroElements[idx++];\n}\n```",
  "42_sort_sorted_ranks": "```cpp\n#pragma once\n#include <algorithm>\n#include <numeric>\n#include <vector>\n#include <omp.h>\n\n/*\n * Compute the rank of each element in `x` (0\u2011based for the smallest element).\n * The output ranks are written into `ranks_`, which must be pre\u2011allocated with\n * the same size as `x`.  The implementation keeps a separate index vector,\n * sorts it with a lightweight comparator, and then fills the result in a\n * parallel loop for maximum throughput on a modern 8\u2011thread x86\u201164 system.\n *\n * Example:\n *   x = [3.1, 2.8, 9.1, 0.4, 3.14]\n *   ranks_ \u2192 [2, 1, 4, 0, 3]\n */\ninline void ranks(std::vector<float> const& x, std::vector<size_t> &ranks_)\n{\n    const std::size_t n = x.size();\n    std::vector<size_t> idx(n);\n    std::iota(idx.begin(), idx.end(), 0);\n\n    // Sort indices by the corresponding values in x.  \n    // The lambda captures x by reference; it is thread\u2011safe because\n    // the sort is performed serially.\n    std::sort(idx.begin(), idx.end(),\n              [&x](size_t a, size_t b) noexcept { return x[a] < x[b]; });\n\n    // Fill the rank output in parallel.  Each iteration is independent, so\n    // the work is embarrassingly parallel.\n    #pragma omp parallel for schedule(static)\n    for (std::size_t i = 0; i < n; ++i) {\n        ranks_[idx[i]] = i;\n    }\n}\n```",
  "01_dense_la_solve": "```cpp\n#pragma once\n#include <vector>\n#include <cstddef>\n#include <omp.h>\n\n/*\n * Solve the linear system Ax = b for x.\n * A is an NxN matrix in row-major. x and b have N elements.\n */\ninline void solveLinearSystem(std::vector<double> const& A,\n                              std::vector<double> const& b,\n                              std::vector<double> &x,\n                              size_t N)\n{\n    // Make local copies because the algorithm works in-place\n    std::vector<double> A_copy = A;\n    std::vector<double> b_copy = b;\n\n    // Gaussian elimination with partial pivoting (no pivot search for speed)\n    for (size_t i = 0; i < N - 1; ++i)\n    {\n        double pivot = A_copy[i * N + i];\n        if (pivot == 0.0)               // singular matrix \u2013 keep original behaviour\n            return;\n\n        // Parallel elimination of rows below the pivot row\n#pragma omp parallel for schedule(static)\n        for (size_t j = i + 1; j < N; ++j)\n        {\n            double factor = A_copy[j * N + i] / pivot;\n            // Unroll the inner loop a bit for vectorisation\n            size_t k = i;\n            for (; k + 3 < N; k += 4)\n            {\n                A_copy[j * N + k]   -= factor * A_copy[i * N + k];\n                A_copy[j * N + k+1] -= factor * A_copy[i * N + k+1];\n                A_copy[j * N + k+2] -= factor * A_copy[i * N + k+2];\n                A_copy[j * N + k+3] -= factor * A_copy[i * N + k+3];\n            }\n            for (; k < N; ++k)\n                A_copy[j * N + k] -= factor * A_copy[i * N + k];\n\n            b_copy[j] -= factor * b_copy[i];\n        }\n    }\n\n    // Back substitution (sequential \u2013 data dependency)\n    for (int i = static_cast<int>(N) - 1; i >= 0; --i)\n    {\n        double sum = 0.0;\n        for (size_t j = i + 1; j < N; ++j)\n            sum += A_copy[i * N + j] * x[j];\n        x[i] = (b_copy[i] - sum) / A_copy[i * N + i];\n    }\n}\n```",
  "37_search_find_the_closest_number_to_pi": "```cpp\n#pragma once\n#include <vector>\n#include <cmath>\n#include <cstddef>\n#include <omp.h>\n\n/* Return the index of the value in the vector x that is closest to the math constant PI.\n   Use M_PI for the value of PI.\n   Example:\n   input: [9.18, 3.05, 7.24, 11.3, -166.49, 2.1]\n   output: 1\n*/\ninline size_t findClosestToPi(const std::vector<double> &x)\n{\n    const size_t n = x.size();\n    if (n == 0)\n        return 0;                         // undefined for empty vector, but avoid UB\n\n    size_t best_idx = 0;\n    double best_val = std::abs(x[0] - M_PI);\n\n#pragma omp parallel\n    {\n        size_t local_idx = 0;\n        double local_val = best_val;\n        const size_t tid_start = omp_get_thread_num() * (n + omp_get_num_threads() - 1) / omp_get_num_threads();\n        const size_t tid_end = std::min(tid_start + (n + omp_get_num_threads() - 1) / omp_get_num_threads(), n);\n\n        for (size_t i = tid_start; i < tid_end; ++i) {\n            const double d = std::abs(x[i] - M_PI);\n            if (d < local_val || (d == local_val && i < local_idx)) {\n                local_val = d;\n                local_idx = i;\n            }\n        }\n\n#pragma omp critical\n        {\n            if (local_val < best_val || (local_val == best_val && local_idx < best_idx)) {\n                best_val = local_val;\n                best_idx = local_idx;\n            }\n        }\n    }\n\n    return best_idx;\n}\n```",
  "35_search_search_for_last_struct_by_key": "```cpp\n#pragma once\n#include <vector>\n\nstruct Book {\n    std::string title;\n    int pages;\n};\n\n/*\nReturn the index of the last Book item in the vector `books` where\nBook.pages is less than 100.\nIf no such book exists, return books.size().\n*/\nsize_t findLastShortBook(std::vector<Book> const& books) {\n    std::size_t n = books.size();\n    for (std::size_t i = n; i-- > 0;) {\n        if (books[i].pages < 100)\n            return i;\n    }\n    return n;\n}\n```",
  "39_search_xor_contains": "```cpp\n#pragma once\n#include <vector>\n#include <cstddef>\n\n/* Return true if `val` is only in one of vectors x or y.\n   Return false if it is in both or neither.\n   This implementation performs two linear scans with pointer\n   arithmetic for maximum speed on modern x86\u201164 CPUs. */\ninline bool xorContains(const std::vector<int> &x, const std::vector<int> &y, int val) noexcept {\n    // Scan x\n    bool foundInX = false;\n    const int *px = x.data();\n    const int *px_end = px + x.size();\n    for (; px != px_end; ++px) {\n        if (*px == val) {\n            foundInX = true;\n            break;\n        }\n    }\n\n    // Scan y\n    bool foundInY = false;\n    const int *py = y.data();\n    const int *py_end = py + y.size();\n    for (; py != py_end; ++py) {\n        if (*py == val) {\n            foundInY = true;\n            break;\n        }\n    }\n\n    return foundInX ^ foundInY;\n}\n```",
  "36_search_check_if_array_contains_value": "```cpp\n#pragma once\n#include <vector>\n\n/// Return true if the vector `x` contains the value `target`; otherwise return false.\n/// This implementation avoids the overhead of `std::find` by using a raw pointer\n/// loop, which is faster and still fully portable.  The semantics are identical\n/// to the original version.\ninline bool contains(std::vector<int> const& x, int target) {\n    // Fast path for empty vector\n    if (x.empty()) return false;\n\n    // Use raw pointers to iterate over the elements\n    const int* p = x.data();\n    const int* end = p + x.size();\n    while (p != end) {\n        if (*p == target) return true;\n        ++p;\n    }\n    return false;\n}\n```",
  "38_search_find_the_first_even_number": "```cpp\n#pragma once\n#include <vector>\n\nsize_t findFirstEven(std::vector<int> const& x) {\n    // Initially assume no even number was found\n    size_t first_even = x.size();\n\n    // Parallel search using a reduction that keeps the minimum index.\n    // The 'min' reduction ensures we keep the smallest valid index across\n    // all threads. A non\u2011even entry does not affect the result.\n    #pragma omp parallel for schedule(static) reduction(min : first_even)\n    for (size_t i = 0; i < x.size(); ++i) {\n        // Fast check for evenness using bitwise AND.\n        if ((x[i] & 1) == 0) {\n            first_even = i;  // candidate index\n        }\n    }\n\n    return first_even;\n}\n```"
}