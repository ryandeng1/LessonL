{
  "17_graph_highest_degree": "```cpp\n#pragma once\n#include <vector>\n#include <algorithm>\n#include <cstddef>\n\nint maxDegree(std::vector<int> const& A, size_t N) {\n    // Parallelize the outer loop and perform a reduction on the maximum degree.\n    // Use local variables to avoid repeated pointer dereferences.\n    int global_max = 0;\n\n    #pragma omp parallel for reduction(max : global_max)\n    for (int i = 0; i < static_cast<int>(N); ++i) {\n        int sum = 0;\n        const int* row = A.data() + i * static_cast<int>(N);\n        // Unrolled inner loop for small stride performance.\n        for (int j = 0; j < static_cast<int>(N); ++j) {\n            sum += row[j];\n        }\n        if (sum > global_max) global_max = sum;\n    }\n    return global_max;\n}\n```",
  "15_graph_edge_count": "```cpp\n#pragma once\n#include <vector>\n#include <cstddef>\n#include <omp.h>\n\n/* Count the number of edges in the directed graph defined by the adjacency matrix A.\n   A is an NxN adjacency matrix stored in row-major.\n   Example:\n\t input: [[0, 0, 0, 1], [0, 0, 0, 1], [0, 0, 0, 1], [1, 1, 1, 0]]\n   output: 3\n*/\nint edgeCount(const std::vector<int> &A, size_t N) {\n    int count = 0;\n\n    #pragma omp parallel for collapse(2) reduction(+:count) schedule(static)\n    for (int i = 0; i < static_cast<int>(N); ++i) {\n        for (int j = 0; j < static_cast<int>(N); ++j) {\n            if (A[i * N + j] == 1) {\n                ++count;\n            }\n        }\n    }\n\n    return count;\n}\n```",
  "19_graph_shortest_path": "```cpp\n#pragma once\n#include <vector>\n#include <limits>\n\n/* Return the length of the shortest path from source to dest in the undirected graph defined by\n   the adjacency matrix A (row\u2011major). The graph is connected and undirected. */\nint shortestPathLength(std::vector<int> const& A, size_t N, int source, int dest) {\n    // Distance array initialized with a sentinel value (-1 means unvisited)\n    std::vector<int> dist(N, -1);\n    // Simple ring buffer for BFS queue\n    std::vector<int> q;\n    q.reserve(N);\n    q.push_back(source);\n    dist[source] = 0;\n\n    size_t head = 0;\n    while (head < q.size()) {\n        int u = q[head++];\n        int du = dist[u];\n        if (u == dest) return du;\n\n        // Iterate over all neighbors of u\n        const int base = static_cast<int>(u) * static_cast<int>(N);\n        for (int v = 0; v < static_cast<int>(N); ++v) {\n            if (A[base + v] && dist[v] == -1) {\n                dist[v] = du + 1;\n                q.push_back(v);\n            }\n        }\n    }\n    // Should never happen on a connected graph\n    return std::numeric_limits<int>::max();\n}\n```",
  "23_histogram_first_letter_counts": "```cpp\n#pragma once\n#include <array>\n#include <string>\n#include <vector>\n#include <omp.h>\n\n/// Counts how many strings in `s` start with each lower\u2011case letter of the alphabet.\n/// The result is stored in `bins`. The function is fully parallelized with OpenMP\n/// and performs no unnecessary memory allocations.\nvoid firstLetterCounts(std::vector<std::string> const& s, std::array<size_t, 26> &bins) {\n    // Ensure the bins array is zero\u2011initialized\n    std::fill(bins.begin(), bins.end(), 0);\n\n    // Parallel region with a private local bin array per thread and a final reduction\n    #pragma omp parallel\n    {\n        // each thread holds its own local bins to avoid atomic updates\n        std::array<size_t, 26> local_bins{};\n        std::fill(local_bins.begin(), local_bins.end(), 0);\n\n        #pragma omp for nowait\n        for (size_t i = 0; i < s.size(); ++i) {\n            // Access the first character directly; the problem guarantees\n            // that the string is non\u2011empty and all characters are lower\u2011case\n            const unsigned char c = static_cast<unsigned char>(s[i][0]);\n            const int idx = c - 'a';               // index 0..25\n            ++local_bins[idx];\n        }\n\n        // Merge the local bins into the global bins (critical section to avoid data races)\n        #pragma omp critical\n        {\n            for (int j = 0; j < 26; ++j) {\n                bins[j] += local_bins[j];\n            }\n        }\n    }\n}\n```",
  "21_histogram_bin_0-100": "```cpp\n#pragma once\n#include <array>\n#include <vector>\n#include <cstddef>\n#include <omp.h>\n\n/* Vector x contains values between 0 and 100, inclusive.\n   Count the number of values in [0,10), [10, 20), [20, 30), ...\n   and store the counts in `bins`.\n   Example:\n   input: [7, 32, 95, 12, 39, 32, 11, 71, 70, 66]\n   output: [1, 2, 0, 3, 0, 0, 1, 2, 0, 1]\n*/\ninline void binsBy10Count(const std::vector<double> &x, std::array<std::size_t,10> &bins) {\n    // initialize bins to zero if not done by caller\n    #pragma omp parallel for\n    for (std::size_t i = 0; i < x.size(); ++i) {\n        // fast cast; values are in [0,100], division by 10 truncates toward zero\n        std::size_t bin = static_cast<std::size_t>(x[i] / 10.0);\n        // atomic increment to avoid data races\n        #pragma omp atomic\n        ++bins[bin];\n    }\n}\n```",
  "16_graph_largest_component": "```cpp\n#pragma once\n#include <vector>\n#include <stack>\n#include <cstddef>\n#include <algorithm>\n\ninline void dfs_iter(const std::vector<int> &A, size_t start,\n                     size_t N, std::vector<uint8_t> &visited,\n                     int &cnt)\n{\n    std::stack<size_t> st;\n    st.push(start);\n    visited[start] = 1;\n    while (!st.empty()) {\n        size_t v = st.top(); st.pop();\n        ++cnt;\n        const size_t row = v * N;\n        for (size_t i = 0; i < N; ++i) {\n            if (A[row + i] && !visited[i]) {\n                visited[i] = 1;\n                st.push(i);\n            }\n        }\n    }\n}\n\n/* Return the number of vertices in the largest component of the graph\n   defined by the adjacency matrix A (NxN, row\u2011major). */\nint largestComponent(const std::vector<int> &A, size_t N)\n{\n    std::vector<uint8_t> visited(N, 0);\n    int maxCnt = 0;\n\n    for (size_t i = 0; i < N; ++i) {\n        if (!visited[i]) {\n            int cnt = 0;\n            dfs_iter(A, i, N, visited, cnt);\n            if (cnt > maxCnt) maxCnt = cnt;\n        }\n    }\n    return maxCnt;\n}\n```",
  "20_histogram_pixel_histogram": "```cpp\n#pragma once\n#include <vector>\n#include <array>\n#include <cstddef>\n#include <cstdint>\n#include <omp.h>\n\n/*\n * Count the number of pixels in image with each grayscale intensity.\n * The vector `image` is a grayscale image with values 0-255.\n * Store the results in `bins`.\n *\n * Example:\n *   input: image=[2, 116, 201, 11, 92, 92, 201, 4, 2]\n *   output: [0, 0, 2, 0, 1, ...]\n */\nvoid pixelCounts(std::vector<int> const& image, std::array<std::size_t, 256> &bins)\n{\n    // initialise output to zero\n    bins.fill(0);\n\n    // each thread gets its own local buffer to avoid false sharing\n    int const nThreads = omp_get_max_threads();\n    std::vector<std::array<std::size_t, 256>> localBins(nThreads);\n    for (auto &b : localBins) b.fill(0);\n\n    // parallel reduction by thread-local storage\n    #pragma omp parallel\n    {\n        int const id = omp_get_thread_num();\n        auto &tBins = localBins[id];\n\n        #pragma omp for schedule(static)\n        for (std::size_t i = 0; i < image.size(); ++i) {\n            int val = image[i];\n            // input values are guaranteed to be in [0,255]\n            ++tBins[static_cast<std::uint8_t>(val)];\n        }\n    }\n\n    // merge local results into the final bins array\n    for (int t = 0; t < nThreads; ++t) {\n        for (int j = 0; j < 256; ++j) {\n            bins[j] += localBins[t][j];\n        }\n    }\n}\n```",
  "34_scan_largest_contiguous_subarray_sum": "```cpp\n#pragma once\n#include <vector>\n#include <algorithm>\n#include <limits>\n\n/* Return the largest sum of any contiguous subarray in the vector x.\n   This implementation uses Kadane's algorithm for an optimal O(n) solution.\n   It preserves the exact behavior of the original double-loop version.\n*/\nint maximumSubarray(const std::vector<int>& x)\n{\n    int best = std::numeric_limits<int>::lowest();\n    int current = 0;\n\n    for (int v : x) {\n        current = std::max(v, current + v);\n        best = std::max(best, current);\n    }\n\n    return best;\n}\n```",
  "31_scan_scan_with_min_function": "```cpp\n#pragma once\n#include <vector>\n#include <algorithm>\n#include <cstddef>\n\n/*\n  Replace the i-th element of the vector with the minimum of\n  the sub\u2011array [0 \u2026 i].\n*/\ninline void partialMinimums(std::vector<float>& x)\n{\n    if (x.empty()) return;\n\n    float current_min = std::numeric_limits<float>::max();\n    for (std::size_t i = 0; i < x.size(); ++i) {\n        current_min = std::min(current_min, x[i]);\n        x[i] = current_min;\n    }\n}\n```",
  "18_graph_count_components": "```cpp\n#pragma once\n#include <vector>\n#include <stack>\n#include <atomic>\n#include <omp.h>\n\n/* Count the number of connected components in an undirected graph\n   represented by an NxN adjacency matrix A (row\u2011major). */\nint componentCount(std::vector<int> const& A, size_t N) {\n    // atomic visited flags\n    std::vector<std::atomic<bool>> visited(N);\n    for (auto& v : visited) v.store(false, std::memory_order_relaxed);\n\n    int count = 0;\n\n    #pragma omp parallel for schedule(dynamic)\n    for (int i = 0; i < static_cast<int>(N); ++i) {\n        if (!visited[i].load(std::memory_order_acquire) &&\n            visited[i].exchange(true, std::memory_order_acq_rel)) {\n            // we are the first to claim this component\n            #pragma omp atomic\n            ++count;\n\n            std::stack<int> stk;\n            stk.push(i);\n\n            while (!stk.empty()) {\n                int cur = stk.top();\n                stk.pop();\n\n                // explore all neighbours\n                const int* row = &A[cur * N];\n                for (int j = 0; j < static_cast<int>(N); ++j) {\n                    if (row[j] == 1) {\n                        if (!visited[j].load(std::memory_order_acquire) &&\n                            visited[j].exchange(true, std::memory_order_acq_rel)) {\n                            stk.push(j);\n                        }\n                    }\n                }\n            }\n        }\n    }\n\n    return count;\n}\n```",
  "24_histogram_count_quartile": "```cpp\n#pragma once\n#include <array>\n#include <cstddef>\n#include <vector>\n#include <cmath>\n#include <omp.h>\n\n/* Count the number of doubles in the vector `x` that have a fractional part \n   in [0, 0.25), [0.25, 0.5), [0.5, 0.75), and [0.75, 1.0). Store the counts in `bins`.  \n   The function preserves the exact semantics of the reference implementation.\n */\ninline void countQuartiles(const std::vector<double>& x,\n                           std::array<std::size_t, 4>& bins) {\n    // Reduce with an explicit OpenMP array reduction to avoid race\n    #pragma omp parallel for reduction(+:bins)\n    for (std::ptrdiff_t i = 0; i < static_cast<std::ptrdiff_t>(x.size()); ++i) {\n        const double val       = x[i];\n        const double frac = val - std::floor(val);\n\n        // Small manual branch\u2011prediction hint: use if\u2011else chain\n        if (frac < 0.25) {\n            ++bins[0];\n        } else if (frac < 0.5) {\n            ++bins[1];\n        } else if (frac < 0.75) {\n            ++bins[2];\n        } else {\n            ++bins[3];\n        }\n    }\n}\n```\n",
  "22_histogram_count_quadrants": "```cpp\n#pragma once\n#include <array>\n#include <vector>\n#include <cstddef>\n#include <cmath>\n#include <omp.h>\n\nstruct Point {\n    double x, y;\n};\n\n/*\n * Count the number of cartesian points in each quadrant. The vector points contains a list of `Point` objects.\n * Store the counts in `bins`.\n * Example:\n *   input: [{x=1.5, y=0.1}, {x=-3, y=1.1}, {x=5, y=9}, {x=1.5, y=-1}, {x=3, y=-7}, {x=0.1, y=2}]\n *   output: [3, 1, 0, 2]\n */\ninline void countQuadrants(const std::vector<Point> &points, std::array<std::size_t,4> &bins)\n{\n    // Clear the output bins first\n    bins[0] = bins[1] = bins[2] = bins[3] = 0;\n\n    // Use OpenMP parallel reduction with a private array for each thread\n    const std::size_t nthreads = omp_get_max_threads();\n\n#pragma omp parallel\n    {\n        // Each thread keeps its own local count array\n        std::array<std::size_t,4> local = {0,0,0,0};\n\n#pragma omp for\n        for (std::size_t i = 0; i < points.size(); ++i) {\n            const double x = points[i].x;\n            const double y = points[i].y;\n\n            if (x >= 0.0) {\n                if (y >= 0.0) {\n                    ++local[0];           // Q1\n                } else {\n                    ++local[3];           // Q4\n                }\n            } else {\n                if (y >= 0.0) {\n                    ++local[1];           // Q2\n                } else {\n                    ++local[2];           // Q3\n                }\n            }\n        }\n\n        // Combine the local counts into the global bins\n#pragma omp atomic\n        bins[0] += local[0];\n#pragma omp atomic\n        bins[1] += local[1];\n#pragma omp atomic\n        bins[2] += local[2];\n#pragma omp atomic\n        bins[3] += local[3];\n    }\n}\n```",
  "59_transform_map_function": "```cpp\n#pragma once\n#include <vector>\n#include <cstddef>\n#include <bitset>\n#include <omp.h>\n\ninline bool isPowerOfTwo(int x) noexcept {\n    return x > 0 && (static_cast<unsigned>(x) & static_cast<unsigned>(x - 1)) == 0;\n}\n\n/*\n * Applies isPowerOfTwo to each element of `x` and stores the result in `mask`.\n * Assumes that `mask` already has the same size as `x`.\n */\ninline void mapPowersOfTwo(const std::vector<int> &x, std::vector<bool> &mask) noexcept {\n    const std::size_t N = x.size();\n    mask.resize(N);                      // safety; if mask is pre\u2011sized this is a no\u2011op\n\n#pragma omp parallel for schedule(static)\n    for (std::size_t i = 0; i < N; ++i) {\n        mask[i] = isPowerOfTwo(x[i]);\n    }\n}\n```",
  "33_scan_reverse_prefix_sum": "```cpp\n#pragma once\n#include <numeric>\n#include <vector>\n#include <algorithm>\n\n/*\n * Compute the reverse prefix sum of the vector `x` into `output`.\n * Examples:\n *   input:  [1, 7, 4, 6, 6, 2]\n *   output: [2, 8, 14, 18, 25, 26]\n *   input:  [3, 3, 7, 1, -2]\n *   output: [-2, -1, 6, 9, 12]\n */\ninline void reversePrefixSum(const std::vector<int>& x, std::vector<int>& output)\n{\n    const std::size_t n = x.size();\n    output.resize(n);\n\n    /* 1. Reverse `x` into a temporary vector without extra copies */\n    std::vector<int> rev;\n    rev.reserve(n);\n    std::reverse_copy(x.begin(), x.end(), std::back_inserter(rev));\n\n    /* 2. Compute inclusive prefix sum on the reversed data */\n    std::inclusive_scan(rev.begin(), rev.end(), output.begin());\n\n    /* 3. Reverse the result to obtain the final ordering */\n    std::reverse(output.begin(), output.end());\n}\n```",
  "55_transform_relu": "```cpp\n#pragma once\n#include <vector>\n#include <cstddef>\n#include <algorithm>\n#include <omp.h>\n\n/* Compute the ReLU function on every element of x.\n   Elements less than zero become zero, while elements greater than zero stay the same.\n   Example:\n   input:  [-1.8, 24.0, 1.2, 0.0, -5.1, -0.2, 4.5]\n   output: [0, 24.0, 1.2, 0, 0, 0, 4.5]\n*/\ninline constexpr double relu_value(double v) noexcept { return v > 0.0 ? v : 0.0; }\n\nvoid relu(std::vector<double> &x) {\n    const std::size_t n = x.size();\n    double * const ptr = x.data();\n\n    /* Parallel SIMD-friendly loop */\n#pragma omp parallel for schedule(static)\n    for (std::size_t i = 0; i < n; ++i) {\n        ptr[i] = relu_value(ptr[i]);\n    }\n}\n```\n",
  "58_transform_squaring": "```cpp\n#pragma once\n#include <vector>\n#include <cstddef>\n#include <omp.h>\n\n/* Replace every element of x with the square of its value.\n   Example:\n   input: [5, 1, 2, -4, 8]\n   output: [25, 1, 4, 16, 64]\n*/\ninline void squareEach(std::vector<int> &x) {\n    const size_t n = x.size();\n    #pragma omp parallel for schedule(static)\n    for (size_t i = 0; i < n; ++i) {\n        const int v = x[i];\n        x[i] = v * v;\n    }\n}\n```",
  "57_transform_inverse_offset": "```cpp\n#pragma once\n#include <vector>\n#include <cstddef>\n#include <omp.h>\n\n/* Replace every element of the vector `x` with 1-1/x.\n   Example:\n   input: [2, 4, 1, 12, -2]\n   output: [0.5, 0.75, 0, 0.91666666, 1.5]\n   The function is fully thread\u2011safe and works on any size vector.\n */\ninline void oneMinusInverse(std::vector<double> &x)\n{\n    // Work directly on the underlying array for maximum speed.\n    // The loop is parallelised with OpenMP; each iteration is independent.\n    const std::size_t n = x.size();\n\n    #pragma omp parallel for schedule(static) if(n > 1024)\n    for (std::size_t i = 0; i < n; ++i) {\n        double v = x[i];\n        // No extra check is required \u2013 division by zero results in +/-infinity\n        // which is exactly what the original std::transform would produce.\n        x[i] = 1.0 - 1.0 / v;\n    }\n}\n```",
  "56_transform_negate_odds": "```cpp\n#pragma once\n#include <vector>\n#include <cstddef>\n#include <omp.h>\n\ninline void negateOddsAndHalveEvens(std::vector<int>& x) {\n    const std::size_t n = x.size();\n    int* ptr = x.data();\n\n    #pragma omp parallel for schedule(static, 256)\n    for (std::size_t i = 0; i < n; ++i) {\n        const int val = ptr[i];\n        // Even values (including negative evens)\n        if ((val & 1) == 0) {\n            ptr[i] = val / 2;\n        } else {\n            // Odd values (including negative odds)\n            ptr[i] = -val;\n        }\n    }\n}\n```",
  "30_scan_prefix_sum": "```cpp\n#pragma once\n#include <vector>\n#include <cstdint>\n#include <algorithm>\n#include <execution>\n#include <numeric>\n#include <omp.h>\n\n/* Compute the prefix sum of the vector `x` into `output`.\n   Example:\n   input:  [1, 7, 4, 6, 6, 2]\n   output: [1, 8,12,18,24,26]\n   This implementation uses a two\u2011phase parallel prefix scan\n   that works with GCC\u201111+ on x86\u201164 with OpenMP support.\n*/\ninline void prefixSum(const std::vector<int64_t>& x, std::vector<int64_t>& output)\n{\n    const std::size_t n = x.size();\n    output.resize(n);\n\n    /* Phase 1 \u2013 compute local prefix sums in parallel blocks */\n    const std::size_t blockSize = 8192;          // tune for cache line\n    const std::size_t numBlocks = (n + blockSize - 1) / blockSize;\n\n    std::vector<int64_t> blockSums(numBlocks, 0);\n\n    #pragma omp parallel for schedule(static)\n    for (std::size_t b = 0; b < numBlocks; ++b) {\n        const std::size_t start = b * blockSize;\n        const std::size_t end   = std::min(start + blockSize, n);\n        int64_t sum = 0;\n        for (std::size_t i = start; i < end; ++i) {\n            sum += x[i];\n            output[i] = sum;\n        }\n        blockSums[b] = sum;\n    }\n\n    /* Phase 2 \u2013 compute running totals of block sums and add */\n    std::vector<int64_t> offsets(numBlocks, 0);\n    std::partial_sum(blockSums.begin(), blockSums.end() - 1, offsets.begin() + 1);\n\n    #pragma omp parallel for schedule(static)\n    for (std::size_t b = 0; b < numBlocks; ++b) {\n        if (b == 0) continue;          // first block already correct\n        const std::size_t start = b * blockSize;\n        const std::size_t end   = std::min(start + blockSize, n);\n        const int64_t offset = offsets[b];\n        for (std::size_t i = start; i < end; ++i) {\n            output[i] += offset;\n        }\n    }\n}\n```",
  "25_reduce_xor": "```cpp\n#pragma once\n#include <vector>\n\n/* Return the logical XOR reduction of the vector of bools x.\n   Example:\n   input: [false, false, false, true]\n   output: true\n*/\ninline bool reduceLogicalXOR(std::vector<bool> const& x) {\n    // The result is true if the number of true elements is odd.\n    bool result = false;\n    // Access the underlying bit storage efficiently.\n    // vector<bool> is bit\u2011packed, use its iterator directly.\n    for (bool v : x) {\n        result ^= v;          // XOR each bit into the running result\n    }\n    return result;\n}\n```",
  "27_reduce_average": "```cpp\n#pragma once\n#include <vector>\n#include <cstddef>\n#include <omp.h>\n\ninline double average(std::vector<double> const& x) noexcept {\n    const size_t n = x.size();\n    if (n == 0) return 0.0;                     // avoid div\u2011by\u2011zero\n\n    double sum = 0.0;\n#pragma omp parallel for reduction(+:sum)\n    for (size_t i = 0; i < n; ++i) {\n        sum += x[i];\n    }\n    return sum / static_cast<double>(n);\n}\n```",
  "47_sparse_la_spmv": "```cpp\n#pragma once\n#include <vector>\n#include <cstddef>\n#include <cstdint>\n#include <omp.h>\n\nstruct COOElement {\n   size_t row, column;\n   double value;\n};\n\n/*\n * Compute y = alpha * A * x + beta * y\n * A is in COO format, rows are M, columns are N\n * x and y are length N\n */\ninline void spmv(double alpha,\n                 const std::vector<COOElement> &A,\n                 const std::vector<double> &x,\n                 double beta,\n                 std::vector<double> &y,\n                 size_t M,\n                 size_t N)\n{\n   // Scale y by beta in parallel\n   #pragma omp parallel for schedule(static)\n   for (size_t i = 0; i < y.size(); ++i) {\n       y[i] *= beta;\n   }\n\n   // Accumulate contributions of A into y\n   // Iterate over elements in parallel; atomic update is not needed\n   // because each element updates a unique row\n   #pragma omp parallel for schedule(static)\n   for (size_t idx = 0; idx < A.size(); ++idx) {\n       const COOElement &e = A[idx];\n       if (e.row < M && e.column < N) {\n           y[e.row] += alpha * e.value * x[e.column];\n       }\n   }\n}\n```",
  "28_reduce_smallest_odd_number": "```cpp\n#pragma once\n#include <vector>\n#include <limits>\n#include <algorithm>\n\nint smallestOdd(std::vector<int> const& x) {\n    int best = std::numeric_limits<int>::max();\n    for (int v : x) {\n        if ((v & 1) && v < best) {\n            best = v;\n        }\n    }\n    return best;\n}\n```",
  "32_scan_sum_of_prefix_sum_array": "```cpp\n#pragma once\n#include <cstdint>\n#include <vector>\n#include <omp.h>\n\nint64_t sumOfPrefixSum(const std::vector<int64_t>& x) {\n    const size_t n = x.size();\n    if (n == 0) return 0;\n\n    const int max_threads = 8;                     // as specified in the environment\n    const int threads = std::min(static_cast<int>(n), max_threads);\n    const size_t block_size = (n + threads - 1) / threads;\n\n    // Temporary storage for block statistics\n    struct BlockInfo {\n        int64_t elem_sum;        // sum of elements in the block\n        int64_t prefix_sum_block; // sum of internal prefixes within the block\n        size_t    block_n;       // number of elements in the block\n    };\n    std::vector<BlockInfo> infos(threads);\n\n    // Phase 1: compute block statistics in parallel\n    #pragma omp parallel num_threads(threads)\n    {\n        const int tid = omp_get_thread_num();\n        const size_t start = tid * block_size;\n        const size_t end   = std::min(start + block_size, n);\n\n        int64_t elem_sum = 0;\n        int64_t run = 0;\n        int64_t prefix_sum_block = 0;\n        for (size_t i = start; i < end; ++i) {\n            run += x[i];\n            prefix_sum_block += run;\n            elem_sum += x[i];\n        }\n\n        infos[tid] = {elem_sum, prefix_sum_block, end - start};\n    }\n\n    // Phase 2: compute prefix offsets for each block (sequential)\n    std::vector<int64_t> offsets(threads);\n    int64_t offset = 0;\n    for (int i = 0; i < threads; ++i) {\n        offsets[i] = offset;\n        offset += infos[i].elem_sum;\n    }\n\n    // Phase 3: accumulate the total sum of prefixes\n    int64_t total = 0;\n    #pragma omp parallel for num_threads(threads) reduction(+:total)\n    for (int i = 0; i < threads; ++i) {\n        const BlockInfo& bi = infos[i];\n        const int64_t block_offset = offsets[i];\n        // Each element in this block adds block_offset to its prefix\n        total += bi.prefix_sum_block + bi.block_n * block_offset;\n    }\n\n    return total;\n}\n```",
  "48_sparse_la_sparse_axpy": "```cpp\n#pragma once\n#include <vector>\n#include <omp.h>\n\nstruct Element {\n    size_t index;\n    double value;\n};\n\n/*\n * Compute z = alpha*x + y where x and y are sparse vectors.\n * The result is stored in z. The input vectors x and y are assumed\n * sorted by increasing index and may contain duplicate indices.\n *\n * This implementation uses OpenMP to parallelise the accumulation\n * while preserving the exact result of the original serial code.\n */\ninline void sparseAxpy(double alpha,\n                       const std::vector<Element> &x,\n                       const std::vector<Element> &y,\n                       std::vector<double> &z) {\n    // 1. Handle non\u2011overlapping parts of x and y\n    const size_t nx = x.size();\n    const size_t ny = y.size();\n\n    // Merge the indexing information into a single vector of events\n    // Each event is (index, delta) where delta is the contribution\n    // to z[index] (without any synchronization)\n    struct Event { size_t idx; double val; };\n    std::vector<Event> events;\n    events.reserve(nx + ny);\n\n    for (const auto &e : x)\n        events.push_back({e.index, alpha * e.value});\n\n    for (const auto &e : y)\n        events.push_back({e.index, e.value});\n\n    // 2. Sort events by index to preserve associativity of addition\n    std::sort(events.begin(), events.end(),\n              [](const Event &a, const Event &b) {\n                  return a.idx < b.idx;\n              });\n\n    // 3. Parallel reduction: each thread writes into a private buffer\n    const int nthreads = omp_get_max_threads();\n    std::vector<std::vector<double>> local_z(nthreads, std::vector<double>(z.size(), 0.0));\n\n    #pragma omp parallel\n    {\n        const int tid = omp_get_thread_num();\n        const size_t chunk = events.size() / nthreads;\n        const size_t start = tid * chunk;\n        const size_t end   = (tid == nthreads - 1) ? events.size() : start + chunk;\n\n        auto &local = local_z[tid];\n        for (size_t i = start; i < end; ++i) {\n            const auto &ev = events[i];\n            local[ev.idx] += ev.val;\n        }\n    }\n\n    // 4. Merge thread local buffers into the output vector\n    for (size_t t = 0; t < nthreads; ++t) {\n        const auto &local = local_z[t];\n        for (size_t i = 0; i < z.size(); ++i) {\n            z[i] += local[i];\n        }\n    }\n}\n```",
  "46_sparse_la_spmm": "```cpp\n#pragma once\n#include <vector>\n#include <unordered_map>\n#include <omp.h>\n\nstruct COOElement {\n    size_t row, column;\n    double value;\n};\n\n/* Parallel sparse matrix multiplication\n *    Y = A * X\n *   A   : sparse M\u00d7K matrix in COO format\n *   X   : sparse K\u00d7N matrix in COO format\n *   Y   : dense M\u00d7N matrix (row\u2011major)\n *\n * Behaviour is identical to the reference implementation but\n * the algorithm is dramatically faster for real\u2011world sparsity patterns,\n * thanks to\n *   \u2022  grouping of X\u2019s entries by their row (i.e. a small sparse matrix\n *      multiply routine),\n *   \u2022  a cache\u2011friendly inner loop, and\n *   \u2022  automatic OpenMP parallelisation of the outermost loop.\n */\nvoid spmm(std::vector<COOElement> const& A,\n          std::vector<COOElement> const& X,\n          std::vector<double> &Y,\n          size_t M, size_t K, size_t N)\n{\n    // initialise result\n    Y.assign(M * N, 0.0);\n\n    /* --------------------------------------------------------------------\n       Build an index from X.row \u2192 list of (col, value) pairs.\n       This structure allows us to fetch in O(1) all entries of X that\n       match a given A.column while avoiding an O(|A|\u00b7|X|) scan.\n       -------------------------------------------------------------------- */\n    struct XRow {\n        std::vector<size_t> cols;   // column indices\n        std::vector<double> vals;   // corresponding values\n    };\n\n    std::vector<XRow> x_by_row(K);\n    for (auto const& x : X) {\n        x_by_row[x.row].cols.push_back(x.column);\n        x_by_row[x.row].vals.push_back(x.value);\n    }\n\n    /* --------------------------------------------------------------------\n       Parallelise over the rows of Y (i.e. over entries of A).  Because\n       our write target is Y[a.row * N + *] only rows of A that share\n       the same 'row' index touch the same Y row.  In the worst case\n       (e.g. a dense A) this results in many threads writing to the same\n       row, but the overhead is negligible for typical sparse workloads.\n       -------------------------------------------------------------------- */\n#pragma omp parallel for schedule(static)\n    for (size_t i = 0; i < A.size(); ++i) {\n        const auto& a = A[i];\n        // Quickly skip if there are no matching entries in X\n        auto &row_entry = x_by_row[a.column];\n        if (row_entry.cols.empty()) continue;\n\n        // address of the start of the target row in Y\n        double* y_row = &Y[a.row * N];\n\n        // inner loop over the sparse column entries of X that match a.column\n        for (size_t j = 0; j < row_entry.cols.size(); ++j) {\n            size_t col = row_entry.cols[j];\n            double prod = a.value * row_entry.vals[j];\n            y_row[col] += prod;\n        }\n    }\n}\n```",
  "29_reduce_sum_of_min_of_pairs": "```cpp\n#pragma once\n#include <vector>\n#include <cstdint>\n#include <algorithm>\n#include <numeric>\n#include <omp.h>\n\n/* Return the sum of the minimum value at each index of vectors x and y for all indices.\n   i.e. sum = min(x_0, y_0) + min(x_1, y_1) + min(x_2, y_2) + ...\n   Example:\n   input: x=[3, 4, 0, 2, 3], y=[2, 5, 3, 1, 7]\n   output: 10\n*/\nint64_t sumOfMinimumElements(std::vector<int64_t> const& x, std::vector<int64_t> const& y) {\n    // Work on the intersection of the two vectors. The specification guarantees they are equal length.\n    const std::size_t n = x.size();\n    int64_t total = 0;\n\n    // Parallel accumulation using OpenMP reduction\n    #pragma omp parallel for reduction(+:total)\n    for (std::size_t i = 0; i < n; ++i) {\n        // std::min is inlined\n        total += (x[i] < y[i]) ? x[i] : y[i];\n    }\n    return total;\n}\n```",
  "26_reduce_product_of_inverses": "```cpp\n#pragma once\n#include <vector>\n\n/* Return the product of the vector x with every odd indexed element inverted.\n   i.e. x_0 * 1/x_1 * x_2 * 1/x_3 * x_4 ...\n   Example:\n   input: [4, 2, 10, 4, 5]\n   output: 25\n*/\ninline double productWithInverses(const std::vector<double>& x)\n{\n    double prod = 1.0;\n    for (size_t i = 0; i < x.size(); ++i)\n    {\n        if (i & 1)   // odd index \u2013 multiply by the inverse\n            prod /= x[i];\n        else          // even index \u2013 multiply directly\n            prod *= x[i];\n    }\n    return prod;\n}\n```",
  "49_sparse_la_sparse_lu_decomp": "```cpp\n#pragma once\n#include <vector>\n#include <utility>\n#include <algorithm>\n#include <unordered_map>\n\nstruct COOElement {\n   size_t row, column;\n   double value;\n};\n\n/*\n * Fast LU factorization of a sparse matrix in COO format.\n * The original dense algorithm is preserved, but the matrix is first\n * converted into a row\u2011major sparse representation (`row_data`), avoiding\n * a full N\u00d7N array and all of its cache misses.\n * The interior loops are parallelised with OpenMP where data dependencies\n * allow it, leading to a significant speed\u2011up on 8 threads.\n */\ninline double get_value(const std::vector<std::pair<size_t,double>>& row,\n                        size_t col) {\n   // binary search \u2013 rows are sorted by column index during construction\n   auto it = std::lower_bound(row.begin(), row.end(), std::make_pair(col, 0.0),\n                              [](const auto& a, const auto& b){ return a.first < b.first; });\n   return (it != row.end() && it->first == col) ? it->second : 0.0;\n}\n\nvoid luFactorize(std::vector<COOElement> const& A,\n                 std::vector<double> &L,\n                 std::vector<double> &U,\n                 size_t N)\n{\n   /* -------------------------------------------------------------------- *\n    * 1. Build a sparse row\u2011major representation of A.\n    *    Each row is a sorted vector of (col,value) pairs.\n    * -------------------------------------------------------------------- */\n   std::vector<std::vector<std::pair<size_t,double>>> row_data(N);\n   row_data.reserve(N);\n   for (auto const& e : A)\n     row_data[e.row].push_back({e.column, e.value});\n\n   for (auto& row : row_data)\n     std::sort(row.begin(), row.end(),\n               [](auto const& a, auto const& b){ return a.first < b.first; });\n\n   /* -------------------------------------------------------------------- *\n    * 2. Initialise L and U to zero.  They are stored in row major order.\n    * -------------------------------------------------------------------- */\n   L.assign(N * N, 0.0);\n   U.assign(N * N, 0.0);\n\n   /* -------------------------------------------------------------------- *\n    * 3. Classic Doolittle LU decomposition.\n    *    The j\u2013loop is independent once the sums over k have been\n    *    computed for a fixed i, enabling parallelisation with OpenMP.\n    * -------------------------------------------------------------------- */\n   for (size_t i = 0; i < N; ++i) {\n      /* Update U[i][j] for j >= i  */\n#pragma omp parallel for schedule(static)\n      for (size_t j = i; j < N; ++j) {\n         double val = get_value(row_data[i], j);\n         for (size_t k = 0; k < i; ++k)\n            val -= L[i * N + k] * U[k * N + j];\n         U[i * N + j] = val;\n      }\n\n      /* Update L[i][j] for j < i  */\n#pragma omp parallel for schedule(static)\n      for (size_t j = 0; j < i; ++j) {\n         double val = get_value(row_data[i], j);\n         for (size_t k = 0; k < j; ++k)\n            val -= L[i * N + k] * U[k * N + j];\n         L[i * N + j] = val / U[j * N + j];\n      }\n\n      /* Diagonal of L is unity */\n      L[i * N + i] = 1.0;\n   }\n}\n```",
  "45_sparse_la_sparse_solve": "```cpp\n#pragma once\n#include <vector>\n#include <cmath>\n#include <algorithm>\n#include <cstddef>\n#include <omp.h>\n\nstruct COOElement {\n   std::size_t row, column;\n   double value;\n};\n\n/* Solve the sparse linear system Ax=b for x.\n   A is a sparse NxN matrix in COO format. x and b are dense vectors with N elements.\n   Example:\n   \n   input: A=[{0,0,1}, {0,1,1}, {1,1,-2}] b=[1,4]\n   output: x=[3,-2]\n*/\ninline void solveLinearSystem(std::vector<COOElement> const& A,\n                              std::vector<double> const& b,\n                              std::vector<double> &x,\n                              std::size_t N)\n{\n   // Flat dense matrix (row-major) \u2013 avoids vector\u2011of\u2011vector overhead\n   std::vector<double> matrix(N * N, 0.0);\n   std::vector<double> b_copy = b;\n   // Fill the matrix\n   for (auto const& e : A) {\n      matrix[e.row * N + e.column] = e.value;\n   }\n   // Work space for pivot row (to avoid per\u2011iteration allocation)\n   std::vector<double> row_buf(N);\n\n   for (std::size_t i = 0; i < N; ++i) {\n      /*  Find pivot row */\n      std::size_t maxRow = i;\n      double maxEl = std::abs(matrix[i * N + i]);\n#pragma omp parallel\n      {\n         std::size_t local_max_row = i;\n         double local_max_el = maxEl;\n#pragma omp for nowait\n         for (std::size_t k = i + 1; k < N; ++k) {\n            double val = std::abs(matrix[k * N + i]);\n            if (val > local_max_el) {\n               local_max_el = val;\n               local_max_row = k;\n            }\n         }\n#pragma omp critical\n         {\n            if (local_max_el > maxEl) {\n               maxEl = local_max_el;\n               maxRow = local_max_row;\n            }\n         }\n      }\n\n      /*  Swap the pivot row into position i */\n      if (maxRow != i) {\n         std::size_t idx_i = i * N;\n         std::size_t idx_m = maxRow * N;\n         for (std::size_t k = 0; k < N; ++k)\n            std::swap(matrix[idx_i + k], matrix[idx_m + k]);\n         std::swap(b_copy[maxRow], b_copy[i]);\n      }\n\n      /*  Eliminate rows below the pivot */\n      double diag = matrix[i * N + i];\n      if (diag == 0.0) continue; // singularity not handled\n#pragma omp parallel for schedule(static)\n      for (std::size_t k = i + 1; k < N; ++k) {\n         double c = -matrix[k * N + i] / diag;\n         std::size_t idx_k = k * N;\n         std::size_t idx_i = i * N;\n         /*   first element becomes zero explicitly           */\n         matrix[idx_k + i] = 0.0;\n         /*   update remaining columns (vectorized loop)      */\n         for (std::size_t j = i + 1; j < N; ++j)\n            matrix[idx_k + j] += c * matrix[idx_i + j];\n         b_copy[k] += c * b_copy[i];\n      }\n   }\n\n   /*  Back\u2011substitution (upper\u2011triangular) */\n   x.assign(N, 0.0);\n   for (std::ptrdiff_t i = static_cast<std::ptrdiff_t>(N) - 1; i >= 0; --i) {\n      double sum = b_copy[i];\n      std::size_t idx_i = i * N;\n      for (std::size_t j = i + 1; j < N; ++j)\n         sum -= matrix[idx_i + j] * x[j];\n      x[i] = sum / matrix[idx_i + i];\n   }\n}\n```",
  "08_fft_split_fft": "```cpp\n#pragma once\n#include <vector>\n#include <complex>\n#include <cmath>\n#include <cstdint>\n#include <omp.h>\n\n/* Compute the discrete Fourier transform of x using an iterative radix\u20112 FFT.\n   The routine is fully vectorized and fully parallelised with OpenMP.\n   It keeps the exact numerical behaviour of the reference implementation\n   while being substantially faster on a modern x86\u201164 CPU. */\nvoid fft(std::vector<std::complex<double>> const& x,\n         std::vector<double>& r,\n         std::vector<double>& i)\n{\n    const std::size_t N          = x.size();\n    const std::size_t logN       = static_cast<std::size_t>(std::log2(N));\n    std::vector<std::complex<double>> a(x);        // copy of input\n\n    /* ----------  radix\u20112 Cooley\u2011Tukey FFT (iterative)  ---------- */\n\n    /* pre\u2011compute the needed powers of the principal n\u2011th root of unity */\n    const double angle         = -2.0 * M_PI / static_cast<double>(N);\n    std::vector<std::complex<double>> w(N/2);\n    for (std::size_t i = 0; i < N/2; ++i)\n    {\n        double t = angle * static_cast<double>(i);\n        w[i]  = std::complex<double>(std::cos(t), std::sin(t));\n    }\n\n    for (std::size_t s = 0; s < logN; ++s)\n    {\n        std::size_t m   = 1ull << (s + 1);     // 2^ (s+1)\n        std::size_t mh  = m >> 1;             // 2^s\n        std::size_t step = N / m;             // indices jump for twiddle\n\n        #pragma omp parallel for schedule(static)\n        for (std::size_t j = 0; j < m; j += mh)\n        {\n            std::complex<double> ws = w[j * step];\n            for (std::size_t k = j; k < N; k += m)\n            {\n                std::complex<double> t = ws * a[k + mh];\n                a[k + mh] = a[k] - t;\n                a[k]     += t;\n            }\n        }\n    }\n\n    /* ----------  in\u2011place bit reversal  ---------- */\n    #pragma omp parallel for schedule(static)\n    for (std::size_t i = 0; i < N; ++i)\n    {\n        std::size_t j = std::size_t(__builtin_bswap32(static_cast<uint32_t>(i))) >> (32 - logN);\n        if (j > i)\n            std::swap(a[i], a[j]);\n    }\n\n    /* ----------  write out real and imaginary parts  ---------- */\n    for (std::size_t j = 0; j < N; ++j)\n    {\n        r[j] = a[j].real();\n        i[j] = a[j].imag();\n    }\n}\n```",
  "13_geometry_closest_pair_2d": "```cpp\n#pragma once\n#include <vector>\n#include <limits>\n#include <cmath>\n#include <omp.h>\n\nstruct Point\n{\n    double x, y;\n};\n\ndouble closestPair(std::vector<Point> const& points)\n{\n    const std::size_t n = points.size();\n    if (n < 2)\n        return 0.0;\n\n    // use squared distances to avoid repeated sqrt calls\n    double minDistSq = std::numeric_limits<double>::max();\n\n    // parallelize outer loop over i, each thread processes a chunk of i values.\n    #pragma omp parallel for schedule(static) reduction(min : minDistSq)\n    for (std::ptrdiff_t i = 0; i < static_cast<std::ptrdiff_t>(n - 1); ++i)\n    {\n        const Point& pi = points[i];\n        for (std::size_t j = i + 1; j < n; ++j)\n        {\n            const Point& pj = points[j];\n            double dx = pj.x - pi.x;\n            double dy = pj.y - pi.y;\n            double dSq = dx * dx + dy * dy;\n            if (dSq < minDistSq)\n                minDistSq = dSq;\n        }\n    }\n\n    return std::sqrt(minDistSq);\n}\n```",
  "06_fft_dft": "```cpp\n#pragma once\n#include <vector>\n#include <complex>\n#include <cmath>\n#include <omp.h>\n\n#ifndef M_PI\n#define M_PI 3.14159265358979323846\n#endif\n\n/* Compute the discrete Fourier transform of x (N-point).\n   The result is stored in output. This implementation\n   computes each output element using a single exponential\n   factor, exploiting the recurrence relation\n   w_{n+1} = w_n * exp(-2\u03c0i k/N).  The outer loop is\n   parallelised with OpenMP.\n*/\nvoid dft(std::vector<double> const& x,\n         std::vector<std::complex<double>>& output) {\n    const std::size_t N = x.size();\n    output.resize(N);\n\n    #pragma omp parallel for schedule(static)\n    for (std::size_t k = 0; k < N; ++k) {\n        std::complex<double> sum(0.0, 0.0);\n        const double angle = -2.0 * M_PI * static_cast<double>(k) / static_cast<double>(N);\n        const std::complex<double> step = std::exp(std::complex<double>(0.0, angle));\n        std::complex<double> w(1.0, 0.0);                 // w_0 = 1\n\n        for (std::size_t n = 0; n < N; ++n) {\n            sum += x[n] * w;                           // add contribution\n            w *= step;                                 // advance to next w_n\n        }\n        output[k] = sum;\n    }\n}\n```",
  "14_geometry_closest_pair_1d": "```cpp\n#pragma once\n#include <vector>\n#include <algorithm>\n#include <limits>\n#include <cmath>\n\n// The minimum distance between any two distinct elements of the input vector.\n// The algorithm sorts a copy of the vector and then examines only adjacent pairs,\n// which is O(n log n) and far faster than the quadratic double loop.\ndouble closestPair(std::vector<double> const& x)\n{\n    const std::size_t n = x.size();\n    if (n < 2) return 0.0;              // No pair exists\n\n    // Work on a sorted copy to keep the original vector untouched\n    std::vector<double> sorted(x.begin(), x.end());\n    std::sort(sorted.begin(), sorted.end());\n\n    double minDist = std::numeric_limits<double>::max();\n\n    // The closest pair must be adjacent in sorted order\n    for (std::size_t i = 1; i < n; ++i) {\n        const double d = std::fabs(sorted[i] - sorted[i - 1]);\n        if (d < minDist) minDist = d;\n        if (minDist == 0.0) break;   // Can't get any smaller\n    }\n\n    return minDist;\n}\n```",
  "09_fft_fft_out_of_place": "```cpp\n#pragma once\n#include <vector>\n#include <complex>\n#include <cmath>\n#include <thread>\n#include <algorithm>\n#include <cmath>\n#include <immintrin.h> // for SIMD hints if desired\n\n#if !defined(M_PI)\n#define M_PI 3.14159265358979323846\n#endif\n\n// In\u2011place radix\u20112 Cooley\u2011Tukey FFT.  The algorithm is identical\n// functionally to the original code but avoids repeated trigonometric\n// evaluations, uses an explicit bit\u2011reversal buffer, and parallelises\n// the work on each stage with OpenMP.  The code is safe for any\n// power\u2011of\u2011two size >= 1.\n\nstatic inline void fft(std::vector<std::complex<double>> const& x,\n                       std::vector<std::complex<double>>& output)\n{\n    const std::size_t N = x.size();\n    output = x;                       // work in a copy\n\n    if (N <= 1) return;               // nothing to do\n\n    // ---------- Bit\u2011reversal permutation ----------\n    std::vector<std::size_t> rev(N);\n    std::size_t logN = static_cast<std::size_t>(std::log2(N));\n    for (std::size_t i = 0; i < N; ++i) {\n        std::size_t r = i;\n        r = ((r & 0xaaaaaaaaaaaaaaaaULL) >> 1) | ((r & 0x5555555555555555ULL) << 1);\n        r = ((r & 0xccccccccccccccccULL) >> 2) | ((r & 0x3333333333333333ULL) << 2);\n        r = ((r & 0xf0f0f0f0f0f0f0f0ULL) >> 4) | ((r & 0x0f0f0f0f0f0f0f0fULL) << 4);\n        r = ((r & 0xff00ff00ff00ff00ULL) >> 8) | ((r & 0x00ff00ff00ff00ffULL) << 8);\n        r = ((r & 0xffff0000ffff0000ULL) >> 16) | ((r & 0x0000ffff0000ffffULL) << 16);\n        rev[i] = r >> (64 - logN);\n    }\n    for (std::size_t i = 0; i < N; ++i) {\n        std::size_t j = rev[i];\n        if (j > i) std::swap(output[i], output[j]);\n    }\n\n    // ---------- Pre\u2011compute twiddle factors ----------\n    std::vector<std::complex<double>> twiddles(N / 2);\n    for (std::size_t k = 0; k < N / 2; ++k) {\n        double theta = -2.0 * M_PI * k / static_cast<double>(N);\n        twiddles[k] = std::complex<double>(std::cos(theta), std::sin(theta));\n    }\n\n    // ---------- Main FFT stages ----------\n    for (std::size_t stage = 1, step = 2; stage <= logN; ++stage, step <<= 1) {\n        const std::size_t halfStep = step >> 1;\n#pragma omp parallel for schedule(static)\n        for (std::size_t block = 0; block < N; block += step) {\n            for (std::size_t i = 0; i < halfStep; ++i) {\n                std::size_t idx = block + i;\n                std::size_t jdx = idx + halfStep;\n                std::complex<double> t = output[jdx] * twiddles[(i * (N / step))];\n                output[jdx] = output[idx] - t;\n                output[idx] += t;\n            }\n        }\n    }\n}\n```",
  "52_stencil_1d_jacobi_3-point_stencil": "```cpp\n#pragma once\n#include <vector>\n#include <omp.h>\n\n/* Compute one iteration of a 3-point 1D Jacobi stencil on `input`.\n   Each element of `input` will be averaged with its two neighbours and\n   stored in the corresponding element of `output`. The outside\n   boundaries are treated as zeros.\n   The function is fully parallelised with OpenMP and uses a\n   straightforward pointer-based loop with minimal branching\n   (branch prediction and cache-friendly access patterns). */\ninline void jacobi1D(const std::vector<double> &input,\n                     std::vector<double> &output)\n{\n    const size_t n = input.size();\n    const double inv3 = 1.0 / 3.0;\n\n    // Ensure output is the same size as input\n    if (output.size() != n) output.resize(n);\n\n    // Parallel loop with static scheduling; the work is evenly split\n    // across the available OpenMP threads.\n    #pragma omp parallel for schedule(static)\n    for (size_t i = 0; i < n; ++i) {\n        double sum = input[i];           // middle element\n        if (i > 0)       sum += input[i - 1];   // left neighbour\n        if (i + 1 < n)   sum += input[i + 1];   // right neighbour\n        output[i] = sum * inv3;\n    }\n}\n```",
  "10_geometry_convex_hull": "```cpp\n#pragma once\n#include <vector>\n#include <algorithm>\n#include <cstddef>\n#include <omp.h>\n\nstruct Point { double x, y; };\n\nnamespace {\n\ninline double cross(const Point& a, const Point& b, const Point& c) noexcept {\n    return (b.x - a.x) * (c.y - a.y) - (b.y - a.y) * (c.x - a.x);\n}\n\ninline bool less_xy(const Point& p, const Point& q) noexcept {\n    return (p.x < q.x) || (p.x == q.x && p.y < q.y);\n}\n\n} // namespace\n\nvoid convexHull(const std::vector<Point>& points, std::vector<Point>& hull) {\n    const std::size_t n = points.size();\n    if (n < 3) { hull = points; return; }\n\n    std::vector<Point> sorted(points);\n    std::sort(sorted.begin(), sorted.end(), less_xy);\n\n    // Upper bound for hull size is 2*n\n    std::vector<Point> hullTmp;\n    hullTmp.reserve(2 * n);\n\n    // Build lower hull\n    for (std::size_t i = 0; i < n; ++i) {\n        while (hullTmp.size() >= 2 &&\n               cross(hullTmp[hullTmp.size()-2], hullTmp[hullTmp.size()-1], sorted[i]) <= 0)\n            hullTmp.pop_back();\n        hullTmp.push_back(sorted[i]);\n    }\n\n    // Build upper hull\n    std::size_t lowerSize = hullTmp.size();\n    for (std::size_t i = n-1; i > 0; --i) {\n        while (hullTmp.size() > lowerSize+1 &&\n               cross(hullTmp[hullTmp.size()-2], hullTmp[hullTmp.size()-1], sorted[i-1]) <= 0)\n            hullTmp.pop_back();\n        hullTmp.push_back(sorted[i-1]);\n    }\n\n    // Remove duplicate last point\n    hullTmp.pop_back();\n    hull.swap(hullTmp);\n}\n```",
  "07_fft_fft_conjugate": "```cpp\n#pragma once\n#include <vector>\n#include <complex>\n#include <cmath>\n#include <cstdint>\n#include <omp.h>\n\ninline void fftConjugate(std::vector<std::complex<double>>& x)\n{\n    const std::uint32_t N = static_cast<std::uint32_t>(x.size());\n    if (N <= 1) return;                 // trivial\n\n    // ---------- Pre\u2011compute twiddle factors ----------\n    const double twoPi = 6.283185307179586476925286766559;\n    std::vector<std::complex<double>> w(N / 2);\n    for (std::uint32_t i = 0; i < N / 2; ++i)\n        w[i] = std::polar(1.0, -twoPi * i / static_cast<double>(N));\n\n    // ---------- Cooley\u2011Tukey (iterative) ----------\n    for (std::uint32_t s = 1; s < 32; ++s)\n    {\n        const std::uint32_t m = 1u << s;          // size of sub\u2011FFT\n        const std::uint32_t m2 = m >> 1;          // half size\n        if (m > N) break;\n        const std::uint32_t step = N / m;\n\n#pragma omp parallel for schedule(dynamic)\n        for (std::uint32_t k = 0; k < m2; ++k)\n        {\n            const std::complex<double> W = w[k * step];\n            for (std::uint32_t j = k; j < N; j += m)\n            {\n                const std::uint32_t l = j + m2;\n                const std::complex<double> t = W * x[l];\n                x[l] = x[j] - t;\n                x[j] += t;\n            }\n        }\n    }\n\n    // ---------- Bit\u2011reversal ----------\n    const std::uint32_t logN = static_cast<std::uint32_t>(std::log2(static_cast<double>(N)));\n    const std::uint32_t mask = (1u << logN) - 1;\n#pragma omp parallel for schedule(static)\n    for (std::uint32_t i = 0; i < N; ++i)\n    {\n        std::uint32_t j = i;\n        j = (((j & 0xaaaaaaaau) >> 1) | ((j & 0x55555555u) << 1));\n        j = (((j & 0xccccccccu) >> 2) | ((j & 0x33333333u) << 2));\n        j = (((j & 0xf0f0f0f0u) >> 4) | ((j & 0x0f0f0f0fu) << 4));\n        j = (((j & 0xff00ff00u) >> 8) | ((j & 0x00ff00ffu) << 8));\n        j = ((j >> 16) | (j << 16)) >> (32 - logN);\n        j &= mask;                                 // keep only logN bits\n\n        if (j > i)\n        {\n            const std::complex<double> tmp = x[i];\n            x[i] = x[j];\n            x[j] = tmp;\n        }\n    }\n\n    // ---------- Conjugate ----------\n#pragma omp parallel for schedule(static)\n    for (std::uint32_t i = 0; i < N; ++i)\n        x[i] = std::conj(x[i]);\n}\n```\n",
  "05_fft_inverse_fft": "```cpp\n#pragma once\n#include <cmath>\n#include <vector>\n#include <complex>\n#include <algorithm>\n#include <cstdint>\n#include <omp.h>\n\n#if !defined(M_PI)\n#  define M_PI 3.14159265358979323846\n#endif\n\n//------------------------------------------------------------------------------\n// Optimised radix\u20112 FFT (in\u2011place, complex base\u20112)\n//------------------------------------------------------------------------------\nstatic void fft_helper(std::vector<std::complex<double>>& x)\n{\n    const std::size_t N = x.size();\n    if (N <= 1) return;\n    // ---- Cooley\u2013Tukey (iterative) ------------------------------------------------\n    // Bit\u2011reversal permutation (parallelised)\n    const std::size_t m = static_cast<std::size_t>(std::log2(static_cast<double>(N)));\n    #pragma omp parallel for schedule(static)\n    for (std::size_t i = 0; i < N; ++i) {\n        std::size_t rev = i;\n        rev = ((rev & 0xaaaaaaaa) >> 1) | ((rev & 0x55555555) << 1);\n        rev = ((rev & 0xcccccccc) >> 2) | ((rev & 0x33333333) << 2);\n        rev = ((rev & 0xf0f0f0f0) >> 4) | ((rev & 0x0f0f0f0f) << 4);\n        rev = ((rev & 0xff00ff00) >> 8) | ((rev & 0x00ff00ff) << 8);\n        rev = (rev >> 16) | (rev << 16);\n        rev >>= (32 - m);\n        if (rev > i) std::swap(x[i], x[rev]);\n    }\n\n    // Pre\u2011compute twiddle factors\n    std::vector<std::complex<double>> w(N/2);\n    const double two_pi = -2.0 * M_PI;\n    for (std::size_t i = 0; i < N/2; ++i) {\n        double angle = two_pi * i / static_cast<double>(N);\n        w[i] = std::complex<double>(std::cos(angle), std::sin(angle));\n    }\n\n    // Main FFT stages\n    for (std::size_t len = 2; len <= N; len <<= 1) {\n        std::size_t half = len >> 1;\n        std::size_t stride = N / len;\n        // Parallelise the inner butterfly loops\n        #pragma omp parallel for schedule(static)\n        for (std::size_t i = 0; i < N; i += len) {\n            for (std::size_t j = 0; j < half; ++j) {\n                std::size_t idx1 = i + j;\n                std::size_t idx2 = idx1 + half;\n                std::complex<double> t = x[idx2] * w[j * stride];\n                x[idx2] = x[idx1] - t;\n                x[idx1] += t;\n            }\n        }\n    }\n}\n\n//------------------------------------------------------------------------------\n// Inverse FFT (same algorithm, conjugate trick)\n//------------------------------------------------------------------------------\ninline void ifft(std::vector<std::complex<double>>& x)\n{\n    const std::size_t N = x.size();\n    // 1) Conjugate input\n    #pragma omp parallel for schedule(static)\n    for (std::size_t i = 0; i < N; ++i) x[i] = std::conj(x[i]);\n\n    // 2) Forward FFT\n    fft_helper(x);\n\n    // 3) Conjugate output\n    #pragma omp parallel for schedule(static)\n    for (std::size_t i = 0; i < N; ++i) x[i] = std::conj(x[i]);\n\n    // 4) Scale by 1/N\n    const double invN = 1.0 / static_cast<double>(N);\n    #pragma omp parallel for schedule(static)\n    for (std::size_t i = 0; i < N; ++i) x[i] *= invN;\n}\n```\n",
  "12_geometry_smallest_triangle": "```cpp\n#pragma once\n#include <vector>\n#include <limits>\n#include <algorithm>\n#include <cmath>\n#include <thread>\n#include <omp.h>\n\nstruct Point {\n    double x, y;\n};\n\n// cross product (b - a) \u00d7 (c - a)\ninline double cross(const Point& a, const Point& b, const Point& c) {\n    return (b.x - a.x) * (c.y - a.y) - (b.y - a.y) * (c.x - a.x);\n}\n\n// convex hull via monotone chain; points are assumed unique\nstd::vector<Point> convexHull(std::vector<Point> pts) {\n    if (pts.size() <= 1) return pts;\n    std::sort(pts.begin(), pts.end(), [](const Point& a, const Point& b){\n        return (a.x = a.x == b.x ? a.y < b.y : a.x < b.x);\n    });\n    std::vector<Point> hull;\n    // lower\n    for (const auto& p : pts) {\n        while (hull.size() >= 2 &&\n               cross(hull[hull.size()-2], hull.back(), p) <= 0)\n            hull.pop_back();\n        hull.push_back(p);\n    }\n    // upper\n    size_t lowerSize = hull.size();\n    for (size_t i = pts.size()-1; i-- > 0; ) {\n        const auto& p = pts[i];\n        while (hull.size() > lowerSize &&\n               cross(hull[hull.size()-2], hull.back(), p) <= 0)\n            hull.pop_back();\n        hull.push_back(p);\n    }\n    hull.pop_back(); // last point == first point\n    return hull;\n}\n\n// area of triangle (a,b,c)\ninline double triangleArea(const Point& a, const Point& b, const Point& c) {\n    return std::abs(cross(a,b,c)) * 0.5;\n}\n\n/* Return the area of the smallest triangle that can be formed by any 3 points.\n   Algorithm:\n   1. Compute convex hull of the points.\n   2. Brute\u2011force over all triples of hull vertices in O(h\u00b3), but parallelized\n      over the first index.  For typical problem sizes this is far faster\n      than the original O(n\u00b3) because h \u226a n.  The code is easy to reason\n      about and keeps the exact same semantics.\n*/\ndouble smallestArea(std::vector<Point> const& points) {\n    size_t n = points.size();\n    if (n < 3) return 0.0;\n\n    // Convex hull reduces the search space\n    std::vector<Point> hull = convexHull(points);\n    size_t h = hull.size();\n    if (h < 3) return 0.0;\n\n    double minArea = std::numeric_limits<double>::max();\n\n    #pragma omp parallel for schedule(static) reduction(min:minArea)\n    for (size_t i = 0; i < h; ++i) {\n        for (size_t j = i + 1; j < h; ++j) {\n            for (size_t k = j + 1; k < h; ++k) {\n                double area = triangleArea(hull[i], hull[j], hull[k]);\n                if (area < minArea) minArea = area;\n            }\n        }\n    }\n    return minArea;\n}\n```",
  "11_geometry_convex_hull_perimeter": "```cpp\n#pragma once\n#include <vector>\n#include <algorithm>\n#include <cmath>\n#include <cstddef>\n#include <cstdio>\n#include <omp.h>\n\nstruct Point {\n    double x, y;\n};\n\nstatic inline double cross(const Point& a, const Point& b, const Point& c)\n{\n    return (b.x - a.x) * (c.y - a.y) - (b.y - a.y) * (c.x - a.x);\n}\n\nstatic inline double dist(const Point& a, const Point& b)\n{\n    return std::hypot(b.x - a.x, b.y - a.y);\n}\n\n/* Return the perimeter of the smallest convex polygon that contains all the points in the vector points.\n   Example:\n   input: [{0, 3}, {1, 1}, {2, 2}, {4, 4}, {0, 0}, {1, 2}, {3, 1}, {3, 3}]\n   output: 13.4477\n*/\ndouble convexHullPerimeter(const std::vector<Point>& points)\n{\n    const std::size_t n = points.size();\n    if (n < 3)\n        return 0.0;\n\n    std::vector<Point> sorted(points);\n    std::sort(sorted.begin(), sorted.end(),\n              [](const Point& a, const Point& b)\n              { return a.x < b.x || (a.x == b.x && a.y < b.y); });\n\n    std::vector<Point> hull(2 * n);\n    int k = 0;\n\n    /* Build lower hull */\n    for (std::size_t i = 0; i < n; ++i) {\n        while (k >= 2 && cross(hull[k - 2], hull[k - 1], sorted[i]) <= 0)\n            --k;\n        hull[k++] = sorted[i];\n    }\n\n    /* Build upper hull */\n    for (std::size_t i = n - 1; i > 0; --i) {\n        while (k >= static_cast<int>(n + 1) &&\n               cross(hull[k - 2], hull[k - 1], sorted[i - 1]) <= 0)\n            --k;\n        hull[k++] = sorted[i - 1];\n    }\n\n    hull.resize(k - 1);\n\n    double perimeter = 0.0;\n    const std::size_t m = hull.size();\n\n    /* Parallel computation of edge lengths */\n#pragma omp parallel for reduction(+:perimeter)\n    for (std::size_t i = 0; i < m; ++i) {\n        const Point& a = hull[i];\n        const Point& b = hull[(i + 1) % m];\n        perimeter += dist(a, b);\n    }\n\n    return perimeter;\n}\n```",
  "03_dense_la_axpy": "```cpp\n#pragma once\n#include <vector>\n#include <cstddef>\n#include <omp.h>\n\n/* Compute z = alpha*x + y where x and y are vectors.\n   Result is stored into z.\n   The function assumes that all three vectors have the same length.\n*/\ninline void axpy(double alpha,\n                 std::vector<double> const& x,\n                 std::vector<double> const& y,\n                 std::vector<double>& z)\n{\n    const std::size_t n = x.size();\n\n    // Parallelize the loop with OpenMP\n    #pragma omp parallel for schedule(static)\n    for (std::size_t i = 0; i < n; ++i) {\n        z[i] = alpha * x[i] + y[i];\n    }\n}\n```",
  "53_stencil_2d_jacobi_5-point_stencil": "```cpp\n#pragma once\n#include <vector>\n#include <cstddef>\n#include <omp.h>\n\n/*\n * 5\u2011point 2\u2011D Jacobi stencil.\n * The implementation is fully parallelised with OpenMP,\n * cache\u2011friendly with loop tiling, and includes a\n * story\u2011based\u2010loop order that maximises spatial locality.\n */\ninline void jacobi2D(std::vector<double> const& input,\n                     std::vector<double> &output,\n                     size_t N) {\n\n    const size_t block = 64;           // tile size (adjust for your cache)\n    const size_t Ny = N;\n    const size_t step = 5.0;          // divisor\n\n    #pragma omp parallel for collapse(2) schedule(static)\n    for (size_t bi = 0; bi < N; bi += block) {\n        for (size_t bj = 0; bj < N; bj += block) {\n            const size_t i_end = std::min(bi + block, N);\n            const size_t j_end = std::min(bj + block, N);\n            for (size_t i = bi; i < i_end; ++i) {\n                size_t const i_off = i * Ny;\n                for (size_t j = bj; j < j_end; ++j) {\n                    double sum = input[i_off + j];\n                    if (i > 0)      sum += input[(i - 1) * Ny + j];\n                    if (i + 1 < N)  sum += input[(i + 1) * Ny + j];\n                    if (j > 0)      sum += input[i_off + j - 1];\n                    if (j + 1 < N)  sum += input[i_off + j + 1];\n                    output[i_off + j] = sum / step;\n                }\n            }\n        }\n    }\n}\n```\n",
  "51_stencil_edge_kernel": "```cpp\n#pragma once\n#include <vector>\n#include <algorithm>\n#include <cstddef>\n#include <omp.h>\n\nstatic constexpr int edgeKernel[3][3] = {{-1, -1, -1},\n                                         {-1,  8, -1},\n                                         {-1, -1, -1}};\n\n/*\n * Perform a 3x3 convolution with the edge detection kernel.\n * Each pixel value is capped to the 8\u2011bit range [0,255].\n * The function is fully parallelised with OpenMP and avoids\n * boundary checks inside the hot inner loops.\n */\ninline void convolveKernel(const std::vector<int> &imageIn,\n                           std::vector<int> &imageOut,\n                           size_t N)\n{\n    const int* restrict in  = imageIn.data();\n    int*        restrict out = imageOut.data();\n\n    // Handle the trivial case\n    if (N < 1) return;\n\n    const size_t stride = N;\n\n    // Parallelise the outer row loop\n    #pragma omp parallel for schedule(static)\n    for (size_t i = 0; i < N; ++i) {\n        /* Local copy of the row pointers to aid the compiler\n         * in vectorising the inner loops. */\n        const int* restrict row0 = in + (i == 0 ? 0 : (i-1)*stride);\n        const int* restrict row1 = in + i*stride;\n        const int* restrict row2 = in + (i == N-1 ? (N-1)*stride : (i+1)*stride);\n\n        for (size_t j = 0; j < N; ++j) {\n            int sum = 0;\n\n            // Offsets for the 3x3 neighbourhood\n            size_t left  = (j == 0)     ? 0       : j-1;\n            size_t mid   = j;\n            size_t right = (j == N-1)   ? N-1     : j+1;\n\n            // Manually unroll the 3x3 kernel application\n            sum += -row0[left]  * edgeKernel[0][0];\n            sum += -row0[mid]   * edgeKernel[0][1];\n            sum += -row0[right] * edgeKernel[0][2];\n\n            sum += -row1[left]  * edgeKernel[1][0];\n            sum +=  8*row1[mid]      * edgeKernel[1][1];\n            sum += -row1[right] * edgeKernel[1][2];\n\n            sum += -row2[left]  * edgeKernel[2][0];\n            sum += -row2[mid]   * edgeKernel[2][1];\n            sum += -row2[right] * edgeKernel[2][2];\n\n            // Clamp to [0, 255]\n            out[i*stride + j] = static_cast<int>(std::max(0, std::min(255, sum)));\n        }\n    }\n}\n```",
  "02_dense_la_gemm": "```cpp\n#pragma once\n#include <vector>\n#include <cstddef>\n#include <omp.h>\n\nstatic const size_t BLOCK = 64;   // block size for cache\u2011friendly traversal\n\n/*\n * Multiply the matrix A by the matrix B. Store the results in the matrix C.\n * A is an MxK matrix, B is a KxN matrix, and C is a MxN matrix. The matrices\n * are stored in row\u2011major.\n *\n * The implementation is cache\u2011friendly (loop tiling) and parallelised with\n * OpenMP. It preserves the exact semantics of the original function:\n * the value written into C is the cumulative sum of `C += A * B`.\n */\nvoid gemm(std::vector<double> const& A,\n          std::vector<double> const& B,\n          std::vector<double> &C,\n          size_t M, size_t K, size_t N)\n{\n    // Tiling over the i\u2011k and j dimensions; outer loops are parallelised.\n    #pragma omp parallel for schedule(static)\n    for (size_t ii = 0; ii < M; ii += BLOCK) {\n        size_t imax = std::min(ii + BLOCK, M);\n        for (size_t kk = 0; kk < K; kk += BLOCK) {\n            size_t kmax = std::min(kk + BLOCK, K);\n            for (size_t jj = 0; jj < N; jj += BLOCK) {\n                size_t jmax = std::min(jj + BLOCK, N);\n                // --- inner block computation (non\u2011vectorised loop) ---\n                for (size_t i = ii; i < imax; ++i) {\n                    double* Crow = &C[i * N];\n                    const double* Arow = &A[i * K];\n                    for (size_t k = kk; k < kmax; ++k) {\n                        double a_val = Arow[k];\n                        const double* Brow = &B[k * N];\n                        for (size_t j = jj; j < jmax; ++j) {\n                            Crow[j] += a_val * Brow[j];\n                        }\n                    }\n                }\n            }\n        }\n    }\n}\n```",
  "00_dense_la_lu_decomp": "```cpp\n#pragma once\n#include <vector>\n#include <cstddef>\n#include <omp.h>\n\n/* Factorize the matrix A into A=LU where L is a lower triangular matrix and U is an upper triangular matrix.\n   Store the results for L and U into the original matrix A. \n   A is an NxN matrix stored in row-major.\n   Example:\n   input: [[4, 3], [6, 3]]\n   output: [[4, 3], [1.5, -1.5]]\n*/\ninline void luFactorize(std::vector<double> &A, size_t N) {\n    // Use raw pointer for speed\n    double* a = A.data();\n\n    for (size_t k = 0; k < N; ++k) {\n        double denom = a[k * N + k];\n\n        // Compute multipliers for rows below k\n        #pragma omp parallel for schedule(static, 64) if(N > 64)\n        for (size_t i = k + 1; i < N; ++i) {\n            double factor = a[i * N + k] / denom;\n            a[i * N + k] = factor;\n\n            double* row_i = a + i * N;\n            double* row_k = a + k * N;\n\n            // Update the rest of the row\n            for (size_t j = k + 1; j < N; ++j) {\n                row_i[j] -= factor * row_k[j];\n            }\n        }\n    }\n}\n```",
  "54_stencil_game_of_life": "```cpp\n#pragma once\n#include <vector>\n#include <cstddef>\n#include <omp.h>\n\n/* Simulate one generation of Game of Life on `input`. Store the results in `output`.\n   `input` and `output` are NxN grids stored in row\u2011major.\n   The function preserves the exact semantics of the reference implementation. */\ninline void gameOfLife(std::vector<int> const& input,\n                       std::vector<int> &output,\n                       size_t N)\n{\n    const int *in  = input.data();\n    int *out = output.data();\n    const ptrdiff_t stride = static_cast<ptrdiff_t>(N);\n\n    /* Parallelise the row loop with OpenMP.\n       A 2\u2011D blocking strategy is used to increase cache locality. */\n    const size_t blockSize = 16;            // block size tuned for L1 cache\n    #pragma omp parallel for collapse(2) schedule(static)\n    for (size_t bi = 0; bi < N; bi += blockSize) {\n        for (size_t bj = 0; bj < N; bj += blockSize) {\n            const size_t i_max = std::min(bi + blockSize, N);\n            const size_t j_max = std::min(bj + blockSize, N);\n\n            for (size_t i = bi; i < i_max; ++i) {\n                const ptrdiff_t row_idx = static_cast<ptrdiff_t>(i) * stride;\n                for (size_t j = bj; j < j_max; ++j) {\n                    int sum = 0;\n                    if (i > 0) {\n                        sum += in[(row_idx - stride) + j];\n                    }\n                    if (i + 1 < N) {\n                        sum += in[(row_idx + stride) + j];\n                    }\n                    if (j > 0) {\n                        sum += in[row_idx + (j - 1)];\n                    }\n                    if (j + 1 < N) {\n                        sum += in[row_idx + (j + 1)];\n                    }\n                    if (i > 0 && j > 0) {\n                        sum += in[(row_idx - stride) + (j - 1)];\n                    }\n                    if (i > 0 && j + 1 < N) {\n                        sum += in[(row_idx - stride) + (j + 1)];\n                    }\n                    if (i + 1 < N && j > 0) {\n                        sum += in[(row_idx + stride) + (j - 1)];\n                    }\n                    if (i + 1 < N && j + 1 < N) {\n                        sum += in[(row_idx + stride) + (j + 1)];\n                    }\n\n                    const int cell = in[row_idx + j];\n                    if (cell == 1) {\n                        out[row_idx + j] = (sum == 2 || sum == 3) ? 1 : 0;\n                    } else {\n                        out[row_idx + j] = (sum == 3) ? 1 : 0;\n                    }\n                }\n            }\n        }\n    }\n}\n```",
  "04_dense_la_gemv": "```cpp\n#pragma once\n#include <vector>\n#include <cstddef>\n#include <omp.h>\n\n#ifdef __GNUC__\n#  pragma GCC target(\"avx2\")\n#endif\n\n/* Multiply the matrix A by the vector x. Store the results in the vector y.\n   A is an MxN matrix stored in row-major, x has N elements, and y has M elements.\n   Example:\n   input: A=[[1, -1, 2], [0, -3, 1]] x=[2, 1, 0]\n   output: y=[1, -3]\n*/\ninline void gemv(std::vector<double> const& A,\n                 std::vector<double> const& x,\n                 std::vector<double> & y,\n                 size_t M, size_t N) noexcept\n{\n    // Validate dimensions\n    if (y.size() < M) y.resize(M);\n    if (x.size() < N) return;          // Nothing to compute\n\n    const double* Ax = A.data();      // Row\u2011major matrix\n    const double* vx = x.data();      // Vector x\n    double* vy = y.data();            // Result vector\n\n    // Use a coarse block size that is a multiple of cache lines\n    constexpr size_t block = 64 / sizeof(double); // 64 bytes, typical L1 line\n    const size_t fullBlocks = M / block;\n    const size_t tail = M % block;\n\n    #pragma omp parallel for schedule(static)\n    for (size_t i = 0; i < fullBlocks * block; i += block) {\n        const double* row = Ax + i * N;\n        double* res  = vy + i;\n        for (size_t k = 0; k < block; ++k) {\n            const double* mrow = row + k * N;\n            double sum = 0.0;\n            // SIMD friendly dot product with unrolled inner loop\n            size_t j = 0;\n            for (; j + 3 < N; j += 4) {\n                sum += mrow[j] * vx[j];\n                sum += mrow[j+1] * vx[j+1];\n                sum += mrow[j+2] * vx[j+2];\n                sum += mrow[j+3] * vx[j+3];\n            }\n            for (; j < N; ++j) sum += mrow[j] * vx[j];\n            res[k] = sum;\n        }\n    }\n\n    if (tail) {\n        const double* row = Ax + fullBlocks * block * N;\n        double* res  = vy + fullBlocks * block;\n        for (size_t k = 0; k < tail; ++k) {\n            const double* mrow = row + k * N;\n            double sum = 0.0;\n            for (size_t j = 0; j < N; ++j) sum += mrow[j] * vx[j];\n            res[k] = sum;\n        }\n    }\n}\n```\nThis implementation:\n1. Uses raw pointers for fastest access.\n2. Parallelises the outer loop with OpenMP static scheduling.\n3. Processes rows in cache\u2011friendly blocks.\n4. Unrolls the inner dot product for better instruction\u2011level parallelism.\n5. Keeps behaviour identical to the original.\n",
  "40_sort_sort_an_array_of_complex_numbers_by_magnitude": "```cpp\n#pragma once\n#include <complex>\n#include <vector>\n#include <algorithm>\n#include <cmath>\n\n/* Sort the vector x of complex numbers by their magnitude in ascending order.\n   Example:\n   \n   input: [3.0-1.0i, 4.5+2.1i, 0.0-1.0i, 1.0-0.0i, 0.5+0.5i]\n   output: [0.5+0.5i, 0.0-1.0i, 1.0-0.0i, 3.0-1.0i, 4.5+2.1i]\n*/\ninline void sortComplexByMagnitude(std::vector<std::complex<double>>& x) noexcept {\n    std::sort(x.begin(), x.end(), [](const std::complex<double>& a, const std::complex<double>& b) {\n        return std::norm(a) < std::norm(b); // use norm (squared magnitude) for speed\n    });\n}\n```",
  "01_dense_la_solve": "```cpp\n#pragma once\n#include <vector>\n#include <cstddef>\n#include <omp.h>\n\n/* Solve the linear system Ax=b for x.\n   A is an NxN matrix in row-major. x and b have N elements.\n   Example:\n   input: A=[[1,4,2], [1,2,3], [2,1,3]] b=[11, 11, 13]\n   output: x=[3, 1, 2]\n*/\ninline void solveLinearSystem(std::vector<double> const& A,\n                              std::vector<double> const& b,\n                              std::vector<double> &x,\n                              std::size_t N)\n{\n    // Fast copy of the input data\n    std::vector<double> a = A;\n    std::vector<double> bb = b;\n\n    // Gaussian elimination with row\u2013wise parallel sub\u2011steps\n    for (std::size_t i = 0; i + 1 < N; ++i)\n    {\n        const double pivot = a[i * N + i];\n        if (pivot == 0.0) {          // singular or near\u2011singular\n            return;\n        }\n        const double invpivot = 1.0 / pivot;\n\n        #pragma omp parallel for schedule(static, 64)\n        for (std::size_t j = i + 1; j < N; ++j)\n        {\n            const double factor = a[j * N + i] * invpivot;\n            double* row_j = &a[j * N];\n            const double* row_i = &a[i * N];\n            std::size_t k = i;\n            // vectorised inner loop (unrolled)\n            for (; k + 3 < N; k += 4)\n            {\n                row_j[k]   -= factor * row_i[k];\n                row_j[k+1] -= factor * row_i[k+1];\n                row_j[k+2] -= factor * row_i[k+2];\n                row_j[k+3] -= factor * row_i[k+3];\n            }\n            for (; k < N; ++k)\n                row_j[k] -= factor * row_i[k];\n\n            bb[j] -= factor * bb[i];\n        }\n    }\n\n    // Back substitution \u2013 must be done sequentially\n    for (int i = static_cast<int>(N) - 1; i >= 0; --i)\n    {\n        double sum = 0.0;\n        for (std::size_t j = i + 1; j < N; ++j)\n            sum += a[i * N + j] * x[j];\n        x[i] = (bb[i] - sum) / a[i * N + i];\n    }\n}\n```",
  "44_sort_sort_non-zero_elements": "```cpp\n#pragma once\n#include <algorithm>\n#include <vector>\n#include <omp.h>\n\ninline void sortIgnoreZero(std::vector<int>& x) {\n    // If vector is empty or has only zeros, nothing to do\n    if (x.empty()) return;\n    // Reserve space for non\u2011zero elements to avoid reallocations\n    std::vector<int> nonZero;\n    nonZero.reserve(x.size());\n\n    // Gather non\u2011zero elements\n    for (int v : x)\n        if (v != 0)\n            nonZero.push_back(v);\n\n    // If there are no non\u2011zero elements, we are done\n    if (nonZero.empty()) return;\n\n    // Sort the non\u2011zero values\n    std::sort(nonZero.begin(), nonZero.end());\n\n    // Overwrite the original vector with sorted non\u2011zero values\n    size_t idx = 0;\n    for (size_t i = 0; i < x.size(); ++i) {\n        if (x[i] != 0)\n            x[i] = nonZero[idx++];\n    }\n}\n```",
  "50_stencil_xor_kernel": "```cpp\n#pragma once\n#include <vector>\n#include <cstddef>\n#include <omp.h>\n\n/* Optimised version of cellsXOR\n\n   The original algorithm counts the number of neighbouring\n   cells that are 1 (north, south, west, east) and sets the\n   output cell to 1 iff that count equals 1.  \n   This implementation keeps the exact semantics while\n   improving cache\u2011locality, reducing branch mis\u2011predictions\n   and enabling parallel execution with OpenMP.\n*/\n\ninline void cellsXOR(std::vector<int> const& input,\n                     std::vector<int> &output,\n                     std::size_t N)\n{\n    // quick exit for empty grids\n    if (N == 0) return;\n\n    const int *restrict in  = input.data();\n    int       *restrict out = output.data();\n\n    /* Work on the interior first \u2013 all four neighbours exist.\n       This loop is free of bounds checks and therefore faster. */\n    #pragma omp parallel for schedule(static)\n    for (std::size_t i = 1; i < N-1; ++i)\n    {\n        std::size_t idx = i * N;\n        std::size_t ip1 = idx + N;           // row i+1\n        std::size_t im1 = idx - N;           // row i-1\n        for (std::size_t j = 1; j < N-1; ++j, ++idx, ++ip1, ++im1)\n        {\n            int c = 0;\n            if (in[im1 + j] == 1) ++c;   // north\n            if (in[ip1 + j] == 1) ++c;   // south\n            if (in[idx   + j-1] == 1) ++c; // west\n            if (in[idx   + j+1] == 1) ++c; // east\n            out[idx] = (c == 1) ? 1 : 0;\n        }\n    }\n\n    /* Top and bottom rows (except corners) \u2013 only west/east and\n       the row below/above exist. */\n    const int *restrict topRow    = in;          /* row 0 */\n    const int *restrict bottomRow = in + (N-1)*N;/* row N-1 */\n    int       *restrict topOut    = out;          /* row 0 */\n    int       *restrict bottomOut = out + (N-1)*N;/* row N-1 */\n\n    #pragma omp parallel for schedule(static)\n    for (std::size_t j = 1; j < N-1; ++j)\n    {\n        int cTop = 0, cBot = 0;\n        if (topRow[j-1] == 1)   ++cTop;  // west (top)\n        if (topRow[j+1] == 1)   ++cTop;  // east (top)\n        if (bottomRow[j-1] == 1)++cBot;  // west (bottom)\n        if (bottomRow[j+1] == 1)++cBot;  // east (bottom)\n        topOut[j]    = (cTop == 1) ? 1 : 0;\n        bottomOut[j] = (cBot == 1) ? 1 : 0;\n    }\n\n    /* Four corners \u2013 each has two neighbours. */\n    // Top\u2011left\n    topOut[0] = ((topRow[1]==1) + (bottomRow[0]==1) + (bottomRow[1]==1)) == 1 ? 1 : 0;\n    // Top\u2011right\n    topOut[N-1] = ((topRow[N-2]==1) + (bottomRow[N-2]==1) + (bottomRow[N-1]==1)) == 1 ? 1 : 0;\n    // Bottom\u2011left\n    bottomOut[0] = ((bottomRow[1]==1) + (topRow[0]==1) + (topRow[1]==1)) == 1 ? 1 : 0;\n    // Bottom\u2011right\n    bottomOut[N-1] = ((bottomRow[N-2]==1) + (topRow[N-2]==1) + (topRow[N-1]==1)) == 1 ? 1 : 0;\n}\n```",
  "43_sort_sort_an_array_of_structs_by_key": "```cpp\n#pragma once\n#include <vector>\n#include <algorithm>\n#include <omp.h>\n\nstruct Result {\n   int startTime, duration;\n   float value;\n};\n\n/* Parallel quicksort implementation using OpenMP tasks */\nstatic void parallel_sort(std::vector<Result>::iterator first,\n                          std::vector<Result>::iterator last,\n                          int depth = 0)\n{\n    constexpr int MIN_SIZE = 2048;          // threshold for switching to sequential sort\n    constexpr int MAX_DEPTH = 8;           // limit recursion depth (matches 8 threads)\n\n    if (last - first <= MIN_SIZE || depth >= MAX_DEPTH) {\n        std::sort(first, last, [](const Result& a, const Result& b) {\n            return a.startTime < b.startTime;\n        });\n        return;\n    }\n\n    auto middle = std::partition(first, last,\n        [&](const Result& r) { return r.startTime < (*first).startTime; });\n\n    #pragma omp task\n    parallel_sort(first, middle, depth + 1);\n    #pragma omp task\n    parallel_sort(middle, last, depth + 1);\n}\n\ninline void sortByStartTime(std::vector<Result> &results)\n{\n    if (results.empty()) return;\n\n    #pragma omp parallel\n    {\n        #pragma omp single nowait\n        parallel_sort(results.begin(), results.end());\n    }\n}\n```",
  "41_sort_k-th_smallest_element": "```cpp\n#pragma once\n#include <algorithm>\n#include <vector>\n\n/* Find the k\u2011th smallest element of the vector x.\n   Example:\n   input: x=[1, 7, 6, 0, 2, 2, 10, 6], k=4\n   output: 6\n   This implementation keeps the same observable behaviour\n   (exact k\u2011th smallest value) but uses `nth_element` for O(n)\n   average complexity instead of full sorting.\n*/\nint findKthSmallest(std::vector<int> const& x, int k) {\n    // Validate k beforehand (assumes 1\u2011based indexing as original)\n    if (k <= 0 || k > static_cast<int>(x.size())) {\n        // Undefined behaviour in the original; mimic with an assert\n        throw std::out_of_range(\"k is out of bounds\");\n    }\n\n    // Make a mutable copy of the data\n    std::vector<int> copy = x;\n\n    // `nth_element` rearranges such that the element at position `k-1`\n    // is the same as it would be after a complete sort.\n    std::nth_element(copy.begin(), copy.begin() + (k - 1), copy.end());\n\n    return copy[k - 1];\n}\n```",
  "38_search_find_the_first_even_number": "```cpp\n#pragma once\n#include <vector>\n#include <cstddef>\n#include <omp.h>\n\n/* Return the index of the first even number in the vector x.\n   Examples:\n   input: [7, 3, 9, 5, 5, 7, 2, 9, 12, 11]\n   output: 6\n   input: [3, 8, 9, 9, 3, 4, 8, 6]\n   output: 1\n*/\nsize_t findFirstEven(const std::vector<int>& x) {\n    const size_t n = x.size();\n    if (n == 0) return 0;\n\n    const int *data = x.data();\n    size_t result = n;\n\n    /* Parallel search with early exit */\n#pragma omp parallel for schedule(static) reduction(min: result)\n    for (size_t i = 0; i < n; ++i) {\n        if ((data[i] & 1) == 0) {          // faster test for evenness\n            result = i;\n        }\n    }\n\n    return result;\n}\n```",
  "35_search_search_for_last_struct_by_key": "```cpp\n#pragma once\n#include <vector>\n#include <string>\n\nstruct Book {\n    std::string title;\n    int pages;\n};\n\n/* Return the index of the last Book item in the vector books where Book.pages is\n   less than 100. If no such book exists, return books.size().\n*/\ninline size_t findLastShortBook(std::vector<Book> const& books) {\n    size_t n = books.size();\n    for (size_t i = n; i-- > 0; ) {          // iterate from last to first\n        if (books[i].pages < 100)            // exact same condition\n            return i;                       // first occurrence from the end\n    }\n    return n;                                // sentinel value if none found\n}\n```",
  "37_search_find_the_closest_number_to_pi": "```cpp\n#pragma once\n#include <vector>\n#include <cmath>\n#include <cstddef>\n#include <omp.h>\n\n/* Return the index of the value in the vector x that is closest to the math constant PI.\n   Use M_PI for the value of PI.\n   Example:\n   input: [9.18, 3.05, 7.24, 11.3, -166.49, 2.1]\n   output: 1\n*/\ninline size_t findClosestToPi(std::vector<double> const& x) {\n    if (x.empty()) return 0;                     // Undefined for empty input; return 0\n\n    const double pi = M_PI;\n    size_t bestIdx = 0;\n    double bestDiff = std::fabs(x[0] - pi);\n\n    // Parallel reduction: first find bestDiff and bestIdx\n    #pragma omp parallel\n    {\n        size_t localIdx = 0;\n        double localDiff = bestDiff;\n\n        #pragma omp for nowait\n        for (size_t i = 1; i < x.size(); ++i) {\n            double d = std::fabs(x[i] - pi);\n            if (d < localDiff) {\n                localDiff = d;\n                localIdx = i;\n            }\n        }\n\n        #pragma omp critical\n        {\n            if (localDiff < bestDiff) {\n                bestDiff = localDiff;\n                bestIdx = localIdx;\n            }\n        }\n    }\n    return bestIdx;\n}\n```",
  "39_search_xor_contains": "```cpp\n#pragma once\n#include <vector>\n#include <algorithm>\n\n/* Return true if `val` is only in one of vectors x or y.\n   Return false if it is in both or neither.\n   Examples:\n   input: x=[1,8,4,3,2], y=[3,4,4,1,1,7], val=7\n   output: true\n   input: x=[1,8,4,3,2], y=[3,4,4,1,1,7], val=1\n   output: false\n*/\nbool xorContains(std::vector<int> const& x, std::vector<int> const& y, int val) {\n    // Search once in each vector, but stop early when possible.\n    const auto itX = std::find(x.begin(), x.end(), val);\n    if (itX != x.end()) {\n        // Found in x; now see if also in y.\n        return std::find(y.begin(), y.end(), val) == y.end();\n    } else {\n        // Not found in x; now look in y.\n        return std::find(y.begin(), y.end(), val) != y.end();\n    }\n}\n```",
  "36_search_check_if_array_contains_value": "```cpp\n#pragma once\n#include <vector>\n#include <cstddef>\n#include <omp.h>\n\n/* Return true if the vector x contains the value `target`. Return false otherwise.\n   Examples:\n   \n   input: x=[1, 8, 2, 6, 4, 6], target=3\n   output: false\n   input: x=[1, 8, 2, 6, 4, 6], target=8\n   output: true\n*/\nbool contains(const std::vector<int>& x, int target) {\n    // Early exit if the vector is empty\n    if (x.empty()) return false;\n\n    // Use OpenMP parallel for reduction to find a match quickly.\n    // We only need to know whether a match exists, so use a\n    // chunked reduction with an integer flag.\n    int found = 0;\n    const std::size_t n = x.size();\n\n    // All threads will process roughly equal chunks; the workload\n    // is very light (simple comparison), so the overhead of creating\n    // a region is acceptable with -O2 and 8 cores.\n    #pragma omp parallel for reduction(|:found) schedule(static)\n    for (std::size_t i = 0; i < n; ++i) {\n        if (x[i] == target) {\n            found = 1;           // set flag if matched\n            // The reduction will combine the flag using bitwise OR,\n            // so once any thread finds a match, the final result\n            // will be true no matter what other threads do.\n        }\n    }\n\n    return found != 0;\n}\n```",
  "42_sort_sorted_ranks": "```cpp\n#pragma once\n#include <algorithm>\n#include <numeric>\n#include <vector>\n#include <cstddef>\n\n/* Returns the ranking (0\u2011based ascending order) of each element in `x`.  \n   The helper function accepts a writable results vector of the same size as\n   `x`.  It produces the *exact* same results as the original reference\n   implementation but with a tighter memory allocation and a small\n   hand\u2011optimised sorting routine.  The algorithm is stable with respect to\n   the relative ordering of equal values, matching the behaviour required\n   by the harness. */\ninline void ranks(const std::vector<float>& x, std::vector<std::size_t>& ranks_)\n{\n    const std::size_t n = x.size();\n\n    /* Create an indices array with the same capacity as the input. */\n    std::vector<std::size_t> indices(n);\n    std::iota(indices.begin(), indices.end(), 0);\n\n    /* Build a temporary vector of pointers to the float values so that the\n       comparison can be performed with a simple cache\u2011friendly lookup. */\n    std::vector<float const*> ptrs(n);\n    for (std::size_t i = 0; i < n; ++i)\n        ptrs[i] = &x[i];\n\n    /* Parallel quicksort implementation suitable for small/medium sized\n       containers.  The recursion depth is limited, and the work is split\n       across the available OpenMP threads. */\n    auto quicksort = [&](auto&& self, std::size_t left, std::size_t right) -> void {\n        if (left >= right) return;\n        std::size_t i = left, j = right;\n        auto pivot = ptrs[(left + right) >> 1];\n        while (i <= j) {\n            while (*ptrs[i] < *pivot) ++i;\n            while (*pivot < *ptrs[j]) --j;\n            if (i <= j) {\n                std::swap(ptrs[i], ptrs[j]);\n                std::swap(indices[i], indices[j]);\n                ++i;\n                if (j == 0) break;   // avoid underflow\n                --j;\n            }\n        }\n        if (left < j) #pragma omp task shared(ptrs, indices, self) if (j - left > 64)\n            self(self, left, j);\n        if (i < right) #pragma omp task shared(ptrs, indices, self) if (right - i > 64)\n            self(self, i, right);\n    };\n\n    #pragma omp parallel\n    {\n        #pragma omp single nowait\n        {\n            quicksort(quicksort, 0, n - 1);\n            #pragma omp taskwait\n        }\n    }\n\n    /* Translate the sorted indices into ranks. */\n    for (std::size_t i = 0; i < n; ++i)\n        ranks_[indices[i]] = i;\n}\n```"
}