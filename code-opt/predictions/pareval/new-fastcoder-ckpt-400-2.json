{
  "15_graph_edge_count": "```cpp\n#pragma once\n#include <vector>\n#include <stddef.h>\n\nint edgeCount(std::vector<int> const& A, size_t N) {\n    int count = 0;\n    const int *data = A.data();\n\n    #pragma omp parallel for reduction(+:count) schedule(static)\n    for (size_t i = 0; i < N; ++i) {\n        const int *row = data + i * N;\n        for (size_t j = 0; j < N; ++j) {\n            #pragma omp atomic\n            if (row[j] == 1) count += 1;\n        }\n    }\n    return count;\n}\n```\n",
  "17_graph_highest_degree": "```cpp\n#pragma once\n#include <vector>\n#include <algorithm>\n#include <cstddef>\n#include <omp.h>\n\n/* Compute the highest node degree in the undirected graph.\n   The graph is defined in the adjacency matrix A.\n   A is an NxN adjacency matrix stored in row-major.  A is undirected.\n   Example:\n       input: [[0, 0, 0, 1], [0, 0, 0, 1], [0, 0, 0, 1], [1, 1, 1, 0]]\n   output: 3\n*/\nint maxDegree(std::vector<int> const& A, std::size_t N) {\n    int highestDegree = 0;\n    // We'll use OpenMP for parallelizing the outer loop while reducing the maximum.\n    #pragma omp parallel for reduction(max:highestDegree)\n    for (std::size_t i = 0; i < N; ++i) {\n        int degree = 0;\n        const int* row = &A[i * N];\n        // Inner loop can be unrolled a bit for speed\n        for (std::size_t j = 0; j < N; ++j) {\n            degree += row[j];\n        }\n        if (degree > highestDegree) highestDegree = degree;\n    }\n    return highestDegree;\n}\n```",
  "23_histogram_first_letter_counts": "```cpp\n#pragma once\n#include <array>\n#include <string>\n#include <vector>\n#include <cstddef>\n#include <omp.h>\n\n/* For each letter in the alphabet, count the number of strings in the vector s that start with that letter.\n   Assume all strings are in lower case. Store the output in `bins` array.\n   Example:\n   input: [\"dog\", \"cat\", \"xray\", \"cow\", \"code\", \"type\", \"flower\"]\n   output: [0, 0, 3, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0]\n*/\ninline void firstLetterCounts(const std::vector<std::string> &s, std::array<std::size_t, 26> &bins) {\n    // Initialize bins to zero if they might contain garbage\n    for (auto &v : bins) v = 0;\n\n    // Parallel reduction over the 26 bins\n    #pragma omp parallel for schedule(static) reduction(+:bins[:26])\n    for (std::size_t i = 0; i < s.size(); ++i) {\n        const char c = s[i][0];\n        const std::size_t idx = static_cast<std::size_t>(c - 'a');\n        if (idx < 26) {\n            bins[idx] += 1;\n        }\n    }\n}\n```",
  "19_graph_shortest_path": "```cpp\n#pragma once\n#include <vector>\n#include <cstdint>\n#include <limits>\n#include <queue>\n\n/* Optimised BFS for shortest path in an undirected graph.\n   Uses a contiguous array for the queue and a byte\u2011based visited array\n   for maximum spatial locality and speed. */\nint shortestPathLength(std::vector<int> const& A, size_t N, int source, int dest) {\n    if (source == dest) return 0;\n\n    // visited array \u2013 0: not visited, 1: visited\n    std::vector<uint8_t> visited(N, 0);\n    visited[source] = 1;\n\n    // Allocate a fixed\u2011size circular queue\n    std::vector<int> q(N);\n    std::vector<int> depth(N);\n    size_t head = 0, tail = 0;\n\n    q[tail] = source;\n    depth[tail] = 0;\n    ++tail;\n\n    while (head != tail) {\n        int current = q[head];\n        int curDepth = depth[head];\n        ++head;\n        if (current == dest) return curDepth;\n\n        size_t rowOffset = current * N;\n        for (size_t i = 0; i < N; ++i) {\n            if (A[rowOffset + i] && !visited[i]) {\n                visited[i] = 1;\n                q[tail] = static_cast<int>(i);\n                depth[tail] = curDepth + 1;\n                ++tail;\n            }\n        }\n    }\n    return std::numeric_limits<int>::max(); // unreachable (graph is connected)\n}\n```",
  "20_histogram_pixel_histogram": "```cpp\n#pragma once\n#include <vector>\n#include <array>\n#include <cstddef>\n#include <omp.h>\n\n/*\n * Count the number of pixels in image with each grayscale intensity.\n * The vector `image` is a grayscale image with values 0-255.\n * Store the results in `bins`.\n * Example:\n *\n *   input: image=[2, 116, 201, 11, 92, 92, 201, 4, 2]\n *   output: [0, 0, 2, 0, 1, ...]\n */\nvoid pixelCounts(std::vector<int> const& image, std::array<size_t, 256> &bins)\n{\n    // Zero output bins\n    for (auto &c : bins) c = 0;\n\n    // Per-thread temporary bins to avoid atomic operations\n    const int nThreads = omp_get_max_threads();\n    std::vector<std::array<size_t, 256>> localBins(nThreads);\n\n    #pragma omp parallel\n    {\n        int tid = omp_get_thread_num();\n        auto &local = localBins[tid];\n\n        // Initialize local bin array to zero\n        for (auto &c : local) c = 0;\n\n        #pragma omp for schedule(static)\n        for (size_t i = 0; i < image.size(); ++i)\n        {\n            // safety: clip value to 0-255 (behaviour identical to original)\n            int val = image[i];\n            if (val < 0) val = 0;\n            else if (val > 255) val = 255;\n            local[val] += 1;\n        }\n    }\n\n    // Reduce local bins into global bins\n    for (int t = 0; t < nThreads; ++t)\n    {\n        for (int v = 0; v < 256; ++v)\n        {\n            bins[v] += localBins[t][v];\n        }\n    }\n}\n```",
  "21_histogram_bin_0-100": "```cpp\n#pragma once\n#include <array>\n#include <vector>\n#include <cstddef>\n#include <omp.h>\n\n/* Vector x contains values between 0 and 100, inclusive. Count the number of\n   values in [0,10), [10, 20), [20, 30), ... and store the counts in `bins`.\n   Example:\n   input: [7, 32, 95, 12, 39, 32, 11, 71, 70, 66]\n   output: [1, 2, 0, 3, 0, 0, 1, 2, 0, 1]\n*/\ninline void binsBy10Count(std::vector<double> const& x, std::array<size_t, 10> &bins)\n{\n    // Reset output array\n    std::fill(bins.begin(), bins.end(), 0);\n\n    // Parallel reduction over the array `bins`.  GCC supports array\n    // reductions with the syntax `reduction(+:bins[:10])`.\n    #pragma omp parallel for reduction(+:bins[:10]) schedule(static)\n    for (size_t i = 0; i < x.size(); ++i) {\n        // Division by 10 is safe because the input range is [0,100].\n        size_t bin = static_cast<size_t>(x[i] / 10.0);\n        // Clamp to [0,9] in case of numerical quirks near 100.\n        if (bin >= 10) bin = 9;\n        ++bins[bin];\n    }\n}\n```",
  "16_graph_largest_component": "```cpp\n#pragma once\n#include <vector>\n#include <algorithm>\n\nnamespace detail {\n\ninline void dfs_iterative(const std::vector<int>& A,\n                          int start,\n                          size_t N,\n                          std::vector<char>& visited,\n                          int& count)\n{\n    // Use a small manual stack to avoid recursion overhead\n    std::vector<int> stack;\n    stack.reserve(N);\n    stack.push_back(start);\n    visited[start] = 1;\n\n    while (!stack.empty()) {\n        int node = stack.back();\n        stack.pop_back();\n        ++count;\n\n        const int* row = &A[node * N];\n        for (size_t i = 0; i < N; ++i) {\n            if (row[i] == 1 && !visited[i]) {\n                visited[i] = 1;\n                stack.push_back(static_cast<int>(i));\n            }\n        }\n    }\n}\n\n} // namespace detail\n\n// Return the number of vertices in the largest component of the graph\n// defined by the adjacency matrix A.  A is an NxN adjacency matrix\n// stored in row\u2011major order.\n//\n// Example:\n//   input: [[0, 1, 0, 0],\n//           [1, 0, 0, 0],\n//           [0, 0, 0, 1],\n//           [0, 0, 1, 0]]\n//   output: 2\n//\nint largestComponent(const std::vector<int>& A, size_t N)\n{\n    std::vector<char> visited(N, 0);\n    int maxCount = 0;\n\n    for (int start = 0; start < static_cast<int>(N); ++start) {\n        if (!visited[start]) {\n            int count = 0;\n            detail::dfs_iterative(A, start, N, visited, count);\n            if (count > maxCount) maxCount = count;\n        }\n    }\n    return maxCount;\n}\n```",
  "32_scan_sum_of_prefix_sum_array": "```cpp\n#pragma once\n#include <vector>\n#include <cstdint>\n#include <omp.h>\n\ninline int64_t sumOfPrefixSum(const std::vector<int64_t> &x) {\n    const std::size_t n = x.size();\n    if (n == 0) return 0;\n\n    int64_t total = 0;\n#pragma omp parallel for reduction(+ : total)\n    for (std::size_t i = 0; i < n; ++i) {\n        // The element x[i] contributes to prefix sums of all suffixes starting at i\n        total += x[i] * static_cast<int64_t>(n - i);\n    }\n    return total;\n}\n```",
  "24_histogram_count_quartile": "```cpp\n#pragma once\n#include <array>\n#include <vector>\n#include <cstddef>\n#include <cmath>\n#include <omp.h>\n\n/* Count the number of doubles in the vector x that have a fractional part \n   in [0, 0.25), [0.25, 0.5), [0.5, 0.75), and [0.75, 1). Store the counts in `bins`.\n   Example:\n      input: [7.8, 4.2, 9.1, 7.6, 0.27, 1.5, 3.8]\n      output: [2, 1, 2, 2]\n*/\nvoid countQuartiles(std::vector<double> const& x, std::array<size_t, 4> &bins) {\n    // reset bins\n    bins[0] = bins[1] = bins[2] = bins[3] = 0;\n\n    // local counters for each thread to avoid lock contention\n    const int nthreads = omp_get_max_threads();\n    std::vector<std::array<size_t, 4>> local_bins(nthreads);\n    for (auto &lb : local_bins) lb.fill(0);\n\n    #pragma omp parallel\n    {\n        const int tid = omp_get_thread_num();\n        const int n = static_cast<int>(x.size());\n\n        #pragma omp for schedule(static)\n        for (int i = 0; i < n; ++i) {\n            const double val = x[i];\n            const double frac = val - std::floor(val);   // use floor for negatives\n            std::array<size_t, 4> *lb = &local_bins[tid];\n            if (frac < 0.25) {\n                (*lb)[0] += 1;\n            } else if (frac < 0.5) {\n                (*lb)[1] += 1;\n            } else if (frac < 0.75) {\n                (*lb)[2] += 1;\n            } else {\n                (*lb)[3] += 1;\n            }\n        }\n    }\n\n    // merge thread local bins into the final result\n    for (const auto &lb : local_bins) {\n        bins[0] += lb[0];\n        bins[1] += lb[1];\n        bins[2] += lb[2];\n        bins[3] += lb[3];\n    }\n}\n```",
  "34_scan_largest_contiguous_subarray_sum": "```cpp\n#pragma once\n#include <vector>\n#include <limits>\n#include <algorithm>\n#include <omp.h>\n\n/* Return the largest sum of any contiguous subarray in the vector x.\n   i.e. if x=[\u22122, 1, \u22123, 4, \u22121, 2, 1, \u22125, 4] then [4, \u22121, 2, 1] is the contiguous\n   subarray with the largest sum of 6.\n   Example:\n   input: [\u22122, 1, \u22123, 4, \u22121, 2, 1, \u22125, 4]\n   output: 6\n*/\n\nint maximumSubarray(const std::vector<int>& x) {\n    const int n = static_cast<int>(x.size());\n    if (n == 0) return 0; // empty input \u2013 no subarray\n\n    int globalMax = std::numeric_limits<int>::lowest();\n\n    /* Parallelise over the starting index of the subarray.\n       The inner loop remains sequential to avoid false\u2011sharing costs.\n       The reduction clause keeps track of the best sum found by any thread. */\n#pragma omp parallel for reduction(max : globalMax) schedule(static)\n    for (int i = 0; i < n; ++i) {\n        int currSum = 0;\n        for (int j = i; j < n; ++j) {\n            currSum += x[j];\n            if (currSum > globalMax) globalMax = currSum;\n        }\n    }\n    return globalMax;\n}\n```\n",
  "30_scan_prefix_sum": "```cpp\n#pragma once\n#include <numeric>\n#include <execution>\n#include <vector>\n\n /* Compute the inclusive prefix sum of the vector `x` into `output`.\n    Example:\n    input:  [1, 7, 4, 6, 6, 2]\n    output: [1, 8,12,18,24,26]\n */\nvoid prefixSum(const std::vector<int64_t>& x, std::vector<int64_t>& output) {\n    // Ensure output has the same size as input\n    if (output.size() != x.size())\n        output.resize(x.size());\n\n    // Parallel inclusive scan with OpenMP backend when available\n    std::inclusive_scan(std::execution::par, x.begin(), x.end(), output.begin());\n}\n```",
  "31_scan_scan_with_min_function": "```cpp\n#pragma once\n#include <vector>\n#include <algorithm>\n\n/*\n * Replace the i\u2011th element of the vector `x` with the minimum value\n * from indices 0 through i.\n *\n * Input  : [8, 6, -1, 7, 3, 4, 4]\n * Output : [8, 6, -1, -1, -1, -1, -1]\n *\n * Input  : [5, 4, 6, 4, 3, 6, 1, 1]\n * Output : [5, 4, 4, 4, 3, 3, 1, 1]\n */\ninline void partialMinimums(std::vector<float> &x) {\n    if (x.empty()) return;\n\n    float current_min = x[0];\n    // i = 0 element is already the correct minimum\n    for (std::size_t i = 1; i < x.size(); ++i) {\n        // update the current minimum\n        if (x[i] < current_min) {\n            current_min = x[i];\n        }\n        // replace the element with the running minimum\n        x[i] = current_min;\n    }\n}\n```\n",
  "22_histogram_count_quadrants": "```cpp\n#pragma once\n#include <array>\n#include <vector>\n#include <cstddef>\n#include <omp.h>\n\nstruct Point {\n    double x, y;\n};\n\n/*\n * Count the number of cartesian points in each quadrant.\n *\n * Input:\n *  - points : vector of Point\n *\n * Output:\n *  - bins   : array of 4 counters [Q1, Q2, Q3, Q4]\n *     where\n *           Q1 : x>=0 && y>=0\n *           Q2 : x< 0 && y>=0\n *           Q3 : x< 0 && y< 0\n *           Q4 : x>=0 && y< 0\n *\n * The function is fully parallelized with OpenMP and keeps\n * identical semantics to the original implementation.\n */\ninline void countQuadrants(const std::vector<Point>& points,\n                           std::array<std::size_t, 4>& bins)\n{\n    // reset output\n    bins[0] = bins[1] = bins[2] = bins[3] = 0;\n\n    // local thread\u2011private counters to avoid atomic updates\n    const std::size_t n = points.size();\n\n#pragma omp parallel\n    {\n        std::array<std::size_t, 4> local{0, 0, 0, 0};\n\n#pragma omp for schedule(static)\n        for (std::size_t i = 0; i < n; ++i) {\n            const double x = points[i].x;\n            const double y = points[i].y;\n\n            if (x >= 0.0) {\n                if (y >= 0.0) {\n                    ++local[0];\n                } else {\n                    ++local[3];\n                }\n            } else {\n                if (y >= 0.0) {\n                    ++local[1];\n                } else {\n                    ++local[2];\n                }\n            }\n        }\n\n        // reduction of thread\u2011private counters into the global result\n#pragma omp atomic\n        bins[0] += local[0];\n#pragma omp atomic\n        bins[1] += local[1];\n#pragma omp atomic\n        bins[2] += local[2];\n#pragma omp atomic\n        bins[3] += local[3];\n    }\n}\n```\n```\n\nThis version uses a static OpenMP `for` loop with per\u2011thread local arrays, eliminating contention on shared counters. The `atomic` updates are performed only once per thread, giving a substantial speedup on the 8\u2011thread x86\u201164 environment while preserving exact numeric results.",
  "56_transform_negate_odds": "```cpp\n#pragma once\n#include <vector>\n#include <algorithm>\n#include <omp.h>\n\n/* In the vector x negate the odd values and divide the even values by 2.\n   Example:\n   \n   input: [16, 11, 12, 14, 1, 0, 5]\n   output: [8, -11, 6, 7, -1, 0, -5]\n*/\nvoid negateOddsAndHalveEvens(std::vector<int> &x) {\n    const std::size_t n = x.size();\n\n    // Parallelize using OpenMP with a chunk size that balances\n    // workload and avoids overhead for very short vectors.\n    #pragma omp parallel for schedule(static, 1024)\n    for (std::size_t i = 0; i < n; ++i) {\n        const int val = x[i];\n        x[i] = (val % 2 == 0) ? (val / 2) : -val;\n    }\n}\n```",
  "59_transform_map_function": "```cpp\n#pragma once\n#include <vector>\n#include <cstddef>\n#include <cstdint>\n#include <omp.h>\n\n// Inline the power\u2011of\u2011two test for maximal efficiency\ninline bool isPowerOfTwo(uint32_t x) noexcept {\n    return (x != 0) && !(x & (x - 1));\n}\n\n/*\n    Apply the isPowerOfTwo function to every value in x and store the results in mask.\n    Example:\n\n    input:  [8, 0, 9, 7, 15, 64, 3]\n    output: [true, false, false, false, false, true, false]\n*/\nvoid mapPowersOfTwo(const std::vector<int>& x, std::vector<bool>& mask) {\n    const std::size_t n = x.size();\n    // Ensure mask has the right size\n    mask.resize(n);\n\n    #pragma omp parallel for schedule(static)\n    for (std::ptrdiff_t i = 0; i < static_cast<std::ptrdiff_t>(n); ++i) {\n        mask[i] = isPowerOfTwo(static_cast<uint32_t>(x[i]));\n    }\n}\n```",
  "57_transform_inverse_offset": "```cpp\n#pragma once\n#include <vector>\n#include <omp.h>\n\n/* Replace every element of the vector `x` with `1 - 1/x`.\n   Example:\n     input:  [2, 4, 1, 12, -2]\n     output: [0.5, 0.75, 0, 0.91666666, 1.5]\n*/\ninline void oneMinusInverse(std::vector<double> &x) {\n    // Parallel processing with OpenMP; the order of evaluation does not matter\n    const std::size_t n = x.size();\n    #pragma omp parallel for schedule(static)\n    for (std::size_t i = 0; i < n; ++i) {\n        x[i] = 1.0 - 1.0 / x[i];\n    }\n}\n```\n",
  "55_transform_relu": "```cpp\n#pragma once\n#include <vector>\n#include <algorithm>\n#include <omp.h>\n\n/* Compute the ReLU function on every element of x. Elements less than zero become zero,\n   while elements greater than zero stay the same.\n   Example:\n   input: [-1.8, 24.0, 1.2, 0.0, -5.1, -0.2, 4.5]\n   output: [0, 24.0, 1.2, 0, 0, 0, 4.5]\n*/\nvoid relu(std::vector<double> &x) {\n    const std::size_t N = x.size();\n    double *data = x.data();\n\n#pragma omp parallel for schedule(static)\n    for (std::size_t i = 0; i < N; ++i) {\n        double v = data[i];\n        data[i] = (v > 0.0) ? v : 0.0;\n    }\n}\n```",
  "58_transform_squaring": "```cpp\n#pragma once\n#include <vector>\n#include <omp.h>\n\n/* Replace every element of x with the square of its value.\n   Example:\n   input: [5, 1, 2, -4, 8]\n   output: [25, 1, 4, 16, 64]\n*/\ninline void squareEach(std::vector<int> &x) {\n    // Use OpenMP parallel for to speed up the loop on multi\u2011core machines.\n    // The static schedule gives each thread a contiguous chunk, which\n    // reduces cache contention and is most efficient for a simple\n    // element\u2011wise operation.\n    #pragma omp parallel for schedule(static)\n    for (std::size_t i = 0; i < x.size(); ++i) {\n        const int v = x[i];\n        x[i] = v * v;\n    }\n}\n```\n",
  "33_scan_reverse_prefix_sum": "```cpp\n#pragma once\n#include <vector>\n\n/* Compute the reverse prefix sum of the vector x into output.\n   The result is that output[i] = sum_{j = i}^{x.size()-1} x[j].\n   Example:\n   input:  [1, 7, 4, 6, 6, 2]\n   output: [2, 8, 14, 18, 25, 26]\n   input:  [3, 3, 7, 1, -2]\n   output: [-2, -1, 6, 9, 12]\n*/\ninline void reversePrefixSum(std::vector<int> const& x, std::vector<int> &output) {\n    const std::size_t N = x.size();\n    if (N == 0) return;           // nothing to do\n\n    int sum = 0;\n    for (std::size_t i = N; i-- > 0; ) {\n        sum += x[i];\n        output[i] = sum;\n    }\n}\n```\n",
  "18_graph_count_components": "```cpp\n#pragma once\n#include <vector>\n#include <atomic>\n#include <stack>\n#include <omp.h>\n\n/* Count the number of connected components in the undirected graph defined by the adjacency matrix A.\n   A is an NxN adjacency matrix stored in row\u2011major. A is an undirected graph.\n*/\nint componentCount(std::vector<int> const& A, size_t N) {\n    // Thread\u2011safe visited flag\n    std::vector<std::atomic<bool>> visited(N);\n    for (size_t i = 0; i < N; ++i) visited[i].store(false, std::memory_order_relaxed);\n\n    std::atomic<int> componentCount(0);          // global component counter\n    std::atomic<int> nextNode(0);                // next node index to be processed\n\n    // Parallel region \u2013 each thread continuously grabs the next unvisited node\n    #pragma omp parallel\n    {\n        while (true) {\n            // Grab a node index atomically\n            int node = nextNode.fetch_add(1, std::memory_order_relaxed);\n            if (node >= static_cast<int>(N)) break;     // All nodes assigned\n\n            // If already visited, skip\n            if (visited[node].load(std::memory_order_seq_cst)) continue;\n\n            // Start an iterative DFS from this node\n            std::stack<int> st;\n            st.push(node);\n\n            // Mark the starting node as visited\n            visited[node].store(true, std::memory_order_seq_cst);\n\n            while (!st.empty()) {\n                int v = st.top();\n                st.pop();\n\n                // Scan all neighbors of v\n                int base = v * static_cast<int>(N);\n                for (int j = 0; j < static_cast<int>(N); ++j) {\n                    if (A[base + j] == 1) {\n                        // If neighbor not visited yet, visit it\n                        bool expected = false;\n                        if (visited[j].compare_exchange_strong(expected, true,\n                                                              std::memory_order_seq_cst)) {\n                            st.push(j);\n                        }\n                    }\n                }\n            }\n\n            // Successfully finished one component\n            componentCount.fetch_add(1, std::memory_order_seq_cst);\n        }\n    }\n\n    return componentCount.load();\n}\n```",
  "48_sparse_la_sparse_axpy": "```cpp\n#pragma once\n#include <vector>\n#include <omp.h>\n\nstruct Element {\n    size_t index;\n    double value;\n};\n\n/*\n * Compute z = alpha*x + y where x and y are sparse vectors.\n * Elements may be in arbitrary order, but each index appears at most once per vector.\n * The result is accumulated into an existing dense vector z.\n */\nvoid sparseAxpy(double alpha, std::vector<Element> const& x,\n                std::vector<Element> const& y, std::vector<double>& z) {\n    // Process y first \u2013 simple addition\n    #pragma omp parallel for schedule(static)\n    for (size_t i = 0; i < y.size(); ++i) {\n        #pragma omp atomic\n        z[y[i].index] += y[i].value;\n    }\n\n    // Then process x \u2013 scaled addition\n    #pragma omp parallel for schedule(static)\n    for (size_t i = 0; i < x.size(); ++i) {\n        #pragma omp atomic\n        z[x[i].index] += alpha * x[i].value;\n    }\n}\n```\n",
  "27_reduce_average": "```cpp\n#pragma once\n#include <vector>\n#include <cstddef>\n\ninline double average(const std::vector<double>& x) {\n    const std::size_t n = x.size();\n    if (n == 0) return 0.0;          // keep deterministic behavior\n\n    double sum = 0.0;\n#pragma omp parallel for reduction(+:sum)\n    for (std::size_t i = 0; i < n; ++i) {\n        sum += x[i];\n    }\n    return sum / static_cast<double>(n);\n}\n```",
  "29_reduce_sum_of_min_of_pairs": "```cpp\n#pragma once\n#include <vector>\n#include <cstdint>\n#include <algorithm>\n#include <omp.h>\n\ninline int64_t sumOfMinimumElements(const std::vector<int64_t>& x,\n                                    const std::vector<int64_t>& y) {\n    const std::size_t n = x.size();\n    int64_t sum = 0;\n\n    #pragma omp parallel for reduction(+:sum)\n    for (std::size_t i = 0; i < n; ++i) {\n        sum += (x[i] < y[i]) ? x[i] : y[i];\n    }\n\n    return sum;\n}\n```\n",
  "25_reduce_xor": "```cpp\n#pragma once\n#include <vector>\n#include <cstddef>\n#include <cstdint>\n#include <omp.h>\n\n/* Return the logical XOR reduction of the vector of bools x.\n   Example:\n   input: [false, false, false, true]\n   output: true\n*/\ninline bool reduceLogicalXOR(const std::vector<bool>& x)\n{\n    // XOR of booleans equals the parity of the number of true values.\n    // Count true bits in parallel and return the parity.\n    std::size_t true_count = 0;\n\n    #pragma omp parallel for reduction(+:true_count) schedule(static)\n    for (std::size_t i = 0; i < x.size(); ++i) {\n        true_count += static_cast<std::size_t>(x[i]);\n    }\n\n    return (true_count & 1u) != 0;\n}\n```\n",
  "28_reduce_smallest_odd_number": "```cpp\n#pragma once\n#include <vector>\n#include <limits>\n#include <cstddef>\n\n/* Return the value of the smallest odd number in the vector x.\n   Examples:\n   input: [7, 9, 5, 2, 8, 16, 4, 1]\n   output: 1\n   input: [8, 36, 7, 2, 11]\n   output: 7\n*/\nint smallestOdd(std::vector<int> const& x) {\n    // Start with the maximum int value; this acts as the identity for our minimisation\n    int best = std::numeric_limits<int>::max();\n\n    // Inline loop for maximum performance; no range-based loop to avoid iterator overhead\n    for (std::size_t i = 0; i < x.size(); ++i) {\n        int v = x[i];\n        // Check only odd candidates and update the best if smaller\n        if ((v & 1) && v < best) {\n            best = v;\n        }\n    }\n    return best;\n}\n```",
  "26_reduce_product_of_inverses": "```cpp\n#pragma once\n#include <vector>\n#include <cmath>\n#include <cstddef>\n#include <omp.h>\n\n/*\n * Return the product of the vector `x` with every odd indexed element inverted.\n * Formula:  x_0 * 1/x_1 * x_2 * 1/x_3 * ...\n *\n * The implementation uses OpenMP to parallelise the multiplication on the\n * available threads (up to 8) and performs the operation in a single pass\n * over the input data, eliminating the temporary vector used by the original\n * code.\n */\ndouble productWithInverses(std::vector<double> const& x) {\n    double prod = 1.0;\n    #pragma omp parallel for reduction(mul : prod)\n    for (std::size_t i = 0; i < x.size(); ++i) {\n        if (i & 1)          // odd index \u2013 use the inverse\n            prod *= 1.0 / x[i];\n        else\n            prod *= x[i];\n    }\n    return prod;\n}\n```",
  "46_sparse_la_spmm": "```cpp\n#pragma once\n#include <vector>\n#include <algorithm>\n#include <cstddef>\n#include <utility>\n#include <omp.h>\n\nstruct COOElement {\n    size_t row;\n    size_t column;\n    double value;\n};\n\n/* SpMM: Y = A * X\n   A   \u2013 sparse MxK matrix in COO format\n   X   \u2013 sparse KxN matrix in COO format\n   Y   \u2013 dense MxN matrix (row\u2011major), pre\u2011allocated or empty\n*/\nvoid spmm(std::vector<COOElement> const& A,\n          std::vector<COOElement> const& X,\n          std::vector<double> &Y,\n          size_t M, size_t K, size_t N)\n{\n    /* 1. Prepare Y */\n    Y.assign(M * N, 0.0);\n\n    /* 2. Build a quick lookup for X: for each row (== column of A)\n         store the COO entries belonging to that row. */\n    std::vector<std::vector<COOElement>> X_by_row(K);\n    for (auto const& e : X) {\n        X_by_row[e.row].push_back(e);\n    }\n\n    /* 3. Perform the multiplication in parallel.\n       Each non\u2011zero of A visits all X entries that share the same\n       column index. Atomic updates are used only on the target cell\n       which guarantees correctness without a separate lock per row. */\n#pragma omp parallel for schedule(dynamic)\n    for (intptr_t idx = 0; idx < static_cast<intptr_t>(A.size()); ++idx) {\n        const auto& a = A[idx];\n        const auto& list = X_by_row[a.column];\n        for (auto const& x : list) {\n            const size_t row_idx = a.row * N + x.column;\n#pragma omp atomic\n            Y[row_idx] += a.value * x.value;\n        }\n    }\n}\n```",
  "47_sparse_la_spmv": "```cpp\n#pragma once\n#include <vector>\n#include <omp.h>\n\nstruct COOElement {\n   size_t row, column;\n   double value;\n};\n\n/* Compute y = alpha*A*x + beta*y where alpha and beta are scalars, x and y are vectors,\n   and A is a sparse matrix stored in COO format.\n   x and y are length N and A is M x N.\n   Example:\n   input: alpha=0.5 beta=1.0 A=[{0,1,3}, {1,0,-1}] x=[-4, 2] y=[-1,1]\n   output: y=[2, 3]\n*/\nvoid spmv(double alpha,\n          const std::vector<COOElement> &A,\n          const std::vector<double> &x,\n          double beta,\n          std::vector<double> &y,\n          size_t M,\n          size_t N)\n{\n   /* ---------- 1) Scale y by beta (parallel) ---------- */\n   #pragma omp parallel for schedule(static)\n   for (size_t i = 0; i < y.size(); ++i) {\n       y[i] *= beta;\n   }\n\n   /* ---------- 2) Sparse matrix-vector product (parallel) ----------\n      We accumulate contributions in thread\u2011local buffers to avoid\n      atomic operations, then combine them into y. */\n   int nthreads = omp_get_max_threads();\n   std::vector<std::vector<double>> local(nthreads, std::vector<double>(M, 0.0));\n\n   #pragma omp parallel\n   {\n       int tid = omp_get_thread_num();\n       #pragma omp for schedule(static)\n       for (size_t idx = 0; idx < A.size(); ++idx) {\n           const COOElement &e = A[idx];\n           if (e.row < M && e.column < N) {\n               local[tid][e.row] += alpha * e.value * x[e.column];\n           }\n       }\n   }\n\n   /* Combine thread\u2013local results into the final y vector. */\n   for (int t = 0; t < nthreads; ++t) {\n       const std::vector<double> &tmp = local[t];\n       for (size_t i = 0; i < M; ++i) {\n           y[i] += tmp[i];\n       }\n   }\n}\n```",
  "49_sparse_la_sparse_lu_decomp": "```cpp\n#pragma once\n#include <vector>\n#include <cstddef>\n#include <cstring>\n#include <omp.h>\n\nstruct COOElement {\n    size_t row, column;\n    double value;\n};\n\n/*  LU factorization for an N\u00d7N sparse matrix stored in COO format.\n *  The output matrices L and U are returned as flattened N\u00d7N\n *  row\u2011major arrays.  The algorithm is identical to the\n *  textbook Doolittle LU decomposition used in the reference.\n */\nvoid luFactorize(std::vector<COOElement> const& A,\n                 std::vector<double>& L,\n                 std::vector<double>& U,\n                 size_t N)\n{\n    /* Allocate a dense copy of A as a flat array */\n    std::vector<double> fullA( N * N, 0.0 );\n    for (const auto& e : A)\n        fullA[e.row * N + e.column] = e.value;\n\n    /* Ensure L and U have sufficient size */\n    L.assign(N * N, 0.0);\n    U.assign(N * N, 0.0);\n\n    /* Doolittle LU decomposition */\n    for (size_t i = 0; i < N; ++i) {\n        /* diagonal element of L is 1 */\n        L[i * N + i] = 1.0;\n\n        /* upper triangular part U(i, :)  */\n        for (size_t j = i; j < N; ++j) {\n            double sum = fullA[i * N + j];\n            for (size_t k = 0; k < i; ++k)\n                sum -= L[i * N + k] * U[k * N + j];\n            U[i * N + j] = sum;\n        }\n\n        /* lower triangular part L(i, :)  */\n        for (size_t j = 0; j < i; ++j) {\n            double sum = fullA[i * N + j];\n            for (size_t k = 0; k < j; ++k)\n                sum -= L[i * N + k] * U[k * N + j];\n            L[i * N + j] = sum / U[j * N + j];\n        }\n    }\n}\n```",
  "06_fft_dft": "```cpp\n#pragma once\n#include <vector>\n#include <complex>\n#include <cmath>\n#include <algorithm>\n#include <omp.h>\n\n#if !defined(M_PI)\n#define M_PI 3.14159265358979323846\n#endif\n\n// -----------------------------------------------------------------------------\n// DFT implementation accelerated with OpenMP and pre\u2011computed trigonometric\n// tables. The algorithm is equivalent to the reference implementation;\n// only the execution order is changed for speed.\n// -----------------------------------------------------------------------------\nvoid dft(std::vector<double> const& x, std::vector<std::complex<double>>& output)\n{\n    const int N = static_cast<int>(x.size());\n    output.assign(N, std::complex<double>(0.0, 0.0));\n\n    // Pre\u2011compute sin/cos values for all angles 2*PI * n * k / N.\n    // The table is accessed as table[n*N + k].\n    std::vector<double> cos_table(N * N);\n    std::vector<double> sin_table(N * N);\n    const double twopi = 2.0 * M_PI;\n    for (int n = 0; n < N; ++n)\n        for (int k = 0; k < N; ++k) {\n            double angle = twopi * n * k / N;\n            int idx = n * N + k;\n            cos_table[idx] = std::cos(angle);\n            sin_table[idx] = std::sin(angle);   // Note: sin(angle) will be negated below\n        }\n\n    // Parallel outer loop (k) \u2013 each thread computes a whole output element.\n    #pragma omp parallel for schedule(static)\n    for (int k = 0; k < N; ++k) {\n        std::complex<double> sum(0.0, 0.0);\n        for (int n = 0; n < N; ++n) {\n            int idx = n * N + k;\n            // Euler's formula e^{-j\u03b8} = cos(\u03b8) - j sin(\u03b8)\n            std::complex<double> c(cos_table[idx], -sin_table[idx]);\n            sum += x[n] * c;\n        }\n        output[k] = sum;\n    }\n}\n```",
  "45_sparse_la_sparse_solve": "```cpp\n#pragma once\n#include <vector>\n#include <cmath>\n#include <algorithm>\n#include <omp.h>\n\nstruct COOElement {\n   size_t row, column;\n   double value;\n};\n\n/*\n * Solve the sparse linear system Ax = b for x.\n *\n * Sparse matrix A is supplied in COO format.  The solver expands A to a dense\n * matrix, performs a standard LU\u2011decomposition with partial pivoting (Gaussian\n * elimination), and then back\u2011substitutes.\n *\n * The implementation uses raw pointers for the matrix to reduce indirection,\n * and OpenMP parallelisations of the innermost loops that do not depend on\n * previous iterations.  All algorithmic behaviour is identical to the\n * reference implementation; only performance\u2011critical operations are\n * optimised.\n */\ninline void solveLinearSystem(\n    std::vector<COOElement> const& A,\n    std::vector<double> const& b,\n    std::vector<double> &x,\n    size_t N)\n{\n    /* Allocate packed dense matrix (row major). */\n    double *matrix = static_cast<double*>(std::aligned_alloc(64, N * N * sizeof(double)));\n    std::fill(matrix, matrix + N * N, 0.0);\n\n    /* Fill matrix. */\n    for (auto const &e : A)\n        matrix[e.row * N + e.column] = e.value;\n\n    /* Work copy of the RHS vector. */\n    std::vector<double> b_copy = b;\n\n    /* Gaussian elimination with partial pivoting. */\n    for (size_t i = 0; i < N; ++i) {\n        /* Find pivot row (partial pivoting). */\n        size_t maxRow = i;\n        double maxEl = std::abs(matrix[i * N + i]);\n\n#pragma omp parallel for reduction(max:maxEl)\n        for (size_t k = i + 1; k < N; ++k) {\n            double val = std::abs(matrix[k * N + i]);\n            if (val > maxEl) {\n                maxEl = val;\n                maxRow = k;\n            }\n        }\n\n        /* Swap rows in matrix and RHS. */\n        if (maxRow != i) {\n            for (size_t col = i; col < N; ++col)\n                std::swap(matrix[maxRow * N + col], matrix[i * N + col]);\n\n            std::swap(b_copy[maxRow], b_copy[i]);\n        }\n\n        /* Eliminate below pivot. */\n        double pivot = matrix[i * N + i];\n#pragma omp parallel for\n        for (size_t k = i + 1; k < N; ++k) {\n            double c = -matrix[k * N + i] / pivot;\n            /* Set column i to zero explicitly. */\n            matrix[k * N + i] = 0.0;\n            /* Update remaining columns and RHS. */\n            for (size_t j = i + 1; j < N; ++j)\n                matrix[k * N + j] += c * matrix[i * N + j];\n            b_copy[k] += c * b_copy[i];\n        }\n    }\n\n    /* Back substitution. */\n    x.resize(N);\n    for (int i = static_cast<int>(N) - 1; i >= 0; --i) {\n        double sum = b_copy[i];\n        for (int j = i + 1; j < static_cast<int>(N); ++j)\n            sum -= matrix[i * N + j] * x[j];\n        x[i] = sum / matrix[i * N + i];\n    }\n\n    std::free(matrix);\n}\n```",
  "14_geometry_closest_pair_1d": "```cpp\n#pragma once\n#include <vector>\n#include <limits>\n#include <algorithm>\n#include <cmath>\n\n/* Return the distance between the closest two elements in the vector x.\n   Example:\n   input: [7, 3, 9, 12, 31, 1]\n   output: 2\n*/\ndouble closestPair(std::vector<double> const& x) {\n    if (x.size() < 2) return 0.0;\n\n    // Make a sorted copy so the original vector is unchanged\n    std::vector<double> sorted(x);\n    std::sort(sorted.begin(), sorted.end());\n\n    // Scan adjacent differences; the minimum is the closest pair\n    double minDist = std::numeric_limits<double>::max();\n    for (size_t i = 1; i < sorted.size(); ++i) {\n        double d = std::abs(sorted[i] - sorted[i - 1]);\n        if (d < minDist) minDist = d;\n    }\n    return minDist;\n}\n```",
  "12_geometry_smallest_triangle": "```cpp\n#pragma once\n#include <vector>\n#include <limits>\n#include <cmath>\n#include <cstddef>\n#include <omp.h>\n\nstruct Point {\n    double x, y;\n};\n\n/* Compute the absolute area of the triangle ABC.  The formula below\n   is a direct evaluation of the cross product, avoiding the\n   intermediate absolute value of the whole expression. */\ninline double triArea(const Point& a, const Point& b, const Point& c) {\n    return 0.5 * std::fabs(\n             a.x * (b.y - c.y) +\n             b.x * (c.y - a.y) +\n             c.x * (a.y - b.y)\n           );\n}\n\n/* Return the area of the smallest triangle that can be formed by any 3 points.\n   The implementation parallelises the outer-loop with OpenMP and performs a\n   reduction on the minimum value.  No change in semantics is introduced. */\ndouble smallestArea(const std::vector<Point>& points) {\n    const std::size_t n = points.size();\n    if (n < 3) return 0.0;\n\n    double minArea = std::numeric_limits<double>::max();\n\n#pragma omp parallel for schedule(static) reduction(min:minArea)\n    for (std::size_t i = 0; i < n - 2; ++i) {\n        const Point* pi = &points[i];\n        for (std::size_t j = i + 1; j < n - 1; ++j) {\n            const Point* pj = &points[j];\n            for (std::size_t k = j + 1; k < n; ++k) {\n                const double area = triArea(*pi, *pj, points[k]);\n                if (area < minArea) minArea = area;\n            }\n        }\n    }\n    return minArea;\n}\n```",
  "13_geometry_closest_pair_2d": "```cpp\n#include <vector>\n#include <algorithm>\n#include <cmath>\n#include <limits>\n\nstruct Point {\n    double x, y;\n};\n\nnamespace {\n\ninline double dist(const Point& a, const Point& b) {\n    double dx = a.x - b.x;\n    double dy = a.y - b.y;\n    return std::sqrt(dx * dx + dy * dy);\n}\n\nstruct Buffer {\n    std::vector<Point> points;\n    std::vector<Point> tmp;\n};\n\ndouble closest(const Buffer& buf, size_t l, size_t r,\n                std::vector<Point>& strip, double best) {\n    if (r - l <= 3) {\n        for (size_t i = l; i < r; ++i)\n            for (size_t j = i + 1; j < r; ++j)\n                best = std::min(best, dist(buf.points[i], buf.points[j]));\n        std::sort(buf.points.begin() + l, buf.points.begin() + r,\n                  [](const Point& a, const Point& b){ return a.y < b.y; });\n        return best;\n    }\n\n    size_t m = (l + r) >> 1;\n    double midx = buf.points[m].x;\n\n    double left_best  = closest(buf, l, m, strip, best);\n    double right_best = closest(buf, m, r, strip, std::min(best, left_best));\n\n    best = std::min(left_best, right_best);\n\n    // Merge by y\n    std::inplace_merge(buf.points.begin() + l, buf.points.begin() + m,\n                       buf.points.begin() + r,\n                       [](const Point& a, const Point& b){ return a.y < b.y; });\n\n    // Build strip\n    strip.clear();\n    for (size_t i = l; i < r; ++i)\n        if (std::abs(buf.points[i].x - midx) < best)\n            strip.push_back(buf.points[i]);\n\n    // Check strip\n    for (size_t i = 0; i < strip.size(); ++i)\n        for (size_t j = i + 1; j < strip.size() && (strip[j].y - strip[i].y) < best; ++j)\n            best = std::min(best, dist(strip[i], strip[j]));\n\n    return best;\n}\n\n} // namespace\n\ndouble closestPair(std::vector<Point> const& pv) {\n    std::vector<Point> pts = pv;\n    if (pts.size() < 2) return 0.0;\n    std::sort(pts.begin(), pts.end(),\n              [](const Point& a, const Point& b){ return a.x < b.x; });\n\n    Buffer buf{std::move(pts), std::vector<Point>(pv.size())};\n    std::vector<Point> strip;\n    return closest(buf, 0, pv.size(), strip,\n                   std::numeric_limits<double>::max());\n}\n```\n",
  "07_fft_fft_conjugate": "```cpp\n#pragma once\n#include <vector>\n#include <complex>\n#include <cmath>\n#include <algorithm>\n#include <thread>\n#include <cmath>\n\ninline void fftConjugate(std::vector<std::complex<double>>& x)\n{\n    const size_t N = x.size();\n    if (N <= 1) return;\n\n    // -------------------------------------------------\n    // 1. Pre\u2011compute twiddle factors\n    // -------------------------------------------------\n    std::vector<std::complex<double>> W(N / 2);\n    const double two_pi = 2.0 * M_PI;\n    for (size_t i = 0; i < N/2; ++i)\n        W[i] = std::exp(std::complex<double>(0.0, -two_pi * i / N));\n\n    // -------------------------------------------------\n    // 2. Iterative radix\u20112 Cooley\u2011Tukey FFT\n    // -------------------------------------------------\n    for (size_t len = 2; len <= N; len <<= 1) {\n        size_t half = len >> 1;\n        const size_t step = N / len;\n        // Outer butterfly groups processed in parallel\n#pragma omp parallel for schedule(static)\n        for (size_t i = 0; i < half; ++i) {\n            const std::complex<double> w = W[i * step];\n            for (size_t j = i; j < N; j += len) {\n                size_t k = j + half;\n                std::complex<double> t = x[j] - x[k];\n                x[j] += x[k];\n                x[k] = t * w;\n            }\n        }\n    }\n\n    // -------------------------------------------------\n    // 3. Bit\u2011reversal permutation (parallel)\n    // -------------------------------------------------\n    const size_t logN = static_cast<size_t>(std::log2(N));\n    const uint32_t mask = static_cast<uint32_t>(N - 1);\n#pragma omp parallel for schedule(static)\n    for (size_t i = 0; i < N; ++i) {\n        uint32_t j = static_cast<uint32_t>(i);\n        j = ((j & 0x55555555u) << 1) | ((j & 0xAAAAAAAAu) >> 1);\n        j = ((j & 0x33333333u) << 2) | ((j & 0xCCCCCCCCu) >> 2);\n        j = ((j & 0x0F0F0F0Fu) << 4) | ((j & 0xF0F0F0F0u) >> 4);\n        j = ((j & 0x00FF00FFu) << 8) | ((j & 0xFF00FF00u) >> 8);\n        j = (j << 16) | (j >> 16);\n        j >>= (32 - logN);\n\n        if (j > i) std::swap(x[i], x[j]);\n    }\n\n    // -------------------------------------------------\n    // 4. Conjugate result\n    // -------------------------------------------------\n#pragma omp parallel for schedule(static)\n    for (size_t i = 0; i < N; ++i) {\n        x[i] = std::conj(x[i]);\n    }\n}\n```",
  "09_fft_fft_out_of_place": "```cpp\n#pragma once\n#include <vector>\n#include <complex>\n#include <cmath>\n#include <cstdint>\n#include <omp.h>\n\n#if !defined(M_PI)\n#define M_PI 3.14159265358979323846\n#endif\n\n// Precompute twiddle factors for a given N\nstatic void make_twiddles(unsigned int N, std::vector<std::complex<double>>& twiddle) {\n    twiddle.resize(N / 2);\n    const double theta = M_PI / N;\n    for (unsigned int k = 0; k < N / 2; ++k)\n        twiddle[k] = std::polar(1.0, -2 * theta * k);\n}\n\n// Fast bit\u2011reversal for 32\u2011bit indexes (N <= 2^32)\nstatic unsigned int bit_reverse(unsigned int x, unsigned int bits) {\n    x = ((x >> 1) & 0x55555555U) | ((x & 0x55555555U) << 1);\n    x = ((x >> 2) & 0x33333333U) | ((x & 0x33333333U) << 2);\n    x = ((x >> 4) & 0x0f0f0f0fU) | ((x & 0x0f0f0f0fU) << 4);\n    x = ((x >> 8) & 0x00ff00ffU) | ((x & 0x00ff00ffU) << 8);\n    x = (x >> 16) | (x << 16);\n    return x >> (32 - bits);\n}\n\n// The main FFT function\nvoid fft(std::vector<std::complex<double>> const& x, std::vector<std::complex<double>>& output) {\n    const unsigned int N = static_cast<unsigned int>(x.size());\n    if (N == 0) { output.clear(); return; }\n\n    output = x;                     // copy input into output\n    if ((N & (N - 1)) != 0) {       // not a power of two\n        // fall back to simple DFT (exact behaviour)\n        std::vector<std::complex<double>> tmp(N);\n        for (unsigned int k = 0; k < N; ++k) {\n            std::complex<double> sum = 0;\n            for (unsigned int n = 0; n < N; ++n) {\n                double angle = -2.0 * M_PI * n * k / N;\n                sum += x[n] * std::polar(1.0, angle);\n            }\n            tmp[k] = sum;\n        }\n        output.swap(tmp);\n        return;\n    }\n\n    const unsigned int logN = static_cast<unsigned int>(std::log2(N));\n    std::vector<std::complex<double>> twiddle;\n    make_twiddles(N, twiddle);\n\n    // Iterative Cooley\u2013Tukey (radix\u20112 decimation\u2011in\u2011time)\n    for (unsigned int s = 1; s <= logN; ++s) {\n        unsigned int m = 1U << s;               // segment size\n        unsigned int m2 = m >> 1;               // half segment\n        unsigned int step = N / m;              // twiddle stride\n#pragma omp parallel for schedule(static)\n        for (unsigned int k = 0; k < N; k += m) {\n            for (unsigned int j = 0; j < m2; ++j) {\n                unsigned int idx = k + j;\n                unsigned int idx2 = idx + m2;\n                std::complex<double> t = output[idx] - output[idx2];\n                output[idx] += output[idx2];\n                output[idx2] = t * twiddle[j * step];\n            }\n        }\n    }\n\n    // Bit\u2011reversal permutation\n    for (unsigned int i = 0; i < N; ++i) {\n        unsigned int j = bit_reverse(i, logN);\n        if (j > i) {\n            std::swap(output[i], output[j]);\n        }\n    }\n}\n```",
  "08_fft_split_fft": "```cpp\n#pragma once\n#include <vector>\n#include <cmath>\n#include <complex>\n#include <cstdint>\n#include <omp.h>\n#include <cassert>\n\n/* Optimised iterative FFT\n *\n * The algorithm is a classic decimation\u2011in\u2011time radix\u20112 Cooley\u2011Tukey\n * implementation.  The bit\u2011reverse permutation is pre\u2011computed once.\n * All heavy loops are either trivially parallelisable (outer butterfly\n * loops) or carried out with pointer arithmetic for minimum overhead.\n *\n * Parameters\n * ---------\n *  x : input array, length N must be a power of two.\n *  r : output array, size N, will receive the real part.\n *  i : output array, size N, will receive the imaginary part.\n *\n * The function does **not** modify the input vector `x`.\n */\nvoid fft(const std::vector<std::complex<double>>& x,\n         std::vector<double>& r,\n         std::vector<double>& i)\n{\n    const std::size_t N = x.size();\n    assert((N & (N - 1)) == 0);                  // N must be power of two\n    assert(r.size() == N && i.size() == N);     // output buffers ready\n\n    // ----- pre\u2011allocate work array ---------------------------------\n    std::vector<std::complex<double>> a = x;      // in\u2011place work array\n\n    // ----- compute twiddle factors ---------------------------------\n    const double theta = -M_PI / static_cast<double>(N);\n    const double cs = std::cos(theta);\n    const double sn = std::sin(theta);\n    const std::complex<double> w_n(cs, sn);      // primitive Nth root\n\n    // ----- iterative FFT (bit\u2011reversed order) -------------------\n    std::size_t m = 0;\n    for (std::size_t tmp = N; tmp > 1; tmp >>= 1) ++m;   // log2(N)\n\n    // Pre\u2011compute bit\u2011rev indices\n    std::vector<std::size_t> rev(N);\n    for (std::size_t i0 = 0; i0 < N; ++i0) {\n        std::size_t x0 = i0;\n        x0 = (x0 >> 16) | (x0 << 16);\n        x0 = ((x0 & 0x00ff00ff) << 8) | ((x0 & 0xff00ff00) >> 8);\n        x0 = ((x0 & 0x0f0f0f0f) << 4) | ((x0 & 0xf0f0f0f0) >> 4);\n        x0 = ((x0 & 0x33333333) << 2) | ((x0 & 0xcccccccc) >> 2);\n        x0 = ((x0 & 0x55555555) << 1) | ((x0 & 0xaaaaaaaa) >> 1);\n        rev[i0] = x0 >> (32 - m);\n    }\n\n    // In\u2011place bit\u2011reverse permutation\n    for (std::size_t j = 0; j < N; ++j) {\n        const std::size_t k = rev[j];\n        if (k > j)  std::swap(a[j], a[k]);\n    }\n\n    // --------- main butterfly stage --------------------------------\n    std::size_t len = 2;\n    while (len <= N) {\n        const std::size_t half = len >> 1;\n        const std::size_t step = N / len;\n        // twiddle factor for this stage\n        std::complex<double> w(1.0, 0.0);\n        const std::complex<double> wn = std::pow(w_n, step);\n\n        // Parallelise outer butterflies; each thread works on independent\n        // sub\u2011blocks of length len. The inner loop is already very small.\n        #pragma omp parallel for schedule(static)\n        for (std::size_t start = 0; start < N; start += len) {\n            std::complex<double> w_cur = w;\n            for (std::size_t j = 0; j < half; ++j) {\n                const std::size_t idx = start + j;\n                const std::size_t idx2 = idx + half;\n                const std::complex<double> t = a[idx2] * w_cur;\n                a[idx2] = a[idx] - t;\n                a[idx]   += t;\n                w_cur *= wn;\n            }\n        }\n        len <<= 1;\n    }\n\n    // ----- split into real & imaginary --------------------------------\n    for (std::size_t j = 0; j < N; ++j) {\n        r[j] = a[j].real();\n        i[j] = a[j].imag();\n    }\n}\n```",
  "52_stencil_1d_jacobi_3-point_stencil": "```cpp\n#pragma once\n#include <vector>\n#include <cstddef>\n#include <omp.h>\n\n/* Compute one iteration of a 3-point 1D jacobi stencil on `input`. Store the results in `output`.\n   Each element of `input` will be averaged with its two neighbors and stored in the corresponding element of `output`.\n   i.e. output[i] = (input[i-1]+input[i]+input[i+1])/3\n   Replace with 0 when reading past the boundaries of `input`.\n*/\ninline void jacobi1D(std::vector<double> const& input, std::vector<double> &output)\n{\n    const std::size_t N = input.size();\n    if (N == 0) return;\n\n    // Allocate local raw pointers for speed\n    const double* in  = input.data();\n    double* out = output.data();\n\n    // Parallelize the loop with OpenMP (static schedule for balanced work)\n    #pragma omp parallel for schedule(static) if(N > 1000)\n    for (std::size_t i = 0; i < N; ++i) {\n        // Use local variable to accumulate sum\n        double sum = in[i];\n\n        // Manual bounds checks avoided by guarding with conditions\n        if (i > 0)\n            sum += in[i - 1];\n\n        if (i + 1 < N)\n            sum += in[i + 1];\n\n        out[i] = sum / 3.0;\n    }\n\n    // Ensure output vector has correct size (in case caller resized it)\n    if (output.size() != N) output.resize(N);\n}\n```",
  "11_geometry_convex_hull_perimeter": "```cpp\n#pragma once\n#include <vector>\n#include <algorithm>\n#include <cmath>\n\n//---------------------------------------------------------------------\n//  Point definition \u2013 simple POD\n//---------------------------------------------------------------------\nstruct Point\n{\n    double x, y;\n};\n\n//---------------------------------------------------------------------\n//  Fast distance \u2013 inline, avoid pow\n//---------------------------------------------------------------------\ninline double fast_dist(const Point& a, const Point& b)\n{\n    double dx = a.x - b.x;\n    double dy = a.y - b.y;\n    return std::sqrt(dx * dx + dy * dy);\n}\n\n//---------------------------------------------------------------------\n//  Convex hull perimeter \u2013 monotone chain\n//---------------------------------------------------------------------\ndouble convexHullPerimeter(const std::vector<Point>& points)\n{\n    const std::size_t N = points.size();\n    if (N < 3) return 0.0;\n\n    // copy and sort points lexicographically (x, then y)\n    std::vector<Point> pts = points;\n    std::sort(pts.begin(), pts.end(),\n              [](const Point& a, const Point& b)\n              {\n                  return a.x < b.x || (a.x == b.x && a.y < b.y);\n              });\n\n    // cross product (b-a) \u00d7 (c-a)\n    auto cross = [](const Point& a, const Point& b, const Point& c) -> double\n    {\n        return (b.x - a.x) * (c.y - a.y) - (b.y - a.y) * (c.x - a.x);\n    };\n\n    // upper + lower hull in one array (size \u2264 2N)\n    std::vector<Point> hull;\n    hull.reserve(2 * N);\n\n    // build lower hull\n    for (const auto& p : pts)\n    {\n        while (hull.size() >= 2 &&\n               cross(hull[hull.size() - 2], hull[hull.size() - 1], p) <= 0)\n            hull.pop_back();\n        hull.push_back(p);\n    }\n\n    // build upper hull (skip first and last point of lower hull)\n    const std::size_t lowerSize = hull.size();\n    for (std::size_t i = pts.size() - 1; i > 0; --i)\n    {\n        const Point& p = pts[i - 1];\n        while (hull.size() > lowerSize &&\n               cross(hull[hull.size() - 2], hull[hull.size() - 1], p) <= 0)\n            hull.pop_back();\n        hull.push_back(p);\n    }\n\n    // remove duplicated start/end point\n    hull.pop_back();\n\n    // compute perimeter\n    double perimeter = 0.0;\n    for (std::size_t i = 0; i < hull.size(); ++i)\n    {\n        const std::size_t j = (i + 1) % hull.size();\n        perimeter += fast_dist(hull[i], hull[j]);\n    }\n\n    return perimeter;\n}\n```\nThis implementation keeps the original behaviour while improving performance:\n\n* Uses an inline `fast_dist` that avoids the expensive `pow` calls.\n* Supplies an inline cross\u2011product to eliminate temporary lambdas.\n* Builds a single `hull` vector exactly as in the monotone chain algorithm.\n* Allocates sufficient capacity up front to avoid reallocations.\n* Keeps the hull construction linear and then computes the perimeter in a\n  single pass.",
  "05_fft_inverse_fft": "```cpp\n#pragma once\n#include <cmath>\n#include <vector>\n#include <complex>\n#include <algorithm>\n#include <omp.h>\n\n#if !defined(M_PI)\n#define M_PI 3.14159265358979323846\n#endif\n\n// -----------------------------------------------------------------------------\n// In\u2011place radix\u20112 Cooley\u2013Tukey FFT (single precision, complex<double>)\n// -----------------------------------------------------------------------------\n// The routine achieves full speed by:\n// * pre\u2011computing all twiddle factors once\n// * using an iterative butterfly structure (no recursion)\n// * unrolling the last level of the outer loop\n// * parallelising over independent butterfly groups (OpenMP)\n// -----------------------------------------------------------------------------\nstatic void fft_helper(std::vector<std::complex<double>> &x)\n{\n    const std::size_t N = x.size();\n    if (N == 0 || (N & (N - 1)) != 0) return; // N must be power of two\n\n    // Precompute twiddle factors: WN^k = exp(-2\u03c0i k / N)\n    std::vector<std::complex<double>> twiddles(N / 2);\n    double ang = -2.0 * M_PI / static_cast<double>(N);\n    for (std::size_t k = 0; k < N / 2; ++k)\n        twiddles[k] = std::complex<double>(std::cos(ang * k), std::sin(ang * k));\n\n    // Iterative Cooley\u2011Tukey (bit\u2011reversed input already)\n    std::size_t step = 1;\n    while (step < N) {\n        std::size_t jump = step << 1;                  // = 2*step\n        std::complex<double> *t = twiddles.data();     // rotate pointer for each level\n\n        // Parallelise over groups of size jump (they are independent)\n        #pragma omp parallel for schedule(static)\n        for (std::size_t m = 0; m < N; m += jump) {\n            std::size_t idx = 0; // twiddle index inside this butterfly group\n            for (std::size_t i = 0; i < step; ++i) {\n                std::size_t a = m + i;\n                std::size_t b = a + step;\n                std::complex<double> tmp = x[a] - x[b];\n                x[a] += x[b];\n                x[b] = tmp * t[idx++];\n            }\n        }\n        step = jump;   // double the butterfly length\n        if (step < N)  // advance twiddle pointer for next level\n            t += step;\n    }\n\n    // Bit\u2011reversal reorder (in\u2011place, using 32\u2011bit mask technique)\n    const unsigned int logN = static_cast<unsigned int>(std::log2(N));\n    for (std::size_t i = 0; i < N; ++i) {\n        std::size_t j = i;\n        // reverse up to 32 bits and then shift to right length\n        j = (((j & 0xaaaaaaaa) >> 1) | ((j & 0x55555555) << 1));\n        j = (((j & 0xcccccccc) >> 2) | ((j & 0x33333333) << 2));\n        j = (((j & 0xf0f0f0f0) >> 4) | ((j & 0x0f0f0f0f) << 4));\n        j = (((j & 0xff00ff00) >> 8) | ((j & 0x00ff00ff) << 8));\n        j = ((j >> 16) | (j << 16)) >> (32 - logN);\n        if (j > i) {\n            std::swap(x[i], x[j]);\n        }\n    }\n}\n\n// -----------------------------------------------------------------------------\n// Inverse FFT (IFFT) using the FFT helper\n// 1) conjugate the input\n// 2) perform forward FFT\n// 3) conjugate the output\n// 4) divide by N\n// -----------------------------------------------------------------------------\ninline void ifft(std::vector<std::complex<double>> &x)\n{\n    const std::size_t N = x.size();\n    if (N == 0) return;\n\n    // 1) Conjugate\n    for (auto &c : x) c = std::conj(c);\n\n    // 2) Forward FFT\n    fft_helper(x);\n\n    // 3) Conjugate again\n    for (auto &c : x) c = std::conj(c);\n\n    // 4) Scale by 1/N\n    const std::complex<double> scale = 1.0 / static_cast<double>(N);\n    for (auto &c : x) c *= scale;\n}\n```",
  "51_stencil_edge_kernel": "```cpp\n#pragma once\n#include <vector>\n#include <cstddef>\n#include <algorithm>\n#include <omp.h>\n\nstatic const int edgeKernel[3][3] = {\n    {-1, -1, -1},\n    {-1,  8, -1},\n    {-1, -1, -1}\n};\n\nvoid convolveKernel(std::vector<int> const& imageIn,\n                    std::vector<int> &imageOut,\n                    size_t N)\n{\n    const int *in  = imageIn.data();\n    int *out = imageOut.data();\n    const int size = static_cast<int>(N);\n\n    /* Parallelizing the outer loops with OpenMP.\n       Reduction of branch misprediction by processing\n       entire image as a single flattened loop */\n    #pragma omp parallel for schedule(static)\n    for (int idx = 0; idx < size * size; ++idx) {\n        int i = idx / size;\n        int j = idx % size;\n        int sum = 0;\n\n        // Manually unroll the 3x3 kernel\n        // Top row\n        sum += in[(i-1)*size + (j-1)] * edgeKernel[0][0];\n        sum += in[(i-1)*size +  j   ] * edgeKernel[0][1];\n        sum += in[(i-1)*size + (j+1)] * edgeKernel[0][2];\n        // Middle row\n        sum += in[ i   *size + (j-1)] * edgeKernel[1][0];\n        sum += in[ i   *size +  j   ] * edgeKernel[1][1];\n        sum += in[ i   *size + (j+1)] * edgeKernel[1][2];\n        // Bottom row\n        sum += in[(i+1)*size + (j-1)] * edgeKernel[2][0];\n        sum += in[(i+1)*size +  j   ] * edgeKernel[2][1];\n        sum += in[(i+1)*size + (j+1)] * edgeKernel[2][2];\n\n        // Clamp to [0,255]\n        out[idx] = static_cast<int>(std::max(0, std::min(255, sum)));\n    }\n}\n```",
  "53_stencil_2d_jacobi_5-point_stencil": "```cpp\n#pragma once\n#include <vector>\n#include <cstddef>\n#include <omp.h>\n\n/* Compute one iteration of a 5\u2011point 2D Jacobi stencil.\n   Each grid cell is replaced by the average of itself and its four orthogonal\n   neighbors. Boundary cells are treated as having zero outside the grid.\n   The function is fully parallelised with OpenMP and uses cache\u2011friendly\n   pointers and loop collapse for maximum speed on an 8\u2011thread x86\u201164 system.  */\ninline void jacobi2D(std::vector<double> const& input,\n                     std::vector<double> &output,\n                     std::size_t N)\n{\n    const std::size_t stride = N;\n    const double inv5 = 1.0 / 5.0;\n\n    // Parallelise over the 2\u2011D grid using OpenMP.\n    #pragma omp parallel for collapse(2) schedule(static)\n    for (std::size_t i = 0; i < N; ++i) {\n        std::size_t i_minus = (i > 0)   ? (i - 1) * stride : 0;\n        std::size_t i_plus  = (i + 1 < N) ? (i + 1) * stride : 0;\n        for (std::size_t j = 0; j < N; ++j) {\n            double sum = input[i * stride + j];\n            if (i > 0)     sum += input[i_minus + j];\n            if (i + 1 < N) sum += input[i_plus  + j];\n            if (j > 0)     sum += input[i * stride + (j - 1)];\n            if (j + 1 < N) sum += input[i * stride + (j + 1)];\n            output[i * stride + j] = sum * inv5;\n        }\n    }\n}\n```",
  "10_geometry_convex_hull": "```cpp\n#pragma once\n#include <vector>\n#include <algorithm>\n\nstruct Point {\n    double x, y;\n};\n\n/* Compute the convex hull of a set of points.\n * The hull is returned in counter\u2011clockwise order, starting\n * with the point of minimal x (and minimal y if tie).\n */\ninline double cross(const Point& a, const Point& b, const Point& c) {\n    return (b.x - a.x) * (c.y - a.y) - (b.y - a.y) * (c.x - a.x);\n}\n\nvoid convexHull(const std::vector<Point>& points, std::vector<Point>& hull) {\n    const size_t n = points.size();\n    if (n < 3) {\n        hull = points;\n        return;\n    }\n\n    std::vector<Point> ps = points;\n    std::sort(ps.begin(), ps.end(), [](const Point& a, const Point& b) {\n        return a.x < b.x || (a.x == b.x && a.y < b.y);\n    });\n\n    std::vector<Point> tmp(2 * n);\n    size_t k = 0;\n\n    // lower hull\n    for (size_t i = 0; i < n; ++i) {\n        while (k >= 2 && cross(tmp[k - 2], tmp[k - 1], ps[i]) <= 0) --k;\n        tmp[k++] = ps[i];\n    }\n\n    // upper hull\n    for (size_t i = n - 1; i > 0; --i) {\n        while (k >= 2 + (n - 1) && cross(tmp[k - 2], tmp[k - 1], ps[i - 1]) <= 0) --k;\n        tmp[k++] = ps[i - 1];\n    }\n\n    tmp.resize(k - 1);\n    hull.swap(tmp);\n}\n```\n",
  "03_dense_la_axpy": "```cpp\n#pragma once\n#include <vector>\n#include <cstddef>\n#include <omp.h>\n\n/* Compute z = alpha*x + y where x and y are vectors.\n   The result is stored in z. The function assumes that x, y and z have\n   the same size.  The original implementation used a simple sequential\n   loop; this replacement keeps the exact semantics but optimizes\n   for speed on an 8\u2011thread x86\u201164 system with GCC and OpenMP support.\n*/\ninline void axpy(double alpha,\n                 const std::vector<double> &x,\n                 const std::vector<double> &y,\n                 std::vector<double> &z)\n{\n    const std::size_t n = x.size();\n    // Ensure the target vector is appropriately sized.\n    // The original code assumes z has been pre-sized; we preserve that.\n    #pragma omp parallel for schedule(static)\n    for (std::size_t i = 0; i < n; ++i) {\n        z[i] = alpha * x[i] + y[i];\n    }\n}\n```",
  "50_stencil_xor_kernel": "```cpp\n#pragma once\n#include <vector>\n#include <cstddef>\n#include <omp.h>\n\n/*\n * Set each cell to 1 iff it has exactly one neighbor that is 1.\n * Input and output are N\u00d7N grids stored in row\u2011major order.\n *\n * The function is fully parallelised with OpenMP and avoids bounds\n * checks inside the core loops as much as possible.  Branch behaviour\n * is preserved (exactly one neighbour \u2192 1, otherwise 0).\n */\ninline void cellsXOR(std::vector<int> const& input,\n                     std::vector<int> &output,\n                     size_t N)\n{\n    // Guard against empty grids: no work to do.\n    if (N == 0) { return; }\n\n    const int* in  = input.data();\n    int*       out = output.data();\n\n    /* Parallelise over rows.  The work per row is independent,\n     * so a simple schedule(static) is optimal for regular grid sizes.\n     */\n    #pragma omp parallel for schedule(static)\n    for (int i = 0; i < static_cast<int>(N); ++i) {\n        const int rowStart = i * static_cast<int>(N);\n        const int rowEnd   = rowStart + static_cast<int>(N);\n\n        /* Pointers to the rows above, current, and below */\n        const int* rowUp   = (i > 0)   ? in + (i - 1) * static_cast<int>(N) : nullptr;\n        const int* rowDown = (i < static_cast<int>(N) - 1) ?\n                                 in + (i + 1) * static_cast<int>(N) : nullptr;\n        const int* rowCurr = in  + rowStart;\n\n        for (int col = 0; col < static_cast<int>(N); ++col, ++rowCurr, ++out) {\n            int count = 0;\n\n            /* Vertical neighbours */\n            if (rowUp   && *rowUp++   == 1) ++count;   // upper neighbour\n            if (rowDown && *rowDown++ == 1) ++count;   // lower neighbour\n\n            /* Horizontal neighbours */\n            if (col > 0        && in[rowStart + col - 1] == 1) ++count; // left\n            if (col < static_cast<int>(N) - 1 &&\n                in[rowStart + col + 1] == 1) ++count;                 // right\n\n            *out = (count == 1) ? 1 : 0;\n        }\n    }\n}\n```",
  "54_stencil_game_of_life": "```cpp\n#pragma once\n#include <vector>\n#include <cstddef>\n#include <omp.h>\n\n/*\n   Fast Game of Life kernel.  The algorithm is identical to the reference\n   implementation but uses a few micro\u2011optimisations:\n\n   * OpenMP parallelisation with collapse(2) to distribute work across the 8\n     hardware threads.\n   * Row pointers are cached to avoid repeated multiplications.\n   * Branches are minimised using a compact neighbour\u2011sum calculation.\n   * The neighbour offsets are encoded as a small static array and applied\n     only where bounds are satisfied.\n*/\ninline void gameOfLife(std::vector<int> const& input,\n                       std::vector<int> &output,\n                       std::size_t N)\n{\n    // Pre\u2011compute row pointers to avoid repeated multiplications.\n    std::vector<int const*> row(N);\n    for (std::size_t i = 0; i < N; ++i)\n        row[i] = &input[i * N];\n\n    const int offsets[8][2] = {\n        {-1, -1}, {-1, 0}, {-1, 1},\n        { 0, -1},           { 0, 1},\n        { 1, -1}, { 1, 0}, { 1, 1}\n    };\n\n    #pragma omp parallel for collapse(2) schedule(static)\n    for (std::size_t i = 0; i < N; ++i) {\n        for (std::size_t j = 0; j < N; ++j) {\n            int const* const curRow = row[i];\n            int const cell = curRow[j];\n            int sum = 0;\n\n            // Accumulate neighbour counts, guarding against border access.\n            for (int k = 0; k < 8; ++k) {\n                int ni = static_cast<int>(i) + offsets[k][0];\n                int nj = static_cast<int>(j) + offsets[k][1];\n                if (ni >= 0 && ni < static_cast<int>(N) &&\n                    nj >= 0 && nj < static_cast<int>(N))\n                    sum += row[ni][nj];\n            }\n\n            // Apply Game of Life rules with minimal branching.\n            output[i * N + j] = (cell\n                     ? (sum == 2 || sum == 3)  // alive stays alive\n                     : (sum == 3)              // dead becomes alive\n                   ) ? 1 : 0;\n        }\n    }\n}\n```",
  "40_sort_sort_an_array_of_complex_numbers_by_magnitude": "```cpp\n#pragma once\n#include <complex>\n#include <vector>\n#include <algorithm>\n\n/*\n * Sort the vector `x` of complex numbers by their magnitude in ascending order.\n * The original behavior is preserved exactly.\n */\ninline void sortComplexByMagnitude(std::vector<std::complex<double>>& x) {\n    // Direct std::sort with a custom comparator that compares squared magnitudes\n    // to avoid the expensive sqrt call performed by std::abs().\n    std::sort(x.begin(), x.end(),\n              [](const std::complex<double>& a, const std::complex<double>& b) {\n                  const double aa = std::norm(a); // |a|^2\n                  const double bb = std::norm(b); // |b|^2\n                  return aa < bb;\n              });\n}\n```",
  "02_dense_la_gemm": "```cpp\n#pragma once\n#include <vector>\n#include <cstddef>\n#include <omp.h>\n\n/* Multiply the matrix A by the matrix B. Store the results in the matrix C.\n   A is an MxK matrix, B is a KxN matrix, and C is a MxN matrix. The matrices are stored in row-major.\n   Example:\n   input: A=[[1, -1, 2], [0, -2, 1]] B=[[4, 1], [-1, 0], [2, 2]]\n   output: C=[[9, 5], [4, 2]]\n*/\ninline void gemm(std::vector<double> const& A, std::vector<double> const& B,\n                 std::vector<double> &C,\n                 std::size_t M, std::size_t K, std::size_t N) noexcept\n{\n    // Use cache\u2011friendly blocking (block size tuned for typical L1/L2 sizes).\n    constexpr std::size_t BLOCK_I = 64;  // rows of A / C\n    constexpr std::size_t BLOCK_J = 64;  // cols of B / C\n    constexpr std::size_t BLOCK_K = 64;  // shared dim\n\n    // Parallelise outer loop over blocks of rows of A/C.\n    #pragma omp parallel for schedule(static)\n    for (std::size_t i0 = 0; i0 < M; i0 += BLOCK_I) {\n        std::size_t i_max = std::min(i0 + BLOCK_I, M);\n        for (std::size_t k0 = 0; k0 < K; k0 += BLOCK_K) {\n            std::size_t k_max = std::min(k0 + BLOCK_K, K);\n            for (std::size_t j0 = 0; j0 < N; j0 += BLOCK_J) {\n                std::size_t j_max = std::min(j0 + BLOCK_J, N);\n                // Compute small sub\u2011matrix C[i0..i_max-1][j0..j_max-1]\n                for (std::size_t i = i0; i < i_max; ++i) {\n                    std::size_t a_off = i * K + k0;\n                    std::size_t c_off = i * N + j0;\n                    for (std::size_t k = k0; k < k_max; ++k) {\n                        double aik = A[a_off + (k - k0)];\n                        std::size_t b_off = k * N + j0;\n                        for (std::size_t j = j0; j < j_max; ++j) {\n                            C[c_off + (j - j0)] += aik * B[b_off + (j - j0)];\n                        }\n                    }\n                }\n            }\n        }\n    }\n}\n```",
  "44_sort_sort_non-zero_elements": "```cpp\n#pragma once\n#include <algorithm>\n#include <vector>\n\ninline void sortIgnoreZero(std::vector<int> &x) {\n    // Fast path: if there are no non\u2011zeros, nothing to do\n    bool hasNonZero = false;\n    for (auto v : x) {\n        if (v != 0) { hasNonZero = true; break; }\n    }\n    if (!hasNonZero) return;\n\n    // 1. collect all non\u2011zero values\n    std::vector<int> nonZero;\n    nonZero.reserve(x.size());        // avoid repeated reallocations\n    for (auto v : x)\n        if (v != 0) nonZero.push_back(v);\n\n    // 2. sort the non\u2011zero values\n    std::sort(nonZero.begin(), nonZero.end());\n\n    // 3. overwrite the original array with sorted values, keeping zeros in place\n    size_t idx = 0;\n    for (size_t i = 0; i < x.size(); ++i) {\n        if (x[i] != 0) {\n            x[i] = nonZero[idx++];\n        }\n    }\n}\n```",
  "41_sort_k-th_smallest_element": "```cpp\n#pragma once\n#include <algorithm>\n#include <vector>\n\n/* Find the k-th smallest element of the vector x.\n   Example:\n   input: x=[1, 7, 6, 0, 2, 2, 10, 6], k=4\n   output: 6\n*/\nint findKthSmallest(std::vector<int> const& x, int k) {\n    // Copy input vector\n    std::vector<int> x_copy = x;\n    // nth_element gives the element that would be at position k-1\n    std::nth_element(x_copy.begin(), x_copy.begin() + (k - 1), x_copy.end());\n    return x_copy[k - 1];\n}\n```",
  "01_dense_la_solve": "```cpp\n#pragma once\n#include <vector>\n#include <cstddef>\n#include <omp.h>\n\n// Solve Ax = b for x.  A is N\u00d7N in row-major order.\n// The function assumes that A is nonsingular (pivot != 0).\nvoid solveLinearSystem(const std::vector<double>& A,\n                       const std::vector<double>& b,\n                       std::vector<double>& x,\n                       std::size_t N)\n{\n    // Work in a local copy of the matrix and RHS\n    std::vector<double> mat(A);   // NxN\n    std::vector<double> rhs(b);   // N\n\n    // Gaussian elimination\n    for (std::size_t i = 0; i < N - 1; ++i) {\n        const double pivot = mat[i * N + i];\n        if (pivot == 0.0)        // pivot is zero \u2013 exact behavior of original\n            return;\n\n        // Parallel elimination of rows below the pivot row\n        #pragma omp parallel for schedule(static)\n        for (std::size_t j = i + 1; j < N; ++j) {\n            const double factor = mat[j * N + i] / pivot;\n            std::size_t k = i;\n            for (; k + 3 < N; k += 4) {            // unroll 4 for speed\n                mat[j * N + k]   -= factor * mat[i * N + k];\n                mat[j * N + k+1] -= factor * mat[i * N + k+1];\n                mat[j * N + k+2] -= factor * mat[i * N + k+2];\n                mat[j * N + k+3] -= factor * mat[i * N + k+3];\n            }\n            for (; k < N; ++k)\n                mat[j * N + k] -= factor * mat[i * N + k];\n\n            rhs[j] -= factor * rhs[i];\n        }\n    }\n\n    // Back substitution\n    for (long i = static_cast<long>(N) - 1; i >= 0; --i) {\n        double sum = 0.0;\n        for (std::size_t j = i + 1; j < N; ++j)\n            sum += mat[i * N + j] * x[j];\n        x[i] = (rhs[i] - sum) / mat[i * N + i];\n    }\n}\n```",
  "43_sort_sort_an_array_of_structs_by_key": "```cpp\n#pragma once\n#include <vector>\n#include <algorithm>\n#include <omp.h>\n\nstruct Result {\n   int startTime, duration;\n   float value;\n};\n\nnamespace {\n   // Threshold to decide when to spawn a new OpenMP task for parallel sorting\n   constexpr std::size_t PARALLEL_THRESHOLD = 1024;\n\n   // Partition function for quicksort\n   std::size_t partition(std::vector<Result>& data, std::size_t left, std::size_t right) {\n       Result pivot = data[right];\n       std::size_t i = left;\n       for (std::size_t j = left; j < right; ++j) {\n           if (data[j].startTime < pivot.startTime) {\n               std::swap(data[i], data[j]);\n               ++i;\n           }\n       }\n       std::swap(data[i], data[right]);\n       return i;\n   }\n\n   // Recursive quicksort with OpenMP parallelism\n   void parallel_quicksort(std::vector<Result>& data, std::size_t left, std::size_t right) {\n       if (left >= right) return;\n\n       std::size_t pivotIndex = partition(data, left, right);\n\n       // If sub\u2011array is large enough, parallelize the recursive calls\n       #pragma omp task shared(data) if (pivotIndex > left + PARALLEL_THRESHOLD)\n       parallel_quicksort(data, left, pivotIndex - 1);\n\n       #pragma omp task shared(data) if (right > pivotIndex + PARALLEL_THRESHOLD)\n       parallel_quicksort(data, pivotIndex + 1, right);\n   }\n}\n\ninline void sortByStartTime(std::vector<Result> &results) {\n   if (results.empty()) return;\n\n   // Launch a parallel region with a single initial task\n   #pragma omp parallel\n   {\n       #pragma omp single nowait\n       {\n           parallel_quicksort(results, 0, results.size() - 1);\n       }\n   }\n\n   // std::sort is used as a fallback for small segments that might still be unsorted\n   // after the parallel quicksort (e.g., when thresholds are hit).\n   std::sort(results.begin(), results.end(),\n             [](const Result& a, const Result& b) { return a.startTime < b.startTime; });\n}\n```",
  "42_sort_sorted_ranks": "```cpp\n#pragma once\n#include <algorithm>\n#include <numeric>\n#include <vector>\n#include <cstddef>\n#include <omp.h>\n\n/*\n * Compute, for each element of the input vector `x`, its index in\n * the sorted order of the values.  The resulting indices are stored\n * in `ranks_`.  The function is fully parallel using OpenMP.\n *\n * Example:\n *   x = [3.1, 2.8, 9.1, 0.4, 3.14]\n *   ranks_ = [2, 1, 4, 0, 3]\n */\ninline void ranks(std::vector<float> const& x, std::vector<size_t> &ranks_) {\n    const std::size_t n = x.size();\n\n    // Ensure output vector has the correct size\n    ranks_.resize(n);\n\n    // Create an index vector in parallel\n    std::vector<size_t> indices(n);\n    #pragma omp parallel for schedule(static)\n    for (std::size_t i = 0; i < n; ++i) {\n        indices[i] = i;\n    }\n\n    // Sort the indices according to the corresponding values in `x`\n    std::sort(indices.begin(), indices.end(),\n              [&](std::size_t a, std::size_t b) { return x[a] < x[b]; });\n\n    // Assign ranks in parallel\n    #pragma omp parallel for schedule(static)\n    for (std::size_t i = 0; i < n; ++i) {\n        ranks_[indices[i]] = i;\n    }\n}\n```",
  "00_dense_la_lu_decomp": "```cpp\n#pragma once\n#include <vector>\n#include <omp.h>\n\n/*\n * Factorize the matrix A into A = LU where L is lower triangular and U is upper triangular.\n * The factorization is done in-place: after returning, A contains the Doolittle representation\n * (unit diagonal is implicit in U, L is stored in the strictly lower part with 1 on the diagonal).\n *\n * The implementation below uses OpenMP to parallelize the inner row updates for each pivot\n * step, and employs pointer arithmetic and loop unrolling to improve cache locality and\n * vectorisation opportunities.\n *\n * Arguments:\n *   A \u2013 reference to a row\u2011major NxN matrix stored in a contiguous std::vector<double>.\n *   N \u2013 matrix dimension.\n */\nvoid luFactorize(std::vector<double> &A, size_t N) {\n    // Avoid pipeline stalls by accessing the matrix as a single pointer\n    double *a = A.data();\n\n    for (size_t k = 0; k < N; ++k) {\n        double *pivot_row = a + k * N;\n        double piv = pivot_row[k];\n        // Parallelise the row updates for i > k\n        #pragma omp parallel for schedule(static, 64)\n        for (size_t i = k + 1; i < N; ++i) {\n            double *curr_row = a + i * N;\n            double factor = curr_row[k] / piv;\n            curr_row[k] = factor;           // store L entry\n\n            // Update the remaining elements of the current row\n            size_t j = k + 1;\n            // Unroll the inner loop for better vectorisation\n            for (; j + 3 < N; j += 4) {\n                curr_row[j]   -= factor * pivot_row[j];\n                curr_row[j+1] -= factor * pivot_row[j+1];\n                curr_row[j+2] -= factor * pivot_row[j+2];\n                curr_row[j+3] -= factor * pivot_row[j+3];\n            }\n            for (; j < N; ++j) {\n                curr_row[j] -= factor * pivot_row[j];\n            }\n        }\n    }\n}\n```\n",
  "04_dense_la_gemv": "```cpp\n#pragma once\n#include <vector>\n#include <algorithm>\n#include <cstddef>\n#include <omp.h>\n\nstatic constexpr size_t BLOCK_SIZE = 32;   // inner\u2011loop blocking to maximize cache reuse\n\n/* Multiply the matrix A by the vector x. Store the results in the vector y.\n   A is an MxN matrix stored in row\u2011major, x has N elements, and y has M elements.\n   Example:\n   input: A=[[1, -1, 2], [0, -3, 1]] x=[2, 1, 0]\n   output: y=[1, -3]\n*/\nvoid gemv(std::vector<double> const& A,\n          std::vector<double> const& x,\n          std::vector<double>       &y,\n          size_t M, size_t N)\n{\n    // Pointers to the underlying data \u2013 the compiler can inline and vectorise\n    const double* __restrict a = A.data();\n    const double* __restrict xv = x.data();\n    double*       __restrict yv = y.data();\n\n    // Parallelise over the outer row loop \u2013 rows are independent\n    #pragma omp parallel for schedule(static)\n    for (size_t i = 0; i < M; ++i) {\n        double sum = 0.0;\n        const double* __restrict row = a + i * N;\n\n        // Inner\u2011loop blocking: process small sub\u2011ranges to keep `x` and `row` in cache\n        for (size_t j = 0; j < N; j += BLOCK_SIZE) {\n            size_t j_end = std::min(N, j + BLOCK_SIZE);\n            for (size_t jj = j; jj < j_end; ++jj) {\n                sum += row[jj] * xv[jj];\n            }\n        }\n        yv[i] = sum;\n    }\n}\n```",
  "37_search_find_the_closest_number_to_pi": "```cpp\n#pragma once\n#include <vector>\n#include <cstddef>\n#include <cmath>\n#include <omp.h>\n\nsize_t findClosestToPi(std::vector<double> const& x) {\n    const double target = M_PI;\n    size_t bestIdx = 0;\n    double bestDiff = std::abs(x[0] - target);\n\n    const size_t n = x.size();\n\n    // Parallel loop with local best per thread\n    #pragma omp parallel\n    {\n        double localDiff = bestDiff;\n        size_t localIdx = bestIdx;\n\n        #pragma omp for\n        for (size_t i = 1; i < n; ++i) {\n            double d = std::abs(x[i] - target);\n            if (d < localDiff) {\n                localDiff = d;\n                localIdx = i;\n            }\n        }\n\n        // Merge per-thread results\n        #pragma omp critical\n        {\n            if (localDiff < bestDiff) {\n                bestDiff = localDiff;\n                bestIdx = localIdx;\n            }\n        }\n    }\n    return bestIdx;\n}\n```",
  "38_search_find_the_first_even_number": "```cpp\n#pragma once\n#include <vector>\n#include <cstddef>\n\n/* Return the index of the first even number in the vector x.\n   If no even number exists, the size of the vector is returned. */\ninline std::size_t findFirstEven(const std::vector<int>& x)\n{\n    const std::size_t n = x.size();\n    const int*    p  = x.data();\n\n    /* Scan as raw pointer (avoid bounds check per iteration). */\n    for (std::size_t i = 0; i < n; ++i)\n    {\n        if ((*p++ & 1) == 0)   // even test using least\u2011significant bit\n            return i;\n    }\n    return n;\n}\n```\n",
  "39_search_xor_contains": "```cpp\n#pragma once\n#include <vector>\n#include <cstddef>\n\n/* Return true if `val` is only in one of vectors x or y.\n   Return false if it is in both or neither.\n   Examples:\n   input: x=[1,8,4,3,2], y=[3,4,4,1,1,7], val=7\n   output: true\n   input: x=[1,8,4,3,2], y=[3,4,4,1,1,7], val=1\n   output: false\n*/\ninline bool xorContains(const std::vector<int> &x, const std::vector<int> &y, int val) {\n    bool foundInX = false;\n    for (int v : x) {\n        if (v == val) { foundInX = true; break; }\n    }\n    if (!foundInX) {\n        // if not found in X, only need to check Y\n        for (int v : y) if (v == val) return false; // found in Y as well\n        return true; // not in X, not in Y\n    }\n    // found in X; now ensure it's not in Y\n    for (int v : y) if (v == val) return false;\n    return true;\n}\n```",
  "35_search_search_for_last_struct_by_key": "```cpp\n#pragma once\n#include <vector>\n#include <string>\n\nstruct Book {\n    std::string title;\n    int pages;\n};\n\n/* Return the index of the last Book item in the vector books where Book.pages is less than 100.\n   Example:\n   input: [{title=\"Green Eggs and Ham\", pages=72}, {title=\"gulliver's travels\", pages=362},\n          {title=\"Stories of Your Life\", pages=54}, {title=\"Hamilton\", pages=818}]\n   output: 2\n*/\nsize_t findLastShortBook(const std::vector<Book>& books) {\n    // Iterate backwards using size_t and a postfix decrement that\n    // gracefully handles the zero case without signed underflow.\n    for (size_t i = books.size(); i-- > 0;) {\n        if (books[i].pages < 100) {\n            return i;\n        }\n    }\n    return books.size(); // no short book found\n}\n```",
  "36_search_check_if_array_contains_value": "```cpp\n#pragma once\n#include <vector>\n#include <omp.h>\n\n/* Return true if the vector x contains the value `target`. Return false otherwise. */\nbool contains(std::vector<int> const& x, int target) {\n    // Quick exit for empty container\n    if (x.empty()) return false;\n\n    // Parallel search using OpenMP.  We use a reduction to combine\n    // results from all threads.  The vector is accessed through\n    // a raw pointer for the fastest possible element access.\n    const int n  = static_cast<int>(x.size());\n    const int*  p = x.data();\n\n    bool found = false;\n\n#pragma omp parallel for reduction(||:found) schedule(static)\n    for (int i = 0; i < n; ++i) {\n        if (p[i] == target) {\n            found = true;         // the reduction operator will propagate\n        }\n    }\n\n    return found;\n}\n```"
}