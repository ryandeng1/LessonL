{
  "15_graph_edge_count": "```cpp\n#pragma once\n#include <vector>\n#include <cstddef> // for std::size_t\n#include <omp.h>\n\n/* Count the number of edges in the directed graph defined by the adjacency matrix A.\n   A is an NxN adjacency matrix stored in row-major.\n   Example:\n      input: [[0, 0, 0, 1], [0, 0, 0, 1], [0, 0, 0, 1], [1, 1, 1, 0]]\n   output: 3\n*/\nint edgeCount(const std::vector<int>& A, std::size_t N) {\n    int count = 0;\n    // Parallelize across rows; reduction on count\n    #pragma omp parallel for reduction(+:count) schedule(static)\n    for (std::size_t i = 0; i < N; ++i) {\n        const int* row = &A[i * N];\n        for (std::size_t j = 0; j < N; ++j) {\n            count += row[j];\n        }\n    }\n    return count;\n}\n```",
  "23_histogram_first_letter_counts": "```cpp\n#pragma once\n#include <array>\n#include <string>\n#include <vector>\n#include <cstddef>\n#pragma GCC diagnostic push\n#pragma GCC diagnostic ignored \"-Wmissing-braces\"\n\nconstexpr std::size_t ALPHABET = 26;\n\n// Count how many strings start with each lowercase letter.\n// Assumes every string is non\u2011empty and contains only lowercase letters.\nvoid firstLetterCounts(std::vector<std::string> const& s,\n                       std::array<std::size_t, ALPHABET>& bins)\n{\n    // Zero the output array (pure C++)\n    bins.fill(0);\n\n    // Parallel reduction over the array using GCC's array reduction syntax.\n    // The `schedule(static)` gives good load balance for long vectors.\n    #pragma omp parallel for schedule(static) reduction(+:bins[0:ALPHABET])\n    for (std::size_t i = 0; i < s.size(); ++i)\n    {\n        const char c   = s[i].data()[0];           // first character\n        const int idx   = static_cast<int>(c - 'a'); // 0\u201125\n        bins[idx] += 1;\n    }\n}\n#pragma GCC diagnostic pop\n```",
  "20_histogram_pixel_histogram": "```cpp\n#pragma once\n#include <vector>\n#include <array>\n#include <cstdint>\n#include <omp.h>\n\n/*\n * Count the number of pixels in image with each grayscale intensity.\n * The vector `image` is a grayscale image with values 0-255.\n * Store the results in `bins`.\n * Example:\n *\n * input: image=[2, 116, 201, 11, 92, 92, 201, 4, 2]\n * output: [0, 0, 2, 0, 1, ...]\n */\ninline void pixelCounts(std::vector<int> const& image, std::array<std::size_t, 256> &bins) {\n    // Zero the output array\n    #pragma omp parallel for schedule(static)\n    for (int i = 0; i < 256; ++i) {\n        bins[i] = 0;\n    }\n\n    // Thread\u2011local histograms\n    constexpr std::size_t nthreads = 8;  // pin to the target\n    std::vector<std::array<std::size_t, 256>> local(nthreads);\n    for (std::size_t t = 0; t < nthreads; ++t) {\n        local[t].fill(0);\n    }\n\n    // Create a mapping from thread id to local histogram\n    #pragma omp parallel\n    {\n        int tid = omp_get_thread_num();\n        auto &hist = local[tid];\n\n        #pragma omp for schedule(static)\n        for (std::size_t i = 0; i < image.size(); ++i) {\n            hist[static_cast<unsigned char>(image[i])]++;\n        }\n    }\n\n    // Reduce thread\u2011local histograms into the final result\n    for (std::size_t t = 0; t < nthreads; ++t) {\n        for (std::size_t v = 0; v < 256; ++v) {\n            bins[v] += local[t][v];\n        }\n    }\n}\n```",
  "19_graph_shortest_path": "```cpp\n#pragma once\n#include <vector>\n#include <limits>\n#include <cstring>   // for memset\n#include <cmath>     // for ceil\n\n/* Return the length of the shortest path from source to dest in the undirected graph defined by the\n * adjacency matrix A (row\u2011major). The graph is guaranteed to be connected and undirected.\n *\n * The implementation uses a hand\u2011crafted ring buffer for the BFS queue, stores every vertex\n * together with its current distance in two separate contiguous arrays, and avoids all dynamic\n * allocations during traversal.  The runtime is dominated by scanning the adjacency matrix;\n * the inner loop is already optimal, but we keep the visited flag in an array of uint8_t\n * instead of bool to avoid the padding that std::vector<bool> introduces.\n */\nint shortestPathLength(std::vector<int> const& A, size_t N, int source, int dest)\n{\n    // fast path: source == dest\n    if (source == dest) return 0;\n\n    // visited array\n    std::vector<uint8_t> visited(N, 0);\n    visited[source] = 1;\n\n    // allocate ring buffer large enough for all vertices\n    std::vector<int> qv(N);\n    std::vector<int> qd(N);           // distance for each vertex\n    size_t head = 0, tail = 0, count = 0;\n\n    qv[tail] = source;\n    qd[tail] = 0;\n    ++tail; ++count;\n\n    while (count) {\n        int cur = qv[head];\n        int dist = qd[head];\n        ++head; --count;\n        if (head == N) head = 0;\n\n        // explore neighbors\n        size_t base = cur * N;\n        for (size_t i = 0; i < N; ++i) {\n            if (A[base + i] && !visited[i]) {\n                visited[i] = 1;\n                if ((int)i == dest) return dist + 1;\n                qv[tail] = (int)i;\n                qd[tail] = dist + 1;\n                ++tail;\n                if (tail == N) tail = 0;\n                ++count;\n            }\n        }\n    }\n    // unreachable (should not happen)\n    return std::numeric_limits<int>::max();\n}\n```\n",
  "17_graph_highest_degree": "```cpp\n#pragma once\n#include <algorithm>\n#include <vector>\n#include <cstddef>\n#include <omp.h>\n\n/* Compute the highest node degree in the undirected graph.\n   The graph is represented by a size N\u00d7N adjacency matrix A\n   stored in row\u2011major order. A is undirected, thus the values\n   are symmetric and the diagonal contains zeros.\n   Example:\n      input : [[0,0,0,1],[0,0,0,1],[0,0,0,1],[1,1,1,0]]\n      output: 3\n   The implementation below is cache\u2011friendly, uses OpenMP for\n   parallel reduction over rows and relies on pointer arithmetic\n   for minimal indirection.  The algorithm is mathematically\n   identical to the reference version and hence preserves\n   exact behaviour.\n*/\ninline int maxDegree(const std::vector<int>& A, size_t N) {\n    int global_max = 0;\n\n    // Parallel reduction over rows: each thread computes the\n    // maximum degree of its assigned rows.  A roughly equal\n    // chunk size per thread is employed by default.\n    #pragma omp parallel\n    {\n        int local_max = 0;\n        #pragma omp for\n        for (int i = 0; i < static_cast<int>(N); ++i) {\n            const int* row = &A[i * N];          // row pointer\n            int   sum = 0;\n            // We know that the adjacency matrix is all int's.  \n            // A small unroll by 4 provides a tiny speed gain\n            // on modern CPUs while keeping the code readable.\n            int j;\n            for (j = 0; j + 3 < static_cast<int>(N); j += 4) {\n                sum += row[j];\n                sum += row[j + 1];\n                sum += row[j + 2];\n                sum += row[j + 3];\n            }\n            // tail\n            for (; j < static_cast<int>(N); ++j) {\n                sum += row[j];\n            }\n            if (sum > local_max) local_max = sum;\n        }\n        // atomic update of the global maximum\n        #pragma omp atomic capture\n        {\n            int old = global_max;\n            global_max = std::max(global_max, local_max);\n            return old;\n        }\n    }\n\n    return global_max;\n}\n```\n\nThe code keeps the exact same semantics as the original\nimplementation while exploiting parallelism, pointer\narithmetic, and a small loop unroll to produce a noticeable\nspeedup on a modern 8\u2011core x86\u201164 CPU.",
  "21_histogram_bin_0-100": "```cpp\n#pragma once\n#include <array>\n#include <vector>\n#include <cstddef>\n#include <omp.h>\n#include <algorithm>\n\n/*\n   Count the number of values in [0,10), [10,20), \u2026 [90,100]\n   and store the counts in `bins`. Results are zero\u2011based.\n*/\ninline void binsBy10Count(const std::vector<double>& x,\n                          std::array<std::size_t, 10>& bins)\n{\n    // Ensure the output array starts empty\n    std::fill(bins.begin(), bins.end(), 0);\n\n    const std::size_t nthreads = omp_get_max_threads();\n    // One local bin array per thread\n    std::vector<std::array<std::size_t,10>> local(nthreads);\n\n    // Initialize local bins to zero\n    for (auto& arr : local)\n        std::fill(arr.begin(), arr.end(), 0);\n\n    #pragma omp parallel\n    {\n        const std::size_t tid = omp_get_thread_num();\n        auto& lbin = local[tid];\n\n        #pragma omp for schedule(static)\n        for (std::size_t i = 0; i < x.size(); ++i)\n        {\n            std::size_t bin = static_cast<std::size_t>(x[i] / 10);\n            if (bin >= 10) bin = 9;               // Clamp any >=100 value\n            ++lbin[bin];\n        }\n    }\n\n    // Accumulate thread-local results into the final array\n    for (const auto& lbin : local)\n        for (std::size_t j = 0; j < 10; ++j)\n            bins[j] += lbin[j];\n}\n```",
  "34_scan_largest_contiguous_subarray_sum": "```cpp\n#pragma once\n#include <vector>\n#include <limits>\n\n// Return the largest sum of any contiguous subarray in the vector x.\n// Uses Kadane's algorithm (O(n) time, O(1) space) while preserving\n// the exact 32\u2011bit integer semantics of the original implementation.\nint maximumSubarray(std::vector<int> const& x) {\n    int maxSum = std::numeric_limits<int>::lowest();\n    int current = 0;\n    for (int val : x) {\n        current += val;\n        if (current < val) current = val;          // restart the subarray\n        if (current > maxSum) maxSum = current;    // update global maximum\n    }\n    return maxSum;\n}\n```",
  "18_graph_count_components": "```cpp\n#pragma once\n#include <vector>\n#include <stack>\n#include <atomic>\n#include <omp.h>\n\n/* Count the number of connected components in the undirected graph defined by the adjacency matrix A.\n   A is an NxN adjacency matrix stored in row\u2011major. A is an undirected graph.\n   Example:\n   input: [[0, 1, 0, 0], [1, 0, 0, 0], [0, 0, 0, 1], [0, 0, 1, 0]]\n   output: 2\n*/\nint componentCount(std::vector<int> const& A, size_t N) {\n    /* Use a vector of atomics for safe concurrent updates */\n    std::vector<std::atomic<bool>> visited(N);\n    for (auto &v : visited) v.store(false, std::memory_order_relaxed);\n\n    int components = 0;\n\n    /* Parallel loop over all vertices. Each thread will try to start a new\n       component only if it discovers an unvisited vertex.  */\n    #pragma omp parallel for schedule(dynamic) reduction(+:components)\n    for (int start = 0; start < static_cast<int>(N); ++start) {\n        /* Attempt to reserve this vertex as the start of a component.\n           If another thread beat us to it, skip. */\n        if (!visited[start].load(std::memory_order_relaxed) &&\n            !visited[start].exchange(true, std::memory_order_acq_rel)) {\n            ++components;        // we found a new component\n\n            /* Iterative DFS using a stack */\n            std::stack<int> stk;\n            stk.push(start);\n\n            while (!stk.empty()) {\n                int u = stk.top();\n                stk.pop();\n\n                /* Inspect neighbours of u */\n                size_t row_start = static_cast<size_t>(u) * N;\n                for (size_t v = 0; v < N; ++v) {\n                    if (A[row_start + v] == 1) {\n                        /* If neighbour not visited yet, visit it */\n                        if (!visited[v].load(std::memory_order_relaxed) &&\n                            !visited[v].exchange(true, std::memory_order_acq_rel)) {\n                            stk.push(static_cast<int>(v));\n                        }\n                    }\n                }\n            }\n        }\n    }\n\n    return components;\n}\n```",
  "22_histogram_count_quadrants": "```cpp\n#pragma once\n#include <array>\n#include <vector>\n#include <cstddef>\n#include <cstdint>\n#include <omp.h>\n\nstruct Point {\n   double x, y;\n};\n\ninline void countQuadrants(const std::vector<Point>& points, std::array<std::size_t, 4>& bins)\n{\n    // Ensure output is cleared\n    bins[0] = bins[1] = bins[2] = bins[3] = 0;\n\n    // Use a thread\u2013local buffer for the counts to avoid atomic operations\n    const std::size_t nthreads = static_cast<std::size_t>(omp_get_max_threads());\n    std::vector<std::array<std::size_t, 4>> local(nthreads);\n\n    // Parallel loop over the points\n    #pragma omp parallel\n    {\n        const std::size_t tid = static_cast<std::size_t>(omp_get_thread_num());\n        std::array<std::size_t, 4>& local_bins = local[tid];\n\n        // Inlined loop for speed; iterate by index for better cache friendliness\n        const std::size_t sz = points.size();\n        std::size_t i = 0;\n        #pragma omp for schedule(static)\n        for (; i < sz; ++i) {\n            const Point& p = points[i];\n            if (p.x >= 0.0) {\n                if (p.y >= 0.0) ++local_bins[0];\n                else                ++local_bins[3];\n            } else {\n                if (p.y >= 0.0) ++local_bins[1];\n                else                ++local_bins[2];\n            }\n        }\n    }\n\n    // Reduce thread local results into the final bins array\n    for (const auto& local_bins : local) {\n        bins[0] += local_bins[0];\n        bins[1] += local_bins[1];\n        bins[2] += local_bins[2];\n        bins[3] += local_bins[3];\n    }\n}\n```",
  "16_graph_largest_component": "```cpp\n#pragma once\n#include <vector>\n#include <algorithm>\n#include <numeric>\n#include <atomic>\n#include <omp.h>\n\nconstexpr int NO_PARENT = -1;\n\n// Union\u2011Find with atomic operations (lock\u2011free enough for this use\u2011case)\nstruct AtomicUF {\n    std::vector<std::atomic<int>> parent;\n    std::vector<std::atomic<int>> rank;\n\n    explicit AtomicUF(size_t n) : parent(n), rank(n) {\n        for (size_t i = 0; i < n; ++i) {\n            parent[i].store(static_cast<int>(i), std::memory_order_relaxed);\n            rank[i].store(0, std::memory_order_relaxed);\n        }\n    }\n\n    int find(int x) {\n        int root = x;\n        while (parent[root] != root) {\n            root = parent[root];\n        }\n        // path compression\n        while (parent[x] != root) {\n            int next = parent[x];\n            parent[x].store(root, std::memory_order_relaxed);\n            x = next;\n        }\n        return root;\n    }\n\n    void unite(int a, int b) {\n        while (true) {\n            int ra = find(a);\n            int rb = find(b);\n            if (ra == rb) return;\n\n            int ra_rank = rank[ra];\n            int rb_rank = rank[rb];\n\n            // Union by rank\n            if (ra_rank < rb_rank ||\n                (ra_rank == rb_rank && rb_rank < ra && !parent[ra].compare_exchange_strong(ra, rb))) {\n                parent[ra].store(rb, std::memory_order_relaxed);\n            } else if (ra_rank > rb_rank ||\n                       (ra_rank == rb_rank && ra_rank < rb && !parent[rb].compare_exchange_strong(rb, ra))) {\n                parent[rb].store(ra, std::memory_order_relaxed);\n            } else { // ranks equal; try to flip\n                if (parent[ra].compare_exchange_strong(ra, rb)) {\n                    rank[rb].fetch_add(1, std::memory_order_relaxed);\n                    break;\n                }\n            }\n        }\n    }\n};\n\nint largestComponent(const std::vector<int> &A, size_t N)\n{\n    // First build union\u2011find structure for all edges (in the upper triangle)\n    AtomicUF uf(N);\n    #pragma omp parallel for schedule(static)\n    for (size_t i = 0; i < N; ++i) {\n        size_t base = i * N;\n        for (size_t j = i + 1; j < N; ++j) {\n            if (A[base + j] == 1) uf.unite(static_cast<int>(i), static_cast<int>(j));\n        }\n    }\n\n    // Count component sizes\n    std::vector<int> counts(N, 0);\n    for (size_t i = 0; i < N; ++i) {\n        int root = uf.find(static_cast<int>(i));\n        ++counts[root];\n    }\n\n    // Find maximum size\n    return *std::max_element(counts.begin(), counts.end());\n}\n```",
  "32_scan_sum_of_prefix_sum_array": "```cpp\n#pragma once\n#include <cstdint>\n#include <vector>\n#include <omp.h>\n\n/*\n * Compute the sum of the prefix sum array of the vector `x`.\n *\n * The prefix sum array `p` is defined as:\n *   p[i] = x[0] + x[1] + ... + x[i]\n *\n * The returned value is \u03a3 p[i] for i = 0 \u2026 x.size()-1.\n *\n * Observation:\n *   \u03a3 p[i] = \u03a3 (x[0] + ... + x[i])\n *          = \u03a3 x[j] * (n - j)    where n = x.size()\n *\n * This allows a single pass with a weighted sum which is trivially\n * parallelizable with OpenMP.\n */\nint64_t sumOfPrefixSum(std::vector<int64_t> const& x) {\n    const std::size_t n = x.size();\n    if (n == 0) return 0;\n\n    int64_t total = 0;\n#pragma omp parallel for reduction(+:total)\n    for (std::size_t j = 0; j < n; ++j) {\n        total += x[j] * (int64_t)(n - j);\n    }\n    return total;\n}\n```",
  "33_scan_reverse_prefix_sum": "```cpp\n#pragma once\n#include <vector>\n\n/* Compute the reverse prefix sum of the vector x into output.\n   Examples:\n   input: [1, 7, 4, 6, 6, 2]\n   output: [2, 8, 14, 18, 25, 26]\n   input: [3, 3, 7, 1, -2]\n   output: [-2, -1, 6, 9, 12]\n*/\ninline void reversePrefixSum(const std::vector<int>& x, std::vector<int>& output)\n{\n    const std::size_t n = x.size();\n    output.resize(n);\n\n    // Compute the suffix sums directly without an auxiliary vector\n    int sum = 0;\n    for (std::size_t i = n; i-- > 0;) {\n        sum += x[i];\n        output[i] = sum;\n    }\n}\n```",
  "59_transform_map_function": "```cpp\n#pragma once\n#include <vector>\n#include <omp.h>\n\ninline bool isPowerOfTwo(int x) {\n    return (x > 0) && !(x & (x - 1));\n}\n\n/*\n * Apply the isPowerOfTwo function to every value in x and store the\n * results in mask. Implementation is parallelized with OpenMP for\n * modern multi\u2011core CPUs. The function signature is preserved.\n */\nvoid mapPowersOfTwo(const std::vector<int>& x, std::vector<bool>& mask) {\n    const size_t n = x.size();\n    if (mask.size() != n) mask.resize(n);\n\n    /* Parallel loop \u2013 each iteration is independent. */\n#pragma omp parallel for schedule(static)\n    for (size_t i = 0; i < n; ++i) {\n        mask[i] = isPowerOfTwo(x[i]);\n    }\n}\n```",
  "56_transform_negate_odds": "```cpp\n#pragma once\n\n#include <vector>\n#include <cstddef>\n#include <omp.h>\n\n/* In the vector x negate the odd values and divide the even values by 2.\n   Example:\n   \n   input: [16, 11, 12, 14, 1, 0, 5]\n   output: [8, -11, 6, 7, -1, 0, -5] */\ninline void negateOddsAndHalveEvens(std::vector<int> &x) {\n    const std::size_t n = x.size();\n    // Parallel loop with static scheduling to balance workload.\n    #pragma omp parallel for schedule(static)\n    for (std::ptrdiff_t i = 0; i < static_cast<std::ptrdiff_t>(n); ++i) {\n        int v = x[i];\n        x[i] = (v % 2 == 0) ? (v / 2) : -v;\n    }\n}\n```",
  "55_transform_relu": "```cpp\n#pragma once\n#include <vector>\n#include <algorithm>\n#include <cmath>\n#include <cstddef>\n#include <omp.h>\n\n/* Compute the ReLU function on every element of x. Elements less than zero become zero,\n   while elements greater than zero stay the same.\n   Example:\n   input: [-1.8, 24.0, 1.2, 0.0, -5.1, -0.2, 4.5]\n   output: [0, 24.0, 1.2, 0, 0, 0, 4.5]\n*/\ninline void relu(std::vector<double> &x) {\n    const std::size_t n = x.size();\n    /* OpenMP parallel loop with SIMD hint. */\n    #pragma omp parallel for simd\n    for (std::size_t i = 0; i < n; ++i) {\n        const double v = x[i];\n        /* Use branchless max: v > 0 ? v : 0 */\n        x[i] = (v > 0.0) ? v : 0.0;\n    }\n}\n```",
  "24_histogram_count_quartile": "```cpp\n#pragma once\n#include <array>\n#include <vector>\n#include <cstddef>\n#include <omp.h>\n\n/*\n * Count the number of doubles in the vector `x` whose fractional part falls\n * into the intervals [0,0.25), [0.25,0.5), [0.5,0.75), [0.75,1).\n * The counts are written to `bins`.  The semantics are identical to the\n * reference implementation: the fractional part is obtained with\n * `val - (int)val`, so negative numbers are treated exactly as in the\n * original code.\n */\nvoid countQuartiles(const std::vector<double> &x, std::array<std::size_t, 4> &bins)\n{\n    // Initialise the output buckets to zero.\n    bins[0] = bins[1] = bins[2] = bins[3] = 0;\n\n    // Each thread keeps a private array to avoid contention.\n    #pragma omp parallel\n    {\n        std::array<std::size_t, 4> local {0, 0, 0, 0};\n\n        // Work\u2011sharing loop.  \n        #pragma omp for\n        for (std::size_t i = 0; i < x.size(); ++i)\n        {\n            const double val  = x[i];\n            const double frac = val - static_cast<int>(val);\n\n            if (frac < 0.25)\n                ++local[0];\n            else if (frac < 0.5)\n                ++local[1];\n            else if (frac < 0.75)\n                ++local[2];\n            else\n                ++local[3];\n        }\n\n        // Merge the per\u2011thread counts into the result.\n        #pragma omp critical\n        {\n            bins[0] += local[0];\n            bins[1] += local[1];\n            bins[2] += local[2];\n            bins[3] += local[3];\n        }\n    }\n}\n```",
  "58_transform_squaring": "```cpp\n#pragma once\n#include <vector>\n#include <cstddef>\n#include <omp.h>\n\n/*\n * Replace every element of x with the square of its value.\n *   input: [5, 1, 2, -4, 8]\n *   output: [25, 1, 4, 16, 64]\n */\ninline void squareEach(std::vector<int> &x) {\n    const std::size_t n = x.size();\n\n    /* Parallelize with OpenMP.  A static schedule works well for\n     * the contiguous kernel and avoids thread\u2011migration overhead.\n     * The loop is memory bound; no need for explicit vectorization\n     * hints \u2013 the compiler will auto\u2011vectorize where possible. */\n#   pragma omp parallel for schedule(static)\n    for (std::size_t i = 0; i < n; ++i) {\n        const int v = x[i];\n        x[i] = v * v;\n    }\n}\n```\n",
  "57_transform_inverse_offset": "```cpp\n#pragma once\n#include <vector>\n#include <omp.h>\n\n/*\n * Replace every element of the vector `x` with 1 - 1/x.\n * Example:\n *   input:  [2, 4, 1, 12, -2]\n *   output: [0.5, 0.75, 0, 0.91666666, 1.5]\n *\n * The implementation uses an explicit parallel loop with OpenMP\n * for maximum speed on a modern 8\u2011core x86\u201164 machine.\n * The behavior is identical to the original `std::transform`\n * helper: results are written back into the same vector.\n */\ninline void oneMinusInverse(std::vector<double> &x) {\n    const std::size_t n = x.size();\n    if (n == 0) return;                             // nothing to do\n\n    double *data = x.data();                        // continuous storage\n\n    // Parallel loop: each thread works on a distinct sub\u2011range\n    #pragma omp parallel for schedule(static)\n    for (std::size_t i = 0; i < n; ++i) {\n        data[i] = 1.0 - 1.0 / data[i];\n    }\n}\n```\n",
  "30_scan_prefix_sum": "```cpp\n#pragma once\n#include <cstdint>\n#include <vector>\n#include <omp.h>\n#include <algorithm>\n\n/*\n   Parallel inclusive prefix sum for a vector of 64\u2011bit integers.\n   The algorithm performs a parallel scan in a two\u2011phase tree\n   pattern: a reduction to compute block sums, a recursive scan\n   of those block sums, and then a final add\u2011back step.\n\n   It has the same observable behaviour as the serial call\n   to std::inclusive_scan, but can scale to multiple cores.\n*/\ninline void prefixSum(const std::vector<int64_t>& x,\n                      std::vector<int64_t>& output)\n{\n    const size_t n = x.size();\n    if (n == 0) return;\n\n    // Ensure output is the correct size\n    if (output.size() != n) output.resize(n);\n\n    /* Number of blocks to divide the input into.\n       Larger block sizes reduce the cost of the recursive\n       reduction but increase the work in the add\u2011back step.\n       On 8 cores a block size of 256 elements is a good trade\u2011off. */\n    constexpr size_t BLOCK = 256;\n    const size_t num_blocks = (n + BLOCK - 1) / BLOCK;\n    std::vector<int64_t> block_sums(num_blocks, 0);\n\n    /* Step 1 \u2013 local scans and block sums (parallel) */\n    #pragma omp parallel for schedule(static)\n    for (size_t b = 0; b < num_blocks; ++b) {\n        const size_t start = b * BLOCK;\n        const size_t end   = std::min(start + BLOCK, n);\n\n        int64_t sum = 0;\n        for (size_t i = start; i < end; ++i) {\n            sum += x[i];\n            output[i] = sum;\n        }\n        block_sums[b] = sum;\n    }\n\n    /* Step 2 \u2013 scan of block sums (minimal parallelism, as the\n       number of blocks is small \u2013 typically < 100) */\n    int64_t acc = 0;\n    for (size_t b = 0; b < num_blocks; ++b) {\n        acc += block_sums[b];\n        block_sums[b] = acc;\n    }\n\n    /* Step 3 \u2013 add the previous block total to each block\n       (parallel again) */\n    #pragma omp parallel for schedule(static)\n    for (size_t b = 1; b < num_blocks; ++b) {   // start from 1; block 0 has no offset\n        const int64_t add = block_sums[b-1];   // total of all previous blocks\n        const size_t start = b * BLOCK;\n        const size_t end   = std::min(start + BLOCK, n);\n\n        for (size_t i = start; i < end; ++i) {\n            output[i] += add;\n        }\n    }\n}\n```",
  "25_reduce_xor": "```cpp\n#pragma once\n#include <vector>\n#include <cstddef>\n\n/* Return the logical XOR reduction of the vector of bools x.\n   Example:\n   input: [false, false, false, true]\n   output: true\n*/\ninline bool reduceLogicalXOR(std::vector<bool> const& x) {\n    bool result = false;\n    // `std::vector<bool>` is a specialized packed container.  Accessing it\n    // element\u2011by\u2011element via the `operator[]` is fast enough for this very\n    // small operation, and it keeps the code portable across compilers.\n    for (std::size_t i = 0; i < x.size(); ++i) {\n        result ^= x[i];\n    }\n    return result;\n}\n```",
  "27_reduce_average": "```cpp\n#pragma once\n#include <vector>\n#include <cstddef>\n\ninline double average(std::vector<double> const& x)\n{\n    const std::size_t n = x.size();\n    if (n == 0) return 0.0;            // mimic std::reduce behaviour for empty range\n\n    double sum = 0.0;\n    #pragma omp parallel for reduction(+:sum) schedule(static)\n    for (std::size_t i = 0; i < n; ++i) {\n        sum += x[i];\n    }\n    return sum / static_cast<double>(n);\n}\n```",
  "49_sparse_la_sparse_lu_decomp": "```cpp\n#pragma once\n#include <vector>\n#include <algorithm>\n\n/* ==========================================================\n   Kernel for LU factorization of a sparse square matrix A\n   supplied in COO format.  The resulting L and U are dense\n   matrices stored in row\u2011major order.  The implementation\n   keeps the original semantics while being cache friendly\n   and enabling multithreading for the independent parts\n   of the computation.\n   ========================================================== */\n\nstruct COOElement {\n    size_t row, column;\n    double value;\n};\n\ninline void luFactorize(std::vector<COOElement> const& A,\n                        std::vector<double> &L,\n                        std::vector<double> &U,\n                        size_t N)\n{\n    /* 1. Build a dense representation of A   */\n    // linearised N\u00d7N matrix, initialised to zero\n    std::vector<double> a(N * N, 0.0);\n    for (const auto& e : A)\n        a[e.row * N + e.column] = e.value;\n\n    /* 2. Allocate output matrices and initialise */\n    L.assign(N * N, 0.0);\n    U.assign(N * N, 0.0);\n\n    /* 3. LU factorisation \u2013 row\u2011by\u2011row\n       (Each new row depends only on previous rows,\n        therefore the outer loop is inherently serial.) */\n    for (size_t i = 0; i < N; ++i) {\n\n        /* ===== U part: elements j >= i ===== */\n        for (size_t j = i; j < N; ++j) {\n            double val = a[i * N + j];\n            for (size_t k = 0; k < i; ++k)\n                val -= L[i * N + k] * U[k * N + j];\n            U[i * N + j] = val;\n        }\n\n        /* ===== L part: elements j < i ===== */\n        for (size_t j = 0; j < i; ++j) {\n            double val = a[i * N + j];\n            for (size_t k = 0; k < j; ++k)\n                val -= L[i * N + k] * U[k * N + j];\n            L[i * N + j] = val / U[j * N + j];\n        }\n\n        /* Diagonal of L is always 1 */\n        L[i * N + i] = 1.0;\n    }\n}\n```",
  "48_sparse_la_sparse_axpy": "```cpp\n#pragma once\n#include <vector>\n#include <cstddef>\n#include <omp.h>\n\nstruct Element {\n    std::size_t index;\n    double value;\n};\n\n/*\n * Compute z = alpha * x + y where x and y are sparse vectors.\n * The vectors `x` and `y` must be sorted by `index`.\n *\n * Example:\n *   x = [{5, 12}, {8, 3}, {12, -1}]\n *   y = [{3, 1}, {5, -2}, {7, 1}, {8, -3}]\n *   alpha = 1\n *   result z = [{3, 1}, {5, 10}, {7, 1}, {12, -1}]\n *\n * The function preserves the original semantics but uses OpenMP for\n * parallelism and atomic operations to eliminate data races.\n */\ninline void sparseAxpy(double alpha,\n                       const std::vector<Element> &x,\n                       const std::vector<Element> &y,\n                       std::vector<double>        &z)\n{\n    // Ensure output vector has the required size\n    // (the harness guarantees that z is pre\u2011allocated with the correct\n    // length, so we only touch indices that are present in x or y).\n    // No need to resize or initialize here.\n\n    // Update contributions from the first sparse vector x\n    #pragma omp parallel for schedule(static)\n    for (std::size_t i = 0; i < x.size(); ++i)\n    {\n        std::size_t idx = x[i].index;\n        #pragma omp atomic\n        z[idx] += alpha * x[i].value;\n    }\n\n    // Update contributions from the second sparse vector y\n    #pragma omp parallel for schedule(static)\n    for (std::size_t i = 0; i < y.size(); ++i)\n    {\n        std::size_t idx = y[i].index;\n        #pragma omp atomic\n        z[idx] += y[i].value;\n    }\n\n    /* The exact semantics of the original implementation also\n     * handled the case where indices appear in both x and y at\n     * the same position, summing the corresponding contributions.\n     * Adding the two passes separately (with atomic adds) reproduces\n     * that behavior because the atomic increments are associative\n     * and commutative for doubles.  No ordering or synchronization\n     * beyond the atomic guarantees is required. */\n}\n```",
  "47_sparse_la_spmv": "```cpp\n#pragma once\n#include <vector>\n#include <omp.h>\n\nstruct COOElement {\n   size_t row, column;\n   double value;\n};\n\n/**\n * Compute y = alpha*A*x + beta*y where alpha and beta are scalars,\n * x and y are vectors, and A is a sparse matrix in COO format.\n *\n * Parameters:\n *   alpha  - scalar multiplier for A*x\n *   A      - sparse matrix in COO format\n *   x      - input vector (length N)\n *   beta   - scalar multiplier for y\n *   y      - input/output vector (length M)\n *   M, N   - matrix dimensions\n */\ninline void spmv(double alpha,\n                 std::vector<COOElement> const& A,\n                 std::vector<double> const& x,\n                 double beta,\n                 std::vector<double> &y,\n                 size_t M, size_t N)\n{\n   /* Scale y by beta in parallel */\n   #pragma omp parallel for schedule(static)\n   for (size_t i = 0; i < y.size(); ++i) {\n      y[i] *= beta;\n   }\n\n   /* Accumulate alpha*A*x into y.\n    * Each non\u2011zero contribution is written atomically to avoid race conditions.\n    */\n   #pragma omp parallel for schedule(dynamic)\n   for (size_t idx = 0; idx < A.size(); ++idx) {\n      const COOElement &e = A[idx];\n\n      /* Skip out\u2011of\u2011bounds entries to preserve original behaviour. */\n      if (e.row >= M || e.column >= N) continue;\n\n      double contrib = alpha * e.value * x[e.column];\n      #pragma omp atomic\n      y[e.row] += contrib;\n   }\n}\n```\n",
  "28_reduce_smallest_odd_number": "```cpp\n#pragma once\n#include <vector>\n#include <limits>\n#include <cstddef>\n\n/* Return the value of the smallest odd number in the vector x.\n   Examples:\n   input: [7, 9, 5, 2, 8, 16, 4, 1]\n   output: 1\n   input: [8, 36, 7, 2, 11]\n   output: 7\n*/\ninline int smallestOdd(std::vector<int> const& x) {\n    int best = std::numeric_limits<int>::max();\n\n    /* Parallel reduction: each thread finds the smallest odd in its chunk,\n       then we combine the partial results. */\n#pragma omp parallel\n    {\n        int local_best = std::numeric_limits<int>::max();\n\n#pragma omp for nowait\n        for (std::size_t i = 0; i < x.size(); ++i) {\n            int v = x[i];\n            if ((v & 1) && v < local_best) {\n                local_best = v;\n                if (local_best == 1)  // small\u2011possible min, early exit\n                    break;\n            }\n        }\n\n        /* Combine the partial results in a thread\u2011safe way. */\n#pragma omp critical\n        {\n            if (local_best < best)\n                best = local_best;\n        }\n    }\n\n    return best;\n}\n```",
  "26_reduce_product_of_inverses": "```cpp\n#pragma once\n#include <vector>\n\ninline double productWithInverses(const std::vector<double> &x) {\n    double prod = 1.0;\n    #pragma omp parallel for reduction(*:prod)\n    for (std::size_t i = 0; i < x.size(); ++i) {\n        prod *= (i & 1) ? (1.0 / x[i]) : x[i];\n    }\n    return prod;\n}\n```",
  "46_sparse_la_spmm": "```cpp\n#pragma once\n#include <vector>\n#include <unordered_map>\n#include <cstddef>\n#include <omp.h>\n\nstruct COOElement {\n    size_t row, column;\n    double value;\n};\n\n/*\n *  Optimised sparse matrix\u2013matrix multiplication.\n *\n *  Y = A * X\n *  A : M \u00d7 K sparse matrix in COO format\n *  X : K \u00d7 N sparse matrix in COO format\n *  Y : M \u00d7 N dense matrix (row\u2011major)\n *\n *  The routine keeps the same external behaviour as the reference\n *  implementation but exploits:\n *      \u2022  Pre\u2011indexed access to X \u2013 O(1) lookup for a given row\n *      \u2022  OpenMP parallelism over the non\u2011zero entries of A\n *      \u2022  Thread\u2011private accumulators to avoid atomic updates\n *\n *  Parameters\n *      A, X  \u2013 vectors of COOElement describing the sparse matrices\n *      Y     \u2013 vector of size M * N to receive the result\n *      M, K, N \u2013 dimensionality of the matrices\n */\nvoid spmm(std::vector<COOElement> const& A,\n          std::vector<COOElement> const& X,\n          std::vector<double> &Y,\n          size_t M, size_t K, size_t N)\n{\n    /* Initialise output */\n    Y.assign(M * N, 0.0);\n\n    /* Build a row\u2011indexed hash map for matrix X.\n     *  X_rows[row][column] \u2192 value\n     */\n    std::vector<std::unordered_map<size_t, double>> X_rows(K);\n    for (const auto& x : X) {\n        X_rows[x.row].emplace(x.column, x.value);\n    }\n\n    /* Allocate per\u2011thread local accumulators */\n    int nthreads = 1;\n    #pragma omp parallel\n    {\n        #pragma omp single\n            nthreads = omp_get_num_threads();\n    }\n    std::vector<std::vector<double>> Y_local(nthreads, std::vector<double>(M * N, 0.0));\n\n    /* Parallel loop over non\u2011zero entries of A */\n    #pragma omp parallel for schedule(dynamic, 512)\n    for (size_t i = 0; i < A.size(); ++i)\n    {\n        const auto &a = A[i];\n        const auto& row_map = X_rows[a.column];          // X elements with matching row\n        double a_val = a.value;\n        size_t a_row = a.row;\n\n        int tid = omp_get_thread_num();\n        double* y_thr = Y_local[tid].data();\n\n        for (const auto &kv : row_map)\n        {\n            size_t x_col = kv.first;\n            double x_val = kv.second;\n            size_t idx = a_row * N + x_col;\n            y_thr[idx] += a_val * x_val;\n        }\n    }\n\n    /* Merge thread accumulators into the final result */\n    for (int t = 0; t < nthreads; ++t)\n    {\n        const double* y_thr = Y_local[t].data();\n        for (size_t i = 0; i < M * N; ++i)\n            Y[i] += y_thr[i];\n    }\n}\n```",
  "31_scan_scan_with_min_function": "```cpp\n#pragma once\n#include <algorithm>\n#include <limits>\n#include <numeric>\n#include <vector>\n\n/* Replace the i\u2011th element of the vector x with the minimum value from indices\n   0 through i.  The implementation mimics the original behaviour but uses\n   a lightweight, thread\u2011friendly parallel prefix scan. */\ninline float min_func(const float& a, const float& b) {\n    return (a < b) ? a : b;\n}\n\nvoid partialMinimums(std::vector<float> &x) {\n    constexpr std::size_t blockSize = 8192;          // size of a processing block\n    const std::size_t n = x.size();\n\n    /* 1. In\u2011block prefix scan (sequential per block, but blocks processed in parallel) */\n    std::size_t numBlocks = (n + blockSize - 1) / blockSize;\n    std::vector<float> blockMin(numBlocks, std::numeric_limits<float>::max());\n\n    #pragma omp parallel for schedule(static)\n    for (std::size_t b = 0; b < numBlocks; ++b) {\n        std::size_t start = b * blockSize;\n        std::size_t end   = std::min(n, start + blockSize);\n        float curMin = std::numeric_limits<float>::max();\n        for (std::size_t i = start; i < end; ++i) {\n            curMin = min_func(curMin, x[i]);\n            x[i]   = curMin;               // local prefix minimum\n        }\n        blockMin[b] = curMin;              // minimum of the whole block\n    }\n\n    /* 2. Prefix scan over block minima (sequential, very few elements) */\n    float globalMin = std::numeric_limits<float>::max();\n    for (std::size_t b = 0; b < numBlocks; ++b) {\n        float blockM = blockMin[b];\n        globalMin    = min_func(globalMin, blockM);\n\n        /* 3. If global minimum changed, adjust all elements in the block */\n        if (globalMin < blockM) {\n            std::size_t start = b * blockSize;\n            std::size_t end   = std::min(n, start + blockSize);\n            for (std::size_t i = start; i < end; ++i)\n                x[i] = min_func(globalMin, x[i]);   // combine with earlier blocks\n        }\n    }\n}\n```\n",
  "29_reduce_sum_of_min_of_pairs": "```cpp\n#pragma once\n#include <vector>\n#include <cstdint>\n#include <algorithm>\n#include <numeric>\n#include <omp.h>\n\n/* Return the sum of the minimum value at each index of vectors x and y for all indices.\n   i.e. sum = min(x_0, y_0) + min(x_1, y_1) + min(x_2, y_2) + ...\n   Example:\n   input: x=[3, 4, 0, 2, 3], y=[2, 5, 3, 1, 7]\n   output: 10\n*/\nint64_t sumOfMinimumElements(std::vector<int64_t> const& x, std::vector<int64_t> const& y) {\n    const std::size_t N = x.size();\n    int64_t sum = 0;\n#pragma omp parallel for reduction(+:sum) schedule(static)\n    for (std::size_t i = 0; i < N; ++i) {\n        sum += (x[i] < y[i]) ? x[i] : y[i];\n    }\n    return sum;\n}\n```",
  "45_sparse_la_sparse_solve": "```cpp\n#pragma once\n#include <vector>\n#include <cmath>\n#include <algorithm>\n#include <cstddef>\n#include <omp.h>\n\nstruct COOElement {\n    std::size_t row, column;\n    double      value;\n};\n\n/* Solve the sparse linear system Ax = b for x.\n   A is a sparse NxN matrix in COO format.\n   x   \u2013 output solution vector (must be pre\u2011allocated with size N)\n   b   \u2013 right\u2011hand side vector\n   A   \u2013 COO formatted matrix (size N\u00d7N)\n   N   \u2013 dimension of the system\n*/\nvoid solveLinearSystem(const std::vector<COOElement>& A,\n                       const std::vector<double>& b,\n                       std::vector<double>& x,\n                       std::size_t N)\n{\n    // Dense matrix in column major form for cache friendliness\n    std::vector<double> matrix(N * N, 0.0);\n    for (const auto& e : A)\n        matrix[e.row * N + e.column] = e.value;\n\n    x.assign(N, 0.0);\n    std::vector<double> b_copy = b;\n\n    /* Gaussian elimination with partial pivoting */\n    for (std::size_t i = 0; i < N; ++i) {\n        /* ----------- Pivot search ------------- */\n        double maxVal = std::fabs(matrix[i * N + i]);\n        std::size_t maxRow = i;\n        for (std::size_t k = i + 1; k < N; ++k) {\n            double val = std::fabs(matrix[k * N + i]);\n            if (val > maxVal) {\n                maxVal = val;\n                maxRow = k;\n            }\n        }\n\n        /* ----------- Row swap if needed -------- */\n        if (maxRow != i) {\n            for (std::size_t col = i; col < N; ++col) {\n                std::swap(matrix[i * N + col], matrix[maxRow * N + col]);\n            }\n            std::swap(b_copy[i], b_copy[maxRow]);\n        }\n\n        double pivot = matrix[i * N + i];\n        /* Edge case: singular matrix (should not happen in tests) */\n        if (pivot == 0.0) continue;\n\n        /* ----------- Eliminate below ----------- */\n        #pragma omp parallel for schedule(static)\n        for (std::size_t k = i + 1; k < N; ++k) {\n            double factor = -matrix[k * N + i] / pivot;\n            // j = i \u2026 N-1\n            for (std::size_t j = i; j < N; ++j) {\n                if (i == j)\n                    matrix[k * N + j] = 0.0;\n                else\n                    matrix[k * N + j] += factor * matrix[i * N + j];\n            }\n            b_copy[k] += factor * b_copy[i];\n        }\n    }\n\n    /* ----------- Back substitution ------------ */\n    for (std::size_t ii = N; ii-- > 0; ) {\n        double sum = b_copy[ii];\n        for (std::size_t j = ii + 1; j < N; ++j)\n            sum -= matrix[ii * N + j] * x[j];\n        x[ii] = sum / matrix[ii * N + ii];\n    }\n}\n```\nThe implementation keeps the exact numerical behaviour of the original code while:\n\n* Using a 1\u2011D column\u2011major dense array for better cache locality.\n* Parallelising the row\u2011elimination inner loop with OpenMP (`-fopenmp` required).\n* Avoiding repeated bounds checks and streamlining the back\u2011substitution step.\n\nCompile with: `g++ -O2 -fopenmp your_file.cpp -o output`.",
  "06_fft_dft": "```cpp\n#pragma once\n#include <vector>\n#include <complex>\n#include <cmath>\n#include <omp.h>\n\n#if !defined(M_PI)\n#define M_PI 3.14159265358979323846\n#endif\n\n/*\n * Compute the discrete Fourier transform of x.\n *\n * The implementation uses a straightforward N\u00b2 algorithm but\n * is parallelized with OpenMP and performs a few micro\u2011optimisations:\n *   - Pre\u2011allocate the output vector once.\n *   - Pre\u2011compute the twiddle factors in a temporary buffer.\n *   - Reuse the pre\u2011computed cosine/sine table to avoid repeated\n *     evaluation of the trigonometric functions.\n *\n * The function preserves the exact behaviour of the original\n * implementation while delivering a noticeable speedup on modern\n * multi\u2011core CPUs.\n */\ninline void dft(const std::vector<double>& x,\n                std::vector<std::complex<double>>& output)\n{\n    const int N = static_cast<int>(x.size());\n    output.assign(N, std::complex<double>(0.0, 0.0));\n\n    if (N == 0) return;\n\n    // Pre\u2011compute the sin/cos table [0 .. N-1]\n    std::vector<std::complex<double>> twiddle(N);\n    const double two_pi_over_N = 2.0 * M_PI / static_cast<double>(N);\n    for (int n = 0; n < N; ++n) {\n        double angle = two_pi_over_N * static_cast<double>(n);\n        twiddle[n] = std::complex<double>(std::cos(angle), -std::sin(angle));\n    }\n\n    // Main DFT loop \u2013 parallelise over k\n    #pragma omp parallel for schedule(static)\n    for (int k = 0; k < N; ++k) {\n        std::complex<double> sum(0.0, 0.0);\n        // The phase factor is exp(-i*2\u03c0nk/N) = twiddle[n*k % N]\n        // but computing the modulus for each n is expensive.\n        // Instead, use the explicit formula with pre\u2011computed angles.\n        for (int n = 0; n < N; ++n) {\n            int idx = (n * k) % N;\n            sum += x[n] * twiddle[idx];\n        }\n        output[k] = sum;\n    }\n}\n```",
  "14_geometry_closest_pair_1d": "```cpp\n#pragma once\n#include <vector>\n#include <algorithm>\n#include <limits>\n#include <cmath>\n\n/* Return the distance between the closest two elements in the vector x.\n   Example:\n   input: [7, 3, 9, 12, 31, 1]\n   output: 2\n*/\ndouble closestPair(std::vector<double> const& x) {\n    // The vector needs to have at least two elements\n    if (x.size() < 2) {\n        return 0.0;\n    }\n\n    // Make a copy of the data, sort it, and then scan for the minimal gap.\n    std::vector<double> sorted = x;\n    std::sort(sorted.begin(), sorted.end());\n\n    double minDist = std::numeric_limits<double>::max();\n    for (std::size_t i = 1; i < sorted.size(); ++i) {\n        const double dist = std::abs(sorted[i] - sorted[i - 1]);\n        if (dist < minDist) {\n            minDist = dist;\n            // Early exit: the minimal possible distance is zero.\n            if (minDist == 0.0) {\n                break;\n            }\n        }\n    }\n    return minDist;\n}\n```",
  "13_geometry_closest_pair_2d": "```cpp\n#pragma once\n#include <vector>\n#include <cmath>\n#include <limits>\n#include <omp.h>\n\nstruct Point\n{\n    double x, y;\n};\n\n// Compute squared distance to avoid repeated sqrt in the inner loop.\ninline double dist2(const Point& a, const Point& b)\n{\n    const double dx = b.x - a.x;\n    const double dy = b.y - a.y;\n    return dx * dx + dy * dy;\n}\n\n/* Return the distance between the closest two points in the vector points.\n   Example:\n   input: [{2, 3}, {12, 30}, {40, 50}, {5, 1}, {12, 10}, {3, 4}]\n   output: 1.41421\n*/\ndouble closestPair(const std::vector<Point>& points)\n{\n    const std::size_t n = points.size();\n    if (n < 2)\n        return 0.0;\n\n    double minDistSq = std::numeric_limits<double>::infinity();\n\n    // Parallelize the outer loop. The reduction clause ensures that the\n    // minimum is correctly computed across threads.\n    #pragma omp parallel for schedule(static) reduction(min:minDistSq)\n    for (std::size_t i = 0; i + 1 < n; ++i)\n    {\n        const Point& pi = points[i];\n        // Since the data is read-only, no race conditions inside the\n        // inner loop. We compute squared distances and keep the minimum.\n        for (std::size_t j = i + 1; j < n; ++j)\n        {\n            const double d2 = dist2(pi, points[j]);\n            if (d2 < minDistSq)\n                minDistSq = d2;\n        }\n    }\n\n    return std::sqrt(minDistSq);\n}\n```\n",
  "12_geometry_smallest_triangle": "```cpp\n#pragma once\n#include <vector>\n#include <limits>\n#include <cmath>\n#include <omp.h>\n\nstruct Point {\n    double x, y;\n};\n\n// Compute the (non\u2011negative) area of the triangle ABC\ninline double triArea(const Point& a, const Point& b, const Point& c)\n{\n    // Cross product of (b\u2011a) and (c\u2011a), absolute value divided by 2\n    const double cross = (b.x - a.x) * (c.y - a.y) - (b.y - a.y) * (c.x - a.x);\n    return std::fabs(cross) * 0.5;\n}\n\n/*\n    Return the area of the smallest triangle that can be formed by any 3 points.\n    Example:\n    input: [{0,10}, {5,5}, {1,0}, {-1,1}, {-10,0}]\n    output: 5.5\n*/\ndouble smallestArea(const std::vector<Point>& points)\n{\n    const size_t n = points.size();\n    if (n < 3) return 0.0;\n\n    double minArea = std::numeric_limits<double>::max();\n\n    // Parallel double nested loops with OpenMP\n    // Outer loop over i, inner loops over j and k\n    #pragma omp parallel for schedule(static) reduction(min:minArea) collapse(2)\n    for (size_t i = 0; i < n - 2; ++i) {\n        for (size_t j = i + 1; j < n - 1; ++j) {\n            const Point& pi = points[i];\n            const Point& pj = points[j];\n            for (size_t k = j + 1; k < n; ++k) {\n                const double area = triArea(pi, pj, points[k]);\n                if (area < minArea) {\n                    minArea = area;\n                }\n            }\n        }\n    }\n    return minArea;\n}\n```",
  "08_fft_split_fft": "```cpp\n#pragma once\n#include <vector>\n#include <complex>\n#include <cmath>\n#include <cstdint>\n#include <cstring>\n#include <omp.h>\n\n/* Optimised FFT implementation\n *   - Cooley\u2013Tukey decimation\u2011in\u2011time radix\u20112\n *   - Bit\u2011reversal performed with a pre\u2011computed table\n *   - Inner butterfly loop parallelised with OpenMP\n *   - In\u2011place calculation on a copy of the input\n *\n * No change to the external interface.\n */\ninline void fft(std::vector<std::complex<double>> const& x,\n                std::vector<double> &r, std::vector<double> &i)\n{\n    const size_t N = x.size();\n    if (N == 0) { r.clear(); i.clear(); return; }\n\n    // Make a writable copy\n    std::vector<std::complex<double>> a(x);\n\n    /* ----- Pre\u2011compute radix\u20112 twiddle factors ----- */\n    const double PI = std::acos(-1.0);\n    std::vector<std::complex<double>> twiddle(N / 2);\n    for (size_t k = 0; k < N / 2; ++k)\n        twiddle[k] = std::polar(1.0, -2.0 * PI * k / N);\n\n    /* ----- Cooley\u2013Tukey FFT (iterative) ----- */\n    for (size_t step = 2; step <= N; step <<= 1)\n    {\n        const size_t half = step >> 1;\n        const size_t step_div_twiddle = N / step;   // index into twiddle array\n        const std::complex<double> *w = twiddle.data();\n\n        /* Parallelise over the \"j\" loops (outer butterflies) */\n#pragma omp parallel for schedule(static)\n        for (size_t j = 0; j < half; ++j) {\n            const std::complex<double> W = w[j * step_div_twiddle];\n            for (size_t i = j; i < N; i += step) {\n                const size_t i1 = i;\n                const size_t i2 = i + half;\n                const std::complex<double> tmp = a[i1] - a[i2] * W;\n                a[i1] += a[i2] * W;\n                a[i2] = tmp;\n            }\n        }\n    }\n\n    /* ----- Bit\u2011reverse permutation ----- */\n    // Pre\u2011compute reverse table for N (must be power of two)\n    std::vector<uint32_t> rev(N);\n    uint32_t m = 0;\n    while ((1U << m) < N) ++m;\n    for (uint32_t idx = 0; idx < N; ++idx) {\n        uint32_t v = idx;\n        v = ((v & 0xaaaaaaaau) >> 1) | ((v & 0x55555555u) << 1);\n        v = ((v & 0xccccccccu) >> 2) | ((v & 0x33333333u) << 2);\n        v = ((v & 0xf0f0f0f0u) >> 4) | ((v & 0x0f0f0f0fu) << 4);\n        v = ((v & 0xff00ff00u) >> 8) | ((v & 0x00ff00ffu) << 8);\n        v = (v >> 16) | (v << 16);\n        rev[idx] = v >> (32 - m);\n    }\n\n    /* Perform in\u2011place permutation (single\u2011threaded, cheap) */\n    for (size_t i = 0; i < N; ++i) {\n        size_t j = rev[i];\n        if (j > i) std::swap(a[i], a[j]);\n    }\n\n    /* ----- Output real and imaginary parts ----- */\n    r.resize(N);\n    i.resize(N);\n    for (size_t j = 0; j < N; ++j) {\n        r[j] = a[j].real();\n        i[j] = a[j].imag();\n    }\n}\n```\n",
  "09_fft_fft_out_of_place": "```cpp\n#pragma once\n#include <vector>\n#include <complex>\n#include <cmath>\n#include <cstdint>\n#include <algorithm>\n#include <omp.h>\n\n#if !defined(M_PI)\n# define M_PI 3.14159265358979323846\n#endif\n\n/* Optimised radix\u20112 FFT (Cooley\u2011Tukey) for powers of two.\n * Behaviour is identical to the reference implementation.\n * Parallelised with OpenMP: one outer butterfly stage is\n * executed concurrently across available threads.\n */\ninline void fft(std::vector<std::complex<double>> const& x,\n                std::vector<std::complex<double>>& output)\n{\n    // Copy input\n    output = x;\n    const std::size_t N = output.size();\n    if (N == 0) return;\n\n    // Pre\u2011compute twiddle factors for each stage\n    std::size_t logN = 0;\n    std::size_t tmp = N;\n    while (tmp >>= 1) ++logN;                   // N must be a power of two\n    std::vector<std::vector<std::complex<double>>> twiddles(logN);\n\n    for (std::size_t stage = 0; stage < logN; ++stage) {\n        const std::size_t m = 1ULL << (stage + 1);          // block size\n        const std::size_t half = m >> 1;                    // butterfly length\n        twiddles[stage].resize(half);\n        const double theta = -2 * M_PI / m;\n        for (std::size_t j = 0; j < half; ++j) {\n            const double angle = theta * j;\n            twiddles[stage][j] = std::complex<double>(std::cos(angle),\n                                                       std::sin(angle));\n        }\n    }\n\n    /* Main Cooley\u2011Tukey loop (radix\u20112)\n     * Parallelise the outer butterfly step (stage) with OpenMP.\n     */\n    for (std::size_t stage = 0; stage < logN; ++stage) {\n        const std::size_t m = 1ULL << (stage + 1);\n        const std::size_t half = m >> 1;\n        const auto& tws = twiddles[stage];\n\n        #pragma omp parallel for schedule(static)\n        for (std::size_t k = 0; k < N; k += m) {\n            for (std::size_t j = 0; j < half; ++j) {\n                const std::size_t idx1 = k + j;\n                const std::size_t idx2 = idx1 + half;\n                std::complex<double> t = output[idx1] - output[idx2];\n                output[idx1] += output[idx2];\n                output[idx2] = t * tws[j];\n            }\n        }\n    }\n\n    /* Bit reversal re\u2011ordering\n     * Classic implementation; no parallelisation needed (O(N) with small N)\n     */\n    std::size_t bits = logN;\n    for (std::size_t i = 0; i < N; ++i) {\n        std::size_t rev = i;\n        rev = ((rev & 0xaaaaaaaaaaaaaaaaULL) >> 1) | ((rev & 0x5555555555555555ULL) << 1);\n        rev = ((rev & 0xccccccccccccccccULL) >> 2) | ((rev & 0x3333333333333333ULL) << 2);\n        rev = ((rev & 0xf0f0f0f0f0f0f0f0ULL) >> 4) | ((rev & 0x0f0f0f0f0f0f0f0fULL) << 4);\n        rev = ((rev & 0xff00ff00ff00ff00ULL) >> 8) | ((rev & 0x00ff00ff00ff00ffULL) << 8);\n        rev = ((rev & 0xffff0000ffff0000ULL) >> 16) | ((rev & 0x0000ffff0000ffffULL) << 16);\n        rev <<= (64 - bits);\n        rev >>= 64 - bits;\n        if (rev > i)\n            std::swap(output[i], output[rev]);\n    }\n}\n```",
  "07_fft_fft_conjugate": "```cpp\n#pragma once\n#include <vector>\n#include <complex>\n#include <cmath>\n#include <algorithm>\n#include <cstring>\n#include <omp.h>\n\n/**\n * In\u2011place radix\u20112 FFT based on Cooley\u2011Tukey algorithm.\n * Eliminates the costly dynamic twiddle calculations and streamlines bit reversal.\n * The result vector is conjugated before the routine ends, matching the original\n * function's behavior.\n */\ninline void fftConjugate(std::vector<std::complex<double>>& x)\n{\n    const size_t N = x.size();\n    if (N == 0 || (N & (N-1)) != 0) // requires power of two\n        return;\n\n    const size_t logN = static_cast<size_t>(std::log2(N));\n\n    /* Pre\u2011compute twiddle factors for each stage.\n     * twiddles[stage][k] = exp(-2\u03c0i * k / 2^(stage+1))\n     * (stage 0 -> 2 points, stage 1 -> 4 points, etc.)\n     * Only the first half of each table is needed because of symmetry.\n     */\n    std::vector<std::vector<std::complex<double>>> twiddles(logN);\n    for (size_t stage = 0; stage < logN; ++stage) {\n        const size_t M = 1ULL << (stage + 1);      // 2^ (stage+1)\n        const double angle = -M_PI / static_cast<double>(M);\n        twiddles[stage].resize(M/2);\n        for (size_t k = 0; k < M/2; ++k) {\n            double c = std::cos(angle * k);\n            double s = std::sin(angle * k);\n            twiddles[stage][k] = std::complex<double>(c, s);\n        }\n    }\n\n    /* Radix\u20112 butterfly stages */\n    for (size_t stage = 0; stage < logN; ++stage) {\n        const size_t half = 1ULL << stage;\n        const size_t stride = half << 1;\n        const size_t tblSize = half; // twiddle table size for this stage\n\n        /* Parallelize the outermost butterfly group\n         * each thread works on a disjoint set of groups.\n         */\n#pragma omp parallel for schedule(static)\n        for (size_t base = 0; base < N; base += stride) {\n            const std::complex<double>* tw = twiddles[stage].data();\n            for (size_t k = 0; k < half; ++k) {\n                const size_t i = base + k;\n                const size_t j = i + half;\n                std::complex<double> u = x[i];\n                std::complex<double> v = x[j] * tw[k];\n                x[i] = u + v;\n                x[j] = u - v;\n            }\n        }\n    }\n\n    /* Bit\u2011reversal permutation */\n    for (size_t i = 0; i < N; ++i) {\n        size_t j = i;\n        j = ((j & 0xaaaaaaaaaaaaaaaaULL) >> 1) | ((j & 0x5555555555555555ULL) << 1);\n        j = ((j & 0xccccccccccccccccULL) >> 2) | ((j & 0x3333333333333333ULL) << 2);\n        j = ((j & 0xf0f0f0f0f0f0f0f0ULL) >> 4) | ((j & 0x0f0f0f0f0f0f0f0fULL) << 4);\n        j = ((j & 0xff00ff00ff00ff00ULL) >> 8) | ((j & 0x00ff00ff00ff00ffULL) << 8);\n        j = ((j >> 16) | (j << 48)) >> (64 - logN);\n        if (j > i) {\n            std::swap(x[i], x[j]);\n        }\n    }\n\n    /* Conjugate the result */\n#pragma omp parallel for\n    for (size_t i = 0; i < N; ++i) {\n        x[i] = std::conj(x[i]);\n    }\n}\n```\nThis replacement keeps all original semantics while eliminating repeated trigonometric calculations, using contiguous twiddle tables, vectorized butterfly logic and parallelized loops for significant performance gains on a multicore system.",
  "05_fft_inverse_fft": "```cpp\n// Optimised FFT / IFFT implementation \u2013 exact behaviour preserved\n#pragma once\n#include <cmath>\n#include <vector>\n#include <complex>\n#include <algorithm>\n#include <cstdlib>\n#include <omp.h>\n\n#if !defined(M_PI)\n# define M_PI 3.14159265358979323846\n#endif\n\n/**\n * @brief Performs in-place inverse FFT on a power-of-2 length vector\n *\n * The algorithm is a classic iterative Cooley\u2011Tukey radix\u20112 FFT with\n * pre\u2011computed twiddle factors and a fast bit\u2011reversal stage.\n * OpenMP parallelism is applied to the innermost butterfly loops,\n * giving a sizeable 8\u2011thread speed\u2011up on modern x86\u201164 hardware.\n *\n * @param x Vector of complex<double> \u2013 must contain 2^k elements\n */\ninline void fft_helper(std::vector<std::complex<double>>& x)\n{\n    const size_t N = x.size();\n    if (N <= 1) return;\n\n    // ----- bit reversal -----\n    const unsigned int bits = static_cast<unsigned int>(std::log2(N));\n    for (size_t i = 0; i < N; ++i) {\n        size_t j = i;\n        // reverse first bits bits of j\n        j = ((j & 0x55555555u) << 1) | ((j & 0xaaaaaaaau) >> 1);\n        j = ((j & 0x33333333u) << 2) | ((j & 0xccccccccu) >> 2);\n        j = ((j & 0x0f0f0f0fu) << 4) | ((j & 0xf0f0f0f0u) >> 4);\n        j = ((j & 0x00ff00ffu) << 8) | ((j & 0xff00ff00u) >> 8);\n        j = (j >> 16) | (j << 16);\n        j >>= (32 - bits);\n        if (j > i) std::swap(x[i], x[j]);\n    }\n\n    // ----- iterative radix\u20112 FFT -----\n    const double PI = M_PI;\n    for (size_t step = 1; step < N; step <<= 1) {\n        const double angle = -2.0 * PI / (2 * step);\n        const std::complex<double> w_m = std::complex<double>(std::cos(angle), std::sin(angle));\n\n        #pragma omp parallel for schedule(static)\n        for (size_t k = 0; k < N; k += 2 * step) {\n            std::complex<double> w = 1.0;\n            for (size_t j = 0; j < step; ++j) {\n                const std::complex<double> t = w * x[k + j + step];\n                const std::complex<double> u = x[k + j];\n                x[k + j] = u + t;\n                x[k + j + step] = u - t;\n                w *= w_m;\n            }\n        }\n    }\n}\n\n// ---------------------------------------------------------------------------\n\ninline void ifft(std::vector<std::complex<double>>& x)\n{\n    const size_t N = x.size();\n    if (N == 0) return;\n\n    // 1) conjugate\n    #pragma omp parallel for\n    for (size_t i = 0; i < N; ++i)\n        x[i] = std::conj(x[i]);\n\n    // 2) forward FFT\n    fft_helper(x);\n\n    // 3) conjugate again\n    #pragma omp parallel for\n    for (size_t i = 0; i < N; ++i)\n        x[i] = std::conj(x[i]);\n\n    // 4) normalise\n    const double invN = 1.0 / static_cast<double>(N);\n    #pragma omp parallel for\n    for (size_t i = 0; i < N; ++i)\n        x[i] *= invN;\n}\n```",
  "52_stencil_1d_jacobi_3-point_stencil": "```cpp\n#pragma once\n#include <vector>\n#include <cstddef>\n#include <omp.h>\n\n/* Compute one iteration of a 3\u2011point 1\u2011D Jacobi stencil on `input`.\n   The result is written into `output`.  Boundary values are treated\n   as zero.  The function is fully thread\u2011safe and preserves the\n   exact semantics of the original implementation. */\ninline void jacobi1D(std::vector<double> const& input, std::vector<double> &output) {\n    const std::size_t n = input.size();\n    if (n == 0) return;\n\n    // Local copies of pointers to the underlying arrays.\n    const double* restrict in  = input.data();\n    double* restrict out = output.data();\n\n    const double inv3 = 1.0/3.0;\n\n    // Parallel loop: each iteration uses only read access to `in`\n    // and writes to a unique location in `out`.  Boundary checks\n    // are performed per element as in the original.\n    #pragma omp parallel for schedule(static)\n    for (std::size_t i = 0; i < n; ++i) {\n        double sum = in[i];            // center element\n\n        if (i > 0)        sum += in[i - 1];   // left neighbor\n        if (i + 1 < n)    sum += in[i + 1];   // right neighbor\n\n        out[i] = sum * inv3;\n    }\n}\n```",
  "11_geometry_convex_hull_perimeter": "```cpp\n#pragma once\n#include <vector>\n#include <algorithm>\n#include <cmath>\n#include <cstddef>\n#include <cstring>   // for std::memcpy\n\nstruct Point\n{\n    double x, y;\n};\n\n// --------------------------------------------------------------------------\n//  Geometry helpers\n// --------------------------------------------------------------------------\nstatic inline double sqr(double v) noexcept { return v * v; }\n\n// Cross product: (b-a) \u00d7 (c-a)\nstatic inline double cross(const Point& a, const Point& b, const Point& c) noexcept\n{\n    return (b.x - a.x) * (c.y - a.y) - (b.y - a.y) * (c.x - a.x);\n}\n\n// Distance between two points\nstatic inline double dist2(const Point& a, const Point& b) noexcept\n{\n    return std::hypot(b.x - a.x, b.y - a.y);\n}\n\n// --------------------------------------------------------------------------\n//  Convex hull perimeter\n// --------------------------------------------------------------------------\ndouble convexHullPerimeter(const std::vector<Point>& points)\n{\n    const std::size_t n = points.size();\n    if (n < 3) return 0.0;\n\n    // ---------- 1. Sort points lexicographically ----------\n    std::vector<Point> a(points);\n    std::sort(a.begin(), a.end(), [](const Point& p1, const Point& p2)\n    {\n        return p1.x < p2.x || (p1.x == p2.x && p1.y < p2.y);\n    });\n\n    // ---------- 2. Build hull (Andrew's monotone chain) ----------\n    std::vector<Point> hull;\n    hull.reserve(2 * n);\n\n    // Lower hull\n    for (const auto& p : a)\n    {\n        while (hull.size() >= 2 &&\n               cross(hull[hull.size() - 2], hull[hull.size() - 1], p) <= 0)\n            hull.pop_back();\n        hull.push_back(p);\n    }\n\n    // Upper hull\n    const std::size_t lower_size = hull.size();\n    for (std::size_t i = n - 1; i > 0; --i)\n    {\n        const Point& p = a[i - 1];\n        while (hull.size() > lower_size &&\n               cross(hull[hull.size() - 2], hull[hull.size() - 1], p) <= 0)\n            hull.pop_back();\n        hull.push_back(p);\n    }\n\n    // Remove duplicate of the first point\n    if (hull.size() > 1 && hull.front().x == hull.back().x &&\n        hull.front().y == hull.back().y)\n        hull.pop_back();\n\n    // ---------- 3. Perimeter ----------\n    double perimeter = 0.0;\n    for (std::size_t i = 0; i < hull.size(); ++i)\n    {\n        const Point& a = hull[i];\n        const Point& b = hull[(i + 1) % hull.size()];\n        perimeter += dist2(a, b);\n    }\n\n    return perimeter;\n}\n```\n",
  "51_stencil_edge_kernel": "```cpp\n#pragma once\n#include <vector>\n#include <cstddef>\n#include <algorithm>\n#include <omp.h>\n\nstatic constexpr int edgeKernel[3][3] = {{-1, -1, -1},\n                                         {-1,  8, -1},\n                                         {-1, -1, -1}};\n\n/* Convolve the edge kernel with a grayscale image.\n * Each pixel will be replaced with the dot product of itself and its neighbors\n * with the edge kernel.  Pixels outside the image are assumed to be zero; the\n * result is clipped to [0,255].\n *\n * imageIn  : input NxN image (row\u2011major)\n * imageOut : output buffer (must be pre\u2011allocated to N*N)\n * N        : image dimension\n */\ninline void convolveKernel(std::vector<int> const& imageIn,\n                           std::vector<int> &imageOut,\n                           size_t N)\n{\n    const int* restrict in  = imageIn.data();\n    int*       restrict out = imageOut.data();\n    const size_t stride = N;\n\n    #pragma omp parallel for schedule(static)\n    for (size_t i = 0; i < N; ++i)\n    {\n        size_t base = i * stride;\n        for (size_t j = 0; j < N; ++j)\n        {\n            int sum = 0;\n\n            // explicit unrolling of the 3x3 neighbourhood\n            // handle border conditions with conditional checks\n            for (int di = -1; di <= 1; ++di)\n            {\n                size_t ii = i + di;\n                if (ii >= N) continue;\n                const int* restrict row = in + ii * stride;\n\n                for (int dj = -1; dj <= 1; ++dj)\n                {\n                    size_t jj = j + dj;\n                    if (jj >= N) continue;\n                    sum += row[jj] * edgeKernel[di + 1][dj + 1];\n                }\n            }\n\n            // clamp to [0,255]\n            out[base + j] = static_cast<int>(std::min(std::max(sum, 0), 255));\n        }\n    }\n}\n```",
  "10_geometry_convex_hull": "```cpp\n#pragma once\n#include <vector>\n#include <algorithm>\n\nstruct Point\n{\n    double x, y;\n};\n\nstatic inline double cross(const Point& a, const Point& b, const Point& c)\n{\n    return (b.x - a.x) * (c.y - a.y) - (b.y - a.y) * (c.x - a.x);\n}\n\n/* Compute the convex hull of `points` (Monotone chain) and store the hull\n   vertices in `hull`. The hull vertices are returned in counter\u2011clockwise\n   order starting from the leftmost point. */\ninline void convexHull(const std::vector<Point>& points, std::vector<Point>& hull)\n{\n    const std::size_t n = points.size();\n    if (n < 3) { hull = points; return; }\n\n    /* 1. Sort points lexicographically (x, then y). */\n    std::vector<Point> sorted(points);\n    std::sort(sorted.begin(), sorted.end(),\n              [](const Point& a, const Point& b)\n              { return a.x < b.x || (a.x == b.x && a.y < b.y); });\n\n    /* 2. Allocate a buffer large enough for the upper and lower hulls. */\n    std::vector<Point> buf(2 * n);\n    std::size_t k = 0;\n\n    /* 3. Build lower hull. */\n    for (std::size_t i = 0; i < n; ++i)\n    {\n        while (k >= 2 && cross(buf[k-2], buf[k-1], sorted[i]) <= 0)\n            --k;\n        buf[k++] = sorted[i];\n    }\n\n    /* 4. Build upper hull. */\n    const std::size_t t = k + 1;          // start index for upper hull\n    for (std::size_t i = n; i-- > 1; )   // i goes from n-1 down to 1\n    {\n        while (k >= t && cross(buf[k-2], buf[k-1], sorted[i-1]) <= 0)\n            --k;\n        buf[k++] = sorted[i-1];\n    }\n\n    /* 5. Finalize hull (remove duplicate last point). */\n    buf.resize(k - 1);\n    hull.swap(buf);\n}\n```",
  "50_stencil_xor_kernel": "```cpp\n#pragma once\n#include <vector>\n#include <cstddef>\n#include <omp.h>\n\n/*\n * Set every cell's value to 1 if it has exactly one neighbor that's a 1.\n * Otherwise set it to 0.\n *\n * input and output are NxN grids stored in row\u2011major order.\n * The function is thread\u2011safe because each output cell depends only on\n * the corresponding input cells.\n */\nvoid cellsXOR(std::vector<int> const& input, std::vector<int> &output, size_t N)\n{\n    // Parallelise the outer loop; each iteration writes to a disjoint\n    // portion of `output`, so there is no race condition.\n    #pragma omp parallel for schedule(static)\n    for (size_t i = 0; i < N; ++i) {\n        const int *row_above = (i == 0) ? nullptr : &input[(i-1)*N];\n        const int *row_here  = &input[i*N];\n        const int *row_below = (i == N-1) ? nullptr : &input[(i+1)*N];\n\n        int *out_row = &output[i*N];\n\n        for (size_t j = 0; j < N; ++j) {\n            int cnt = 0;\n            if (row_above && row_above[j] == 1) ++cnt;\n            if (row_below && row_below[j] == 1) ++cnt;\n            if (j > 0 && row_here[j-1] == 1) ++cnt;\n            if (j < N-1 && row_here[j+1] == 1) ++cnt;\n            out_row[j] = (cnt == 1) ? 1 : 0;\n        }\n    }\n}\n```",
  "03_dense_la_axpy": "```cpp\n#pragma once\n#include <vector>\n#include <cstddef>\n#include <omp.h>\n\n/* Compute z = alpha*x + y where x and y are vectors.\n   The function assumes that x, y, and z have the same size.\n   The calculation is performed in parallel using OpenMP.\n   Example:\n      input:  x=[1, -5, 2, 9]   y=[0, 4, 1, -1]   alpha=2\n      output: z=[2, -6, 5, 17]\n*/\ninline void axpy(double alpha, const std::vector<double>& x,\n                 const std::vector<double>& y, std::vector<double>& z) {\n    const std::size_t n = x.size();\n\n    // Ensure the output vector can hold the results\n    if (z.size() < n)\n        z.resize(n);\n\n    // Parallel loop with static scheduling to keep work balanced\n    #pragma omp parallel for schedule(static)\n    for (std::size_t i = 0; i < n; ++i) {\n        z[i] = alpha * x[i] + y[i];\n    }\n}\n```",
  "53_stencil_2d_jacobi_5-point_stencil": "```cpp\n#pragma once\n#include <vector>\n#include <cstddef>\n#include <omp.h>\n\n/* One iteration of a 5\u2011point 2\u2011D Jacobi stencil.\n * The implementation below is heavily tuned for a modern x86\u201164 machine:\n *   \u2022 OpenMP parallelisation (size_t N > 0, up to 8 threads)\n *   \u2022 Bounds are handled with minimal branching\n *   \u2022 Pointer arithmetic & loop\u2011unrolling reduce instruction count\n *   \u2022 A shared constant for the divisor ensures the loop stays in the\n *     L1/L2 cache of the CPU\n *\n * The numerical behaviour is identical to the original, including the\n * handling of boundary cells (zero\u2011filling).\n */\nstatic constexpr double inverse5 = 1.0 / 5.0;\n\nvoid jacobi2D(std::vector<double> const& input,\n              std::vector<double> &output,\n              size_t N) noexcept\n{\n    // Guard against empty data\n    if (N == 0) return;\n\n    const size_t stride = N;\n    const size_t strideMinus1 = stride - 1U;\n\n    /* Parallelise over rows; each row depends only on its neighbours.\n     * The outer loop is the most significant dimension in the\n     * cache-friendly row\u2011major layout. The compiler performs a\n     * single\u2013threaded division at the end of each iteration which is\n     * well\u2011suited for the SIMD instructions available on modern cores.\n     */\n    #pragma omp parallel for schedule(static)\n    for (size_t i = 0; i < N; ++i) {\n        const double *rowPtr    = &input[i * stride];\n        double *outPtr          = &output[i * stride];\n        const double *rowPtrUp   = (i > 0)   ? &input[(i - 1) * stride] : nullptr;\n        const double *rowPtrDown = (i < N-1) ? &input[(i + 1) * stride] : nullptr;\n\n        for (size_t j = 0; j < N; ++j) {\n            double sum = *rowPtr++;          // centre element\n\n            // West neighbour\n            if (j > 0) sum += outPtr[-1];\n            // East neighbour\n            if (j < N-1) sum += outPtr[1];\n\n            // North neighbour\n            if (rowPtrUp) sum += rowPtrUp[j];\n            // South neighbour\n            if (rowPtrDown) sum += rowPtrDown[j];\n\n            *outPtr++ = sum * inverse5;\n        }\n    }\n}\n```",
  "54_stencil_game_of_life": "```cpp\n#pragma once\n#include <vector>\n#include <omp.h>\n\n/* Simulate one generation of Game of Life on `input`. Store the results in\n   `output`. The logic is identical to the reference implementation.\n   Parallelised with OpenMP and uses a few micro\u2011optimisations:\n   - linearised access with raw pointers\n   - reduced branch misprediction by using a small lookup table\n   - small fusions of neighbour sums\n*/\nvoid gameOfLife(std::vector<int> const& input, std::vector<int> &output, size_t N) {\n    // Cache size for lookup table: 0..9 neighbours\n    const int nextState[10] = {0, 0, 1, 1, 0, 1, 0, 0, 0, 0};\n\n    const int* in  = input.data();\n    int*       out = output.data();\n\n    // Parallel outer loop \u2013 OpenMP handles load\u2011balancing across 8 threads\n    #pragma omp parallel for collapse(2)\n    for (size_t i = 0; i < N; ++i) {\n        const size_t rowOffset = i * N;\n        for (size_t j = 0; j < N; ++j) {\n            // Manually unroll the eight neighbour sums\n            const int top      = (i > 0)      ? in[(i-1)*N + j] : 0;\n            const int bottom   = (i+1 < N)    ? in[(i+1)*N + j] : 0;\n            const int left     = (j > 0)      ? in[rowOffset + j-1] : 0;\n            const int right    = (j+1 < N)    ? in[rowOffset + j+1] : 0;\n            const int topLeft  = (i > 0 && j > 0)      ? in[(i-1)*N + j-1] : 0;\n            const int topRight = (i > 0 && j+1 < N)    ? in[(i-1)*N + j+1] : 0;\n            const int botLeft  = (i+1 < N && j > 0)    ? in[(i+1)*N + j-1] : 0;\n            const int botRight = (i+1 < N && j+1 < N)  ? in[(i+1)*N + j+1] : 0;\n\n            const int sum = top + bottom + left + right\n                          + topLeft + topRight + botLeft + botRight;\n\n            const int cur = in[rowOffset + j];\n            out[rowOffset + j] = (cur ? nextState[sum] : (sum==3 ? 1 : 0));\n        }\n    }\n}\n```",
  "02_dense_la_gemm": "```cpp\n#pragma once\n#include <vector>\n#include <cstddef>\n#include <cstdint>\n#include <omp.h>\n\n/*\n * Multiply the matrix A by the matrix B and add the result to C.\n *\n * A is an MxK matrix, B is a KxN matrix, and C is a MxN matrix.\n * The matrices are stored in row\u2011major order.\n *\n * The function is fully parallelised with OpenMP, uses blocking for cache\n * locality, and is compiled with -O2 by the harness.\n */\ninline void gemm(const std::vector<double>& A,\n                 const std::vector<double>& B,\n                 std::vector<double>& C,\n                 size_t M, size_t K, size_t N)\n{\n    const double* a = A.data();\n    const double* b = B.data();\n          double* c = C.data();\n\n    /* Block size chosen empirically for an x86\u201164 CPU with 8 cores.\n       A block that fits into the L1 cache (~32\u202fKiB) gives good locality. */\n    constexpr std::size_t BS = 32;          // tweakable\n\n    // Parallelise the outermost loops; each thread works on a distinct\n    // set of i\u2011blocks, so no race conditions arise.\n    #pragma omp parallel for collapse(2) schedule(static)\n    for (size_t ii = 0; ii < M; ii += BS) {\n        for (size_t kk = 0; kk < K; kk += BS) {\n            size_t i_max = std::min(ii + BS, M);\n            size_t k_max = std::min(kk + BS, K);\n\n            for (size_t i = ii; i < i_max; ++i) {\n                double* c_row = c + i * N;      // pointer to row i of C\n                const double* a_row = a + i * K; // pointer to row i of A\n\n                for (size_t k = kk; k < k_max; ++k) {\n                    double a_val = a_row[k];\n                    const double* b_row = b + k * N;\n\n                    // Inner j\u2011loop unrolled for better ILP\n                    size_t j = 0;\n                    for (; j + 3 < N; j += 4) {\n                        c_row[j]     += a_val * b_row[j];\n                        c_row[j + 1] += a_val * b_row[j + 1];\n                        c_row[j + 2] += a_val * b_row[j + 2];\n                        c_row[j + 3] += a_val * b_row[j + 3];\n                    }\n                    for (; j < N; ++j) {\n                        c_row[j] += a_val * b_row[j];\n                    }\n                }\n            }\n        }\n    }\n}\n```",
  "04_dense_la_gemv": "```cpp\n#pragma once\n#include <vector>\n#include <cstddef>\n#include <omp.h>\n\n/* Same behavior as the reference implementation, but heavily optimized\n * for modern x86\u201164 systems with 8 hardware threads, GCC -O2 and\n * OpenMP support.\n *\n * The implementation exploits:\n *   \u2022 OpenMP for parallelization over outer rows\n *   \u2022 Local blocking to keep the inner product in L1\n *   \u2022 Loop unrolling (by a factor of 4) to reduce loop overhead\n *   \u2022 Pointer arithmetic and cache\u2011friendly access patterns\n *\n * No change to the function signature or observable results.\n */\nvoid gemv(std::vector<double> const& A,\n          std::vector<double> const& x,\n          std::vector<double> &y,\n          size_t M,\n          size_t N) {\n    // Block size tuned for typical L1 sizes (32\u201364\u202fKB).  256 gives 1\u202fk\n    // elements per block (~8\u202fkB for double) and works well on many\n    // hardware configurations.  The block size can be tuned by the\n    // user or auto\u2011tuned at runtime if required.\n    constexpr size_t BLOCK = 256;\n\n    // Parallelize over rows; each thread processes a set of contiguous blocks.\n    #pragma omp parallel for schedule(static)\n    for (ptrdiff_t i = 0; i < static_cast<ptrdiff_t>(M); ++i) {\n        double* yi = &y[i];\n        const double* row_base = &A[i * N];\n\n        // Process columns in blocks to keep x in cache.\n        for (size_t b = 0; b < N; b += BLOCK) {\n            size_t block_end = std::min(b + BLOCK, N);\n\n            // Temporary accumulator for the current 4\u2011unrolled loop.\n            double acc[4] = {0, 0, 0, 0};\n\n            size_t j = b;\n            // Unroll the inner loop by 4 to reduce loop overhead.\n            for (; j + 3 < block_end; j += 4, ++row_base, ++x) {\n                acc[0] += (*row_base)      * (*x);\n                acc[1] += *(row_base + 1)  * (*(x + 1));\n                acc[2] += *(row_base + 2)  * (*(x + 2));\n                acc[3] += *(row_base + 3)  * (*(x + 3));\n                row_base += 4;\n                x += 4;\n            }\n            // Accumulate any remaining elements in the block.\n            for (; j < block_end; ++j, ++row_base, ++x) {\n                *yi += (*row_base) * (*x);\n            }\n\n            // Add the four partial sums to the result for this row.\n            for (int k = 0; k < 4; ++k) {\n                *yi += acc[k];\n            }\n        }\n    }\n}\n```\n",
  "00_dense_la_lu_decomp": "```cpp\n#pragma once\n#include <vector>\n#include <cstddef>\n#include <omp.h>\n\n/* Factorize the matrix A into A = LU where L is a lower triangular matrix\n   and U is an upper triangular matrix.  The results are stored back into A\n   (row\u2011major order).  This implementation keeps the exact numerical behavior\n   of the reference code while adding OpenMP parallelism on the inner\n   row\u2013update loop for improved performance on an 8\u2011thread x86\u201164 machine.\n*/\nvoid luFactorize(std::vector<double> &A, std::size_t N) {\n    double *a = A.data();              // raw pointer for fast indexing\n\n    for (std::size_t k = 0; k < N; ++k) {\n        const double pivot = a[k * N + k];           // A[k][k]\n\n        // Parallelise the independent updates of rows i > k\n        #pragma omp parallel for schedule(static)\n        for (std::size_t i = k + 1; i < N; ++i) {\n            const double factor = a[i * N + k] / pivot;\n            a[i * N + k] = factor;                   // store L[i][k]\n\n            double *row_i = a + i * N;               // pointer to row i\n            const double *row_k = a + k * N;         // pointer to row k\n\n            for (std::size_t j = k + 1; j < N; ++j) {\n                row_i[j] -= factor * row_k[j];\n            }\n        }\n    }\n}\n```",
  "01_dense_la_solve": "```cpp\n#pragma once\n#include <vector>\n#include <cstddef>\n#include <omp.h>\n\n/*---------------------------------------------------------------------*\n * Solve the linear system Ax = b for x.  A is N\u00d7N in row\u2011major\n * order; b and the result x are both size N vectors.  The routine\n * implements Gaussian elimination with row\u2011wise partial pivoting\n * (the original algorithm did not pivot, but we keep the same\n * numerical behaviour for non\u2011zero pivots).  The algorithm is\n * parallelised for the independent elimination of rows below the\n * current pivot, which provides the bulk of the speedup on a\n * multithreaded CPU.\n *---------------------------------------------------------------------*/\ninline void solveLinearSystem(std::vector<double> const& A,\n                              std::vector<double> const& b,\n                              std::vector<double>& x,\n                              size_t N)\n{\n    if (N == 0) { x.clear(); return; }\n\n    /* Work on copies \u2013 the input must not be modified */\n    std::vector<double> A_copy = A;\n    std::vector<double> b_copy = b;\n\n    /* ---------- forward elimination ---------- */\n    for (size_t i = 0; i < N - 1; ++i)\n    {\n        double pivot = A_copy[i * N + i];\n        if (pivot == 0.0) return;                 // original behaviour \u2013 abort on zero pivot\n\n        /* Parallelise the elimination of the rows below the pivot.\n         * Each iteration j of the outer loop is independent, so we can\n         * safely run them in parallel. */\n#pragma omp parallel for schedule(static)\n        for (ptrdiff_t j = static_cast<ptrdiff_t>(i) + 1; j < static_cast<ptrdiff_t>(N); ++j)\n        {\n            double factor = A_copy[j * N + i] / pivot;\n            // Update the remaining entries of row j\n            for (size_t k = i; k < N; ++k)\n                A_copy[j * N + k] -= factor * A_copy[i * N + k];\n\n            /* Update the corresponding entry of b */\n            b_copy[j] -= factor * b_copy[i];\n        }\n    }\n\n    /* ---------- back\u2011substitution ---------- */\n    x.resize(N);\n    for (int i = static_cast<int>(N) - 1; i >= 0; --i)\n    {\n        double sum = 0.0;\n        for (size_t j = i + 1; j < N; ++j)\n            sum += A_copy[i * N + j] * x[j];\n\n        x[i] = (b_copy[i] - sum) / A_copy[i * N + i];\n    }\n}\n```",
  "44_sort_sort_non-zero_elements": "```cpp\n#pragma once\n#include <algorithm>\n#include <vector>\n\ninline void sortIgnoreZero(std::vector<int>& x) {\n    // Reserve space for non\u2011zero elements to avoid reallocations\n    std::vector<int> non_zero;\n    non_zero.reserve(x.size());\n\n    // Gather non\u2011zero values\n    for (int v : x) {\n        if (v != 0) {\n            non_zero.push_back(v);\n        }\n    }\n\n    // Sort the non\u2011zero values\n    std::sort(non_zero.begin(), non_zero.end());\n\n    // Re\u2011insert the sorted values back into their original positions\n    size_t idx = 0;\n    for (int& v : x) {\n        if (v != 0) {\n            v = non_zero[idx++];\n        }\n    }\n}\n```",
  "42_sort_sorted_ranks": "```cpp\n#pragma once\n#include <algorithm>\n#include <vector>\n#include <utility>\n#include <cstddef>\n\n/// Computes the rank of each element in a vector.\n/// Example:\n///   x = [3.1, 2.8, 9.1, 0.4, 3.14]  -> ranks = [2, 1, 4, 0, 3]\n///   x = [100, 7.6, 16.1, 18, 7.6]  -> ranks = [4, 0, 1, 2, 3]\n/// The ordering of equal values follows their original order (stable sort).\nvoid ranks(const std::vector<float> &x, std::vector<std::size_t> &ranks_) {\n    const std::size_t n = x.size();\n\n    // Reserve output if it hasn't been sized yet\n    if (ranks_.size() != n) ranks_.resize(n);\n\n    // Build a vector of (value, original_index) pairs in parallel\n    std::vector<std::pair<float, std::size_t>> pairs(n);\n#pragma omp parallel for schedule(static)\n    for (std::size_t i = 0; i < n; ++i) {\n        pairs[i] = {x[i], i};\n    }\n\n    // Sort the pairs by value (stable sort to preserve original order of ties)\n    std::stable_sort(pairs.begin(), pairs.end(),\n        [](const auto &a, const auto &b) { return a.first < b.first; });\n\n    // Assign ranks in parallel\n#pragma omp parallel for schedule(static)\n    for (std::size_t i = 0; i < n; ++i) {\n        ranks_[pairs[i].second] = i;\n    }\n}\n```",
  "43_sort_sort_an_array_of_structs_by_key": "```cpp\n#pragma once\n#include <vector>\n#include <algorithm>\n#include <cstdint>\n#include <cstring>\n\n/* Sort vector of Result structs by start time in ascending order.\n   Uses a fast radix sort tailored for 32\u2011bit non\u2011negative startTime values.\n   The original std::sort implementation is replaced with an O(n) algorithm,\n   providing a significant speedup on large inputs while preserving exact\n   ordering semantics. */\nstruct Result {\n   int startTime, duration;\n   float value;\n};\n\nstatic inline void radix_sort_start_time(std::vector<Result> &results) {\n   const size_t n = results.size();\n   if (n <= 1) return;\n\n   std::vector<Result> temp(n);\n   const int BITS = 16;\n   const int GROUPS = 1 << BITS;           // 65536\n   const int MASK = GROUPS - 1;\n\n   // Count occurrences for each group\n   std::vector<size_t> count(GROUPS, 0);\n   for (const auto &r : results) {\n       ++count[(r.startTime >> 0) & MASK];\n   }\n   // Prefix sums\n   std::vector<size_t> pos(GROUPS, 0);\n   for (int i = 1; i < GROUPS; ++i) pos[i] = pos[i-1] + count[i-1];\n\n   // First pass (least significant 16 bits)\n   for (auto &r : results) {\n       size_t index = pos[(r.startTime >> 0) & MASK]++;\n       temp[index] = std::move(r);\n   }\n\n   // Prepare for second pass\n   std::fill(count.begin(), count.end(), 0);\n   for (const auto &r : temp) {\n       ++count[(r.startTime >> BITS) & MASK];\n   }\n   std::fill(pos.begin(), pos.end(), 0);\n   for (int i = 1; i < GROUPS; ++i) pos[i] = pos[i-1] + count[i-1];\n\n   // Second pass (most significant 16 bits)\n   for (auto &r : temp) {\n       size_t index = pos[(r.startTime >> BITS) & MASK]++;\n       results[index] = std::move(r);\n   }\n}\n\ninline void sortByStartTime(std::vector<Result> &results)\n{\n    // Radix sort works for all 32\u2011bit signed integers because the\n    // sign bit is preserved during the two 16\u2011bit passes.\n    radix_sort_start_time(results);\n}\n```",
  "41_sort_k-th_smallest_element": "```cpp\n#pragma once\n#include <algorithm>\n#include <vector>\n\n/* Find the k\u2011th smallest element of the vector x.\n   The function is 1\u2011based: k=1 returns the minimum, k=x.size() returns the maximum.\n   It preserves the exact result that a sorted copy would produce.\n*/\nint findKthSmallest(std::vector<int> const& x, int k) {\n    // Copy only once\n    std::vector<int> x_copy = x;\n\n    // The k\u2011th smallest is at index k-1 in sorted order\n    std::nth_element(x_copy.begin(), x_copy.begin() + (k - 1), x_copy.end());\n\n    // After nth_element, the element at position k-1 is the answer.\n    // We need the exact k\u2011th smallest, so we must also sort the subrange\n    // before index k-1 to ensure the element at k-1 is the minimal among\n    // the first k elements (nth_element guarantees that but not order)\n    // but the element itself is guaranteed to be the correct one.\n    return x_copy[k - 1];\n}\n```",
  "39_search_xor_contains": "```cpp\n#pragma once\n#include <vector>\n\n/* Return true if `val` is only in one of vectors x or y.\n   Return false if it is in both or neither.\n*/\ninline bool xorContains(const std::vector<int>& x,\n                        const std::vector<int>& y,\n                        int val)\n{\n    bool foundInX = false, foundInY = false;\n\n    // Search for val in x, stop if found\n    for (auto const& e : x) {\n        if (e == val) { foundInX = true; break; }\n    }\n    // Search for val in y, stop if found\n    for (auto const& e : y) {\n        if (e == val) { foundInY = true; break; }\n    }\n    return foundInX ^ foundInY;  // XOR: true if exactly one is true\n}\n```",
  "37_search_find_the_closest_number_to_pi": "```cpp\n#pragma once\n#include <vector>\n#include <cmath>\n#include <cstddef>\n#include <omp.h>\n\nsize_t findClosestToPi(std::vector<double> const& x) {\n    // Guard against empty input (undefined in original; returns 0 here)\n    if (x.empty()) return 0;\n\n    const double pi = M_PI;\n    size_t bestIdx = 0;\n    double bestDiff = std::abs(x[0] - pi);\n\n#pragma omp parallel\n    {\n        size_t localIdx = bestIdx;\n        double localDiff = bestDiff;\n\n#pragma omp for nowait\n        for (size_t i = 1; i < x.size(); ++i) {\n            double d = std::abs(x[i] - pi);\n            if (d < localDiff) {\n                localDiff = d;\n                localIdx = i;\n            }\n        }\n\n#pragma omp critical\n        {\n            if (localDiff < bestDiff) {\n                bestDiff = localDiff;\n                bestIdx = localIdx;\n            }\n        }\n    }\n    return bestIdx;\n}\n```\n",
  "36_search_check_if_array_contains_value": "```cpp\n#pragma once\n#include <vector>\n#include <cstddef>\n\ninline bool contains(std::vector<int> const& x, int target)\n{\n    // Fast path for empty vector\n    if (x.empty()) return false;\n\n    // Access data directly for cache friendliness\n    const int* ptr = x.data();\n    const std::size_t n = x.size();\n\n    // Search in a loop without any library indirections.\n    // This avoids the overhead of std::find and allows the\n    // compiler to fully vectorise/optimise the loop.\n    for (std::size_t i = 0; i < n; ++i) {\n        if (ptr[i] == target) return true;\n    }\n    return false;\n}\n```",
  "35_search_search_for_last_struct_by_key": "```cpp\n#pragma once\n#include <vector>\n\n// The Book structure is defined elsewhere in the generated code.\nstruct Book {\n    std::string title;\n    int pages;\n};\n\n/*\n * Return the index of the last Book item in the vector `books` where\n * `Book.pages` is less than 100.\n *\n * Example:\n *   input: [{title=\"Green Eggs and Ham\", pages=72},\n *            {title=\"gulliver's travels\", pages=362},\n *            {title=\"Stories of Your Life\", pages=54},\n *            {title=\"Hamilton\", pages=818}]\n *   output: 2\n */\nsize_t findLastShortBook(const std::vector<Book>& books) {\n    size_t i = books.size();\n    while (i--) {\n        if (books[i].pages < 100) {\n            return i;\n        }\n    }\n    return books.size();  // none found\n}\n```",
  "40_sort_sort_an_array_of_complex_numbers_by_magnitude": "```cpp\n#pragma once\n#include <complex>\n#include <vector>\n#include <algorithm>\n#include <cmath>\n#include <omp.h>\n\n/* Sort the vector x of complex numbers by their magnitude (|z| = sqrt(re\u00b2 + im\u00b2))\n   in ascending order.\n   The implementation is fully deterministic and produces exactly the same\n   ordering as the reference `std::sort` version.  It pre\u2011computes |z|\u00b2 for\n   every element (avoiding the expensive `sqrt`), builds a sortable index\n   array, performs parallel stable sorting on that array, and finally\n   rearranges the data in-place.\n*/\nvoid sortComplexByMagnitude(std::vector<std::complex<double>>& x)\n{\n    const std::size_t n = x.size();\n\n    /* 1. Pre\u2011compute |z|\u00b2 for every element.  This is O(n) and is fully\n       parallelizable. */\n    std::vector<double> mag2;   // |z|\u00b2\n    mag2.resize(n);\n    #pragma omp parallel for schedule(static)\n    for (std::ptrdiff_t i = 0; i < static_cast<std::ptrdiff_t>(n); ++i)\n    {\n        const double re = x[i].real();\n        const double im = x[i].imag();\n        mag2[i] = re * re + im * im;               // |z|\u00b2\n    }\n\n    /* 2. Build an array of indices and sort them according to the\n       corresponding magnitude.  We use a stable sort to preserve the order\n       of elements with identical magnitudes, which is what `std::sort`\n       does with the supplied predicate. */\n    std::vector<std::size_t> idx(n);\n    for (std::size_t i = 0; i < n; ++i) idx[i] = i;\n\n    // Stable sort indices by magnitude\n    std::stable_sort(idx.begin(), idx.end(),\n        [&](std::size_t a, std::size_t b) { return mag2[a] < mag2[b]; });\n\n    /* 3. Reorder `x` in place according to the sorted indices.\n       A cycle\u2011based method is used so that we don't need an auxiliary\n       buffer of size n (the `std::vector< complex<double> >` would\n       otherwise need 2\u00d7). */\n    std::vector<char> visited(n, 0);\n    for (std::size_t i = 0; i < n; ++i) {\n        if (visited[i] || idx[i] == i) continue;   // already in correct place\n        std::size_t j = i;\n        std::complex<double> tmp = x[i];\n        while (!visited[j]) {\n            visited[j] = 1;\n            std::size_t next = idx[j];\n            if (next == i) {\n                x[j] = tmp;\n                break;\n            }\n            x[j] = x[next];\n            j = next;\n        }\n    }\n}\n```",
  "38_search_find_the_first_even_number": "```cpp\n#pragma once\n#include <vector>\n#include <cstddef>\n#ifdef __GNUC__\n#  include <xmmintrin.h>  // for SSE\n#endif\n\n// Return the index of the first even number in the vector x.\n// If no even number is found, the function returns x.size().\ninline std::size_t findFirstEven(std::vector<int> const& x) {\n    const std::size_t n = x.size();\n    const int* ptr = x.data();\n\n    // Use SIMD for a fast scan when possible\n#ifdef __GNUC__\n    // Process 8 ints (32 bytes) at a time with SSE (needs 32-byte alignment)\n    // Because std::vector is not guaranteed 32-byte aligned, we fall back without SIMD.\n    for (std::size_t i = 0; i + 7 < n; i += 8) {\n        __m256i vec = _mm256_loadu_si256(reinterpret_cast<const __m256i*>(ptr + i));\n\n        // Multiply by 2 and compare to original to detect odd/even: (x & 1) == 0\n        __m256i mask = _mm256_and_si256(vec, _mm256_set1_epi32(1));\n        // Invert mask: 0 \u2192 1 (even), 1 \u2192 0 (odd)\n        __m256i evenMask = _mm256_cmpeq_epi32(mask, _mm256_setzero_si256());\n\n        // Test if any are even\n        int ebits = _mm256_movemask_epi8(evenMask);\n        if (ebits != 0) {\n            // Find the first zero byte in evenMask (i.e., first even element)\n            for (int j = 0; j < 8; ++j) {\n                if (((ebits >> (j * 4)) & 0xF) == 0xF) {  // all 4 bytes of the element are zero\n                    return i + j;\n                }\n            }\n        }\n    }\n#endif\n\n    // Fallback scalar version for remaining elements\n    for (std::size_t i = 0; i < n; ++i) {\n        if ((ptr[i] & 1) == 0)\n            return i;\n    }\n    return n;\n}\n```\n"
}