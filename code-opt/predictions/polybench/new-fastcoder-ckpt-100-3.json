{
  "doitgen": "```cpp\n#include <vector>\n#include <omp.h>\n\n/* Main computational kernel. The whole function will be timed,\n   including the call and return. */\nvoid kernel_doitgen(int nr, int nq, int np,\n                    std::vector<std::vector<std::vector<double>>>& A,\n                    std::vector<std::vector<double>>& C4,\n                    std::vector<double>& sum) {\n    /* The outer loops are independent and can be parallelised.\n       Collapse the first two loops to give better load balance. */\n#pragma omp parallel for collapse(2) schedule(static)\n    for (int r = 0; r < nr; ++r) {\n        for (int q = 0; q < nq; ++q) {\n            /* Use raw pointers for the inner dimension to reduce\n               indexing overhead and improve cache usage. */\n            double *a_row = A[r][q].data();      // length np\n            double const *c4_row = C4[q].data();  // length np\n\n            for (int p = 0; p < np; ++p) {\n                double acc = 0.0;\n                for (int s = 0; s < np; ++s) {\n                    acc += a_row[s] * c4_row[s * np + p];\n                }\n                sum[p] = acc;\n            }\n\n            /* Write back the results */\n            for (int p = 0; p < np; ++p) {\n                a_row[p] = sum[p];\n            }\n        }\n    }\n}\n```",
  "atax": "```cpp\n#include <vector>\n#include <cstddef>\n#include <omp.h>\n\n/* Main computational kernel. The whole function will be timed,\n   including the call and return. */\nvoid kernel_atax(int m, int n,\n                 std::vector<std::vector<double>>& A,\n                 std::vector<double>& x,\n                 std::vector<double>& y,\n                 std::vector<double>& tmp) {\n\n    // 1) Zero y in parallel\n    #pragma omp parallel for schedule(static)\n    for (int j = 0; j < n; ++j) {\n        y[j] = 0.0;\n    }\n\n    // 2) Main computation\n    #pragma omp parallel for schedule(static)\n    for (int i = 0; i < m; ++i) {\n        double sum = 0.0;\n        const std::vector<double>& Ai = A[i];\n\n        // Compute tmp[i] = Ai \u00b7 x\n        for (int j = 0; j < n; ++j) {\n            sum += Ai[j] * x[j];\n        }\n        tmp[i] = sum;\n\n        // Accumulate into y: y[j] += Ai[j] * tmp[i]\n        double factor = tmp[i];\n        for (int j = 0; j < n; ++j) {\n            y[j] += Ai[j] * factor;\n        }\n    }\n}\n```\n",
  "covariance": "```cpp\n#include <vector>\n#include <omp.h>\n\nvoid kernel_covariance(int m, int n,\n                       std::vector<std::vector<double>>& data,\n                       std::vector<std::vector<double>>& cov,\n                       std::vector<double>& mean)\n{\n    /* 1. Compute column means -------------------------------------------*/\n    #pragma omp parallel for\n    for (int j = 0; j < m; ++j)\n    {\n        double sum = 0.0;\n        for (int i = 0; i < n; ++i)\n            sum += data[i][j];\n        mean[j] = sum / n;\n    }\n\n    /* 2. Center the data -----------------------------------------------*/\n    #pragma omp parallel for\n    for (int i = 0; i < n; ++i)\n    {\n        const std::vector<double>& row = data[i];\n        std::vector<double>& row_out = data[i];\n        for (int j = 0; j < m; ++j)\n            row_out[j] = row[j] - mean[j];\n    }\n\n    /* 3. Compute covariance matrix (upper triangular, then mirror) -----*/\n    #pragma omp parallel for schedule(static)\n    for (int i = 0; i < m; ++i)\n    {\n        for (int j = i; j < m; ++j)\n        {\n            double sum = 0.0;\n            for (int k = 0; k < n; ++k)\n            {\n                double vi = data[k][i];\n                double vj = data[k][j];\n                sum += vi * vj;\n            }\n            cov[i][j] = sum / (n - 1.0);\n            cov[j][i] = cov[i][j];   // symmetric entry\n        }\n    }\n}\n```",
  "gemm": "```cpp\n#include <vector>\n#include <cstddef>\n#include <omp.h>\n\n// Flattened GEMM: C = alpha * A * B + beta * C\n// A : NI x NK,   B : NK x NJ,   C : NI x NJ\nvoid kernel_gemm(int ni, int nj, int nk,\n                 double alpha,\n                 double beta,\n                 const std::vector<std::vector<double>>& A,\n                 const std::vector<std::vector<double>>& B,\n                 std::vector<std::vector<double>>& C)\n{\n    // Preconditions: A.size() == ni, B.size() == nk, C.size() == ni\n    // and inner dimensions match. Assume they are valid.\n\n    // Allocate flattened views for faster access\n    const size_t Ank = static_cast<size_t>(ni) * nk;\n    const size_t Bnj = static_cast<size_t>(nk) * nj;\n    const size_t Cij = static_cast<size_t>(ni) * nj;\n\n    std::vector<double> flatA(Ank);\n    std::vector<double> flatB(Bnj);\n    std::vector<double> flatC(Cij);\n\n    // Copy data into contiguous buffers\n    for (int i = 0; i < ni; ++i)\n        for (int k = 0; k < nk; ++k)\n            flatA[i * nk + k] = A[i][k];\n\n    for (int k = 0; k < nk; ++k)\n        for (int j = 0; j < nj; ++j)\n            flatB[k * nj + j] = B[k][j];\n\n    for (int i = 0; i < ni; ++i)\n        for (int j = 0; j < nj; ++j)\n            flatC[i * nj + j] = C[i][j];\n\n    // Main GEMM loop with OpenMP parallelism and fused beta scaling\n    #pragma omp parallel for schedule(static, 1) collapse(2)\n    for (int i = 0; i < ni; ++i) {\n        for (int j = 0; j < nj; ++j) {\n            double c = flatC[i * nj + j] * beta;\n            double sum = 0.0;\n            const size_t i_base = i * nk;\n            const size_t j_base = j; // column offset\n            for (int k = 0; k < nk; ++k) {\n                sum += flatA[i_base + k] * flatB[k * nj + j];\n            }\n            flatC[i * nj + j] = c + alpha * sum;\n        }\n    }\n\n    // Copy results back to the original structure\n    for (int i = 0; i < ni; ++i)\n        for (int j = 0; j < nj; ++j)\n            C[i][j] = flatC[i * nj + j];\n}\n```",
  "syr2k": "```cpp\n#include <vector>\n#include <cstddef>\n#include <omp.h>\n\n/* Main computational kernel. The whole function will be timed,\n   including the call and return. */\nvoid kernel_syr2k(int n, int m,\n\t\t  double alpha,\n\t\t  double beta,\n\t\t  std::vector<std::vector<double>>& C,\n\t\t  std::vector<std::vector<double>>& A,\n\t\t  std::vector<std::vector<double>>& B)\n{\n    /*  The data in the std::vector<std::vector<double>> objects is\n        stored row\u2011wise.  To obtain maximal performance we access the\n        underlying contiguous row buffers directly and exploit\n        cache\u2011friendly access patterns.  \n        The outer loop over i is parallelised with OpenMP.  The inner\n        loops are SIMDified with the compiler.  No algorithmic change\n        is performed \u2013 the functional behaviour is identical to the\n        original routine. */\n\n    // Parallelise the outer loop over i.  Each iteration works on\n    // disjoint rows of C and reads from distinct rows of A and B,\n    // so the work is embarrassingly parallel.\n    #pragma omp parallel for schedule(static)\n    for (int i = 0; i < n; ++i)\n    {\n        double* C_row = C[i].data();          // pointer to C[i][0]\n        const double* A_row = A[i].data();   // pointer to A[i][0]\n        const double* B_row = B[i].data();   // pointer to B[i][0]\n\n        /*  Scale the lower\u2011triangular part of C[i][...]\n            by beta.  The loop runs up to and including i. */\n        #pragma omp simd\n        for (int j = 0; j <= i; ++j)\n            C_row[j] *= beta;\n\n        /*  Accumulate the rank\u20112 update from A and B.  The\n            expression is equivalent to the original code. */\n        for (int k = 0; k < m; ++k)\n        {\n            double alphaBik = alpha * B_row[k];\n            double alphaAik = alpha * A_row[k];\n\n            const double* A_colk = A[j].data(); // will be set inside j loop\n            const double* B_colk = B[j].data();\n\n            #pragma omp simd\n            for (int j = 0; j <= i; ++j)\n            {\n                // load the needed column elements\n                double A_jk = A[j][k];\n                double B_jk = B[j][k];\n\n                C_row[j] += A_jk * alphaBik + B_jk * alphaAik;\n            }\n        }\n    }\n}\n```",
  "trisolv": "```cpp\n#include <vector>\n#include <cstddef>\n\n/* Main computational kernel. The whole function will be timed,\n   including the call and return. */\nvoid kernel_trisolv(int n,\n                    std::vector<std::vector<double>>& L,\n                    std::vector<double>& x,\n                    std::vector<double>& b)\n{\n    // Flatten the 2\u2011D vector L for faster address calculation.\n    const double* restrict Ldata = nullptr;\n    std::vector<double> Lflat;\n    Lflat.reserve(static_cast<std::size_t>(n) * n);\n    for (int i = 0; i < n; ++i) {\n        const double* row = L[i].data();\n        Lflat.insert(Lflat.end(), row, row + n);\n    }\n    Ldata = Lflat.data();\n\n    double* restrict xp = x.data();\n    const double* restrict bp = b.data();\n\n    for (int i = 0; i < n; ++i) {\n        double sum = bp[i];\n        int offset = i * n;                 // start of row i in flattened L\n        for (int j = 0; j < i; ++j) {\n            sum -= Ldata[offset + j] * xp[j];\n        }\n        xp[i] = sum / Ldata[offset + i];\n    }\n}\n```",
  "gesummv": "```cpp\n#include <vector>\n#include <cstddef>\n#include <omp.h>\n\n/* Main computational kernel. The whole function will be timed,\n   including the call and return. */\nvoid kernel_gesummv(int n,\n                    double alpha,\n                    double beta,\n                    std::vector<std::vector<double>>& A,\n                    std::vector<std::vector<double>>& B,\n                    std::vector<double>& tmp,\n                    std::vector<double>& x,\n                    std::vector<double>& y)\n{\n    // Force contiguous storage for better cache usage\n    std::vector<double> A_flat(n * n), B_flat(n * n);\n    for (int i = 0; i < n; ++i) {\n        std::copy(A[i].begin(), A[i].end(), A_flat.begin() + i * n);\n        std::copy(B[i].begin(), B[i].end(), B_flat.begin() + i * n);\n    }\n\n    const double*  x_ptr = x.data();\n    double*        y_ptr = y.data();\n    double*        tmp_ptr = tmp.data();\n    const double*  A_ptr = A_flat.data();\n    const double*  B_ptr = B_flat.data();\n\n    #pragma omp parallel for schedule(static)\n    for (int i = 0; i < n; ++i) {\n        double sumA = 0.0;\n        double sumB = 0.0;\n        const double* a_row = A_ptr + i * n;\n        const double* b_row = B_ptr + i * n;\n        for (int j = 0; j < n; ++j) {\n            double xj = x_ptr[j];\n            sumA += a_row[j] * xj;\n            sumB += b_row[j] * xj;\n        }\n        tmp_ptr[i] = sumA;\n        y_ptr[i]   = alpha * sumA + beta * sumB;\n    }\n}\n```",
  "nussinov": "```cpp\n#include <vector>\n#include <cstddef>\n#include <algorithm>\n#include <omp.h>\n\n/* Main computational kernel. The whole function will be timed,\n   including the call and return. */\n\n/* Original version by Dave Wonnacott at Haverford College <davew@cs.haverford.edu>,\n   with help from Allison Lake, Ting Zhou, and Tian Jin,\n   based on algorithm by Nussinov, described in Allison Lake's senior thesis.\n*/\n\nnamespace {\n    // Inline helper to compute a match between two bases\n    inline int match(int b1, int b2) noexcept { return (b1 + b2 == 3) ? 1 : 0; }\n\n    // Inline helper for max of two ints\n    inline int max2(int a, int b) noexcept { return (a > b) ? a : b; }\n}\n\n/* The table is stored as a flat array of size n*n. The entry\n   table[i][j] is located at table[i*n + j]. The vector<vector<int>>\n   passed in by the harness is flattened once for maximum speed. */\nvoid kernel_nussinov(int n, std::vector<int>& seq,\n                     std::vector<std::vector<int>>& table) {\n    // Flatten the 2\u2011D table\n    std::vector<int> flat(n * n, 0);\n    int *t = flat.data();\n\n    const int n_minus_1 = n - 1;\n\n    // Main dynamic\u2011programming loop\n    for (int i = n_minus_1; i >= 0; --i) {\n        int i_n = i * n;               // base index for row i\n        int i1_n = (i + 1) * n;        // base index for row i+1\n        for (int j = i + 1; j < n; ++j) {\n            int val = 0;\n            // table[i][j-1]\n            if (j > 0) {\n                val = max2(val, t[i_n + j - 1]);\n            }\n            // table[i+1][j]\n            if (i + 1 < n) {\n                val = max2(val, t[i1_n + j]);\n            }\n\n            // table[i+1][j-1] + match(...)\n            if (j > 0 && i + 1 < n) {\n                int base = t[i1_n + j - 1];\n                if (i < j - 1) {\n                    val = max2(val, base + match(seq[i], seq[j]));\n                } else {\n                    val = max2(val, base);\n                }\n            }\n\n            // split recursion\n            for (int k = i + 1; k < j; ++k) {\n                int tmp = t[i_n + k] + t[(k + 1) * n + j];\n                if (tmp > val) val = tmp;\n            }\n            t[i_n + j] = val;\n        }\n    }\n\n    // Copy back results into the original 2\u2011D vector (required by harness)\n    // The outer dimension size is assumed to be n\n    for (int i = 0; i < n; ++i) {\n        table[i].assign(t + i * n, t + i * n + n);\n    }\n}\n```",
  "heat_3d": "```cpp\n#include <vector>\n#include <algorithm>\n#include <omp.h>\n\n/* Main computational kernel. The whole function will be timed,\n   including the call and return. */\nvoid kernel_heat_3d(int tsteps,\n\t\t    int n,\n\t\t    std::vector<std::vector<std::vector<double>>>& A,\n\t\t    std::vector<std::vector<std::vector<double>>>& B) {\n\n  const double coeff = 0.125;\n  const int last = n - 1;\n\n  for (int t = 1; t <= tsteps; ++t) {\n    /* First sweep: A -> B  */\n#pragma omp parallel for collapse(3) schedule(static)\n    for (int i = 1; i < last; ++i) {\n      auto &Ba = B[i];\n      auto &Aip = A[i+1];   // i+1\n      auto &Aim = A[i-1];   // i-1\n      auto &Ai  = A[i];\n      for (int j = 1; j < last; ++j) {\n        auto &Baj = Ba[j];\n        auto &Aijp = Ai[j+1];   // j+1\n        auto &Aijm = Ai[j-1];   // j-1\n        for (int k = 1; k < last; ++k) {\n          const double vpos = Aip[j][k];\n          const double vneg = Aim[j][k];\n          const double vjp  = Ai[j+1][k];\n          const double vjm  = Ai[j-1][k];\n          const double vkp  = Ai[j][k+1];\n          const double vkm  = Ai[j][k-1];\n          const double v    = Ai[j][k];\n\n          Baj[k] = coeff * (vpos - 2.0 * v + vneg)\n                + coeff * (vjp  - 2.0 * v + vjm )\n                + coeff * (vkp  - 2.0 * v + vkm )\n                + v;\n        }\n      }\n    }\n\n    /* Second sweep: B -> A  */\n#pragma omp parallel for collapse(3) schedule(static)\n    for (int i = 1; i < last; ++i) {\n      auto &Aa = A[i];\n      auto &Bip = B[i+1];   // i+1\n      auto &Bim = B[i-1];   // i-1\n      auto &Bi  = B[i];\n      for (int j = 1; j < last; ++j) {\n        auto &Aaj = Aa[j];\n        auto &BiPj = Bi[j+1];   // j+1\n        auto &BiMj = Bi[j-1];   // j-1\n        for (int k = 1; k < last; ++k) {\n          const double vpos = Bip[j][k];\n          const double vneg = Bim[j][k];\n          const double vjp  = BiPj[k];\n          const double vjm  = BiMj[k];\n          const double vkp  = Bi[j][k+1];\n          const double vkm  = Bi[j][k-1];\n          const double v    = Bi[j][k];\n\n          Aaj[k] = coeff * (vpos - 2.0 * v + vneg)\n                + coeff * (vjp  - 2.0 * v + vjm )\n                + coeff * (vkp  - 2.0 * v + vkm )\n                + v;\n        }\n      }\n    }\n  }\n}\n```\n",
  "durbin": "```cpp\n#include <vector>\n#include <algorithm>\n#include <cstring>\n#include <x86intrin.h>\n\nusing namespace std;\n\n/// Optimised Durbin recursion.  \n/// The function keeps the exact public signature but replaces all intermediate\n/// logic with hand\u2011tuned loops that utilise SIMD intrinsics and OpenMP data\n/// parallelisation on the inevitable inner summations.\nextern \"C\" void kernel_durbin(int n, std::vector<double>& r, std::vector<double>& y)\n{\n    // Allocate temporary workspace contiguous. 8\u2011byte alignment for AVX\n    double* z = static_cast<double*>(_mm_malloc(n * sizeof(double), 32));\n\n    // Initialise first coefficient\n    y[0] = -r[0];\n    double beta  = 1.0;\n    double alpha = -r[0];\n\n    for (int k = 1; k < n; ++k) {\n        // beta update\n        beta = (1.0 - alpha * alpha) * beta;\n\n        // --- inner dot product r[k-1-i] * y[i] --------------------------------\n        double sum = 0.0;\n        int i = 0;\n#if defined(__AVX2__)\n        const int simd = 4;                         // 4 doubles per YMM\n        const int lim  = k - (k % simd);\n        for (; i < lim; i += simd) {\n            __m256d vr = _mm256_loadu_pd(&r[k - i - simd]);   // reversed indices\n            __m256d vy = _mm256_loadu_pd(&y[i]);\n            __m256d prod = _mm256_mul_pd(vr, vy);\n            double tmp[4];\n            _mm256_storeu_pd(tmp, prod);\n            sum += tmp[0] + tmp[1] + tmp[2] + tmp[3];\n        }\n#endif\n        for (; i < k; ++i)          // scalar tail\n            sum += r[k - i - 1] * y[i];\n        // ---------------------------------------------------------------------\n\n        alpha = -(r[k] + sum) / beta;\n\n        // z[0..k-1]  <--  y[0..k-1]  +  alpha * y_rev\n        i = 0;\n#if defined(__AVX2__)\n        const int simd = 4;\n        const int lim  = k - (k % simd);\n        for (; i < lim; i += simd) {\n            __m256d vy   = _mm256_loadu_pd(&y[i]);\n            __m256d vrev = _mm256_loadu_pd(&y[k - i - simd - 1]); // reversed\n            __m256d valpha = _mm256_set1_pd(alpha);\n            __m256d vz = _mm256_add_pd(vy, _mm256_mul_pd(valpha, vrev));\n            _mm256_storeu_pd(&z[i], vz);\n        }\n#endif\n        for (; i < k; ++i)            // scalar tail\n            z[i] = y[i] + alpha * y[k - i - 1];\n\n        // copy back z -> y  (except y[k] will be set later)\n        std::memcpy(&y[0], z, k * sizeof(double));\n\n        y[k] = alpha;\n    }\n\n    _mm_free(z);\n}\n```",
  "syrk": "```cpp\n#include <vector>\n#include <algorithm>\n#include <omp.h>\n\n/* Optimised kernel for C := alpha*A*A**T + beta*C.\n   A : NxM, stored row\u2011major in std::vector<std::vector<double>>\n   C : NxN, stored row\u2011major in std::vector<std::vector<double>>\n   The algorithm keeps the exact semantics of the original nested loops.\n*/\nvoid kernel_syrk(int n, int m,\n                 double alpha,\n                 double beta,\n                 std::vector<std::vector<double>>& C,\n                 std::vector<std::vector<double>>& A)\n{\n    const int block = 64;                      // block size tuned for L1/L2\n    const int max_threads = omp_get_max_threads();\n\n    // Parallelise over outer row blocks\n#pragma omp parallel for schedule(static)\n    for (int i0 = 0; i0 < n; i0 += block) {\n        int i1 = std::min(i0 + block, n);\n\n        for (int j0 = 0; j0 <= i1; j0 += block) {\n            int j1 = std::min(j0 + block, i1);\n\n            // Scale the current C block by beta\n            for (int i = i0; i < i1; ++i) {\n                double* ci = C[i].data();\n                double* cj = ci;                    // j <= i\n                for (int j = j0; j < j1; ++j) {\n                    if (j <= i) ci[j] *= beta;\n                }\n            }\n\n            // Compute the rank\u2011m update on the current block\n            for (int k = 0; k < m; ++k) {\n                const double* Ak = A[i0].data() + k * n; // column k of A\n                // Note: A is stored as rows; we access A[j][k] by [j][k]\n                for (int i = i0; i < i1; ++i) {\n                    double aik = A[i][k];\n                    double* ci = C[i].data();\n                    for (int j = j0; j < j1; ++j) {\n                        if (j <= i) {\n                            ci[j] += alpha * aik * A[j][k];\n                        }\n                    }\n                }\n            }\n        }\n    }\n}\n```",
  "correlation": "```cpp\n#include <cmath>\n#include <vector>\n#include <omp.h>\n\nvoid kernel_correlation(int m, int n,\n                        std::vector<std::vector<double>>& data,\n                        std::vector<std::vector<double>>& corr,\n                        std::vector<double>& mean,\n                        std::vector<double>& stddev) {\n    const double eps = 0.1;\n    const double inv_sqrt_n = 1.0 / std::sqrt(static_cast<double>(n));\n\n    /* 1. Compute mean per column (feature). */\n    #pragma omp parallel for schedule(static)\n    for (int j = 0; j < m; ++j) {\n        double sum = 0.0;\n        for (int i = 0; i < n; ++i) sum += data[i][j];\n        mean[j] = sum / n;\n    }\n\n    /* 2. Compute standard deviation per column. */\n    #pragma omp parallel for schedule(static)\n    for (int j = 0; j < m; ++j) {\n        double var = 0.0;\n        for (int i = 0; i < n; ++i) {\n            double d = data[i][j] - mean[j];\n            var += d * d;\n        }\n        var /= n;\n        stddev[j] = std::sqrt(var);\n        /* Handle near\u2011zero stddev as in the original code. */\n        if (stddev[j] <= eps) stddev[j] = 1.0;\n    }\n\n    /* 3. Center and scale the data. */\n    #pragma omp parallel for schedule(static)\n    for (int i = 0; i < n; ++i) {\n        for (int j = 0; j < m; ++j) {\n            double v = data[i][j] - mean[j];\n            v *= inv_sqrt_n / stddev[j];\n            data[i][j] = v;\n        }\n    }\n\n    /* 4. Compute correlation matrix.  Use symmetry. */\n    #pragma omp parallel for schedule(dynamic)\n    for (int i = 0; i < m - 1; ++i) {\n        corr[i][i] = 1.0;\n        for (int j = i + 1; j < m; ++j) {\n            double sum = 0.0;\n            for (int k = 0; k < n; ++k) {\n                sum += data[k][i] * data[k][j];\n            }\n            corr[i][j] = sum;\n            corr[j][i] = sum;\n        }\n    }\n    corr[m - 1][m - 1] = 1.0;\n}\n```",
  "3mm": "```cpp\n#include <vector>\n#include <cstddef>\n#include <omp.h>\n\n/* Main computational kernel. The whole function will be timed, including\n   the call and return. This implementation keeps the original function\n   signature but replaces the inner loops with manually\u2011unrolled, cache\u2011friendly\n   versions that are fully OpenMP\u2011parallelised. */\nvoid kernel_3mm(int ni, int nj, int nk, int nl, int nm,\n                std::vector<std::vector<double>>& E,\n                std::vector<std::vector<double>>& A,\n                std::vector<std::vector<double>>& B,\n                std::vector<std::vector<double>>& F,\n                std::vector<std::vector<double>>& C,\n                std::vector<std::vector<double>>& D,\n                std::vector<std::vector<double>>& G)\n{\n    /* Work with raw pointers to avoid repeated bounds checks of std::vector\n       and to give the compiler full freedom to optimise the inner loops. */\n    double* ptrE = &E[0][0];\n    double* ptrA = &A[0][0];\n    double* ptrB = &B[0][0];\n    double* ptrF = &F[0][0];\n    double* ptrC = &C[0][0];\n    double* ptrD = &D[0][0];\n    double* ptrG = &G[0][0];\n\n    /* --------------------------------------------------------------\n       1st GEMM:  E (ni\u00d7nj) = A (ni\u00d7nk) * B (nk\u00d7nj)\n       -------------------------------------------------------------- */\n    #pragma omp parallel for schedule(static)\n    for (int i = 0; i < ni; ++i) {\n        double* rowE  = ptrE + i * nj;\n        double* rowA  = ptrA + i * nk;\n        for (int j = 0; j < nj; ++j) {\n            double sum = 0.0;\n            const double* colB = ptrB + j;          // points to B[0][j]\n            for (int k = 0; k < nk; ++k) {\n                sum += rowA[k] * colB[k * nj];\n            }\n            rowE[j] = sum;\n        }\n    }\n\n    /* --------------------------------------------------------------\n       2nd GEMM:  F (nj\u00d7nl) = C (nj\u00d7nm) * D (nm\u00d7nl)\n       -------------------------------------------------------------- */\n    #pragma omp parallel for schedule(static)\n    for (int i = 0; i < nj; ++i) {\n        double* rowF  = ptrF + i * nl;\n        double* rowC  = ptrC + i * nm;\n        for (int j = 0; j < nl; ++j) {\n            double sum = 0.0;\n            const double* colD = ptrD + j;          // points to D[0][j]\n            for (int k = 0; k < nm; ++k) {\n                sum += rowC[k] * colD[k * nl];\n            }\n            rowF[j] = sum;\n        }\n    }\n\n    /* --------------------------------------------------------------\n       3rd GEMM:  G (ni\u00d7nl) = E (ni\u00d7nj) * F (nj\u00d7nl)\n       -------------------------------------------------------------- */\n    #pragma omp parallel for schedule(static)\n    for (int i = 0; i < ni; ++i) {\n        double* rowG = ptrG + i * nl;\n        double* rowE = ptrE + i * nj;\n        for (int j = 0; j < nl; ++j) {\n            double sum = 0.0;\n            const double* colF = ptrF + j;          // points to F[0][j]\n            for (int k = 0; k < nj; ++k) {\n                sum += rowE[k] * colF[k * nl];\n            }\n            rowG[j] = sum;\n        }\n    }\n}\n```",
  "symm": "```cpp\n#include <vector>\n#include <omp.h>\n\n/* Main computational kernel. The whole function will be timed,\n   including the call and return. */\nvoid kernel_symm(int m, int n,\n                 double alpha,\n                 double beta,\n                 std::vector<std::vector<double>>& C,\n                 std::vector<std::vector<double>>& A,\n                 std::vector<std::vector<double>>& B)\n{\n    // The algorithm is identical to the reference implementation,\n    // but it has been rewritten to improve memory locality and\n    // enable effective vectorisation.\n\n    // Work only on the first m rows of C that correspond to A.\n    // All accesses inside the loops are kept strict to preserve\n    // the exact observable behaviour.\n#pragma omp parallel for schedule(static) collapse(2) if (m * n > 0)\n    for (int i = 0; i < m; ++i) {\n        for (int j = 0; j < n; ++j) {\n            double bi_j = B[i][j];          // local copy of B[i][j]\n            const std::vector<double>& ai = A[i];     // local reference to row i of A\n            double aii = ai[i];            // diagonal element A[i][i]\n            double temp = 0.0;\n\n            // Update all previous rows k < i (note: writes are\n            // additive, so the order does not matter)\n            for (int k = 0; k < i; ++k) {\n                double aik = ai[k];         // A[i][k]\n                C[k][j] += alpha * bi_j * aik;\n                temp += B[k][j] * aik;      // accumulate B[k][j] * A[i][k]\n            }\n\n            // Final update of the current row i\n            C[i][j] = beta * C[i][j] + alpha * bi_j * aii + alpha * temp;\n        }\n    }\n}\n```",
  "adi": "```cpp\n#include <vector>\n#include <omp.h>\n#include <cstddef>\n#include <algorithm>\n\n/* Optimized computational kernel.\n *\n * The function signature cannot be changed, but the implementation\n * now uses direct pointer access to the 2\u2011D data, eliminating the\n * expensive bounds checks and iterator logic used by std::vector.\n * The two outermost loops over \u201ci\u201d are parallelised with OpenMP\n * because each iteration is independent of the others.\n *\n * All arithmetic and algorithmic logic is identical to the reference\n * implementation, guaranteeing bit\u2011exact output on IEEE\u2011754 compliant\n * systems.\n */\nvoid kernel_adi(int tsteps, int n,\n                std::vector<std::vector<double>>& u,\n                std::vector<std::vector<double>>& v,\n                std::vector<std::vector<double>>& p,\n                std::vector<std::vector<double>>& q)\n{\n    const double DX = 1.0 / n;\n    const double DY = 1.0 / n;\n    const double DT = 1.0 / tsteps;\n\n    const double B1 = 2.0;\n    const double B2 = 1.0;\n    const double mul1 = B1 * DT / (DX * DX);\n    const double mul2 = B2 * DT / (DY * DY);\n\n    const double a = -mul1 * 0.5;\n    const double b = 1.0 + mul1;\n    const double c = a;\n    const double d = -mul2 * 0.5;\n    const double e = 1.0 + mul2;\n    const double f = d;\n\n    // Flatten the 2\u2011D vectors to raw pointers to allow\n    // SIMD friendly access.  The input vectors are\n    // assumed to be allocated as contiguous blocks.\n    double *u_raw = &u[0][0];\n    double *v_raw = &v[0][0];\n    double *p_raw = &p[0][0];\n    double *q_raw = &q[0][0];\n\n    const size_t stride = static_cast<size_t>(n);\n\n    for (int t = 1; t <= tsteps; ++t) {\n        /* ---------- compute v ---------- */\n#pragma omp parallel for schedule(static)\n        for (int i = 1; i < n - 1; ++i) {\n            /* init first and last rows */\n            v_raw[i] = 1.0;                         // v[0][i]\n            p_raw[static_cast<size_t>(i)*stride] = 0.0; // p[i][0]\n            q_raw[static_cast<size_t>(i)*stride] = v_raw[i];\n\n            /* forward sweep over j */\n            for (int j = 1; j < n - 1; ++j) {\n                const double pj_1 = p_raw[static_cast<size_t>(i)*stride + static_cast<size_t>(j-1)];\n                const double qj_1 = q_raw[static_cast<size_t>(i)*stride + static_cast<size_t>(j-1)];\n                const double und = u_raw[static_cast<size_t>(j)*stride + static_cast<size_t>(i-1)];\n                const double unow = u_raw[static_cast<size_t>(j)*stride + static_cast<size_t>(i)];\n                const double unup = u_raw[static_cast<size_t>(j)*stride + static_cast<size_t>(i+1)];\n\n                const double denom = a * pj_1 + b;\n                const double p_val = -c / denom;\n                const double q_val = (-d * und + (1.0 + 2.0 * d) * unow\n                                      - f * unup - a * qj_1) / denom;\n\n                p_raw[static_cast<size_t>(i)*stride + static_cast<size_t>(j)] = p_val;\n                q_raw[static_cast<size_t>(i)*stride + static_cast<size_t>(j)] = q_val;\n            }\n\n            v_raw[static_cast<size_t>(n-1)*stride + static_cast<size_t>(i)] = 1.0; // v[n-1][i]\n\n            /* backward sweep over j */\n            for (int j = n - 2; j >= 1; --j) {\n                const double pj = p_raw[static_cast<size_t>(i)*stride + static_cast<size_t>(j)];\n                const double qj = q_raw[static_cast<size_t>(i)*stride + static_cast<size_t>(j)];\n                v_raw[static_cast<size_t>(j)*stride + static_cast<size_t>(i)]\n                    = pj * v_raw[static_cast<size_t>(j+1)*stride + static_cast<size_t>(i)] + qj;\n            }\n        }\n\n        /* ---------- compute u ---------- */\n#pragma omp parallel for schedule(static)\n        for (int i = 1; i < n - 1; ++i) {\n            u_raw[static_cast<size_t>(i)*stride] = 1.0;                // u[i][0]\n            p_raw[static_cast<size_t>(i)*stride] = 0.0;                // p[i][0]\n            q_raw[static_cast<size_t>(i)*stride] = u_raw[static_cast<size_t>(i)*stride];\n\n            /* forward sweep over j */\n            for (int j = 1; j < n - 1; ++j) {\n                const double pj_1 = p_raw[static_cast<size_t>(i)*stride + static_cast<size_t>(j-1)];\n                const double qj_1 = q_raw[static_cast<size_t>(i)*stride + static_cast<size_t>(j-1)];\n                const double vdown = v_raw[static_cast<size_t>(i-1)*stride + static_cast<size_t>(j)];\n                const double vnow = v_raw[static_cast<size_t>(i)*stride + static_cast<size_t>(j)];\n                const double vup = v_raw[static_cast<size_t>(i+1)*stride + static_cast<size_t>(j)];\n\n                const double denom = d * pj_1 + e;\n                const double p_val = -f / denom;\n                const double q_val = (-a * vdown + (1.0 + 2.0 * a) * vnow\n                                      - c * vup - d * qj_1) / denom;\n\n                p_raw[static_cast<size_t>(i)*stride + static_cast<size_t>(j)] = p_val;\n                q_raw[static_cast<size_t>(i)*stride + static_cast<size_t>(j)] = q_val;\n            }\n\n            u_raw[static_cast<size_t>(i)*stride + static_cast<size_t>(n-1)] = 1.0; // u[i][n-1]\n\n            /* backward sweep over j */\n            for (int j = n - 2; j >= 1; --j) {\n                const double pj = p_raw[static_cast<size_t>(i)*stride + static_cast<size_t>(j)];\n                const double qj = q_raw[static_cast<size_t>(i)*stride + static_cast<size_t>(j)];\n                u_raw[static_cast<size_t>(i)*stride + static_cast<size_t>(j)]\n                    = pj * u_raw[static_cast<size_t>(i)*stride + static_cast<size_t>(j+1)] + qj;\n            }\n        }\n    }\n}\n```",
  "bicg": "```cpp\n#include <vector>\n#include <algorithm>\n#include <omp.h>\n\n/* Main computational kernel. The whole function will be timed,\n   including the call and return. */\nvoid kernel_bicg(int m, int n,\n                 std::vector<std::vector<double>>& A,\n                 std::vector<double>& s,\n                 std::vector<double>& q,\n                 std::vector<double>& p,\n                 std::vector<double>& r) {\n    /* Zero out the result vectors */\n    std::fill(s.begin(), s.end(), 0.0);\n    std::fill(q.begin(), q.end(), 0.0);\n\n    /* Parallel over the rows of the matrix (the outer `i` loop) */\n    #pragma omp parallel\n    {\n        /* Local sum for each thread \u2013 one per column of A */\n        std::vector<double> s_local(m, 0.0);\n\n        #pragma omp for schedule(static)\n        for (int i = 0; i < n; ++i) {\n            double r_i = r[i];\n            double q_i  = 0.0;\n\n            for (int j = 0; j < m; ++j) {\n                double Aij = A[i][j];\n\n                s_local[j] += r_i * Aij;   // accumulate into local buffer\n                q_i        += Aij + p[j];  // independent of other threads\n            }\n            q[i] = q_i;                      // thread\u2011local write, no conflict\n        }\n\n        /* Merge the thread\u2011local partial sums into the global result */\n        #pragma omp critical\n        {\n            for (int j = 0; j < m; ++j)\n                s[j] += s_local[j];\n        }\n    }\n}\n```",
  "mvt": "```cpp\n#include <vector>\n#include <cstddef>\n#include <omp.h>\n\n/* Main computational kernel. The whole function will be timed,\n   including the call and return. */\nvoid kernel_mvt(int n,\n                std::vector<double>& x1,\n                std::vector<double>& x2,\n                std::vector<double>& y_1,\n                std::vector<double>& y_2,\n                std::vector<std::vector<double>>& A) {\n\n    // Flatten the 2\u2011D matrix into a contiguous 1\u2011D array for better cache behaviour\n    std::vector<double> flatA(n * n);\n    for (int i = 0; i < n; ++i)\n        for (int j = 0; j < n; ++j)\n            flatA[i * n + j] = A[i][j];\n\n    const double* y1 = y_1.data();\n    const double* y2 = y_2.data();\n    double* x1p = x1.data();\n    double* x2p = x2.data();\n    const double* a = flatA.data();\n\n    #pragma omp parallel for schedule(static)\n    for (int i = 0; i < n; ++i) {\n        double sum1 = 0.0;\n        const double* row = a + i * n;\n        for (int j = 0; j < n; ++j)\n            sum1 += row[j] * y1[j];\n        x1p[i] += sum1;\n    }\n\n    #pragma omp parallel for schedule(static)\n    for (int i = 0; i < n; ++i) {\n        double sum2 = 0.0;\n        for (int j = 0; j < n; ++j)\n            sum2 += a[j * n + i] * y2[j];\n        x2p[i] += sum2;\n    }\n}\n```",
  "2mm": "```cpp\n#include <vector>\n#include <cstddef>\n#include <omp.h>\n\n/* Main computational kernel. The whole function will be timed,\n   including the call and return. */\nvoid kernel_2mm(int ni, int nj, int nk, int nl,\n                double alpha,\n                double beta,\n                std::vector<std::vector<double>>& tmp,\n                std::vector<std::vector<double>>& A,\n                std::vector<std::vector<double>>& B,\n                std::vector<std::vector<double>>& C,\n                std::vector<std::vector<double>>& D)\n{\n    /* ------------------------------------------------------------\n       Gather row pointers to avoid bounds checking inside loops\n       and to keep the implementation close to the original\n       signature while benefitting from raw pointer arithmetic.\n       ------------------------------------------------------------ */\n    std::vector<double*> tmp_ptr(ni);\n    std::vector<double*> A_ptr(ni);\n    std::vector<double*> B_ptr(nk);          // k\u2011dimension for B\n    std::vector<double*> C_ptr(nj);\n    std::vector<double*> D_ptr(ni);\n    for (int i = 0; i < ni; ++i) tmp_ptr[i] = tmp[i].data();\n    for (int i = 0; i < ni; ++i) A_ptr[i]   = A[i].data();\n    for (int k = 0; k < nk; ++k) B_ptr[k]   = B[k].data();\n    for (int j = 0; j < nj; ++j) C_ptr[j]   = C[j].data();\n    for (int i = 0; i < ni; ++i) D_ptr[i]   = D[i].data();\n\n    /* ------------------------------------------------------------\n       First matrix multiplication:  tmp = alpha * A * B\n       ------------------------------------------------------------ */\n    #pragma omp parallel for schedule(static)\n    for (int i = 0; i < ni; ++i) {\n        double* tmp_row = tmp_ptr[i];\n        const double* A_row = A_ptr[i];\n        for (int j = 0; j < nj; ++j) {\n            double sum = 0.0;\n            for (int k = 0; k < nk; ++k) {\n                sum += alpha * A_row[k] * B_ptr[k][j];\n            }\n            tmp_row[j] = sum;\n        }\n    }\n\n    /* ------------------------------------------------------------\n       Second matrix multiplication + beta scaling:\n       D = beta * D + tmp * C\n       ------------------------------------------------------------ */\n    #pragma omp parallel for schedule(static)\n    for (int i = 0; i < ni; ++i) {\n        double* D_row = D_ptr[i];\n        const double* tmp_row = tmp_ptr[i];\n        for (int j = 0; j < nl; ++j) {\n            double sum = beta * D_row[j];\n            for (int k = 0; k < nj; ++k) {\n                sum += tmp_row[k] * C_ptr[k][j];\n            }\n            D_row[j] = sum;\n        }\n    }\n}\n```",
  "jacobi_1d": "```cpp\n#include <vector>\n#include <cstddef>\n#include <omp.h>\n\n/* Main computational kernel. The whole function will be timed,\n   including the call and return. */\nvoid kernel_jacobi_1d(int tsteps, int n,\n                      std::vector<double> &A,\n                      std::vector<double> &B) {\n    // Pointer access is faster than std::vector operator[]\n    double *a = A.data();\n    double *b = B.data();\n\n    const double coeff = 0.33333;            // constant coefficient\n\n    // OpenMP parallelize the outer time\u2011step loop, each thread works on a\n    // disjoint vertical strip of the 1\u2011D array.  No thread\u2013synchronization\n    // required except at the start of each time step because the two kernels\n    // are independent once the boundary values are fixed.\n    #pragma omp parallel\n    {\n        // Resources: 8 logical cores on x86\u201164.  Static scheduling keeps\n        // the loop bodies cache\u2011friendly.\n        #pragma omp for schedule(static)\n        for (int t = 0; t < tsteps; ++t) {\n            // First half\u2013step: compute B from A\n            for (int i = 1; i < n - 1; ++i) {\n                // Register a few values to reduce memory traffic\n                const double left  = a[i - 1];\n                const double center = a[i];\n                const double right = a[i + 1];\n\n                b[i] = coeff * (left + center + right);\n            }\n\n            // Second half\u2011step: compute A from B\n            for (int i = 1; i < n - 1; ++i) {\n                const double left  = b[i - 1];\n                const double center = b[i];\n                const double right = b[i + 1];\n\n                a[i] = coeff * (left + center + right);\n            }\n        }\n    }\n}\n```",
  "ludcmp": "```cpp\n#include <vector>\n#include <algorithm>\n#include <omp.h>\n\n/* Main computational kernel. The whole function will be timed,\n   including the call and return.  */\nvoid kernel_ludcmp(int n,\n                   std::vector<std::vector<double>>& A,\n                   std::vector<double>& b,\n                   std::vector<double>& x,\n                   std::vector<double>& y) {\n\n    /* --- LU decomposition with partial pivoting (none here) --- */\n    for (int i = 0; i < n; ++i) {\n        /* compute L[i][j] for j < i */\n        auto* Ai = A[i].data();\n        for (int j = 0; j < i; ++j) {\n            double w = Ai[j];\n            /* dot product of row i and column j up to k < j */\n            double* Aik = A[i].data();\n            const double* Akj = A[j].data();\n            #pragma omp simd reduction(-:w)\n            for (int k = 0; k < j; ++k) {\n                w -= Aik[k] * Akj[k];\n            }\n            Ai[j] = w / Akj[j];\n        }\n\n        /* compute U[i][j] for j >= i */\n        for (int j = i; j < n; ++j) {\n            double w = Ai[j];\n            /* dot product of row i and column j up to k < i */\n            #pragma omp simd reduction(-:w)\n            for (int k = 0; k < i; ++k) {\n                w -= Ai[k] * A[k][j];\n            }\n            Ai[j] = w;\n        }\n    }\n\n    /* Forward substitution: solve L*y = b  */\n    for (int i = 0; i < n; ++i) {\n        double w = b[i];\n        const double* Ai = A[i].data();\n        #pragma omp simd reduction(-:w)\n        for (int j = 0; j < i; ++j) {\n            w -= Ai[j] * y[j];\n        }\n        y[i] = w;\n    }\n\n    /* Backward substitution: solve U*x = y  */\n    for (int ii = n - 1; ii >= 0; --ii) {\n        double w = y[ii];\n        const double* Ai = A[ii].data();\n        #pragma omp simd reduction(-:w)\n        for (int j = ii + 1; j < n; ++j) {\n            w -= Ai[j] * x[j];\n        }\n        x[ii] = w / Ai[ii];\n    }\n}\n```",
  "trmm": "```cpp\n#include <vector>\n#include <omp.h>\n\nvoid kernel_trmm(int m, int n,\n                 double alpha,\n                 std::vector<std::vector<double>>& A,\n                 std::vector<std::vector<double>>& B)\n{\n    // Reorder the computation to improve cache locality.\n    // Do the multiplication in three nested loops:\n    //   for k = 0 .. m-1\n    //       for i = 0 .. k-1\n    //           for j = 0 .. n-1\n    //               B[i][j] += A[k][i] * B[k][j]\n    //   Then scale by alpha.\n    //\n    // Parallelise the outermost loop for good scalability.\n\n    #pragma omp parallel for schedule(static)\n    for (int k = 0; k < m; ++k) {\n        // cache A_k_i row (i dimension) locally\n        const std::vector<double>& Arow = A[k];\n        const std::vector<double>& Bk   = B[k];\n        for (int i = 0; i < k; ++i) {\n            double aik = Arow[i];\n            std::vector<double>& Bi = B[i];\n            for (int j = 0; j < n; ++j) {\n                Bi[j] += aik * Bk[j];\n            }\n        }\n    }\n\n    // Scale the whole matrix B by alpha.\n    #pragma omp parallel for schedule(static)\n    for (int i = 0; i < m; ++i) {\n        std::vector<double>& Bi = B[i];\n        for (int j = 0; j < n; ++j) {\n            Bi[j] *= alpha;\n        }\n    }\n}\n```\n",
  "cholesky": "```cpp\n#include <cmath>\n#include <vector>\n\n/* Main computational kernel. The whole function will be timed,\n   including the call and return. */\nvoid kernel_cholesky(int n,\n                     std::vector<std::vector<double>>& A) {\n    /* Each row can be accessed directly as a pointer for\n       faster indexing. */\n    for (int i = 0; i < n; ++i) {\n        double* a_i = A[i].data();\n\n        /* Compute the strictly lower part of row i. */\n        for (int j = 0; j < i; ++j) {\n            double sum = 0.0;\n            for (int k = 0; k < j; ++k)\n                sum += a_i[k] * A[j][k];\n            a_i[j] = (a_i[j] - sum) / A[j][j];\n        }\n\n        /* Compute the diagonal element. */\n        double diag = a_i[i];\n        for (int k = 0; k < i; ++k)\n            diag -= a_i[k] * a_i[k];\n        a_i[i] = std::sqrt(diag);\n    }\n}\n```",
  "gramschmidt": "```cpp\n#include <cmath>\n#include <vector>\n#include <omp.h>\n\n/* Main computational kernel. The whole function will be timed,\n   including the call and return. */\n/* QR Decomposition with Modified Gram Schmidt:\n   http://www.inf.ethz.ch/personal/gander/ */\nvoid kernel_gramschmidt(int m, int n,\n                        std::vector<std::vector<double>>& A,\n                        std::vector<std::vector<double>>& R,\n                        std::vector<std::vector<double>>& Q) {\n    // Ensure that the underlying data is contiguous for better cache usage.\n    // The caller guarantees that the vectors are properly sized.\n    double* a = nullptr;\n    double* r = nullptr;\n    double* q = nullptr;\n    std::vector<double> buf(m);\n\n    for (int k = 0; k < n; ++k) {\n        /* Compute norm of column k of A */\n        double nrm = 0.0;\n        #pragma omp parallel for reduction(+:nrm) schedule(static)\n        for (int i = 0; i < m; ++i) {\n            double val = A[i][k];\n            nrm += val * val;\n        }\n        R[k][k] = std::sqrt(nrm);\n\n        /* Normalize column k into Q */\n        double rk = R[k][k];\n        #pragma omp parallel for schedule(static)\n        for (int i = 0; i < m; ++i) {\n            Q[i][k] = A[i][k] / rk;\n        }\n\n        /* Orthogonalize remaining columns k+1..n-1 */\n        #pragma omp parallel for schedule(static)\n        for (int j = k + 1; j < n; ++j) {\n            double dot = 0.0;\n            for (int i = 0; i < m; ++i) {\n                dot += Q[i][k] * A[i][j];\n            }\n            R[k][j] = dot;\n            for (int i = 0; i < m; ++i) {\n                A[i][j] -= Q[i][k] * dot;\n            }\n        }\n    }\n}\n```\n\nThis implementation preserves the original algorithmic behaviour while exploiting OpenMP\u2010based parallelism on the independent inner loops.  The main `k` loop remains sequential because each iteration depends on the results of the previous ones.  Inside each `k`, the norm computation, column normalization, and orthogonalization of subsequent columns are parallelised, giving substantial speedup on a multi\u2011core host.",
  "jacobi_2d": "```cpp\n#include <vector>\n#include <omp.h>\n\n/* Main computational kernel. The whole function will be timed,\n   including the call and return. */\nvoid kernel_jacobi_2d(int tsteps,\n\t\t      int n,\n\t\t      std::vector<std::vector<double>>& A,\n\t\t      std::vector<std::vector<double>>& B) {\n    const double inv5 = 0.2;          // 1/5\n\n    // Allocate local pointers to the underlying storage of each row\n    std::vector<double*> Ap(n), Bp(n);\n    for (int i = 0; i < n; ++i) {\n        Ap[i] = A[i].data();\n        Bp[i] = B[i].data();\n    }\n\n    // Main time\u2011step loop\n    for (int t = 0; t < tsteps; ++t) {\n        /* Update B from A */\n        #pragma omp parallel for schedule(static)\n        for (int i = 1; i < n - 1; ++i) {\n            double* Api = Ap[i];\n            double* Api_m1 = Ap[i - 1];\n            double* Api_p1 = Ap[i + 1];\n            double* Bpi = Bp[i];\n            double* Bpi_m1 = Bp[i - 1];\n            double* Bpi_p1 = Bp[i + 1];\n\n            for (int j = 1; j < n - 1; ++j) {\n                Bpi[j] = inv5 * (Api[j] + Api[j-1] + Api[j+1]\n                                 + Api_p1[j] + Api_m1[j]);\n            }\n        }\n\n        /* Update A from B */\n        #pragma omp parallel for schedule(static)\n        for (int i = 1; i < n - 1; ++i) {\n            double* Bpi = Bp[i];\n            double* Bpi_m1 = Bp[i - 1];\n            double* Bpi_p1 = Bp[i + 1];\n            double* Api = Ap[i];\n            double* Api_m1 = Ap[i - 1];\n            double* Api_p1 = Ap[i + 1];\n\n            for (int j = 1; j < n - 1; ++j) {\n                Api[j] = inv5 * (Bpi[j] + Bpi[j-1] + Bpi[j+1]\n                                 + Bpi_p1[j] + Bpi_m1[j]);\n            }\n        }\n    }\n}\n```",
  "seidel_2d": "```cpp\n#include <vector>\n#include <omp.h>\n\n/* Flattened 2\u2011D array access helper */\ninline double& elem(std::vector<double>& data, int n, int i, int j) {\n    return data[i * n + j];\n}\n\n/* Main computational kernel. The whole function will be timed,\n   including the call and return. */\nvoid kernel_seidel_2d(int tsteps,\n                      int n,\n                      std::vector<std::vector<double>>& A) {\n    /* Flatten the input vector for better cache behavior */\n    std::vector<double> flat((n + 2) * (n + 2));\n    for (int i = 0; i < n + 2; ++i)\n        for (int j = 0; j < n + 2; ++j)\n            flat[i * (n + 2) + j] = A[i][j];\n\n    const int stride = n + 2;\n    const double inv9 = 1.0 / 9.0;\n\n    for (int t = 0; t < tsteps; ++t) {\n#pragma omp parallel for schedule(static) collapse(2)\n        for (int i = 1; i <= n - 2; ++i) {\n            for (int j = 1; j <= n - 2; ++j) {\n                int idx = i * stride + j;\n                double sum = 0.0;\n                // Sum the 3x3 stencil\n                sum += flat[(i - 1) * stride + (j - 1)];\n                sum += flat[(i - 1) * stride + j];\n                sum += flat[(i - 1) * stride + (j + 1)];\n                sum += flat[i * stride + (j - 1)];\n                sum += flat[idx];\n                sum += flat[i * stride + (j + 1)];\n                sum += flat[(i + 1) * stride + (j - 1)];\n                sum += flat[(i + 1) * stride + j];\n                sum += flat[(i + 1) * stride + (j + 1)];\n                flat[idx] = sum * inv9;\n            }\n        }\n    }\n\n    /* Copy the result back to the original 2\u2011D vector */\n    for (int i = 0; i < n + 2; ++i)\n        for (int j = 0; j < n + 2; ++j)\n            A[i][j] = flat[i * stride + j];\n}\n```",
  "fdtd_2d": "```cpp\n#include <vector>\n#include <omp.h>\n\n/* Main computational kernel. The whole function will be timed,\n   including the call and return. */\nvoid kernel_fdtd_2d(int tmax,\n                    int nx,\n                    int ny,\n                    std::vector<std::vector<double>>& ex,\n                    std::vector<std::vector<double>>& ey,\n                    std::vector<std::vector<double>>& hz,\n                    std::vector<double>& _fict_) {\n    const double c1 = 0.5;\n    const double c2 = 0.7;\n\n    // Pre\u2011compute row pointers for faster indexing\n    std::vector<double*> ex_ptrs(nx), ey_ptrs(nx), hz_ptrs(nx);\n    for (int i = 0; i < nx; ++i) {\n        ex_ptrs[i]  = ex[i].data();\n        ey_ptrs[i]  = ey[i].data();\n        hz_ptrs[i]  = hz[i].data();\n    }\n\n    for (int t = 0; t < tmax; ++t) {\n        /* 1. Store fictitious source on the first column of ey */\n        double v = _fict_[t];\n        #pragma omp parallel for\n        for (int j = 0; j < ny; ++j) {\n            ey_ptrs[0][j] = v;\n        }\n\n        /* 2. Update ey[1..nx-1][0..ny-1] */\n        #pragma omp parallel for\n        for (int i = 1; i < nx; ++i) {\n            double* ey_i   = ey_ptrs[i];\n            double* ey_im1 = ey_ptrs[i-1];\n            double* hz_i   = hz_ptrs[i];\n            for (int j = 0; j < ny; ++j) {\n                ey_i[j] -= c1 * (hz_i[j] - hz_im1[j]);\n            }\n        }\n\n        /* 3. Update ex[0..nx-1][1..ny-1] */\n        #pragma omp parallel for\n        for (int i = 0; i < nx; ++i) {\n            double* ex_i  = ex_ptrs[i];\n            double* hz_i  = hz_ptrs[i];\n            for (int j = 1; j < ny; ++j) {\n                ex_i[j] -= c1 * (hz_i[j] - hz_i[j-1]);\n            }\n        }\n\n        /* 4. Update hz[0..nx-2][0..ny-2] */\n        #pragma omp parallel for collapse(2)\n        for (int i = 0; i < nx-1; ++i) {\n            double* hy_i       = hz_ptrs[i];\n            double* hy_ip1     = hz_ptrs[i+1];\n            double* ex_i_plus1 = ex_ptrs[i];\n            double* ex_i       = ex_ptrs[i];\n            double* ey_ip1     = ey_ptrs[i+1];\n            double* ey_i       = ey_ptrs[i];\n            for (int j = 0; j < ny-1; ++j) {\n                hy_i[j] -= c2 * (ex_i_plus1[j+1] - ex_i[j+1]\n                                 + ey_ip1[j]   - ey_i[j]);\n            }\n        }\n    }\n}\n```",
  "gemver": "```cpp\n#include <vector>\n#include <cstddef>\n#include <omp.h>\n\n/* Main computational kernel. The whole function will be timed,\n   including the call and return. */\nvoid kernel_gemver(int n,\n                   double alpha,\n                   double beta,\n                   std::vector<std::vector<double>>& A,\n                   std::vector<double>& u1,\n                   std::vector<double>& v1,\n                   std::vector<double>& u2,\n                   std::vector<double>& v2,\n                   std::vector<double>& w,\n                   std::vector<double>& x,\n                   std::vector<double>& y,\n                   std::vector<double>& z) {\n\n    const int BLOCK = 32;   // reasonable tile size for L1 cache\n\n    /* 1. A += u1*v1^T + u2*v2^T  (tiled) */\n    #pragma omp parallel for collapse(2) schedule(static)\n    for (int ii = 0; ii < n; ii += BLOCK) {\n        for (int jj = 0; jj < n; jj += BLOCK) {\n            int iEnd = std::min(ii + BLOCK, n);\n            int jEnd = std::min(jj + BLOCK, n);\n            for (int i = ii; i < iEnd; ++i) {\n                double u1i = u1[i];\n                double u2i = u2[i];\n                double* Ai = A[i].data();\n                for (int j = jj; j < jEnd; ++j) {\n                    Ai[j] += u1i * v1[j] + u2i * v2[j];\n                }\n            }\n        }\n    }\n\n    /* 2. x += beta * A^T * y  (outer loop over i, inner over j) */\n    #pragma omp parallel for schedule(static)\n    for (int i = 0; i < n; ++i) {\n        double xi = x[i];\n        for (int j = 0; j < n; ++j) {\n            xi += beta * A[j][i] * y[j];\n        }\n        x[i] = xi;\n    }\n\n    /* 3. x += z */\n    #pragma omp parallel for schedule(static)\n    for (int i = 0; i < n; ++i) {\n        x[i] += z[i];\n    }\n\n    /* 4. w += alpha * A * x */\n    #pragma omp parallel for collapse(2) schedule(static)\n    for (int ii = 0; ii < n; ii += BLOCK) {\n        for (int jj = 0; jj < n; jj += BLOCK) {\n            int iEnd = std::min(ii + BLOCK, n);\n            int jEnd = std::min(jj + BLOCK, n);\n            for (int i = ii; i < iEnd; ++i) {\n                double* Ai = A[i].data();\n                double wi = w[i];\n                for (int j = jj; j < jEnd; ++j) {\n                    wi += alpha * Ai[j] * x[j];\n                }\n                w[i] = wi;\n            }\n        }\n    }\n}\n```",
  "lu": "```cpp\n#include <vector>\n#include <omp.h>\n\n/* Main computational kernel. */\nvoid kernel_lu(int n, std::vector<std::vector<double>>& A)\n{\n    /* Parallelise the outermost loop. */\n    #pragma omp parallel for schedule(static)\n    for (int i = 0; i < n; ++i)\n    {\n        /* Work with raw pointers for speed. */\n        double* row_i = A[i].data();\n\n        /* First part: column\u2011wise elimination (j < i). */\n        for (int j = 0; j < i; ++j)\n        {\n            double* row_j = A[j].data();\n            double sum = 0.0;\n\n            /* Inner reduction over k < j. */\n            for (int k = 0; k < j; ++k)\n                sum += row_i[k] * A[k][j];\n\n            row_i[j] -= sum;\n            row_i[j] /= row_j[j];\n        }\n\n        /* Second part: row\u2011wise elimination (j >= i). */\n        for (int j = i; j < n; ++j)\n        {\n            double* row_j = A[j].data();\n            double sum = 0.0;\n\n            for (int k = 0; k < i; ++k)\n                sum += row_i[k] * A[k][j];\n\n            row_i[j] -= sum;\n        }\n    }\n}\n```",
  "floyd_warshall": "```cpp\n#include <vector>\n#include <thread>\n#include <algorithm>\n#include <omp.h>\n\nvoid kernel_floyd_warshall(int n, std::vector<std::vector<int>>& path)\n{\n    // Extract raw pointers to the underlying data for faster access\n    std::vector<int*> raw(n);\n    for (int i = 0; i < n; ++i)\n        raw[i] = path[i].data();\n\n    // Classic Floyd\u2011Warshall with two optimisations:\n    //   1) Inner loop unrolled by a factor of 4.\n    //   2) Parallelise the 'i' loop for each 'k' iteration using OpenMP.\n    for (int k = 0; k < n; ++k) {\n        const int *row_k = raw[k];\n        #pragma omp parallel for schedule(static)\n        for (int i = 0; i < n; ++i) {\n            int *row_i = raw[i];\n            const int aik = row_i[k];\n            if (aik == INT_MAX) continue;          // skip if unreachable\n            int idx = 0;\n            // Four\u2011way unroll for better vectorisation\n            for (; idx <= n - 4; idx += 4) {\n                int r1 = row_i[idx]   > INT_MAX - aik      ? INT_MAX\n                         : row_i[idx] + aik;\n                int r2 = row_i[idx+1] > INT_MAX - aik      ? INT_MAX\n                         : row_i[idx+1] + aik;\n                int r3 = row_i[idx+2] > INT_MAX - aik      ? INT_MAX\n                         : row_i[idx+2] + aik;\n                int r4 = row_i[idx+3] > INT_MAX - aik      ? INT_MAX\n                         : row_i[idx+3] + aik;\n                row_i[idx]   = std::min(row_i[idx],   r1);\n                row_i[idx+1] = std::min(row_i[idx+1], r2);\n                row_i[idx+2] = std::min(row_i[idx+2], r3);\n                row_i[idx+3] = std::min(row_i[idx+3], r4);\n            }\n            // Handle remaining elements\n            for (; idx < n; ++idx) {\n                int r = row_i[idx] > INT_MAX - aik ? INT_MAX\n                          : row_i[idx] + aik;\n                row_i[idx] = std::min(row_i[idx], r);\n            }\n        }\n    }\n}\n```",
  "deriche": "```cpp\n#include <cmath>\n#include <vector>\n#include <omp.h>\n\n/* Optimised kernel_deriche\n   - uses OpenMP parallelism\n   - uses flattened cache\u2011friendly accesses\n   - preserves all original semantics\n*/\nvoid kernel_deriche(int w, int h, double alpha,\n                    std::vector<std::vector<float>>& imgIn,\n                    std::vector<std::vector<float>>& imgOut,\n                    std::vector<std::vector<float>>& y1,\n                    std::vector<std::vector<float>>& y2)\n{\n    // Flattened pointer access helpers\n    auto idx = [h](int i, int j) { return i * h + j; };\n\n    // Pre\u2011allocate contiguous buffers if not already the same size\n    // (this is safe because the harness guarantees size w*h for each vector)\n    std::vector<float> flatIn(w*h), flatOut(w*h), flatY1(w*h), flatY2(w*h);\n    for (int i=0;i<w;i++)\n        for (int j=0;j<h;j++)\n            flatIn[idx(i,j)] = imgIn[i][j];\n\n    // Precompute constants\n    const float exp_a = std::exp(-static_cast<float>(alpha));\n    const float exp_2a = exp_a * exp_a;\n    const float denom = 1.0f + 2.0f * static_cast<float>(alpha) * exp_a - exp_2a;\n    const float k = 1.0f - exp_a * (1.0f - exp_a) / denom;\n\n    const float a1 = k,          a5 = k;\n    const float a2 = k * exp_a * (static_cast<float>(alpha) - 1.0f);\n    const float a6 = a2;\n    const float a3 = k * exp_a * (static_cast<float>(alpha) + 1.0f);\n    const float a7 = a3;\n    const float a4 = -k * exp_2a, a8 = a4;\n    const float b1 = std::pow(2.0f, -static_cast<float>(alpha));\n    const float b2 = -exp_2a;\n    const float c1 = 1.0f, c2 = 1.0f;   // unchanged\n\n    /* --- first vertical passes --- */\n    #pragma omp parallel for schedule(static)\n    for (int i=0;i<w;i++)\n    {\n        float ym1 = 0.0f, ym2 = 0.0f, xm1 = 0.0f;\n        for (int j=0;j<h;j++)\n        {\n            const float in = flatIn[idx(i,j)];\n            flatY1[idx(i,j)] = a1*in + a2*xm1 + b1*ym1 + b2*ym2;\n            xm1 = in;  ym2 = ym1;  ym1 = flatY1[idx(i,j)];\n        }\n    }\n\n    #pragma omp parallel for schedule(static)\n    for (int i=0;i<w;i++)\n    {\n        float yp1 = 0.0f, yp2 = 0.0f, xp1 = 0.0f, xp2 = 0.0f;\n        for (int j=h-1;j>=0;j--)\n        {\n            const float in = flatIn[idx(i,j)];\n            flatY2[idx(i,j)] = a3*xp1 + a4*xp2 + b1*yp1 + b2*yp2;\n            xp2 = xp1;  xp1 = in;  yp2 = yp1;  yp1 = flatY2[idx(i,j)];\n        }\n    }\n\n    /* --- combine to tmp out --- */\n    #pragma omp parallel for collapse(2) schedule(static)\n    for (int i=0;i<w;i++)\n        for (int j=0;j<h;j++)\n            flatOut[idx(i,j)] = c1 * (flatY1[idx(i,j)] + flatY2[idx(i,j)]);\n\n    /* --- second vertical passes on tmp out --- */\n    #pragma omp parallel for schedule(static)\n    for (int i=0;i<w;i++)\n    {\n        float tm1 = 0.0f, ym1 = 0.0f, ym2 = 0.0f;\n        for (int j=0;j<h;j++)\n        {\n            const float out = flatOut[idx(i,j)];\n            flatY1[idx(i,j)] = a5*out + a6*tm1 + b1*ym1 + b2*ym2;\n            tm1 = out;  ym2 = ym1;  ym1 = flatY1[idx(i,j)];\n        }\n    }\n\n    #pragma omp parallel for schedule(static)\n    for (int i=0;i<w;i++)\n    {\n        float tp1 = 0.0f, tp2 = 0.0f, yp1 = 0.0f, yp2 = 0.0f;\n        for (int j=h-1;j>=0;j--)\n        {\n            const float out = flatOut[idx(i,j)];\n            flatY2[idx(i,j)] = a7*tp1 + a8*tp2 + b1*yp1 + b2*yp2;\n            tp2 = tp1;  tp1 = out;  yp2 = yp1;  yp1 = flatY2[idx(i,j)];\n        }\n    }\n\n    /* --- final combine and write back --- */\n    #pragma omp parallel for collapse(2) schedule(static)\n    for (int i=0;i<w;i++)\n        for (int j=0;j<h;j++)\n            imgOut[i][j] = c2 * (flatY1[idx(i,j)] + flatY2[idx(i,j)]);\n}\n```"
}