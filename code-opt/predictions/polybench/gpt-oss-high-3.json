{
  "gemm": "```cpp\n#include <vector>\n#include <omp.h>\n\nvoid kernel_gemm(int ni, int nj, int nk,\n                 double alpha,\n                 double beta,\n                 std::vector<std::vector<double>>& A,\n                 std::vector<std::vector<double>>& B,\n                 std::vector<std::vector<double>>& C)\n{\n    /* Parallelise the outer loop over the rows of C.\n       Each thread works on a distinct row, avoiding any race\n       conditions or false\u2011sharing.  Inside the row loop we use\n       a small amount of explicit SIMD hinting to aid the\n       compiler's auto\u2011vectorisation. */\n\n#pragma omp parallel for schedule(static)\n    for (int i = 0; i < ni; ++i) {\n        double *Ci = C[i].data();\n\n        /* Scale the current row of C by \u03b2. */\n#pragma omp simd\n        for (int j = 0; j < nj; ++j) {\n            Ci[j] *= beta;\n        }\n\n        /* Accumulate \u03b1 * A[i][:] * B[ : ][j] into C[i][j]. */\n        for (int k = 0; k < nk; ++k) {\n            const double aik = alpha * A[i][k];\n            const double *Bk  = B[k].data();\n\n#pragma omp simd\n            for (int j = 0; j < nj; ++j) {\n                Ci[j] += aik * Bk[j];\n            }\n        }\n    }\n}\n```",
  "trisolv": "```cpp\n#include <vector>\n\nvoid kernel_trisolv(int n,\n                    std::vector<std::vector<double>>& L,\n                    std::vector<double>& x,\n                    std::vector<double>& b) {\n    double* x_ptr = x.data();          // writable pointer to x\n    const double* b_ptr = b.data();    // read\u2011only pointer to b\n\n    for (int i = 0; i < n; ++i) {\n        const double* row = L[i].data();   // pointer to the i\u2011th row of L\n        double acc = 0.0;\n        /* The inner loop has no data dependency, so vectorisation is safe */\n        #pragma GCC ivdep\n        for (int j = 0; j < i; ++j) {\n            acc += row[j] * x_ptr[j];\n        }\n        x_ptr[i] = (b_ptr[i] - acc) / row[i];\n    }\n}\n```\n",
  "gesummv": "```cpp\n#include <vector>\n#include <omp.h>\n\nvoid kernel_gesummv(int n,\n                    double alpha,\n                    double beta,\n                    std::vector<std::vector<double>>& A,\n                    std::vector<std::vector<double>>& B,\n                    std::vector<double>& tmp,\n                    std::vector<double>& x,\n                    std::vector<double>& y)\n{\n    const double* xdata = x.data();\n\n    #pragma omp parallel for schedule(static)\n    for (int i = 0; i < n; ++i) {\n        const double* __restrict Ai = A[i].data();\n        const double* __restrict Bi = B[i].data();\n        double ti = 0.0;\n        double yi = 0.0;\n\n        const double* __restrict xptr = xdata;\n        for (int j = 0; j < n; ++j) {\n            double xv = *xptr++;\n            ti += *Ai++ * xv;\n            yi += *Bi++ * xv;\n        }\n\n        tmp[i] = ti;\n        y[i]   = alpha * ti + beta * yi;\n    }\n}\n```",
  "covariance": "```cpp\n#include <vector>\n\nvoid kernel_covariance(int m, int n,\n                       std::vector<std::vector<double>>& data,\n                       std::vector<std::vector<double>>& cov,\n                       std::vector<double>& mean)\n{\n    /* 1. Compute column means */\n    #pragma omp parallel for schedule(static)\n    for (int j = 0; j < m; ++j) {\n        double sum = 0.0;\n        for (int i = 0; i < n; ++i) {\n            sum += data[i][j];\n        }\n        mean[j] = sum / static_cast<double>(n);\n    }\n\n    /* 2. Subtract mean from each element */\n    #pragma omp parallel for schedule(static)\n    for (int i = 0; i < n; ++i) {\n        auto& row = data[i];\n        for (int j = 0; j < m; ++j) {\n            row[j] -= mean[j];\n        }\n    }\n\n    /* 3. Compute covariance matrix (upper triangle) and mirror */\n    #pragma omp parallel for collapse(2) schedule(static)\n    for (int i = 0; i < m; ++i) {\n        for (int j = i; j < m; ++j) {\n            double sum = 0.0;\n            for (int k = 0; k < n; ++k) {\n                sum += data[k][i] * data[k][j];\n            }\n            double val = sum / (static_cast<double>(n) - 1.0);\n            cov[i][j] = val;\n            cov[j][i] = val;\n        }\n    }\n}\n```",
  "jacobi_1d": "```cpp\n#include <vector>\n#include <omp.h>\n\nvoid kernel_jacobi_1d(int tsteps,\n                      int n,\n                      std::vector<double>& A,\n                      std::vector<double>& B) {\n    const double c = 0.33333;\n    #pragma omp parallel\n    {\n        for (int t = 0; t < tsteps; ++t) {\n            /* B  <-  A   */\n            #pragma omp for schedule(static)\n            for (int i = 1; i < n - 1; ++i) {\n                const double v = A[i-1] + A[i] + A[i+1];\n                B[i] = c * v;\n            }\n\n            /* A  <-  B   */\n            #pragma omp for schedule(static)\n            for (int i = 1; i < n - 1; ++i) {\n                const double v = B[i-1] + B[i] + B[i+1];\n                A[i] = c * v;\n            }\n        }\n    }\n}\n```",
  "heat_3d": "```cpp\n#include <vector>\n#include <omp.h>\n\nvoid kernel_heat_3d(int tsteps,\n                   int n,\n                   std::vector<std::vector<std::vector<double>>>& A,\n                   std::vector<std::vector<std::vector<double>>>& B) {\n    constexpr double coef = 0.125;\n    for (int t = 1; t <= tsteps; ++t) {\n        // Compute B from A\n        #pragma omp parallel for collapse(3) schedule(static)\n        for (int i = 1; i < n - 1; ++i) {\n            for (int j = 1; j < n - 1; ++j) {\n                for (int k = 1; k < n - 1; ++k) {\n                    double a_center = A[i][j][k];\n                    double sum = 0.0;\n                    sum += coef * (A[i + 1][j][k] - 2.0 * a_center + A[i - 1][j][k]);\n                    sum += coef * (A[i][j + 1][k] - 2.0 * a_center + A[i][j - 1][k]);\n                    sum += coef * (A[i][j][k + 1] - 2.0 * a_center + A[i][j][k - 1]);\n                    B[i][j][k] = sum + a_center;\n                }\n            }\n        }\n\n        // Compute A from B\n        #pragma omp parallel for collapse(3) schedule(static)\n        for (int i = 1; i < n - 1; ++i) {\n            for (int j = 1; j < n - 1; ++j) {\n                for (int k = 1; k < n - 1; ++k) {\n                    double b_center = B[i][j][k];\n                    double sum = 0.0;\n                    sum += coef * (B[i + 1][j][k] - 2.0 * b_center + B[i - 1][j][k]);\n                    sum += coef * (B[i][j + 1][k] - 2.0 * b_center + B[i][j - 1][k]);\n                    sum += coef * (B[i][j][k + 1] - 2.0 * b_center + B[i][j][k - 1]);\n                    A[i][j][k] = sum + b_center;\n                }\n            }\n        }\n    }\n}\n```",
  "atax": "```cpp\n#include <vector>\n#include <algorithm>\n#include <omp.h>\n\nvoid kernel_atax(int m, int n,\n                 std::vector<std::vector<double>>& A,\n                 std::vector<double>& x,\n                 std::vector<double>& y,\n                 std::vector<double>& tmp) {\n    // Initialise output vector\n    std::fill(y.begin(), y.end(), 0.0);\n\n    const double* x_ptr = x.data();   // pointer to input vector\n    double* y_ptr = y.data();         // pointer to output vector\n    const int N = n;                  // local copy of column count\n\n    #pragma omp parallel for schedule(static) reduction(+: y_ptr[:N])\n    for (int i = 0; i < m; ++i) {\n        const double* row = A[i].data();  // pointer to the i\u2011th row of A\n        double dot = 0.0;\n\n        // First pass: compute dot product of A[i] with x\n        for (int j = 0; j < N; ++j) {\n            dot += row[j] * x_ptr[j];\n        }\n\n        tmp[i] = dot;          // store the dot product\n\n        // Second pass: accumulate into y\n        for (int j = 0; j < N; ++j) {\n            y_ptr[j] += dot * row[j];\n        }\n    }\n}\n```",
  "3mm": "```cpp\n#include <vector>\n#include <omp.h>\n\nvoid kernel_3mm(int ni, int nj, int nk, int nl, int nm,\n                std::vector<std::vector<double>>& E,\n                std::vector<std::vector<double>>& A,\n                std::vector<std::vector<double>>& B,\n                std::vector<std::vector<double>>& F,\n                std::vector<std::vector<double>>& C,\n                std::vector<std::vector<double>>& D,\n                std::vector<std::vector<double>>& G)\n{\n    /* E = A * B */\n    #pragma omp parallel for schedule(static)\n    for (int i = 0; i < ni; ++i) {\n        std::vector<double>& e_row = E[i];\n        const std::vector<double>& a_row = A[i];\n\n        for (int j = 0; j < nj; ++j)\n            e_row[j] = 0.0;\n\n        for (int k = 0; k < nk; ++k) {\n            double aik = a_row[k];\n            const std::vector<double>& Bk = B[k];\n            #pragma omp simd\n            for (int j = 0; j < nj; ++j)\n                e_row[j] += aik * Bk[j];\n        }\n    }\n\n    /* F = C * D */\n    #pragma omp parallel for schedule(static)\n    for (int i = 0; i < nj; ++i) {\n        std::vector<double>& f_row = F[i];\n        const std::vector<double>& c_row = C[i];\n\n        for (int j = 0; j < nl; ++j)\n            f_row[j] = 0.0;\n\n        for (int k = 0; k < nm; ++k) {\n            double c_ik = c_row[k];\n            const std::vector<double>& Dk = D[k];\n            #pragma omp simd\n            for (int j = 0; j < nl; ++j)\n                f_row[j] += c_ik * Dk[j];\n        }\n    }\n\n    /* G = E * F */\n    #pragma omp parallel for schedule(static)\n    for (int i = 0; i < ni; ++i) {\n        std::vector<double>& g_row = G[i];\n        const std::vector<double>& e_row = E[i];\n\n        for (int j = 0; j < nl; ++j)\n            g_row[j] = 0.0;\n\n        for (int k = 0; k < nj; ++k) {\n            double e_ik = e_row[k];\n            const std::vector<double>& Fk = F[k];\n            #pragma omp simd\n            for (int j = 0; j < nl; ++j)\n                g_row[j] += e_ik * Fk[j];\n        }\n    }\n}\n```",
  "doitgen": "```cpp\n#include <vector>\n#include <algorithm>\n#include <omp.h>\n\n/*\u00a0Optimised kernel: matrix\u2011vector multiplication of each row of\u00a0A\u00a0by matrix\u00a0C4.\n   Parallelised over the outer indices (r, q) with a private scratch buffer.\n   The vector\u00a0sum\u00a0is preserved to contain the last computed row, just like the\n   original serial implementation. */\nvoid kernel_doitgen(int nr, int nq, int np,\n                    std::vector<std::vector<std::vector<double>>>& A,\n                    std::vector<std::vector<double>>& C4,\n                    std::vector<double>& sum)\n{\n    /* Parallel section over outer loops\u00a0r\u00a0and\u00a0q\u00a0*/\n    #pragma omp parallel\n    {\n        /* Private scratch buffer per thread */\n        std::vector<double> scratch(np);\n\n        /* Distribute (r, q) pairs across threads */\n        #pragma omp for collapse(2) schedule(static)\n        for (int r = 0; r < nr; ++r) {\n            for (int q = 0; q < nq; ++q) {\n\n                double* row = A[r][q].data();\n\n                /* Zero scratch */\n                std::fill(scratch.begin(), scratch.end(), 0.0);\n\n                /* Compute row\u2011wise product: row = row * C4 */\n                for (int s = 0; s < np; ++s) {\n                    const double  a = row[s];\n                    const double* c4row = C4[s].data();\n                    double* out = scratch.data();\n\n                    #pragma omp simd\n                    for (int p = 0; p < np; ++p) {\n                        out[p] += a * c4row[p];\n                    }\n                }\n\n                /* Copy result back into A */\n                std::copy(scratch.begin(), scratch.end(), row);\n            }\n        }\n    }\n\n    /* Preserve original behaviour: after the loops the vector\u00a0sum\u00a0holds\n       the last computed row. Copy it from A[nr\u20111][nq\u20111] if the vector is valid. */\n    if (!sum.empty() && nr > 0 && nq > 0) {\n        const auto& lastRow = A[nr - 1][nq - 1];\n        std::copy(lastRow.begin(), lastRow.end(), sum.begin());\n    }\n}\n```",
  "bicg": "```cpp\n#include <vector>\n#include <omp.h>\n\nvoid kernel_bicg(int m, int n,\n                 std::vector<std::vector<double>>& A,\n                 std::vector<double>& s,\n                 std::vector<double>& q,\n                 std::vector<double>& p,\n                 std::vector<double>& r)\n{\n    /* Sum of the p vector \u2013 will be added to every q[i]. */\n    double sum_p = 0.0;\n    #pragma omp parallel for reduction(+:sum_p)\n    for (int j = 0; j < m; ++j) {\n        sum_p += p[j];\n    }\n\n    /* s[j] = sum_i r[i] * A[i][j]   (matrix\u2011vector product A\u1d40\u00b7r). */\n    #pragma omp parallel for schedule(static)\n    for (int j = 0; j < m; ++j) {\n        double val = 0.0;\n        #pragma omp simd\n        for (int i = 0; i < n; ++i) {\n            val += r[i] * A[i][j];\n        }\n        s[j] = val;\n    }\n\n    /* q[i] = sum_j (A[i][j] + p[j]) = (sum_j A[i][j]) + sum_p. */\n    #pragma omp parallel for schedule(static)\n    for (int i = 0; i < n; ++i) {\n        double sumA = 0.0;\n        const double* row = A[i].data();\n        #pragma omp simd\n        for (int j = 0; j < m; ++j) {\n            sumA += row[j];\n        }\n        q[i] = sumA + sum_p;\n    }\n}\n```",
  "durbin": "```cpp\n#include <vector>\n#include <cstring>\n\nvoid kernel_durbin(int n,\n                   std::vector<double>& r,\n                   std::vector<double>& y)\n{\n    if (n <= 0) return;\n\n    std::vector<double> z(n);          // temporary buffer\n    const double* r_arr = r.data();   // read\u2011only alias of r\n    double*       y_arr = y.data();   // writable alias of y\n    double*       z_arr = z.data();   // writable alias of z\n\n    y_arr[0] = -r_arr[0];\n    double beta  = 1.0;\n    double alpha = -r_arr[0];\n\n    for (int k = 1; k < n; ++k)\n    {\n        beta = (1.0 - alpha * alpha) * beta;\n\n        /* compute s = \u03a3_{i=0}^{k-1} r[k-1-i] * y[i] */\n        double sum = 0.0;\n        const double* r_ptr = r_arr + k - 1;\n        for (int i = 0; i < k; ++i)\n            sum += (*r_ptr--) * y_arr[i];\n\n        alpha = -(r_arr[k] + sum) / beta;\n\n        /* compute z[i] = y[i] + alpha * y[k-1-i] */\n        for (int i = 0; i < k; ++i)\n            z_arr[i] = y_arr[i] + alpha * y_arr[k - 1 - i];\n\n        /* copy new values back to y */\n        std::memcpy(y_arr, z_arr, static_cast<std::size_t>(k) * sizeof(double));\n\n        y_arr[k] = alpha;\n    }\n}\n```",
  "nussinov": "```cpp\n#include <vector>\n#include <omp.h>\n\nvoid kernel_nussinov(int n, std::vector<int>& seq,\n                     std::vector<std::vector<int>>& table) {\n    // Precompute a pointer to each row for fast access\n    std::vector<int*> rows(n);\n    for (int i = 0; i < n; ++i) rows[i] = table[i].data();\n\n    const int* seqp = seq.data();   // local copy for faster indexing\n\n    for (int d = 2; d <= n; ++d) {\n        #pragma omp parallel for schedule(static)\n        for (int i = 0; i + d - 1 < n; ++i) {\n            int j = i + d - 1;\n            int* row_i   = rows[i];\n            int* row_ip1 = rows[i + 1];\n\n            int best = row_i[j];  // start with the current value\n\n            // Neighbor cells\n            int tmp = row_i[j - 1];\n            if (tmp > best) best = tmp;\n            tmp = row_ip1[j];\n            if (tmp > best) best = tmp;\n\n            // Match handling (adjacent bases are not allowed to pair)\n            if (i < j - 1) {\n                int match_val = ((seqp[i] + seqp[j]) == 3) ? 1 : 0;\n                tmp = row_ip1[j - 1] + match_val;\n                if (tmp > best) best = tmp;\n            } else {\n                tmp = row_ip1[j - 1];\n                if (tmp > best) best = tmp;\n            }\n\n            // Split the interval\n            for (int k = i + 1; k < j; ++k) {\n                int sum = row_i[k] + rows[k + 1][j];\n                if (sum > best) best = sum;\n            }\n\n            row_i[j] = best;  // store the computed value\n        }\n    }\n}\n```",
  "syr2k": "```cpp\n#include <vector>\n\nvoid kernel_syr2k(int n, int m,\n                  double alpha,\n                  double beta,\n                  std::vector<std::vector<double>>& C,\n                  std::vector<std::vector<double>>& A,\n                  std::vector<std::vector<double>>& B) {\n    #pragma omp parallel for schedule(static)\n    for (int i = 0; i < n; ++i) {\n        double* C_i = C[i].data();\n        const double* A_i = A[i].data();\n        const double* B_i = B[i].data();\n\n        // Scale the lower\u2011triangular part of C\n        for (int j = 0; j <= i; ++j) {\n            C_i[j] *= beta;\n        }\n\n        // Outer\u2013product updates\n        for (int k = 0; k < m; ++k) {\n            const double B_ik = B_i[k];\n            const double A_ik = A_i[k];\n            for (int j = 0; j <= i; ++j) {\n                C_i[j] += (A[j][k] * alpha) * B_ik + (B[j][k] * alpha) * A_ik;\n            }\n        }\n    }\n}\n```\n",
  "2mm": "```cpp\n#include <vector>\n#include <omp.h>\n\nvoid kernel_2mm(int ni, int nj, int nk, int nl,\n                double alpha, double beta,\n                std::vector<std::vector<double>>& tmp,\n                std::vector<std::vector<double>>& A,\n                std::vector<std::vector<double>>& B,\n                std::vector<std::vector<double>>& C,\n                std::vector<std::vector<double>>& D)\n{\n    /* Pre\u2011compute row pointers to avoid repeated indexing of the inner\n       vectors of B and C. These arrays are read\u2011only and are safely\n       shared among all OpenMP threads. */\n    std::vector<const double*> B_rows(nk);\n    for (int k = 0; k < nk; ++k) B_rows[k] = B[k].data();\n\n    std::vector<const double*> C_rows(nj);\n    for (int k = 0; k < nj; ++k) C_rows[k] = C[k].data();\n\n    #pragma omp parallel\n    {\n        /* 1st phase \u2013 compute tmp = alpha * (A * B) */\n        #pragma omp for schedule(static)\n        for (int i = 0; i < ni; ++i) {\n            double* tmpRow = tmp[i].data();\n            const double* aRow = A[i].data();\n\n            /* initialise tmp[i] to zero */\n            for (int j = 0; j < nj; ++j) tmpRow[j] = 0.0;\n\n            /* performing i\u2011k\u2011j order improves cache reuse on B */\n            for (int k = 0; k < nk; ++k) {\n                double coeff = alpha * aRow[k];\n                const double* bRow = B_rows[k];\n                #pragma omp simd\n                for (int j = 0; j < nj; ++j) {\n                    tmpRow[j] += coeff * bRow[j];\n                }\n            }\n        }\n\n        /* 2nd phase \u2013 D = beta * D + tmp * C */\n        #pragma omp for schedule(static)\n        for (int i = 0; i < ni; ++i) {\n            double* dRow = D[i].data();\n            const double* tmpRow = tmp[i].data();\n\n            /* scale the D row by beta */\n            #pragma omp simd\n            for (int j = 0; j < nl; ++j) dRow[j] *= beta;\n\n            /* accumulate the product of tmp[i] and C */\n            for (int k = 0; k < nj; ++k) {\n                double t = tmpRow[k];\n                const double* cRow = C_rows[k];\n                #pragma omp simd\n                for (int j = 0; j < nl; ++j) {\n                    dRow[j] += t * cRow[j];\n                }\n            }\n        }\n    }\n}\n```",
  "mvt": "```cpp\n#include <vector>\n#include <omp.h>\n\nvoid kernel_mvt(int n,\n                std::vector<double>& x1,\n                std::vector<double>& x2,\n                std::vector<double>& y_1,\n                std::vector<double>& y_2,\n                std::vector<std::vector<double>>& A) {\n    const double *y1 = y_1.data();\n    const double *y2 = y_2.data();\n\n    /* Pre\u2011compute row pointers for efficient column access in the\n       transpose part of the kernel. */\n    std::vector<const double*> row_ptrs(n);\n    for (int j = 0; j < n; ++j) {\n        row_ptrs[j] = A[j].data();\n    }\n\n    /* Parallelize the two independent matrix\u2011vector products. */\n    #pragma omp parallel\n    {\n        /* x1 += A * y1  (row\u2011major access) */\n        #pragma omp for schedule(static)\n        for (int i = 0; i < n; ++i) {\n            double sum = x1[i];\n            const double *row = A[i].data();\n            const double *p_y1 = y1;\n            for (int j = 0; j < n; ++j) {\n                sum += row[j] * p_y1[j];\n            }\n            x1[i] = sum;\n        }\n\n        /* x2 += A\u1d40 * y2  (column\u2011major access) */\n        #pragma omp for schedule(static)\n        for (int i = 0; i < n; ++i) {\n            double sum = x2[i];\n            for (int j = 0; j < n; ++j) {\n                sum += row_ptrs[j][i] * y2[j];\n            }\n            x2[i] = sum;\n        }\n    }\n}\n```",
  "seidel_2d": "```cpp\n#include <vector>\n\nconstexpr double INV9 = 1.0 / 9.0;\n\n/* Main computational kernel. The whole function will be timed,\n   including the call and return. */\nvoid kernel_seidel_2d(int tsteps,\n\t\t      int n,\n\t\t      std::vector<std::vector<double>>& A) {\n    for (int t = 0; t < tsteps; ++t) {\n        for (int i = 1; i <= n - 2; ++i) {\n            double* __restrict up   = A[i - 1].data();  // row above\n            double* __restrict cur  = A[i].data();      // current row\n            double* __restrict down = A[i + 1].data();  // row below\n\n            /* The pointers point to column j\u20111 before the first iteration.\n               They are advanced in the loop header so that inside the body\n               we access the three consecutive columns [j\u20111, j, j+1]. */\n            for (int j = 1; j <= n - 2; ++j, ++up, ++cur, ++down) {\n                const double old = cur[1];               // A[i][j] before update\n                const double sum = up[0] + up[1] + up[2] +  // row above\n                                   cur[0] + old + cur[2] + // current row\n                                   down[0] + down[1] + down[2]; // row below\n                cur[1] = sum * INV9;\n            }\n        }\n    }\n}\n```",
  "cholesky": "```cpp\n#include <cmath>\n#include <vector>\n\nvoid kernel_cholesky(int n, std::vector<std::vector<double>>& A) {\n    for (int i = 0; i < n; ++i) {\n        double* row_i = A[i].data();\n        for (int j = 0; j < i; ++j) {\n            double sum = row_i[j];\n            const double* row_j = A[j].data();\n            for (int k = 0; k < j; ++k) {\n                sum -= row_i[k] * row_j[k];\n            }\n            row_i[j] = sum / row_j[j];\n        }\n        double diag = row_i[i];\n        for (int k = 0; k < i; ++k) {\n            diag -= row_i[k] * row_i[k];\n        }\n        row_i[i] = std::sqrt(diag);\n    }\n}\n```",
  "syrk": "```cpp\n#include <vector>\n#include <omp.h>\n\n/* Main computational kernel.  The whole function will be timed,\n   including the call and return. */\nvoid kernel_syrk(int n, int m,\n                 double alpha,\n                 double beta,\n                 std::vector<std::vector<double>>& C,\n                 std::vector<std::vector<double>>& A)\n{\n#pragma omp parallel for schedule(static)\n    for (int i = 0; i < n; ++i) {\n        double* Ci = C[i].data();          // C[i][]\n        const double* Ai = A[i].data();    // A[i][]\n\n        /* Scale the lower\u2010triangular part of C by beta */\n        for (int j = 0; j <= i; ++j) {\n            Ci[j] *= beta;\n        }\n\n        /* Accumulate alpha*A*A\u1d40 into C */\n        for (int k = 0; k < m; ++k) {\n            double aik_alpha = alpha * Ai[k];   // (\u03b1 * A[i][k])\n            for (int j = 0; j <= i; ++j) {\n                const double* Aj = A[j].data(); // A[j][]\n                Ci[j] += aik_alpha * Aj[k];\n            }\n        }\n    }\n}\n```",
  "adi": "```cpp\n#include <vector>\n#include <omp.h>\n\nvoid kernel_adi(int tsteps, int n,\n                std::vector<std::vector<double>>& u,\n                std::vector<std::vector<double>>& v,\n                std::vector<std::vector<double>>& p,\n                std::vector<std::vector<double>>& q)\n{\n    const double DX = 1.0 / n;\n    const double DY = 1.0 / n;\n    const double DT = 1.0 / tsteps;\n    const double B1 = 2.0;\n    const double B2 = 1.0;\n    const double mul1 = B1 * DT / (DX * DX);\n    const double mul2 = B2 * DT / (DY * DY);\n    const double a = -mul1 / 2.0;\n    const double b = 1.0 + mul1;\n    const double c = a;\n    const double d = -mul2 / 2.0;\n    const double e = 1.0 + mul2;\n    const double f = d;\n\n    const double one_plus_2d = 1.0 + 2.0 * d;\n    const double one_plus_2a = 1.0 + 2.0 * a;\n\n    #pragma omp parallel\n    {\n        for (int t = 1; t <= tsteps; ++t)\n        {\n            /* --- compute v --- */\n            #pragma omp for schedule(static)\n            for (int i = 1; i < n - 1; ++i)\n            {\n                v[0][i] = 1.0;\n                p[i][0] = 0.0;\n                q[i][0] = v[0][i];\n\n                double p_prev = p[i][0];\n                double q_prev = q[i][0];\n\n                for (int j = 1; j < n - 1; ++j)\n                {\n                    double denom   = a * p_prev + b;\n                    double inv_den = 1.0 / denom;\n                    double p_cur   = (-c) * inv_den;\n\n                    const double* urow = u[j].data();\n                    double u_left  = urow[i - 1];\n                    double u_mid   = urow[i];\n                    double u_right = urow[i + 1];\n\n                    double q_num = (-d) * u_left + one_plus_2d * u_mid - f * u_right - a * q_prev;\n                    double q_cur = q_num * inv_den;\n\n                    p[i][j] = p_cur;\n                    q[i][j] = q_cur;\n\n                    p_prev = p_cur;\n                    q_prev = q_cur;\n                }\n\n                v[n - 1][i] = 1.0;\n                for (int j = n - 2; j >= 1; --j)\n                {\n                    double v_next = v[j + 1][i];\n                    v[j][i] = p[i][j] * v_next + q[i][j];\n                }\n            }\n\n            /* --- compute u --- */\n            #pragma omp for schedule(static)\n            for (int i = 1; i < n - 1; ++i)\n            {\n                u[i][0] = 1.0;\n                p[i][0] = 0.0;\n                q[i][0] = u[i][0];\n\n                double p_prev = p[i][0];\n                double q_prev = q[i][0];\n\n                const double* v_up   = v[i - 1].data();\n                const double* v_mid  = v[i].data();\n                const double* v_down = v[i + 1].data();\n\n                for (int j = 1; j < n - 1; ++j)\n                {\n                    double denom   = d * p_prev + e;\n                    double inv_den = 1.0 / denom;\n                    double p_cur   = (-f) * inv_den;\n\n                    double v_up_j   = v_up[j];\n                    double v_mid_j  = v_mid[j];\n                    double v_down_j = v_down[j];\n\n                    double q_num = (-a) * v_up_j + one_plus_2a * v_mid_j - c * v_down_j - d * q_prev;\n                    double q_cur = q_num * inv_den;\n\n                    p[i][j] = p_cur;\n                    q[i][j] = q_cur;\n\n                    p_prev = p_cur;\n                    q_prev = q_cur;\n                }\n\n                u[i][n - 1] = 1.0;\n                for (int j = n - 2; j >= 1; --j)\n                {\n                    double u_next = u[i][j + 1];\n                    u[i][j] = p[i][j] * u_next + q[i][j];\n                }\n            }\n        }\n    }\n}\n```",
  "jacobi_2d": "```cpp\n#include <vector>\n#include <omp.h>\n\n/* Main computational kernel. The whole function will be timed,\n   including the call and return. */\nvoid kernel_jacobi_2d(int tsteps,\n                      int n,\n                      std::vector<std::vector<double>>& A,\n                      std::vector<std::vector<double>>& B)\n{\n    const double alpha = 0.2;   // 1/5\n\n    #pragma omp parallel\n    {\n        for (int t = 0; t < tsteps; ++t)\n        {\n            /* B \u2190 0.2 * (A + neighbor averages) */\n            #pragma omp for schedule(static)\n            for (int i = 1; i < n - 1; ++i)\n            {\n                const double * Ai  = A[i].data();\n                const double * Aip = A[i + 1].data();\n                const double * Aim = A[i - 1].data();\n                double       * Bi  = B[i].data();\n\n#pragma omp simd\n                for (int j = 1; j < n - 1; ++j)\n                {\n                    Bi[j] = alpha * (Ai[j]  + Ai[j-1] + Ai[j+1]\n                                    + Aip[j] + Aim[j]);\n                }\n            }\n\n            /* A \u2190 0.2 * (B + neighbor averages) */\n            #pragma omp for schedule(static)\n            for (int i = 1; i < n - 1; ++i)\n            {\n                const double * Bi  = B[i].data();\n                const double * Bip = B[i + 1].data();\n                const double * Bim = B[i - 1].data();\n                double       * Ai  = A[i].data();\n\n#pragma omp simd\n                for (int j = 1; j < n - 1; ++j)\n                {\n                    Ai[j] = alpha * (Bi[j]  + Bi[j-1] + Bi[j+1]\n                                    + Bip[j] + Bim[j]);\n                }\n            }\n        }\n    }\n}\n```",
  "fdtd_2d": "```cpp\n#include <vector>\n#include <omp.h>\n\nvoid kernel_fdtd_2d(int tmax,\n                    int nx,\n                    int ny,\n                    std::vector<std::vector<double>>& ex,\n                    std::vector<std::vector<double>>& ey,\n                    std::vector<std::vector<double>>& hz,\n                    std::vector<double>& _fict_) {\n    // Create row pointers for faster indexing\n    std::vector<double*> ex_rows(nx), ey_rows(nx), hz_rows(nx);\n    for (int i = 0; i < nx; ++i) {\n        ex_rows[i] = ex[i].data();\n        ey_rows[i] = ey[i].data();\n        hz_rows[i] = hz[i].data();\n    }\n\n    const double coeff_ey = 0.5;\n    const double coeff_ex = 0.5;\n    const double coeff_hz = 0.7;\n\n    #pragma omp parallel\n    {\n        for (int t = 0; t < tmax; ++t) {\n            const double fict_val = _fict_[t];\n\n            /* Update ey[0][j] */\n            #pragma omp for schedule(static)\n            for (int j = 0; j < ny; ++j) {\n                ey_rows[0][j] = fict_val;\n            }\n\n            /* Update ey[i][j] */\n            #pragma omp for schedule(static)\n            for (int i = 1; i < nx; ++i) {\n                double* ey_i    = ey_rows[i];\n                double* ey_prev = ey_rows[i - 1];\n                double* hz_i    = hz_rows[i];\n                double* hz_prev = hz_rows[i - 1];\n                #pragma omp simd\n                for (int j = 0; j < ny; ++j) {\n                    ey_i[j] -= coeff_ey * (hz_i[j] - hz_prev[j]);\n                }\n            }\n\n            /* Update ex[i][j] */\n            #pragma omp for schedule(static)\n            for (int i = 0; i < nx; ++i) {\n                double* ex_i = ex_rows[i];\n                double* hz_i = hz_rows[i];\n                #pragma omp simd\n                for (int j = 1; j < ny; ++j) {\n                    ex_i[j] -= coeff_ex * (hz_i[j] - hz_i[j - 1]);\n                }\n            }\n\n            /* Update hz[i][j] */\n            #pragma omp for schedule(static)\n            for (int i = 0; i < nx - 1; ++i) {\n                double* hz_i    = hz_rows[i];\n                double* ex_i    = ex_rows[i];\n                double* ey_ip1  = ey_rows[i + 1];\n                double* ey_i    = ey_rows[i];\n                #pragma omp simd\n                for (int j = 0; j < ny - 1; ++j) {\n                    hz_i[j] -= coeff_hz *\n                        (ex_i[j + 1] - ex_i[j] + ey_ip1[j] - ey_i[j]);\n                }\n            }\n        }\n    }\n}\n```",
  "correlation": "```cpp\n#include <vector>\n#include <cmath>\n#include <omp.h>\n\nvoid kernel_correlation(int m, int n,\n            std::vector<std::vector<double>>& data,\n            std::vector<std::vector<double>>& corr,\n            std::vector<double>& mean,\n            std::vector<double>& stddev) {\n    if (m <= 0 || n <= 0) return;\n\n    const double eps = 0.1;\n\n    /* 1. Compute column means */\n    #pragma omp parallel for schedule(static)\n    for (int j = 0; j < m; ++j) {\n        double s = 0.0;\n        for (int i = 0; i < n; ++i) {\n            s += data[i][j];\n        }\n        mean[j] = s / n;\n    }\n\n    /* 2. Compute column standard deviations */\n    #pragma omp parallel for schedule(static)\n    for (int j = 0; j < m; ++j) {\n        double ssq = 0.0;\n        double mj = mean[j];\n        for (int i = 0; i < n; ++i) {\n            double d = data[i][j] - mj;\n            ssq += d * d;\n        }\n        double stdv = std::sqrt(ssq / n);\n        stddev[j] = (stdv <= eps) ? 1.0 : stdv;\n    }\n\n    /* 3. Standardise the data in place */\n    const double sqrt_n = std::sqrt(static_cast<double>(n));\n    std::vector<double> denom(m);\n    for (int j = 0; j < m; ++j) denom[j] = sqrt_n * stddev[j];\n\n    #pragma omp parallel for schedule(static)\n    for (int i = 0; i < n; ++i) {\n        double* row = data[i].data();\n        for (int j = 0; j < m; ++j) {\n            row[j] = (row[j] - mean[j]) / denom[j];\n        }\n    }\n\n    /* 4. Compute the correlation matrix */\n    #pragma omp parallel for schedule(static)\n    for (int i = 0; i < m; ++i) {\n        double* corr_i = corr[i].data();\n\n        /* Diagonal entry */\n        corr_i[i] = 1.0;\n\n        /* Zero the upper\u2011triangle for this row */\n        for (int j = i + 1; j < m; ++j) {\n            corr_i[j] = 0.0;\n        }\n\n        /* Accumulate dot products for all contributing rows */\n        for (int k = 0; k < n; ++k) {\n            double* data_k = data[k].data();\n            double xi = data_k[i];\n            for (int j = i + 1; j < m; ++j) {\n                corr_i[j] += xi * data_k[j];\n            }\n        }\n\n        /* Mirror into the lower\u2011triangle */\n        for (int j = i + 1; j < m; ++j) {\n            corr[j][i] = corr_i[j];\n        }\n    }\n}\n```",
  "gemver": "```c++\n#include <vector>\n#include <omp.h>\n\nvoid kernel_gemver(int n,\n                   double alpha,\n                   double beta,\n                   std::vector<std::vector<double>>& A,\n                   std::vector<double>& u1,\n                   std::vector<double>& v1,\n                   std::vector<double>& u2,\n                   std::vector<double>& v2,\n                   std::vector<double>& w,\n                   std::vector<double>& x,\n                   std://std::vector<double>& y,\n                   std::vector<double>& z) {\n    if (n <= 0) return;\n\n    /* 1. A += outer(u1, v1) + outer(u2, v2) */\n    #pragma omp parallel for schedule(static)\n    for (int i = 0; i < n; ++i) {\n        const double u1i = u1[i];\n        const double u2i = u2[i];\n        double* Ai = A[i].data();\n        const double* v1_data = v1.data();\n        const double* v2_data = v2.data();\n        #pragma omp simd\n        for (int j = 0; j < n; ++j) {\n            Ai[j] += u1i * v1_data[j] + u2i * v2_data[j];\n        }\n    }\n\n    /* 2. x += beta * A^T * y */\n    const double* y_data = y.data();\n    double* x_data = x.data();\n    #pragma omp parallel for schedule(static)\n    for (int i = 0; i < n; ++i) {\n        double sum = x_data[i];\n        for (int j = 0; j < n; ++j) {\n            sum += beta * A[j][i] * y_data[j];\n        }\n        x_data[i] = sum;\n    }\n\n    /* 3. x += z */\n    #pragma omp parallel for schedule(static)\n    for (int i = 0; i < n; ++i) {\n        x_data[i] += z[i];\n    }\n\n    /* 4. w += alpha * A * x */\n    const double* x_data2 = x_data;\n    double* w_data = w.data();\n    #pragma omp parallel for schedule(static)\n    for (int i = 0; i < n; ++i) {\n        double sum = w_data[i];\n        const double* Ai = A[i].data();\n        #pragma omp simd\n        for (int j = 0; j < n; ++j) {\n            sum += alpha * Ai[j] * x_data2[j];\n        }\n        w_data[i] = sum;\n    }\n}\n```\n",
  "ludcmp": "```cpp\n#include <vector>\n\nvoid kernel_ludcmp(int n,\n                   std::vector<std::vector<double>>& A,\n                   std::vector<double>& b,\n                   std::vector<double>& x,\n                   std::vector<double>& y)\n{\n    if (n <= 0) return;\n\n    /* Create fast row pointers. */\n    std::vector<double*> Ap(n);\n    for (int i = 0; i < n; ++i)\n        Ap[i] = A[i].data();\n\n    /* LU decomposition */\n    for (int i = 0; i < n; ++i) {\n        double* Ai = Ap[i];\n\n        /* --- Lower triangular part (L) --- */\n        for (int j = 0; j < i; ++j) {\n            double w   = Ai[j];\n            double sum = 0.0;\n            for (int k = 0; k < j; ++k)\n                sum += Ai[k] * Ap[k][j];\n            Ai[j] = (w - sum) / Ap[j][j];\n        }\n\n        /* --- Upper triangular part (U) --- */\n        for (int k = 0; k < i; ++k) {\n            double aik   = Ai[k];\n            double* Ak    = Ap[k];\n            double* AiPtr = Ai + i;   // address of A[i][i]\n            double* AkPtr = Ak + i;   // address of A[k][i]\n            int     len   = n - i;\n            for (int jj = 0; jj < len; ++jj)\n                AiPtr[jj] -= aik * AkPtr[jj];\n        }\n    }\n\n    /* Forward substitution: y = L \\ b */\n    double* bp = b.data();\n    double* yp = y.data();\n    for (int i = 0; i < n; ++i) {\n        double sum = bp[i];\n        double* Ai = Ap[i];\n        for (int j = 0; j < i; ++j)\n            sum -= Ai[j] * yp[j];\n        yp[i] = sum;\n    }\n\n    /* Backward substitution: x = U \\ y */\n    double* xp = x.data();\n    for (int i = n - 1; i >= 0; --i) {\n        double sum = yp[i];\n        double* Ai = Ap[i];\n        for (int j = i + 1; j < n; ++j)\n            sum -= Ai[j] * xp[j];\n        xp[i] = sum / Ai[i];\n    }\n}\n```",
  "deriche": "```cpp\n#include <cmath>\n#include <vector>\n#include <omp.h>\n\nvoid kernel_deriche(int w, int h, double alpha,\n                    std::vector<std::vector<float>>& imgIn,\n                    std::vector<std::vector<float>>& imgOut,\n                    std::vector<std::vector<float>>& y1,\n                    std::vector<std::vector<float>>& y2)\n{\n    /* Pre\u2011compute constants (same as the original C++ code) */\n    const double alphaD            = alpha;\n    const double expAlpha          = std::exp(-alphaD);    // e^(-\u03b1)\n    const double expPos2Alpha      = std::exp( 2.0*alphaD); // e^(+2\u03b1)\n    const double expMinus2Alpha    = std::exp(-2.0*alphaD); // e^(-2\u03b1)\n    const double pow2NegAlpha      = std::pow(2.0, -alphaD); // 2^(-\u03b1)\n\n    const double numerator   = expAlpha * (1.0 - expAlpha); \n    const double denominator = 1.0 + 2.0 * alphaD * expAlpha - expPos2Alpha;\n    const float k = static_cast<float>(1.0 - numerator / denominator);\n\n    const float a1 = k,          a5 = k;\n    const float a2 = static_cast<float>(static_cast<double>(k) * expAlpha * (alphaD - 1.0));\n    const float a6 = a2;\n    const float a3 = static_cast<float>(static_cast<double>(k) * expAlpha * (alphaD + 1.0));\n    const float a7 = a3;\n    const float a4 = static_cast<float>(-static_cast<double>(k) * expMinus2Alpha);\n    const float a8 = a4;\n\n    const float b1 = static_cast<float>(pow2NegAlpha);\n    const float b2 = static_cast<float>(-expMinus2Alpha);\n    const float c1 = 1.0f,  c2 = 1.0f;\n\n    /* Build row pointers for more efficient indexing */\n    std::vector<float*> imgIn_rows(w);\n    std::vector<float*> imgOut_rows(w);\n    std::vector<float*> y1_rows(w);\n    std::vector<float*> y2_rows(w);\n    for (int i = 0; i < w; ++i)\n    {\n        imgIn_rows[i]  = imgIn[i].data();\n        imgOut_rows[i] = imgOut[i].data();\n        y1_rows[i]     = y1[i].data();\n        y2_rows[i]     = y2[i].data();\n    }\n\n    /* Row\u2011wise forward pass */\n#pragma omp parallel for schedule(static)\n    for (int i = 0; i < w; ++i)\n    {\n        const float*   rowIn  = imgIn_rows[i];\n        float*         rowY1  = y1_rows[i];\n        float ym1 = 0.0f, ym2 = 0.0f, xm1 = 0.0f;\n        for (int j = 0; j < h; ++j)\n        {\n            float cur = a1 * rowIn[j] + a2 * xm1 + b1 * ym1 + b2 * ym2;\n            rowY1[j]  = cur;\n            xm1       = rowIn[j];\n            ym2       = ym1;\n            ym1       = cur;\n        }\n    }\n\n    /* Row\u2011wise backward pass */\n#pragma omp parallel for schedule(static)\n    for (int i = 0; i < w; ++i)\n    {\n        const float*   rowIn  = imgIn_rows[i];\n        float*         rowY2  = y2_rows[i];\n        float yp1 = 0.0f, yp2 = 0.0f, xp1 = 0.0f, xp2 = 0.0f;\n        for (int j = h - 1; j >= 0; --j)\n        {\n            float cur = a3 * xp1 + a4 * xp2 + b1 * yp1 + b2 * yp2;\n            rowY2[j]  = cur;\n            xp2       = xp1;\n            xp1       = rowIn[j];\n            yp2       = yp1;\n            yp1       = cur;\n        }\n    }\n\n    /* Merge row results into imgOut */\n#pragma omp parallel for schedule(static)\n    for (int i = 0; i < w; ++i)\n    {\n        float*        rowOut = imgOut_rows[i];\n        const float*  rowY1  = y1_rows[i];\n        const float*  rowY2  = y2_rows[i];\n        for (int j = 0; j < h; ++j)\n        {\n            rowOut[j] = c1 * (rowY1[j] + rowY2[j]);   // c1 == 1\n        }\n    }\n\n    /* Column\u2011wise forward pass */\n#pragma omp parallel for schedule(static)\n    for (int j = 0; j < h; ++j)\n    {\n        float tm1 = 0.0f, ym1 = 0.0f, ym2 = 0.0f;\n        for (int i = 0; i < w; ++i)\n        {\n            float*        rowY1 = y1_rows[i];\n            const float*  rowOut = imgOut_rows[i];\n            float cur = a5 * rowOut[j] + a6 * tm1 + b1 * ym1 + b2 * ym2;\n            rowY1[j] = cur;\n            tm1      = rowOut[j];\n            ym2      = ym1;\n            ym1      = cur;\n        }\n    }\n\n    /* Column\u2011wise backward pass */\n#pragma omp parallel for schedule(static)\n    for (int j = 0; j < h; ++j)\n    {\n        float xp1 = 0.0f, xp2 = 0.0f, yp1 = 0.0f, yp2 = 0.0f;\n        for (int i = w - 1; i >= 0; --i)\n        {\n            float*        rowY2 = y2_rows[i];\n            const float*  rowOut = imgOut_rows[i];\n            float cur = a7 * xp1 + a8 * xp2 + b1 * yp1 + b2 * yp2;\n            rowY2[j] = cur;\n            xp2      = xp1;\n            xp1      = rowOut[j];\n            yp2      = yp1;\n            yp1      = cur;\n        }\n    }\n\n    /* Final merge into imgOut */\n#pragma omp parallel for schedule(static)\n    for (int i = 0; i < w; ++i)\n    {\n        float*        rowOut = imgOut_rows[i];\n        const float*  rowY1  = y1_rows[i];\n        const float*  rowY2  = y2_rows[i];\n        for (int j = 0; j < h; ++j)\n        {\n            rowOut[j] = c2 * (rowY1[j] + rowY2[j]);   // c2 == 1\n        }\n    }\n}\n```",
  "symm": "```cpp\n#include <vector>\n#include <omp.h>\n\nvoid kernel_symm(int m, int n,\n                 double alpha,\n                 double beta,\n                 std::vector<std::vector<double>>& C,\n                 std::vector<std::vector<double>>& A,\n                 std::vector<std::vector<double>>& B) {\n    /* Pre\u2011compute row pointers for better locality */\n    std::vector<double*> A_rows(m), B_rows(m), C_rows(m);\n    for (int i = 0; i < m; ++i) {\n        A_rows[i] = A[i].data();\n        B_rows[i] = B[i].data();\n        C_rows[i] = C[i].data();\n    }\n\n    /* Parallel over columns for each row i */\n    #pragma omp parallel\n    {\n        for (int i = 0; i < m; ++i) {\n            double* Arow = A_rows[i];\n            double* Brow = B_rows[i];\n            double* Crow = C_rows[i];\n            double   Aii   = Arow[i];\n            double   diagCoeff = alpha * Aii;   // alpha * A[i][i]\n\n            #pragma omp for schedule(static)\n            for (int j = 0; j < n; ++j) {\n                double Bj      = Brow[j];\n                double alphaBj = alpha * Bj;\n                double temp2   = 0.0;\n\n                for (int k = 0; k < i; ++k) {\n                    double aik   = Arow[k];\n                    double* CkRow = C_rows[k];\n                    double Bkj   = B_rows[k][j];\n                    CkRow[j]     += alphaBj * aik;\n                    temp2        += Bkj * aik;\n                }\n\n                Crow[j] = beta * Crow[j] + diagCoeff * Bj + alpha * temp2;\n            }\n        }\n    }\n}\n```",
  "floyd_warshall": "```cpp\n#include <vector>\n#include <omp.h>      // OpenMP directives\n\nvoid kernel_floyd_warshall(int n, std::vector<std::vector<int>>& path) {\n    for (int k = 0; k < n; ++k) {\n        // Sequential part \u2013 process the k\u2011th row itself\n        int* row_k  = path[k].data();\n        int  aik_k   = row_k[k];\n        for (int j = 0; j < n; ++j) {\n            int alt = aik_k + row_k[j];\n            if (row_k[j] > alt) {\n                row_k[j] = alt;\n            }\n        }\n\n        // Parallel part \u2013 all other rows\n        #pragma omp parallel for schedule(static)\n        for (int i = 0; i < n; ++i) {\n            if (i == k) continue;            // already handled\n            int* row_i = path[i].data();\n            int  aik   = row_i[k];\n            const int* rk = row_k;             // read\u2011only\n            #pragma omp simd\n            for (int j = 0; j < n; ++j) {\n                int alt = aik + rk[j];\n                if (row_i[j] > alt) {\n                    row_i[j] = alt;\n                }\n            }\n        }\n    }\n}\n```\n",
  "trmm": "```cpp\n#include <vector>\n#include <algorithm>\n#include <omp.h>\n\nvoid kernel_trmm(int m, int n,\n                 double alpha,\n                 std::vector<std::vector<double>>& A,\n                 std::vector<std::vector<double>>& B)\n{\n    if (m <= 0 || n <= 0) return;\n\n    #pragma omp parallel\n    {\n        int tid      = omp_get_thread_num();\n        int threads  = omp_get_num_threads();\n        int block    = (m + threads - 1) / threads;\n        int start = tid * block;\n        int end   = std::min(start + block, m);\n\n        // Accumulate contributions\n        for (int k = 0; k < m; ++k) {\n            const double *__restrict src = B[k].data();   // source row\n            int upper = std::min(k, end);\n            if (start >= upper) continue;\n            for (int i = start; i < upper; ++i) {\n                double aik = A[k][i];\n                double *__restrict dst = B[i].data();\n                #pragma omp simd\n                for (int j = 0; j < n; ++j) {\n                    dst[j] += aik * src[j];\n                }\n            }\n        }\n\n        #pragma omp barrier   // ensure all contributions are complete\n\n        // Scale assigned rows\n        for (int i = start; i < end; ++i) {\n            double *__restrict dst = B[i].data();\n            #pragma omp simd\n            for (int j = 0; j < n; ++j) {\n                dst[j] *= alpha;\n            }\n        }\n    }\n}\n```",
  "lu": "```cpp\n#include <vector>\n#ifdef _OPENMP\n#include <omp.h>\n#endif\n\nvoid kernel_lu(int n, std::vector<std::vector<double>>& A) {\n    // Precompute raw pointers to each row for faster indexing\n    std::vector<double*> rows(n);\n    for (int i = 0; i < n; ++i) {\n        rows[i] = A[i].data();\n    }\n\n#pragma omp parallel\n    {\n        for (int i = 0; i < n; ++i) {\n            // Lower part (L[i][j] for j < i) \u2013 executed by a single thread\n#pragma omp single\n            {\n                double* Ai = rows[i];\n                for (int j = 0; j < i; ++j) {\n                    double* Aj = rows[j];\n                    for (int k = 0; k < j; ++k) {\n                        Ai[j] -= Ai[k] * Aj[k];\n                    }\n                    Ai[j] /= Aj[j];\n                }\n            }\n\n            // Upper part (U[i][j] for j >= i) \u2013 parallelised over j\n#pragma omp for schedule(static)\n            for (int j = i; j < n; ++j) {\n                double* Ai = rows[i];\n                for (int k = 0; k < i; ++k) {\n                    Ai[j] -= Ai[k] * rows[k][j];\n                }\n            }\n        }\n    }\n}\n```",
  "gramschmidt": "```cpp\n#include <cmath>\n#include <vector>\n#include <omp.h>\n\nvoid kernel_gramschmidt(int m, int n,\n                        std::vector<std::vector<double>>& A,\n                        std::vector<std::vector<double>>& R,\n                        std::vector<std::vector<double>>& Q) {\n    /* The whole computation is performed inside a single parallel region.\n       A single thread orchestrates the outer kernel loop (k).  For each k\n       we fork tasks over the remaining columns (j).  The tasks operate in\n       full parallelism and run to completion before the next k is\n       processed.  This avoids the overhead of spawning a new parallel\n       region at every k and preserves the exact sequential dependencies\n       of the classical Modified Gram\u2013Schmidt algorithm. */\n    #pragma omp parallel\n    {\n        #pragma omp single\n        {\n            for (int k = 0; k < n; ++k) {\n                /* Compute the norm of column k of A. */\n                double norm2 = 0.0;\n                for (int i = 0; i < m; ++i) {\n                    double v = A[i][k];\n                    norm2 += v * v;\n                }\n                double Rkk = std::sqrt(norm2);\n                R[k][k] = Rkk;\n\n                /* Compute the k\u2011th orthonormal column of Q. */\n                for (int i = 0; i < m; ++i) {\n                    Q[i][k] = A[i][k] / Rkk;\n                }\n\n                /* Parallel orthogonalise the remaining columns j > k. */\n                for (int j = k + 1; j < n; ++j) {\n                    #pragma omp task firstprivate(k, j, m) shared(A, R, Q)\n                    {\n                        double rkj = 0.0;\n                        /* dot product of column k of Q with column j of A */\n                        for (int i = 0; i < m; ++i) {\n                            rkj += Q[i][k] * A[i][j];\n                        }\n                        R[k][j] = rkj;\n                        /* subtract the projection from column j of A */\n                        for (int i = 0; i < m; ++i) {\n                            A[i][j] -= Q[i][k] * rkj;\n                        }\n                    }\n                }\n                /* Ensure all j\u2011tasks for this k finish before proceeding */\n                #pragma omp taskwait\n            }\n        }\n    }\n}\n```"
}