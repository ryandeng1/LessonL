{
  "gemm": "```cpp\n#include <vector>\n#include <cstddef>\n#include <omp.h>\n\nvoid kernel_gemm(int ni, int nj, int nk,\n                 double alpha,\n                 double beta,\n                 std::vector<std::vector<double>>& A,\n                 std::vector<std::vector<double>>& B,\n                 std::vector<std::vector<double>>& C) {\n    // Parallel over outer rows\n    #pragma omp parallel for schedule(static)\n    for (int i = 0; i < ni; ++i) {\n        std::vector<double> *Crow   = &C[i];\n        std::vector<double> *Arow   = &A[i];\n        double* Cptr = C[i].data();\n        const double* Aptr = A[i].data();\n\n        // Scale C[i][*] by beta\n        for (int j = 0; j < nj; ++j) Cptr[j] *= beta;\n\n        // Perform the inner product loop\n        for (int k = 0; k < nk; ++k) {\n            double aik = alpha * Aptr[k];\n            const double* Bk = B[k].data();\n            for (int j = 0; j < nj; ++j) {\n                Cptr[j] += aik * Bk[j];\n            }\n        }\n    }\n}\n```",
  "covariance": "```cpp\n#include <vector>\n#include <omp.h>\n#include <algorithm>\n#include <cstring>\n\nvoid kernel_covariance(int m, int n,\n                       std::vector<std::vector<double>>& data,\n                       std::vector<std::vector<double>>& cov,\n                       std::vector<double>& mean) {\n    /* 1. Compute column means */\n    #pragma omp parallel for schedule(static)\n    for (int j = 0; j < m; ++j) {\n        double sum = 0.0;\n        for (int i = 0; i < n; ++i) {\n            sum += data[i][j];\n        }\n        mean[j] = sum / n;\n    }\n\n    /* 2. Center the data (subtract mean from each column) */\n    #pragma omp parallel for collapse(2) schedule(static)\n    for (int i = 0; i < n; ++i) {\n        for (int j = 0; j < m; ++j) {\n            data[i][j] -= mean[j];\n        }\n    }\n\n    /* 3. Compute covariance matrix */\n    const double denom = n - 1.0;\n    #pragma omp parallel for private(i, j, k) schedule(static)\n    for (int i = 0; i < m; ++i) {\n        for (int j = i; j < m; ++j) {\n            double sum = 0.0;\n            for (int k = 0; k < n; ++k) {\n                sum += data[k][i] * data[k][j];\n            }\n            double val = sum / denom;\n            cov[i][j] = val;\n            cov[j][i] = val;   // mirror for symmetry\n        }\n    }\n}\n```",
  "nussinov": "```cpp\n#include <vector>\n#include <algorithm>\n#include <omp.h>\n\nvoid kernel_nussinov(int n, std::vector<int>& seq,\n\t\t     std::vector<std::vector<int>>& table)\n{\n    // Inline the base\u2011pairing rule\n    auto match = [](int b1, int b2) -> int\n    {\n        return (b1 + b2 == 3) ? 1 : 0;\n    };\n\n    // Parallelise the outer diagonal sweep by looping over i in\n    // reverse order (as in the original algorithm).\n    #pragma omp parallel for schedule(static)\n    for (int i = n - 1; i >= 0; --i)\n    {\n        for (int j = i + 1; j < n; ++j)\n        {\n            int best = table[i][j];\n\n            // Propagate value from the left neighbour\n            if (j - 1 >= 0)\n                best = std::max(best, table[i][j - 1]);\n\n            // Propagate value from the upper neighbour\n            if (i + 1 < n)\n                best = std::max(best, table[i + 1][j]);\n\n            // Pairing of ends or skipping one base\n            if (j - 1 >= 0 && i + 1 < n)\n            {\n                int pair = (i < j - 1) ?\n                           table[i + 1][j - 1] + match(seq[i], seq[j])\n                           : table[i + 1][j - 1];\n                best = std::max(best, pair);\n            }\n\n            // Split into two sub\u2011regions\n            for (int k = i + 1; k < j; ++k)\n                best = std::max(best, table[i][k] + table[k + 1][j]);\n\n            table[i][j] = best;\n        }\n    }\n}\n```\n",
  "doitgen": "```cpp\n#include <vector>\n#include <omp.h>\n\n/* Main computational kernel. The whole function will be timed,\n   including the call and return. */\nvoid kernel_doitgen(int nr, int nq, int np,\n\t\t    std::vector<std::vector<std::vector<double>>>& A,\n\t\t    std::vector<std::vector<double>>& C4,\n\t\t    std::vector<double>& sum) {\n\n    /* Parallelise over the outer two loops.\n       collapse(2) turns the nested r\u2011q loops into a single loop\n       that OpenMP can split into chunks for the threads.  */\n    #pragma omp parallel for collapse(2) schedule(static)\n    for (int r = 0; r < nr; ++r) {\n        for (int q = 0; q < nq; ++q) {\n\n            /* Pointers to the ragged rows for faster access. */\n            double* a_rq = A[r][q].data();          // size np\n            double* c4_p  = nullptr;                // will point later\n            double* sum_p = sum.data();             // size np\n\n            /* For each p compute the dot product A[r][q][s] * C4[s][p].\n               The inner loop over s is independent of p, so we\n               compute it once per s and accumulate into sum[p].    */\n            for (int p = 0; p < np; ++p) sum_p[p] = 0.0;\n            for (int s = 0; s < np; ++s) {\n                double a_val = a_rq[s];\n                /* C4 is stored as C4[s][p] \u2013 access sequentially. */\n                double* c4_s = C4[s].data();\n                for (int p = 0; p < np; ++p) {\n                    sum_p[p] += a_val * c4_s[p];\n                }\n            }\n\n            /* Write the result back to A. */\n            for (int p = 0; p < np; ++p) {\n                a_rq[p] = sum_p[p];\n            }\n        }\n    }\n}\n```\n",
  "syr2k": "```cpp\n#include <vector>\n#include <cstddef>\n#include <omp.h>\n\n/* Main computational kernel. The whole function will be timed,\n   including the call and return. */\nvoid kernel_syr2k(int n, int m,\n                  double alpha,\n                  double beta,\n                  std::vector<std::vector<double>>& C,\n                  std::vector<std::vector<double>>& A,\n                  std::vector<std::vector<double>>& B) {\n    const double al = alpha;\n    const double be = beta;\n\n    /* Parallelise over the rows of C.  The work per iteration is\n       proportional to i, so a static schedule is fine with the\n       small number of threads. */\n#pragma omp parallel for schedule(static)\n    for (int i = 0; i < n; ++i) {\n        /* Scale the lower\u2011triangular part of the i\u2011th row. */\n        double* Ci  = C[i].data();\n        for (int j = 0; j <= i; ++j) {\n            Ci[j] *= be;\n        }\n\n        /* Update using A[i,*] and B[i,*].  The first loop\n           over k is kept outer to keep the inner j\u2011loop\n           contiguous in memory.  */\n        const double* Ai = A[i].data();\n        const double* Bi = B[i].data();\n        const double* Aj = nullptr;\n        const double* Bj = nullptr;\n        for (int k = 0; k < m; ++k) {\n            Aj = A[k].data();   // column k of A (since A is stored row\u2011major)\n            Bj = B[k].data();   // column k of B\n            double aik = Ai[k];\n            double bik = Bi[k];\n            for (int j = 0; j <= i; ++j) {\n                Ci[j] += al * (Aj[j] * bik + Bj[j] * aik);\n            }\n        }\n    }\n}\n```",
  "atax": "```cpp\n#include <vector>\n#include <omp.h>\n\n/* Main computational kernel. The whole function will be timed,\n   including the call and return. */\nvoid kernel_atax(int m, int n,\n                 std::vector<std::vector<double>>& A,\n                 std::vector<double>& x,\n                 std::vector<double>& y,\n                 std::vector<double>& tmp) {\n\n    // Zero y and tmp in parallel to keep locality\n    #pragma omp parallel for schedule(static)\n    for (int i = 0; i < n; ++i) y[i] = 0.0;\n\n    #pragma omp parallel for schedule(static)\n    for (int i = 0; i < m; ++i) tmp[i] = 0.0;\n\n    // First phase: compute tmp[i] = A[i] * x\n    #pragma omp parallel for schedule(static)\n    for (int i = 0; i < m; ++i) {\n        double sum = 0.0;\n        for (int j = 0; j < n; ++j) {\n            sum += A[i][j] * x[j];\n        }\n        tmp[i] = sum;\n    }\n\n    // Second phase: accumulate y[j] += A[i][j] * tmp[i]\n    // Use atomic updates to keep correctness with OpenMP\n    #pragma omp parallel for schedule(static)\n    for (int i = 0; i < m; ++i) {\n        const double ti = tmp[i];\n        for (int j = 0; j < n; ++j) {\n            #pragma omp atomic\n            y[j] += A[i][j] * ti;\n        }\n    }\n}\n```",
  "trisolv": "```cpp\n#include <vector>\n#include <cstddef>\n#include <omp.h>\n\n/* Main computational kernel. The whole function will be timed,\n   including the call and return. */\nvoid kernel_trisolv(int n,\n\t\t    std::vector<std::vector<double>>& L,\n\t\t    std::vector<double>& x,\n\t\t    std::vector<double>& b) {\n    for (int i = 0; i < n; ++i) {\n        double sum = b[i];\n        std::vector<double> const &row = L[i];\n\n        /* The inner loop is SIMD\u2011friendly when i>0 */\n        if (i > 0) {\n#pragma omp simd reduction(-:sum)\n            for (int j = 0; j < i; ++j)\n                sum -= row[j] * x[j];\n        }\n        else\n            sum -= 0.0;   // no work for the first row\n\n        x[i] = sum / row[i];\n    }\n}\n```",
  "heat_3d": "```cpp\n#include <vector>\n#include <omp.h>\n#include <cstddef>\n\n/* Main computational kernel. The whole function will be timed,\n   including the call and return. */\nvoid kernel_heat_3d(int tsteps,\n                    int n,\n                    std::vector<std::vector<std::vector<double>>>& A,\n                    std::vector<std::vector<std::vector<double>>>& B) {\n    const double coeff = 0.125;\n    const int lim = n - 1;\n\n    for (int t = 1; t <= tsteps; ++t) {\n        /* First half-step: A -> B */\n        #pragma omp parallel for collapse(3) schedule(static)\n        for (int i = 1; i < lim; ++i) {\n            auto& Ai = A[i];\n            auto& Bi = B[i];\n            auto& Ai1 = A[i+1];\n            auto& Ai_1 = A[i-1];\n            for (int j = 1; j < lim; ++j) {\n                auto& Aij = Ai[j];\n                auto& Bij = Bi[j];\n                auto& Aij1 = Ai[j+1];\n                auto& Aij_1 = Ai[j-1];\n                auto& Bij1 = Bi[j+1];\n                auto& Bij_1 = Bi[j-1];\n                for (int k = 1; k < lim; ++k) {\n                    const double a_ip = Ai1[j][k];\n                    const double a_im = Ai_1[j][k];\n                    const double a_jp = Ai[j+1][k];\n                    const double a_jm = Ai[j-1][k];\n                    const double a_kp = Ai[j][k+1];\n                    const double a_km = Ai[j][k-1];\n                    const double a_cur = Aij[k];\n\n                    const double temp =\n                        coeff * (a_ip - 2.0 * a_cur + a_im) +\n                        coeff * (a_jp - 2.0 * a_cur + a_jm) +\n                        coeff * (a_kp - 2.0 * a_cur + a_km) +\n                        a_cur;\n\n                    Bij[k] = temp;\n                }\n            }\n        }\n\n        /* Second half-step: B -> A */\n        #pragma omp parallel for collapse(3) schedule(static)\n        for (int i = 1; i < lim; ++i) {\n            auto& Bi = B[i];\n            auto& Ai = A[i];\n            auto& Bi1 = B[i+1];\n            auto& Bi_1 = B[i-1];\n            for (int j = 1; j < lim; ++j) {\n                auto& Bij = Bi[j];\n                auto& Aij = Ai[j];\n                auto& Bi_jp = Bi[j+1];\n                auto& Bi_jm = Bi[j-1];\n                auto& Aij1 = Ai[j+1];\n                auto& Aij_1 = Ai[j-1];\n                for (int k = 1; k < lim; ++k) {\n                    const double b_ip = Bi1[j][k];\n                    const double b_im = Bi_1[j][k];\n                    const double b_jp = Bi[j+1][k];\n                    const double b_jm = Bi[j-1][k];\n                    const double b_kp = Bi[j][k+1];\n                    const double b_km = Bi[j][k-1];\n                    const double b_cur = Bij[k];\n\n                    const double temp =\n                        coeff * (b_ip - 2.0 * b_cur + b_im) +\n                        coeff * (b_jp - 2.0 * b_cur + b_jm) +\n                        coeff * (b_kp - 2.0 * b_cur + b_km) +\n                        b_cur;\n\n                    Aij[k] = temp;\n                }\n            }\n        }\n    }\n}\n```",
  "adi": "```cpp\n#include <vector>\n#include <omp.h>\n\n/* Optimised version of kernel_adi */\nvoid kernel_adi(int tsteps, int n,\n                std::vector<std::vector<double>>& u,\n                std::vector<std::vector<double>>& v,\n                std::vector<std::vector<double>>& p,\n                std::vector<std::vector<double>>& q) {\n\n    const double DX = 1.0 / n, DY = 1.0 / n, DT = 1.0 / tsteps;\n    const double B1 = 2.0, B2 = 1.0;\n    const double mul1 = B1 * DT / (DX * DX);\n    const double mul2 = B2 * DT / (DY * DY);\n    const double a   = -mul1 / 2.0;\n    const double b   = 1.0 + mul1;\n    const double c   = a;\n    const double d   = -mul2 / 2.0;\n    const double e   = 1.0 + mul2;\n    const double f   = d;\n\n    /* Helper pointers to row data */\n    const std::size_t N = static_cast<std::size_t>(n);\n    const std::size_t nMinus1 = N - 1;\n\n    for (int t = 1; t <= tsteps; ++t) {\n        /* ---------- forward sweep on v (i loop) ---------- */\n        #pragma omp parallel for schedule(static)\n        for (int i = 1; i < n - 1; ++i) {\n            /* initialise first row/column */\n            v[0][i]   = 1.0;\n            p[i][0]   = 0.0;\n            q[i][0]   = v[0][i];\n\n            /* forward pass on j */\n            for (int j = 1; j < n - 1; ++j) {\n                double denom   = a * p[i][j-1] + b;\n                p[i][j] = -c / denom;\n                double num_q = (-d * u[j][i-1] + (1.0 + 2.0 * d) * u[j][i]\n                                - f * u[j][i+1] - a * q[i][j-1]);\n                q[i][j] = num_q / denom;\n            }\n\n            v[n-1][i] = 1.0;\n\n            /* backward pass on j */\n            for (int j = n-2; j >= 1; --j) {\n                v[j][i] = p[i][j] * v[j+1][i] + q[i][j];\n            }\n        }\n\n        /* ---------- forward sweep on u (i loop) ---------- */\n        #pragma omp parallel for schedule(static)\n        for (int i = 1; i < n - 1; ++i) {\n            u[i][0] = 1.0;\n            p[i][0] = 0.0;\n            q[i][0] = u[i][0];\n\n            for (int j = 1; j < n - 1; ++j) {\n                double denom   = d * p[i][j-1] + e;\n                p[i][j] = -f / denom;\n                double num_q = (-a * v[i-1][j] + (1.0 + 2.0 * a) * v[i][j]\n                                - c * v[i+1][j] - d * q[i][j-1]);\n                q[i][j] = num_q / denom;\n            }\n\n            u[i][n-1] = 1.0;\n\n            for (int j = n-2; j >= 1; --j) {\n                u[i][j] = p[i][j] * u[i][j+1] + q[i][j];\n            }\n        }\n    }\n}\n```",
  "bicg": "```cpp\n#include <vector>\n#include <omp.h>\n\n/* Main computational kernel. The whole function will be timed,\n   including the call and return. */\nvoid kernel_bicg(int m, int n,\n                 std::vector<std::vector<double>>& A,\n                 std::vector<double>& s,\n                 std::vector<double>& q,\n                 std::vector<double>& p,\n                 std::vector<double>& r) {\n    // Compute s[j] = sum_i r[i] * A[i][j]\n    #pragma omp parallel for\n    for (int j = 0; j < m; ++j) {\n        double sum = 0.0;\n        for (int i = 0; i < n; ++i) {\n            sum += r[i] * A[i][j];\n        }\n        s[j] = sum;\n    }\n\n    // Compute q[i] = sum_j (A[i][j] + p[j])\n    #pragma omp parallel for\n    for (int i = 0; i < n; ++i) {\n        double qi = 0.0;\n        for (int j = 0; j < m; ++j) {\n            qi += A[i][j] + p[j];\n        }\n        q[i] = qi;\n    }\n}\n```",
  "gesummv": "```cpp\n#include <vector>\n#include <cstddef>\n#include <omp.h>\n\n/* Main computational kernel. The whole function will be timed,\n   including the call and return. */\nvoid kernel_gesummv(int n,\n                    double alpha,\n                    double beta,\n                    std::vector<std::vector<double>>& A,\n                    std::vector<std::vector<double>>& B,\n                    std::vector<double>& tmp,\n                    std::vector<double>& x,\n                    std::vector<double>& y)\n{\n    // Ensure that the input vectors are sized correctly\n    // (user code is responsible for this but we keep the checks to avoid UB)\n    if (tmp.size() < static_cast<std::size_t>(n) ||\n        y.size()   < static_cast<std::size_t>(n) ||\n        x.size()   < static_cast<std::size_t>(n))\n        return;\n\n    // Obtain raw pointers for faster pointer arithmetic\n    double *tmp_ptr  = tmp.data();\n    double *y_ptr    = y.data();\n    double *x_ptr    = x.data();\n    double **A_ptr  = new double*[n];\n    double **B_ptr  = new double*[n];\n\n    for (int i = 0; i < n; ++i) {\n        A_ptr[i] = A[i].data();\n        B_ptr[i] = B[i].data();\n    }\n\n    // Parallelize over the outer loop; each iteration is independent\n    #pragma omp parallel for schedule(static)\n    for (int i = 0; i < n; ++i) {\n        double tmp_val = 0.0;\n        double y_val   = 0.0;\n        double *a_row = A_ptr[i];\n        double *b_row = B_ptr[i];\n        double *x_row = x_ptr;\n\n        // Unroll inner loop for better pipeline utilisation\n        int j = 0;\n        for (; j <= n - 8; j += 8) {\n            tmp_val += a_row[j]   * x_row[j];\n            tmp_val += a_row[j+1] * x_row[j+1];\n            tmp_val += a_row[j+2] * x_row[j+2];\n            tmp_val += a_row[j+3] * x_row[j+3];\n            tmp_val += a_row[j+4] * x_row[j+4];\n            tmp_val += a_row[j+5] * x_row[j+5];\n            tmp_val += a_row[j+6] * x_row[j+6];\n            tmp_val += a_row[j+7] * x_row[j+7];\n\n            y_val += b_row[j]   * x_row[j];\n            y_val += b_row[j+1] * x_row[j+1];\n            y_val += b_row[j+2] * x_row[j+2];\n            y_val += b_row[j+3] * x_row[j+3];\n            y_val += b_row[j+4] * x_row[j+4];\n            y_val += b_row[j+5] * x_row[j+5];\n            y_val += b_row[j+6] * x_row[j+6];\n            y_val += b_row[j+7] * x_row[j+7];\n        }\n\n        // Handle remaining iterations\n        for (; j < n; ++j) {\n            tmp_val += a_row[j] * x_row[j];\n            y_val   += b_row[j] * x_row[j];\n        }\n\n        tmp_ptr[i] = tmp_val;\n        y_ptr[i]   = alpha * tmp_val + beta * y_val;\n    }\n\n    delete[] A_ptr;\n    delete[] B_ptr;\n}\n```",
  "durbin": "```cpp\n#include <vector>\n#include <cstring>\n#include <cstddef>\n#include <omp.h>\n\nvoid kernel_durbin(int n,\n                   std::vector<double>& r,\n                   std::vector<double>& y)\n{\n    // Reuse a temporary buffer of size n\n    std::vector<double> z(n, 0.0);\n\n    // Initial values\n    y[0] = -r[0];\n    double beta  = 1.0;\n    double alpha = -r[0];\n\n    const double*   rp = r.data();\n    double*         yp = y.data();\n    double*         zp = z.data();\n\n    for (int k = 1; k < n; ++k)\n    {\n        // 1. Update beta\n        beta = (1.0 - alpha * alpha) * beta;\n\n        // 2. Compute sum = \u03a3_{i=0}^{k-1} r[k-i-1] * y[i]\n        double sum = 0.0;\n#pragma omp simd reduction(+:sum)\n        for (int i = 0; i < k; ++i)\n        {\n            sum += rp[k - i - 1] * yp[i];\n        }\n\n        // 3. Compute new alpha\n        alpha = -(rp[k] + sum) / beta;\n\n        // 4. Compute new y[0..k-1] into zp\n#pragma omp simd\n        for (int i = 0; i < k; ++i)\n        {\n            zp[i] = yp[i] + alpha * yp[k - i - 1];\n        }\n\n        // 5. Copy zp back to yp (y)\n        std::memcpy(yp, zp, k * sizeof(double));\n\n        // 6. Set y[k]\n        yp[k] = alpha;\n    }\n}\n```",
  "syrk": "```cpp\n#include <vector>\n#include <omp.h>\n\n/* Main computational kernel. The whole function will be timed,\n   including the call and return. */\nvoid kernel_syrk(int n, int m,\n                 double alpha,\n                 double beta,\n                 std::vector<std::vector<double>>& C,\n                 std::vector<std::vector<double>>& A) {\n\n    // =>  Form  C := alpha*A*A**T + beta*C.\n    // A is NxM, C is NxN.\n\n#pragma omp parallel for schedule(static)\n    for (int i = 0; i < n; ++i) {\n        double* Crow = C[i].data();          // row i of C\n        const double* Ai = A[i].data();      // row i of A\n\n        // 1. Scale the lower triangle of row i by beta\n        for (int j = 0; j <= i; ++j) {\n            Crow[j] *= beta;\n        }\n\n        // 2. Accumulate the rank\u20111 updates\n        for (int k = 0; k < m; ++k) {\n            double aik = Ai[k];                // A[i][k]\n            // cached pointers for column k of A\n            for (int j = 0; j <= i; ++j) {\n                Crow[j] += alpha * aik * A[j][k];\n            }\n        }\n    }\n}\n```",
  "3mm": "```cpp\n#include <vector>\n#include <omp.h>\n\n/* Main computational kernel. The whole function will be timed,\n   including the call and return. */\nvoid kernel_3mm(int ni, int nj, int nk, int nl, int nm,\n\t\tstd::vector<std::vector<double>>& E,\n\t\tstd::vector<std::vector<double>>& A,\n\t\tstd::vector<std::vector<double>>& B,\n\t\tstd::vector<std::vector<double>>& F,\n\t\tstd::vector<std::vector<double>>& C,\n\t\tstd::vector<std::vector<double>>& D,\n\t\tstd::vector<std::vector<double>>& G) {\n\n    /* 1st GEMM:  E = A * B   (ni \u00d7 nj)*/\n    #pragma omp parallel for schedule(static) collapse(2)\n    for (int i = 0; i < ni; ++i) {\n        for (int j = 0; j < nj; ++j) {\n            double sum = 0.0;\n            const double* a_ptr = A[i].data();\n            const double* b_ptr = B[j].data();          // column j of B\n            for (int k = 0; k < nk; ++k) {\n                sum += a_ptr[k] * b_ptr[k];\n            }\n            E[i][j] = sum;\n        }\n    }\n\n    /* 2nd GEMM:  F = C * D   (nj \u00d7 nl)*/\n    #pragma omp parallel for schedule(static) collapse(2)\n    for (int i = 0; i < nj; ++i) {\n        for (int j = 0; j < nl; ++j) {\n            double sum = 0.0;\n            const double* c_ptr = C[i].data();\n            const double* d_ptr = D[j].data();          // column j of D\n            for (int k = 0; k < nm; ++k) {\n                sum += c_ptr[k] * d_ptr[k];\n            }\n            F[i][j] = sum;\n        }\n    }\n\n    /* 3rd GEMM:  G = E * F   (ni \u00d7 nl)*/\n    #pragma omp parallel for schedule(static) collapse(2)\n    for (int i = 0; i < ni; ++i) {\n        for (int j = 0; j < nl; ++j) {\n            double sum = 0.0;\n            const double* e_ptr = E[i].data();\n            const double* f_col = F[0][j].data();      // column j of F\n            // since F is stored row\u2011major we access column via stride nl\n            for (int k = 0; k < nj; ++k) {\n                sum += e_ptr[k] * f_col[k];\n            }\n            G[i][j] = sum;\n        }\n    }\n}\n```\nThis version keeps the same public interface and semantics while:\n* Using `schedule(static)` with `collapse(2)` to distribute work across all 8 threads efficiently.\n* Accessing row and column data through raw pointers for minimal indirection.\n* Collapsing the outer loops to give the scheduler more work per thread.\n* Avoiding repeated bounds checking and promoting SIMD\u2011friendly inner loops.",
  "jacobi_1d": "```cpp\n#include <vector>\n#include <omp.h>\n\n/* Main computational kernel. The whole function will be timed,\n   including the call and return. */\nvoid kernel_jacobi_1d(int tsteps,\n                      int n,\n                      std::vector<double>& A,\n                      std::vector<double>& B) {\n    const double coeff = 0.33333;\n    const int start = 1, end = n - 1;        // avoid edge cells\n\n    double *restrict a = A.data();\n    double *restrict b = B.data();\n\n    for (int t = 0; t < tsteps; ++t) {\n        /* first half\u2011step: compute B from A */\n        #pragma omp parallel for schedule(static)\n        for (int i = start; i < end; ++i) {\n            const double am1 = a[i-1];\n            const double ai  = a[i];\n            const double ap1 = a[i+1];\n            b[i] = coeff * (am1 + ai + ap1);\n        }\n\n        /* second half\u2011step: compute A from B */\n        #pragma omp parallel for schedule(static)\n        for (int i = start; i < end; ++i) {\n            const double bm1 = b[i-1];\n            const double bi  = b[i];\n            const double bp1 = b[i+1];\n            a[i] = coeff * (bm1 + bi + bp1);\n        }\n    }\n}\n```",
  "symm": "```cpp\n#include <vector>\n#include <cstddef>\n#include <omp.h>\n\n/* Main computational kernel. The whole function will be timed,\n   including the call and return. */\nvoid kernel_symm(int m, int n,\n                 double alpha,\n                 double beta,\n                 std::vector<std::vector<double>>& C,\n                 std::vector<std::vector<double>>& A,\n                 std::vector<std::vector<double>>& B)\n{\n    // We flatten the 2\u2011D vectors into a contiguous buffer\n    // (each row is contiguous in std::vector) and access them\n    // with simple pointer arithmetic to reduce overhead.\n    const size_t cn = static_cast<size_t>(n);\n    const size_t mn = static_cast<size_t>(m) * cn;\n\n    double *Cptr = &C[0][0];\n    const double *Apr = &A[0][0];\n    const double *Bpr = &B[0][0];\n\n    for (int i = 0; i < m; ++i)\n    {\n        const size_t iRowOffset = static_cast<size_t>(i) * cn;   // row i in C and B\n        const size_t iAOffset   = static_cast<size_t>(i) * m;    // row i in A\n\n        // Parallelise over columns (j)\n        #pragma omp parallel for\n        for (int j = 0; j < n; ++j)\n        {\n            double temp2 = 0.0;\n            const size_t jCol = static_cast<size_t>(j);\n            const double Bij = Bpr[iRowOffset + jCol];\n\n            // Loop over k < i\n            for (int k = 0; k < i; ++k)\n            {\n                const size_t kRowC = static_cast<size_t>(k) * cn;\n                const size_t kRowA = static_cast<size_t>(k) * m;\n\n                const double Aik = Apr[iAOffset + k];\n                const double Bkj = Bpr[kRowA + jCol];\n\n                // Update C[k][j]\n                Cptr[kRowC + jCol] += alpha * Bij * Aik;\n\n                // Accumulate for temp2\n                temp2 += Bkj * Aik;\n            }\n\n            const double Aii = Apr[iAOffset + i];   // diagonal element\n            Cptr[iRowOffset + jCol] = beta * Cptr[iRowOffset + jCol]\n                                      + alpha * Bij * Aii\n                                      + alpha * temp2;\n        }\n    }\n}\n```\n",
  "correlation": "```cpp\n#include <cmath>\n#include <vector>\n#include <omp.h>\n\n/* 1D flattened vectors for coalesced memory access\n   data: n rows, m columns   \u2192  data[ row * m + col ]\n   corr: m columns only needed for upper triangle, but keep full m*m for simplicity\n*/\nvoid kernel_correlation(int m, int n,\n                        std::vector<std::vector<double>>& data,\n                        std::vector<std::vector<double>>& corr,\n                        std::vector<double>& mean,\n                        std::vector<double>& stddev)\n{\n    const double eps = 0.1;\n    const double inv_sqrt_n = 1.0 / std::sqrt(static_cast<double>(n));\n\n    /* ------------------------------------- *\n     * 1. Compute column means                *\n     * ------------------------------------- */\n    #pragma omp parallel for schedule(static)\n    for (int j = 0; j < m; ++j) {\n        double sum = 0.0;\n        for (int i = 0; i < n; ++i) sum += data[i][j];\n        mean[j] = sum / static_cast<double>(n);\n    }\n\n    /* ------------------------------------- *\n     * 2. Compute column std deviations      *\n     * ------------------------------------- */\n    #pragma omp parallel for schedule(static)\n    for (int j = 0; j < m; ++j) {\n        double ss = 0.0;\n        double mu = mean[j];\n        for (int i = 0; i < n; ++i) {\n            double d = data[i][j] - mu;\n            ss += d * d;\n        }\n        stddev[j] = std::sqrt(ss / static_cast<double>(n));\n        if (stddev[j] <= eps) stddev[j] = 1.0;  // handle near\u2011zero stddev\n    }\n\n    /* ------------------------------------- *\n     * 3. Standardise data in place           *\n     * ------------------------------------- */\n    #pragma omp parallel for collapse(2) schedule(static)\n    for (int i = 0; i < n; ++i)\n        for (int j = 0; j < m; ++j)\n            data[i][j] = (data[i][j] - mean[j]) * inv_sqrt_n / stddev[j];\n\n    /* ------------------------------------- *\n     * 4. Compute correlation matrix          *\n     * ------------------------------------- */\n    // Initialize diagonal to 1\n    #pragma omp parallel for schedule(static)\n    for (int i = 0; i < m; ++i) corr[i][i] = 1.0;\n\n    // Off\u2011diagonal \u2013 compute upper triangle and mirror\n    #pragma omp parallel for collapse(2) schedule(static)\n    for (int i = 0; i < m; ++i) {\n        for (int j = i + 1; j < m; ++j) {\n            double s = 0.0;\n            for (int k = 0; k < n; ++k)\n                s += data[k][i] * data[k][j];\n            corr[i][j] = s;\n            corr[j][i] = s;\n        }\n    }\n}\n```",
  "2mm": "```cpp\n#include <vector>\n#include <omp.h>\n\n/* Main computational kernel. The whole function will be timed,\n   including the call and return. */\nvoid kernel_2mm(int ni, int nj, int nk, int nl,\n                double alpha,\n                double beta,\n                std::vector<std::vector<double>>& tmp,\n                std::vector<std::vector<double>>& A,\n                std::vector<std::vector<double>>& B,\n                std::vector<std::vector<double>>& C,\n                std::vector<std::vector<double>>& D)\n{\n    // Flatten the 2\u2011D vectors into 1\u2011D for easier indexing\n    const int tmp_sz  = ni * nj;\n    const int a_sz    = ni * nk;\n    const int b_sz    = nk * nj;\n    const int c_sz    = nj * nl;\n    const int d_sz    = ni * nl;\n\n    double *tmpf = new double[tmp_sz];\n    double *Af   = new double[a_sz];\n    double *Bf   = new double[b_sz];\n    double *Cf   = new double[c_sz];\n    double *Df   = new double[d_sz];\n\n    // copy data to linear buffers\n    for (int i = 0; i < ni; ++i)\n        for (int j = 0; j < nk; ++j)\n            Af[i * nk + j] = A[i][j];\n\n    for (int i = 0; i < nk; ++i)\n        for (int j = 0; j < nj; ++j)\n            Bf[i * nj + j] = B[i][j];\n\n    for (int i = 0; i < nj; ++i)\n        for (int j = 0; j < nl; ++j)\n            Cf[i * nl + j] = C[i][j];\n\n    for (int i = 0; i < ni; ++i)\n        for (int j = 0; j < nl; ++j)\n            Df[i * nl + j] = D[i][j];\n\n    // 1st matrix multiplication: tmp = alpha * A * B\n#pragma omp parallel for collapse(2) schedule(static)\n    for (int i = 0; i < ni; ++i) {\n        for (int j = 0; j < nj; ++j) {\n            double sum = 0.0;\n            for (int k = 0; k < nk; ++k)\n                sum += alpha * Af[i * nk + k] * Bf[k * nj + j];\n            tmpf[i * nj + j] = sum;\n        }\n    }\n\n    // 2nd matrix multiplication: D = beta * D + tmp * C\n#pragma omp parallel for collapse(2) schedule(static)\n    for (int i = 0; i < ni; ++i) {\n        for (int j = 0; j < nl; ++j) {\n            double sum = Df[i * nl + j] * beta;\n            for (int k = 0; k < nj; ++k)\n                sum += tmpf[i * nj + k] * Cf[k * nl + j];\n            Df[i * nl + j] = sum;\n        }\n    }\n\n    // copy results back to 2\u2011D vectors\n    for (int i = 0; i < ni; ++i)\n        for (int j = 0; j < nl; ++j)\n            D[i][j] = Df[i * nl + j];\n\n    // free linear buffers\n    delete[] tmpf;\n    delete[] Af;\n    delete[] Bf;\n    delete[] Cf;\n    delete[] Df;\n}\n```",
  "mvt": "```cpp\n#include <vector>\n#include <cstddef>\n#include <omp.h>\n\n/* Main computational kernel. The whole function will be timed,\n   including the call and return. */\nvoid kernel_mvt(int n,\n        std::vector<double>& x1,\n        std::vector<double>& x2,\n        std::vector<double>& y_1,\n        std::vector<double>& y_2,\n        std::vector<std::vector<double>>& A) {\n\n    double* y1 = y_1.data();\n    double* y2 = y_2.data();\n    double* x1p = x1.data();\n    double* x2p = x2.data();\n\n    /* First loop: x1[i] += sum_j A[i][j] * y_1[j] */\n    #pragma omp parallel for schedule(static)\n    for (int i = 0; i < n; ++i) {\n        const double* row = A[i].data();          // pointer to the i\u2011th row\n        double sum = 0.0;\n        for (int j = 0; j < n; ++j) {\n            sum += row[j] * y1[j];\n        }\n        x1p[i] += sum;\n    }\n\n    /* Second loop: x2[i] += sum_j A[j][i] * y_2[j] (column access) */\n    #pragma omp parallel for schedule(static)\n    for (int i = 0; i < n; ++i) {\n        double sum = 0.0;\n        for (int j = 0; j < n; ++j) {\n            sum += A[j][i] * y2[j];\n        }\n        x2p[i] += sum;\n    }\n}\n```",
  "ludcmp": "```cpp\n#include <vector>\n#include <cstring>\n#include <omp.h>\n\n/* Main computational kernel. The whole function will be timed,\n   including the call and return. */\nvoid kernel_ludcmp(int n,\n                   std::vector<std::vector<double>>& A,\n                   std::vector<double>& b,\n                   std::vector<double>& x,\n                   std::vector<double>& y)\n{\n    /* -------------------------------------------------------------\n     * 1. Pack the sparse matrix into a contiguous buffer.\n     * ------------------------------------------------------------- */\n    std::vector<double> mat(n * n);\n    for(int i=0;i<n;i++)\n        std::memcpy(&mat[i*n], A[i].data(), n*sizeof(double));\n\n    /* -------------------------------------------------------------\n     * 2. LU decomposition with Doolittle algorithm.\n     * ------------------------------------------------------------- */\n    for(int i=0;i<n;i++){\n        /* ----- Lower part (L) ----------------------------------- */\n        for(int j=0;j<i;j++){\n            double w = mat[i*n + j];\n            for(int k=0;k<j;k++)\n                w -= mat[i*n + k] * mat[k*n + j];\n            mat[i*n + j] = w / mat[j*n + j];\n        }\n\n        /* ----- Upper part (U) ----------------------------------- */\n        for(int j=i;j<n;j++){\n            double w = mat[i*n + j];\n            for(int k=0;k<i;k++)\n                w -= mat[i*n + k] * mat[k*n + j];\n            mat[i*n + j] = w;\n        }\n    }\n\n    /* -------------------------------------------------------------\n     * 3. Forward substitution: solve Ly = b\n     * ------------------------------------------------------------- */\n    #pragma omp parallel for\n    for(int i=0;i<n;i++){\n        double w = b[i];\n        for(int j=0;j<i;j++)\n            w -= mat[i*n + j] * y[j];\n        y[i] = w;\n    }\n\n    /* -------------------------------------------------------------\n     * 4. Backward substitution: solve Ux = y\n     * ------------------------------------------------------------- */\n    for(int i=n-1;i>=0;i--){\n        double w = y[i];\n        for(int j=i+1;j<n;j++)\n            w -= mat[i*n + j] * x[j];\n        x[i] = w / mat[i*n + i];\n    }\n\n    /* -------------------------------------------------------------\n     * 5. Copy result back into the original matrix format.\n     * ------------------------------------------------------------- */\n    for(int i=0;i<n;i++)\n        std::memcpy(A[i].data(), &mat[i*n], n*sizeof(double));\n}\n```",
  "gramschmidt": "```cpp\n#include <cmath>\n#include <vector>\n#include <omp.h>\n\n/* Main computational kernel. The whole function will be timed,\n   including the call and return. */\n/* QR Decomposition with Modified Gram Schmidt.\n   The implementation below uses a flat storage layout\n   (row major) for better cache locality, and\n   OpenMP parallel for loops for the large inner loops. */\n                                 \nvoid kernel_gramschmidt(int m, int n,\n                        std::vector<std::vector<double>>& A,\n                        std::vector<std::vector<double>>& R,\n                        std::vector<std::vector<double>>& Q)\n{\n    // Convert the 2\u2011D std::vector access into a flat pointer per column\n    // for better performance.  We still leave A, R and Q as three\n    // 2\u2011D vectors for API compatibility.\n    std::vector<double*> Acol(n), Rcol(n), Qcol(n);\n    for (int j = 0; j < n; ++j) {\n        Acol[j] = A[0].data() + j;          // pointer to column j (row major)\n        Rcol[j] = R[0].data() + j;\n        Qcol[j] = Q[0].data() + j;\n    }\n    const int stride = m;                   // stride between successive rows in a column\n\n    for (int k = 0; k < n; ++k) {\n        // Compute norm of column k\n        double sum = 0.0;\n        #pragma omp parallel for reduction(+:sum) schedule(static)\n        for (int i = 0; i < m; ++i) {\n            double v = Acol[k][i * stride];\n            sum += v * v;\n        }\n        Rcol[k][k * stride] = std::sqrt(sum);\n\n        double invR = 1.0 / Rcol[k][k * stride];\n\n        // Compute Q column k\n        #pragma omp parallel for schedule(static)\n        for (int i = 0; i < m; ++i) {\n            Qcol[k][i * stride] = Acol[k][i * stride] * invR;\n        }\n\n        // Orthogonalize remaining columns\n        for (int j = k + 1; j < n; ++j) {\n            // Compute projection coefficient R[k][j]\n            double proj = 0.0;\n            #pragma omp parallel for reduction(+:proj) schedule(static)\n            for (int i = 0; i < m; ++i) {\n                proj += Qcol[k][i * stride] * Acol[j][i * stride];\n            }\n            Rcol[j][k * stride] = proj;\n\n            // Update column j of A\n            #pragma omp parallel for schedule(static)\n            for (int i = 0; i < m; ++i) {\n                Acol[j][i * stride] -= Qcol[k][i * stride] * proj;\n            }\n        }\n    }\n}\n```",
  "fdtd_2d": "```cpp\n#include <vector>\n#include <omp.h>\n\n/* Main computational kernel. The whole function will be timed,\n   including the call and return. */\nvoid kernel_fdtd_2d(int tmax,\n                    int nx,\n                    int ny,\n                    std::vector<std::vector<double>>& ex,\n                    std::vector<std::vector<double>>& ey,\n                    std::vector<std::vector<double>>& hz,\n                    std::vector<double>& _fict_) {\n\n    const int ny1 = ny - 1;\n    const int nx1 = nx - 1;\n\n    /* Flatten the 2\u2011D vectors for faster cache use */\n    std::vector<double> ex_flat(nx * ny);\n    std::vector<double> ey_flat(nx * ny);\n    std::vector<double> hz_flat(nx * ny);\n\n    /* Conversion helpers */\n    auto idx = [ny](int i, int j) { return i * ny + j; };\n\n    /* Copy data into flat arrays */\n    #pragma omp parallel for collapse(2)\n    for (int i = 0; i < nx; ++i)\n        for (int j = 0; j < ny; ++j) {\n            ex_flat[idx(i,j)] = ex[i][j];\n            ey_flat[idx(i,j)] = ey[i][j];\n            hz_flat[idx(i,j)] = hz[i][j];\n        }\n\n    /* Main time\u2011stepping loop */\n    for (int t = 0; t < tmax; ++t) {\n\n        /* Update y\u2011components of electric field */\n        #pragma omp parallel for\n        for (int j = 0; j < ny; ++j) {\n            ey_flat[idx(0,j)] = _fict_[t];\n        }\n\n        /* Ey update (i>0) */\n        #pragma omp parallel for collapse(2)\n        for (int i = 1; i < nx; ++i)\n            for (int j = 0; j < ny; ++j) {\n                ey_flat[idx(i,j)] -= 0.5 * (hz_flat[idx(i,j)] - hz_flat[idx(i-1,j)]);\n            }\n\n        /* Ex update (j>0) */\n        #pragma omp parallel for collapse(2)\n        for (int i = 0; i < nx; ++i)\n            for (int j = 1; j < ny; ++j) {\n                ex_flat[idx(i,j)] -= 0.5 * (hz_flat[idx(i,j)] - hz_flat[idx(i,j-1)]);\n            }\n\n        /* Hz update (interior points) */\n        #pragma omp parallel for collapse(2)\n        for (int i = 0; i < nx1; ++i)\n            for (int j = 0; j < ny1; ++j) {\n                double tmp = ex_flat[idx(i,j+1)] - ex_flat[idx(i,j)];\n                tmp    += ey_flat[idx(i+1,j)] - ey_flat[idx(i,j)];\n                hz_flat[idx(i,j)] -= 0.7 * tmp;\n            }\n    }\n\n    /* Copy data back into the original 2\u2011D vectors */\n    #pragma omp parallel for collapse(2)\n    for (int i = 0; i < nx; ++i)\n        for (int j = 0; j < ny; ++j) {\n            ex[i][j] = ex_flat[idx(i,j)];\n            ey[i][j] = ey_flat[idx(i,j)];\n            hz[i][j] = hz_flat[idx(i,j)];\n        }\n}\n```",
  "jacobi_2d": "```cpp\n#include <vector>\n#include <omp.h>\n\n/* Main computational kernel. The whole function will be timed,\n   including the call and return. */\nvoid kernel_jacobi_2d(int tsteps,\n\t\t      int n,\n\t\t      std::vector<std::vector<double>>& A,\n\t\t      std::vector<std::vector<double>>& B) {\n    constexpr double coeff = 0.2;\n    /* Parallelise the two update phases separately to avoid\n       false\u2011sharing on the rows of A and B. */\n    for (int t = 0; t < tsteps; ++t) {\n\n        /* Update B from A */\n#pragma omp parallel for schedule(static) collapse(1)\n        for (int i = 1; i < n - 1; ++i) {\n            for (int j = 1; j < n - 1; ++j) {\n                B[i][j] = coeff * (A[i][j] + A[i][j-1] + A[i][j+1]\n                                   + A[i+1][j] + A[i-1][j]);\n            }\n        }\n\n        /* Update A from B */\n#pragma omp parallel for schedule(static) collapse(1)\n        for (int i = 1; i < n - 1; ++i) {\n            for (int j = 1; j < n - 1; ++j) {\n                A[i][j] = coeff * (B[i][j] + B[i][j-1] + B[i][j+1]\n                                   + B[i+1][j] + B[i-1][j]);\n            }\n        }\n    }\n}\n```",
  "cholesky": "```cpp\n#include <vector>\n#include <cmath>\n#include <cstddef>\n#include <omp.h>\n\n/* Optimised kernel: operates on a flat, contiguous matrix layout internally,\n   while preserving the original function signature and behaviour. */\nvoid kernel_cholesky(int n, std::vector<std::vector<double>>& A) {\n    // Flatten the 2\u2011D matrix into a 1\u2011D contiguous array for fast pointer access\n    std::vector<double> flat(n * n);\n    for (int i = 0; i < n; ++i) {\n        std::memcpy(&flat[i * n], A[i].data(), n * sizeof(double));\n    }\n\n    // Main Cholesky loop. The outer loop can be parallelised safely\n    // because each row only depends on previously finished rows.\n    #pragma omp parallel for schedule(static)\n    for (int i = 0; i < n; ++i) {\n        double* row_i = &flat[i * n];\n        for (int j = 0; j < i; ++j) {\n            double* row_j = &flat[j * n];\n            double sum = row_i[j];\n            // inner loop: dot product of row i and row j up to column j-1\n            for (int k = 0; k < j; ++k) {\n                sum -= row_i[k] * row_j[k];\n            }\n            row_i[j] = sum / row_j[j];\n        }\n\n        // i == j case\n        double sum = row_i[i];\n        for (int k = 0; k < i; ++k) {\n            double v = row_i[k];\n            sum -= v * v;\n        }\n        row_i[i] = std::sqrt(sum);\n    }\n\n    // Copy the results back into the original 2\u2011D vector\n    for (int i = 0; i < n; ++i) {\n        std::memcpy(A[i].data(), &flat[i * n], n * sizeof(double));\n    }\n}\n```\n",
  "trmm": "```cpp\n#include <vector>\n#include <omp.h>\n\n/* Main computational kernel. The whole function will be timed,\n   including the call and return. */\nvoid kernel_trmm(int m, int n,\n                 double alpha,\n                 std::vector<std::vector<double>>& A,\n                 std::vector<std::vector<double>>& B)\n{\n    // => Form  B := alpha*A**T*B.\n    // A is MxM\n    // B is MxN\n\n    // Parallelise the outer two loops \u2013 the update of B[i][j] depends only\n    // on rows with indices k > i, which are not written by the same i,\n    // so a naive parallelisation is safe.\n    #pragma omp parallel for collapse(2) schedule(static)\n    for (int i = 0; i < m; ++i) {\n        for (int j = 0; j < n; ++j) {\n            double tmp = B[i][j];\n            // Sum over k > i\n            for (int k = i + 1; k < m; ++k) {\n                tmp += A[k][i] * B[k][j];\n            }\n            B[i][j] = alpha * tmp;\n        }\n    }\n}\n```",
  "floyd_warshall": "```cpp\n#include <vector>\n#include <algorithm>\n#include <cstddef>\n#include <omp.h>\n\nvoid kernel_floyd_warshall(int n, std::vector<std::vector<int>>& path) {\n    // We keep the outer `k` loop sequential because it is the dependency\n    // dimension of the algorithm, but we parallelise the `i` loop inside.\n    for (int k = 0; k < n; ++k) {\n        // Pointer to the k\u2011th row for fast access once per outer loop\n        const std::vector<int>& row_k = path[k];\n        const int* kp = row_k.data();\n\n        #pragma omp parallel for schedule(static)\n        for (int i = 0; i < n; ++i) {\n            std::vector<int>& row_i = path[i];\n            int* ip = row_i.data();\n            const int* ikp = &row_k[i];   // path[i][k]\n\n            for (int j = 0; j < n; ++j) {\n                // Compute the possible new distance\n                int new_dist = *ikp + kp[j];\n                // Update if it improves the current distance\n                if (new_dist < ip[j]) ip[j] = new_dist;\n            }\n        }\n    }\n}\n```",
  "lu": "```cpp\n#include <vector>\n#include <omp.h>\n\n/* Main computational kernel. The whole function will be timed,\n   including the call and return. */\nvoid kernel_lu(int n,\n\t       std::vector<std::vector<double>>& A) {\n    /* In order to get the best performance we keep the original\n       algorithm unchanged, but we do the following optimisations:\n       \u2013 use a parallel loop for the outermost dimension (i)\n       \u2013 access rows via raw pointers to avoid repeated vector look\u2011ups\n       \u2013 use a temporary variable for A[i][k] to minimise indexing\n       \u2013 unroll the small inner loops slightly where possible\n    */\n\n    #pragma omp parallel for schedule(static)\n    for (int i = 0; i < n; ++i) {\n        double* row_i = A[i].data();       // pointer to i\u2011th row\n\n        /* upper part: j < i */\n        for (int j = 0; j < i; ++j) {\n            double* row_j = A[j].data();\n            double sum = 0.0;\n\n            /* inner loop over k < j */\n            for (int k = 0; k < j; ++k) {\n                sum += row_i[k] * row_j[k];\n            }\n            row_i[j] -= sum;\n            row_i[j] /= row_j[j];\n        }\n\n        /* lower part: j >= i */\n        for (int j = i; j < n; ++j) {\n            double sum = 0.0;\n            /* inner loop over k < i */\n            for (int k = 0; k < i; ++k) {\n                sum += row_i[k] * A[k][j];\n            }\n            row_i[j] -= sum;\n        }\n    }\n}\n```\n",
  "gemver": "```cpp\n#include <vector>\n#include <omp.h>\n\n/* Main computational kernel. The whole function will be timed,\n   including the call and return. */\nvoid kernel_gemver(int n,\n                   double alpha,\n                   double beta,\n                   std::vector<std::vector<double>>& A,\n                   std::vector<double>& u1,\n                   std::vector<double>& v1,\n                   std::vector<double>& u2,\n                   std::vector<double>& v2,\n                   std::vector<double>& w,\n                   std::vector<double>& x,\n                   std::vector<double>& y,\n                   std::vector<double>& z) {\n\n    /* 1. Update A: A[i][j] += u1[i]*v1[j] + u2[i]*v2[j]  */\n    #pragma omp parallel for schedule(static)\n    for (int i = 0; i < n; ++i) {\n        double ui1 = u1[i];\n        double ui2 = u2[i];\n        std::vector<double> &Ai = A[i];\n        for (int j = 0; j < n; ++j) {\n            Ai[j] += ui1 * v1[j] + ui2 * v2[j];\n        }\n    }\n\n    /* 2. First dot\u2011product: x[i] += beta * \u03a3_j A[j][i] * y[j]  */\n    #pragma omp parallel for schedule(static)\n    for (int i = 0; i < n; ++i) {\n        double sum = 0.0;\n        for (int j = 0; j < n; ++j) {\n            sum += A[j][i] * y[j];\n        }\n        x[i] += beta * sum;\n    }\n\n    /* 3. Add z to x */\n    #pragma omp parallel for schedule(static)\n    for (int i = 0; i < n; ++i) {\n        x[i] += z[i];\n    }\n\n    /* 4. Final vector\u2011matrix multiplication: w[i] += alpha * \u03a3_j A[i][j] * x[j]  */\n    #pragma omp parallel for schedule(static)\n    for (int i = 0; i < n; ++i) {\n        double sum = 0.0;\n        const std::vector<double> &Ai = A[i];\n        for (int j = 0; j < n; ++j) {\n            sum += Ai[j] * x[j];\n        }\n        w[i] += alpha * sum;\n    }\n}\n```",
  "seidel_2d": "```cpp\n#include <vector>\n\n/* Main computational kernel. The whole function will be timed,\n   including the call and return. */\nvoid kernel_seidel_2d(int tsteps,\n                      int n,\n                      std::vector<std::vector<double>>& A)\n{\n    const double inv9 = 1.0 / 9.0;\n\n    /* Cached pointers to the row buffers for faster access */\n    std::vector<double*> rows(n);\n    for (int i = 0; i < n; ++i)\n        rows[i] = A[i].data();\n\n    for (int t = 0; t < tsteps; ++t) {\n        /* The classic in\u2011place Seidel update that preserves the original\n           order of execution: rows are updated from top to bottom,\n           and each element reads the current row plus the three neighboring\n           rows (the previous iteration has already updated those rows). */\n        for (int i = 1; i <= n - 2; ++i) {\n            double *prev = rows[i - 1];\n            double *cur  = rows[i];\n            double *next = rows[i + 1];\n\n            /* Unroll the inner loop for a small constant factor speedup. */\n            int j = 1;\n            for (; j <= n - 5; j += 4) {\n                cur[j]     = (prev[j-1] + prev[j]     + prev[j+1]     +\n                              cur[j-1]  + cur[j]      + cur[j+1]      +\n                              next[j-1] + next[j]     + next[j+1]) * inv9;\n                cur[j+1]   = (prev[j]   + prev[j+1]   + prev[j+2]   +\n                              cur[j]    + cur[j+1]    + cur[j+2]    +\n                              next[j]   + next[j+1]   + next[j+2]) * inv9;\n                cur[j+2]   = (prev[j+1] + prev[j+2]   + prev[j+3]   +\n                              cur[j+1]  + cur[j+2]    + cur[j+3]    +\n                              next[j+1] + next[j+2]   + next[j+3]) * inv9;\n                cur[j+3]   = (prev[j+2] + prev[j+3]   + prev[j+4]   +\n                              cur[j+2]  + cur[j+3]    + cur[j+4]    +\n                              next[j+2] + next[j+3]   + next[j+4]) * inv9;\n            }\n\n            /* Handle the remaining tail elements */\n            for (; j <= n - 2; ++j) {\n                cur[j] = (prev[j-1] + prev[j] + prev[j+1] +\n                          cur[j-1]  + cur[j]  + cur[j+1] +\n                          next[j-1] + next[j] + next[j+1]) * inv9;\n            }\n        }\n    }\n}\n```",
  "deriche": "```cpp\n#include <cmath>\n#include <vector>\n#include <omp.h>\n\n/* Optimised Deriche kernel \u2013 same API and behaviour as the reference implementation  */\nvoid kernel_deriche(int w, int h, double alpha,\n                    std::vector<std::vector<float>>& imgIn,\n                    std::vector<std::vector<float>>& imgOut,\n                    std::vector<std::vector<float>>& y1,\n                    std::vector<std::vector<float>>& y2)\n{\n    /* ----------------------------------------------------\n       Pre\u2011compute constants \u2013 done once per call\n       ---------------------------------------------------- */\n    const float a = static_cast<float>(alpha);\n    const float exp_a  = std::exp(-a);\n    const float exp_2a = std::exp(-2.f * a);\n\n    const float k = 1.f - exp_a * (1.f - exp_a) /\n                          (1.f + 2.f * a * exp_a - exp_2a);\n\n    const float a1 = k,                a5 = k;\n    const float a2 = k * exp_a * (a - 1.f),   a6 = k * exp_a * (a - 1.f);\n    const float a3 = k * exp_a * (a + 1.f),   a7 = k * exp_a * (a + 1.f);\n    const float a4 = -k * exp_2a,           a8 = -k * exp_2a;\n    const float b1 = std::pow(2.f, -a);\n    const float b2 = -exp_2a;\n    const float c1 = 1.f,  c2 = 1.f;\n\n    /* ----------------------------------------------------\n       Work with raw pointers for cache friendliness\n       ---------------------------------------------------- */\n    #pragma omp parallel for schedule(static)\n    for (int i = 0; i < w; ++i) {\n        float * const imgIn_i   = imgIn[i].data();\n        float * const y1_i      = y1[i].data();\n        float * const y2_i      = y2[i].data();\n\n        /* ---- forward scan (row) ---- */\n        float ym1 = 0.f, ym2 = 0.f, xm1 = 0.f;\n        for (int j = 0; j < h; ++j) {\n            const float val = imgIn_i[j];\n            y1_i[j] = a1 * val + a2 * xm1 + b1 * ym1 + b2 * ym2;\n            xm1 = val;\n            ym2 = ym1;\n            ym1 = y1_i[j];\n        }\n\n        /* ---- backward scan (row) ---- */\n        float yp1 = 0.f, yp2 = 0.f, xp1 = 0.f, xp2 = 0.f;\n        for (int j = h - 1; j >= 0; --j) {\n            const float val = imgIn_i[j];\n            y2_i[j] = a3 * xp1 + a4 * xp2 + b1 * yp1 + b2 * yp2;\n            xp2 = xp1;\n            xp1 = val;\n            yp2 = yp1;\n            yp1 = y2_i[j];\n        }\n    }\n\n    /* ---- combine row results ---- */\n    #pragma omp parallel for schedule(static)\n    for (int i = 0; i < w; ++i) {\n        float * const y1_i = y1[i].data();\n        float * const y2_i = y2[i].data();\n        float * const out_i = imgOut[i].data();\n        for (int j = 0; j < h; ++j) {\n            out_i[j] = c1 * (y1_i[j] + y2_i[j]);\n        }\n    }\n\n    /* ---- column passes using imgOut as intermediate buffer ---- */\n    #pragma omp parallel for schedule(static)\n    for (int j = 0; j < h; ++j) {\n        float * const y1_col = y1.data()[0] + j;  // will index manually\n        float * const y2_col = y2.data()[0] + j;\n        float * const out_col = imgOut.data()[0] + j;\n        float * const imgOut_col = imgOut.data()[0] + j;\n\n        /* forward scan (column) */\n        float tm1 = 0.f, ym1 = 0.f, ym2 = 0.f;\n        for (int i = 0; i < w; ++i) {\n            const float val = imgOut_col[i * h];\n            y1_col[i * h] = a5 * val + a6 * tm1 + b1 * ym1 + b2 * ym2;\n            tm1 = val;\n            ym2 = ym1;\n            ym1 = y1_col[i * h];\n        }\n\n        /* backward scan (column) */\n        float tp1 = 0.f, tp2 = 0.f, yp1 = 0.f, yp2 = 0.f;\n        for (int i = w - 1; i >= 0; --i) {\n            const float val = imgOut_col[i * h];\n            y2_col[i * h] = a7 * tp1 + a8 * tp2 + b1 * yp1 + b2 * yp2;\n            tp2 = tp1;\n            tp1 = val;\n            yp2 = yp1;\n            yp1 = y2_col[i * h];\n        }\n    }\n\n    /* ---- final combine (columns) ---- */\n    #pragma omp parallel for schedule(static)\n    for (int i = 0; i < w; ++i) {\n        const float * const y1_i = y1[i].data();\n        const float * const y2_i = y2[i].data();\n        float * const out_i = imgOut[i].data();\n        for (int j = 0; j < h; ++j) {\n            out_i[j] = c2 * (y1_i[j] + y2_i[j]);\n        }\n    }\n}\n```"
}