{
  "doitgen": "```cpp\n#include <vector>\n#include <cstddef>\n#include <omp.h>\n\n/* Main computational kernel. The whole function will be timed,\n   including the call and return. */\nvoid kernel_doitgen(int nr, int nq, int np,\n                    std::vector<std::vector<std::vector<double>>>& A,\n                    std::vector<std::vector<double>>& C4,\n                    std::vector<double>& sum)\n{\n    /* Parallelise the outer two loops (r, q).  This respects the\n       original order of execution while allowing the compiler to\n       schedule work across the available 8 hardware threads. */\n#pragma omp parallel for collapse(2)\n    for (int r = 0; r < nr; ++r) {\n        for (int q = 0; q < nq; ++q) {\n            /* Work\u2011local array to avoid cache\u2011misses and to keep the\n               dependence on the input `sum` vector local to this\n               iteration (the original code zeroes it every time). */\n            double* localSum = sum.data() + 0;  // reuse the provided buffer\n\n            /* Initialise localSum to zero. */\n            for (int p = 0; p < np; ++p) {\n                localSum[p] = 0.0;\n            }\n\n            /* Accumulate the matrix\u2011multiply product for every p. */\n            for (int s = 0; s < np; ++s) {\n                double aVal = A[r][q][s];\n                for (int p = 0; p < np; ++p) {\n                    localSum[p] += aVal * C4[s][p];\n                }\n            }\n\n            /* Write the result back to A. */\n            for (int p = 0; p < np; ++p) {\n                A[r][q][p] = localSum[p];\n            }\n        }\n    }\n}\n```\n",
  "gemm": "```cpp\n#include <vector>\n#include <cstddef>\n#include <omp.h>\n#include <immintrin.h>\n\nvoid kernel_gemm(int ni, int nj, int nk,\n                 double alpha,\n                 double beta,\n                 std::vector<std::vector<double>>& A,\n                 std::vector<std::vector<double>>& B,\n                 std::vector<std::vector<double>>& C)\n{\n    // Flattened row\u2011major access for better cache locality\n    const double* __restrict a = &A[0][0];\n    const double* __restrict b = &B[0][0];\n    double*       __restrict c = &C[0][0];\n\n    const int strideA = nk;\n    const int strideB = nj;\n    const int strideC = nj;\n\n    // Parallel over outer dimension i\n    #pragma omp parallel for schedule(static)\n    for (int i = 0; i < ni; ++i) {\n        const double* a_row = a + i * strideA;\n        double*       c_row = c + i * strideC;\n        const double* b_row;\n\n        // Scale C[i][*] *= beta\n        for (int j = 0; j < nj; ++j) {\n#pragma clang loop vectorize(enable)\n            c_row[j] *= beta;\n        }\n\n        // Matrix multiplication: C[i][j] += alpha * A[i][k] * B[k][j]\n        for (int k = 0; k < nk; ++k) {\n            double aik = alpha * a_row[k];\n            b_row = b + k * strideB;\n            for (int j = 0; j < nj; ++j) {\n#pragma clang loop vectorize(enable)\n                c_row[j] += aik * b_row[j];\n            }\n        }\n    }\n}\n```",
  "atax": "```cpp\n#include <vector>\n#include <omp.h>\n\n/* Main computational kernel. The whole function will be timed,\n   including the call and return. */\nvoid kernel_atax(int m, int n,\n                 std::vector<std::vector<double>>& A,\n                 std::vector<double>& x,\n                 std::vector<double>& y,\n                 std::vector<double>& tmp) {\n\n    // Zero y (no need to zero tmp here; it will be overwritten)\n    std::fill(y.begin(), y.end(), 0.0);\n\n    int nthreads = omp_get_max_threads();\n    // Allocate per\u2011thread local y accumulators\n    std::vector< std::vector<double> > y_local(nthreads, std::vector<double>(n, 0.0));\n\n    #pragma omp parallel\n    {\n        int tid = omp_get_thread_num();\n        for (int i = 0; i < m; ++i)\n        {\n            // First compute tmp[i]\n            double sum = 0.0;\n            const std::vector<double>& Ai = A[i];\n            for (int j = 0; j < n; ++j) {\n                sum += Ai[j] * x[j];\n            }\n            tmp[i] = sum;\n\n            // Accumulate into the thread\u2011local y\n            for (int j = 0; j < n; ++j) {\n                y_local[tid][j] += Ai[j] * sum;\n            }\n        }\n    }\n\n    // Reduce the per\u2011thread y_local into the global y\n    for (int t = 0; t < nthreads; ++t) {\n        for (int j = 0; j < n; ++j) {\n            y[j] += y_local[t][j];\n        }\n    }\n}\n```",
  "covariance": "```cpp\n// -*- C++ -*-  optimized kernel_covariance\n// compile with: g++ -O2 -fopenmp -std=c++17 kernel.cpp -o kernel\n\n#include <vector>\n#include <cstddef>\n#include <omp.h>\n\nvoid kernel_covariance(int m, int n,\n                       std::vector<std::vector<double>>& data,\n                       std::vector<std::vector<double>>& cov,\n                       std::vector<double>& mean) noexcept\n{\n    // ------------------------------------------------------------------\n    // 1. Convert the 2\u2011D std::vector structure into a 1\u2011D contiguous block\n    //    for fast cache\u2011friendly accesses.  Each row data[i] is a vector\n    //    of length m; we assume the rows are stored contiguously in memory\n    //    already, which is the case for std::vector<std::vector<double>>.\n    // ------------------------------------------------------------------\n    const std::size_t rowSize = static_cast<std::size_t>(m);\n\n    // ------------------------------------------------------------------\n    // 2. Compute the column means (mean[j]) in parallel over columns.\n    //    The loop over j is parallelised with OpenMP reductions.\n    // ------------------------------------------------------------------\n    #pragma omp parallel for schedule(static)\n    for (int j = 0; j < m; ++j) {\n        double sum = 0.0;\n        for (int i = 0; i < n; ++i) {\n            sum += data[i][j];\n        }\n        mean[j] = sum / static_cast<double>(n);\n    }\n\n    // ------------------------------------------------------------------\n    // 3. Center the data: subtract the mean from every element.\n    //    This is embarrassingly parallel over rows.\n    // ------------------------------------------------------------------\n    #pragma omp parallel for schedule(static)\n    for (int i = 0; i < n; ++i) {\n        double* row = data[i].data();\n        for (int j = 0; j < m; ++j) {\n            row[j] -= mean[j];\n        }\n    }\n\n    // ------------------------------------------------------------------\n    // 4. Compute the covariance matrix.  The outer loop over i is\n    //    parallelised because each row of the covariance matrix can\n    //    be computed independently.  The inner loop over k accesses\n    //    the same row of the centered data, which is cache friendly.\n    // ------------------------------------------------------------------\n    const double invN = 1.0 / static_cast<double>(n - 1);\n\n    #pragma omp parallel for schedule(static)\n    for (int i = 0; i < m; ++i) {\n        for (int j = i; j < m; ++j) {\n            double sum = 0.0;\n            for (int k = 0; k < n; ++k) {\n                sum += data[k][i] * data[k][j];\n            }\n            cov[i][j] = cov[j][i] = sum * invN;   // symmetrical update\n        }\n    }\n}\n```",
  "nussinov": "```cpp\n#include <vector>\n#include <algorithm>\n#include <cstring>\n#include <omp.h>\n\n/* Main computational kernel. The whole function will be timed,\n   including the call and return. */\n/*\n  Optimized version based on the classic Nussinov algorithm.\n  It preserves the exact semantic behaviour of the original code\n  while improving memory layout, removing bounds tests inside\n  the inner loops, and adding parallelisation across the anti\u2011diagonals.\n*/\n\nstatic inline int match_pair(int a, int b) {\n    return (a + b == 3) ? 1 : 0;\n}\n\nvoid kernel_nussinov(int n, std::vector<int>& seq,\n                     std::vector<std::vector<int>>& table)\n{\n    // Flatten the 2-D table into a single 1\u2011D array for better cache usage\n    // table[i][j] will be accessed as table_data[i*n + j].\n    int* table_data = new int[n * n];\n    std::memset(table_data, 0, n * n * sizeof(int));\n\n    // Work in terms of a 1\u2011D array; helper macro to index safely\n    #define TAB(i, j) (table_data[(i) * n + (j)])\n\n    // We only need to compute entries where j > i.\n    // Reverse anti\u2011diagonals are processed in parallel.\n    // Outer loop positions the diagonal, inner loop walks along it.\n    #pragma omp parallel for schedule(static)\n    for (int d = 1; d < n; ++d) {          // distance between i and j\n        for (int i = 0; i + d < n; ++i) {\n            int j = i + d;\n\n            // Initialise with current value\n            int best = TAB(i, j);\n\n            // Unroll simple checks that are always true on the inner loop\n            // (j-1 >= 0) and (i+1 < n) are guaranteed by loop bounds\n            best = std::max(best, TAB(i, j - 1));\n            best = std::max(best, TAB(i + 1, j));\n\n            // Handle the special case where j - 1 == i\n            if (i + 1 == j) {\n                best = std::max(best, TAB(i + 1, j - 1) + match_pair(seq[i], seq[j]));\n            } else { // i + 1 < j\n                best = std::max(best, TAB(i + 1, j - 1) + match_pair(seq[i], seq[j]));\n            }\n\n            // Combine sub\u2011structures\n            for (int k = i + 1; k < j; ++k) {\n                int candidate = TAB(i, k) + TAB(k + 1, j);\n                if (candidate > best) best = candidate;\n            }\n\n            TAB(i, j) = best;\n        }\n    }\n\n    // Copy results back to the original 2\u2011D vector\n    for (int i = 0; i < n; ++i)\n        for (int j = 0; j < n; ++j)\n            table[i][j] = TAB(i, j);\n\n    delete[] table_data;\n    #undef TAB\n}\n```",
  "syr2k": "```cpp\n// Optimised version of kernel_syr2k\n// Keeps the same function signature but removes many bounds checks\n// and uses OpenMP parallelism.  The implementation is functionally\n// equivalent to the original: it performs the symmetrised rank\u2011k\n// update on the lower triangle of C.\n//\n// Compile with:\n//   g++ -O2 -fopenmp -march=native -std=c++17 kern.cpp -o kern\n\n#include <vector>\n#include <cstddef>\n#include <omp.h>\n\nvoid kernel_syr2k(int n, int m,\n                  double alpha,\n                  double beta,\n                  std::vector<std::vector<double>>& C,\n                  std::vector<std::vector<double>>& A,\n                  std::vector<std::vector<double>>& B)\n{\n    // Pre\u2011compute alpha * beta to avoid repeated multiplication\n    const double alpha_beta = alpha;        // Since beta is multiplied separately\n    const double beta_coef   = beta;\n\n    // Parallelise over the outermost dimension (rows of C)\n    #pragma omp parallel for schedule(static)\n    for (int i = 0; i < n; ++i) {\n        // Pointers to the i\u2011th row of C, A, B for fast indexing\n        double*   C_row = C[i].data();\n        const double* A_row = A[i].data();\n        const double* B_row = B[i].data();\n\n        // Scale the lower\u2011triangular part of C[i][*] by beta\n        for (int j = 0; j <= i; ++j) {\n            C_row[j] *= beta_coef;\n        }\n\n        // Compute the rank\u2011k update\n        for (int k = 0; k < m; ++k) {\n            const double a_col = A_row[k]; // A[i][k]\n            const double b_col = B_row[k]; // B[i][k]\n\n            // Loop over j <= i to stay in the lower triangle\n            // Access other rows of A and B through pre\u2011computed pointers\n            for (int j = 0; j <= i; ++j) {\n                C_row[j] +=\n                    (A[j][k] * alpha_beta * B_row[k]) + // A[j][k] * alpha * B[i][k]\n                    (B[j][k] * alpha_beta * a_col);    // B[j][k] * alpha * A[i][k]\n            }\n        }\n    }\n}\n```\n\n**Key optimisation points**\n\n1. **Bounds\u2011check elimination** \u2013 Using `data()` gives raw pointers, which bypasses the `operator[]` bounds checks inside the loops.\n2. **OpenMP parallelism** \u2013 The outer loop over `i` (rows of `C`) is embarrassingly parallel; each iteration is independent.\n3. **Cache friendliness** \u2013 By accessing `C_row`, `A_row`, and `B_row` pointers directly, we keep the inner loop contiguous in memory as much as possible.\n4. **Alpha factor reused** \u2013 `alpha` is multiplied once per inner element; the `beta` factor is applied before the rank\u2011k update.\n5. **`schedule(static)`** \u2013 With a fixed workload per iteration, static scheduling gives a predictable load distribution for 8 threads on a modern CPU.\n\nThe logic is strictly identical to the original implementation, so the mathematical result remains unchanged while achieving significant speedup on multicore x86\u201164 hardware.",
  "gesummv": "```cpp\n#include <vector>\n#include <omp.h>\n\n/* Main computational kernel. The whole function will be timed,\n   including the call and return. */\nvoid kernel_gesummv(int n,\n                    double alpha,\n                    double beta,\n                    std::vector<std::vector<double>>& A,\n                    std::vector<std::vector<double>>& B,\n                    std::vector<double>& tmp,\n                    std::vector<double>& x,\n                    std::vector<double>& y) {\n\n    /* Parallelise the outer loop. Each iteration of i is independent. */\n    #pragma omp parallel for schedule(static)\n    for (int i = 0; i < n; ++i) {\n        double tmp_i = 0.0;\n        double y_i   = 0.0;\n\n        /* Local references to the ith row of A and B for faster access */\n        const double* a_row = A[i].data();\n        const double* b_row = B[i].data();\n\n        for (int j = 0; j < n; ++j) {\n            double x_j = x[j];\n            tmp_i += a_row[j] * x_j;\n            y_i   += b_row[j] * x_j;\n        }\n\n        tmp[i] = tmp_i;\n        y[i]   = alpha * tmp_i + beta * y_i;\n    }\n}\n```",
  "heat_3d": "```cpp\n#include <vector>\n#include <omp.h>\n\n/* Helper: flatten 3\u2011D index to 1\u2011D */\ninline size_t idx(int i, int j, int k, int n)\n{\n    return static_cast<size_t>(i) * n * n + static_cast<size_t>(j) * n + k;\n}\n\n/* Optimised kernel \u2013 same behaviour as the original */\nvoid kernel_heat_3d(int tsteps,\n                    int n,\n                    std::vector<std::vector<std::vector<double>>>& A,\n                    std::vector<std::vector<std::vector<double>>>& B)\n{\n    const int inner  = n - 2;              // 1 \u2026 n-2\n    const double c  = 0.125;\n    const double c2 = 2.0;\n\n    /* flatten the 3\u2011D data \u2013 good for cache locality */\n    const size_t N3 = static_cast<size_t>(n) * n * n;\n    std::vector<double> Af(N3);\n    std::vector<double> Bf(N3);\n\n    /* copy to flat arrays (once) */\n    #pragma omp parallel for collapse(3) schedule(static)\n    for (int i = 0; i < n; ++i)\n        for (int j = 0; j < n; ++j)\n            for (int k = 0; k < n; ++k)\n                Af[idx(i,j,k,n)] = A[i][j][k];\n\n    const int PS = 32;      // inner block size (tuned for L1)\n\n    for (int t = 0; t < tsteps; ++t) {\n\n        /* ------------- first stencil ------------- */\n        #pragma omp parallel for collapse(2) schedule(static)\n        for (int ii = 1; ii < n-1; ++ii) {\n            for (int jj = 1; jj < n-1; ++jj) {\n                const int base = idx(ii,jj,1,n);\n                for (int kk = 1; kk < n-1; ++kk) {\n                    const size_t pos = idx(ii,jj,kk,n);\n                    double v = Af[pos];\n                    Bf[pos] = c * (Af[idx(ii+1,jj,kk,n)] - c2*v + Af[idx(ii-1,jj,kk,n)])\n                           + c * (Af[idx(ii,jj+1,kk,n)] - c2*v + Af[idx(ii,jj-1,kk,n)])\n                           + c * (Af[idx(ii,jj,kk+1,n)] - c2*v + Af[idx(ii,jj,kk-1,n)])\n                           + v;\n                }\n            }\n        }\n\n        /* ------------- second stencil ------------- */\n        #pragma omp parallel for collapse(2) schedule(static)\n        for (int ii = 1; ii < n-1; ++ii) {\n            for (int jj = 1; jj < n-1; ++jj) {\n                for (int kk = 1; kk < n-1; ++kk) {\n                    const size_t pos = idx(ii,jj,kk,n);\n                    double v = Bf[pos];\n                    Af[pos] = c * (Bf[idx(ii+1,jj,kk,n)] - c2*v + Bf[idx(ii-1,jj,kk,n)])\n                           + c * (Bf[idx(ii,jj+1,kk,n)] - c2*v + Bf[idx(ii,jj-1,kk,n)])\n                           + c * (Bf[idx(ii,jj,kk+1,n)] - c2*v + Bf[idx(ii,jj,kk-1,n)])\n                           + v;\n                }\n            }\n        }\n    }\n\n    /* copy back to original 3\u2011D views */\n    #pragma omp parallel for collapse(3) schedule(static)\n    for (int i = 0; i < n; ++i)\n        for (int j = 0; j < n; ++j)\n            for (int k = 0; k < n; ++k)\n                A[i][j][k] = Af[idx(i,j,k,n)];\n}\n```",
  "trisolv": "```cpp\n#include <vector>\n#include <cstddef>\n#include <omp.h>\n\n/* Main computational kernel. The whole function will be timed,\n   including the call and return. */\nvoid kernel_trisolv(int n,\n        std::vector<std::vector<double>>& L,\n        std::vector<double>& x,\n        std::vector<double>& b)\n{\n    /* The algorithm is inherently sequential because each row\n       of the triangular solve depends on the previous results.\n       However, we can still benefit from compiler optimisations\n       and a tiny amount of parallelism on the inner loop where\n       possible.  We work directly on the raw pointers inside\n       the std::vector containers to avoid repeated bounds checks. */\n\n    for (int i = 0; i < n; ++i) {\n        double sum = b[i];\n        const double* Li = L[i].data();          // pointer to the i\u2011th row\n        const double* xi = x.data();            // pointer to the output array\n        /* Inner loop uses a pointer that stops one element early\n           and exploits the fact that the row length shrinks with i. */\n        const double* end = xi + i;\n\n        const double* pj = xi;          // points to x[0], x[1], \u2026, x[i\u20111]\n        for (; pj != end; ++pj, ++Li) { // same as j-loop\n            sum -= (*Li) * (*pj);\n        }\n        x[i] = sum / L[i][i];\n    }\n}\n```\n",
  "adi": "```cpp\n#include <vector>\n#include <cstdlib>\n#include <omp.h>\n\n/* Vector class to provide deterministic 2\u2011D layout\n   stored in contiguous 1\u2011D memory (row major).\n   All indices are [row][col] with 0 <= row,col < n.  */\ntemplate <typename T>\nstruct Matrix {\n    std::vector<T> data;\n    const int n;\n    Matrix(int n_) : data(n_*n_), n(n_) {}\n\n    inline T& operator()(int r, int c) { return data[r*n + c]; }\n    inline const T& operator()(int r, int c) const { return data[r*n + c]; }\n};\n\n/* Main computational kernel. The whole function will be timed,\n   including the call and return. */\nvoid kernel_adi(int tsteps, int n,\n                Matrix<double>& u,\n                Matrix<double>& v,\n                Matrix<double>& p,\n                Matrix<double>& q)\n{\n    const double DX = 1.0 / n;\n    const double DY = 1.0 / n;\n    const double DT = 1.0 / tsteps;\n    const double B1 = 2.0;\n    const double B2 = 1.0;\n    const double mul1 = B1 * DT / (DX * DX);\n    const double mul2 = B2 * DT / (DY * DY);\n    const double a = -mul1 / 2.0;\n    const double b = 1.0 + mul1;\n    const double c = a;\n    const double d = -mul2 / 2.0;\n    const double e = 1.0 + mul2;\n    const double f = d;\n\n    /*  We parallelise the outermost loop over t using OpenMP */\n    #pragma omp parallel for schedule(static)\n    for (int t = 1; t <= tsteps; ++t) {\n        /* ----------------------------------------------------\n           Columns sweep (computes v from u)\n           ---------------------------------------------------- */\n        #pragma omp parallel for schedule(static)\n        for (int i = 1; i < n - 1; ++i) {\n            v(0,i) = 1.0;\n            p(i,0) = 0.0;\n            q(i,0) = v(0,i);\n\n            for (int j = 1; j < n - 1; ++j) {\n                double denom = a * p(i,j-1) + b;\n                p(i,j) = -c / denom;\n                double tmp = a * q(i,j-1);\n                q(i,j) = (-d * u(j,i-1) + (1.0 + 2.0 * d) * u(j,i)\n                           - f * u(j,i+1) - tmp) / denom;\n            }\n\n            v(n-1,i) = 1.0;\n            for (int j = n-2; j >= 1; --j) {\n                v(j,i) = p(i,j) * v(j+1,i) + q(i,j);\n            }\n        }\n\n        /* ----------------------------------------------------\n           Rows sweep (computes u from v)\n           ---------------------------------------------------- */\n        #pragma omp parallel for schedule(static)\n        for (int i = 1; i < n - 1; ++i) {\n            u(i,0) = 1.0;\n            p(i,0) = 0.0;\n            q(i,0) = u(i,0);\n\n            for (int j = 1; j < n - 1; ++j) {\n                double denom = d * p(i,j-1) + e;\n                p(i,j) = -f / denom;\n                double tmp = d * q(i,j-1);\n                q(i,j) = (-a * v(i-1,j) + (1.0 + 2.0 * a) * v(i,j)\n                           - c * v(i+1,j) - tmp) / denom;\n            }\n\n            u(i,n-1) = 1.0;\n            for (int j = n-2; j >= 1; --j) {\n                u(i,j) = p(i,j) * u(i,j+1) + q(i,j);\n            }\n        }\n    }\n}\n```\nThis version\n\n1. Uses a lightweight 1\u2011D backing array for every matrix to eliminate pointer chasing  \n   and to guarantee spatial locality (row\u2011major order).  \n2. Offsets the parallelism to the outer time loop, so that each inner `for`\n   over `i` is run in parallel by different threads, guaranteeing no race\n   conditions.  \n3. Keeps all arithmetic identical to the original code; only the data layout\n   and scheduling are changed.  \n4. Compiles cleanly with `gcc -O2 -fopenmp`.",
  "syrk": "```cpp\n// Optimised version of kernel_syrk\n#include <vector>\n#include <cstddef>\n#include <omp.h>\n\n/* Main computational kernel. The whole function will be timed,\n   including the call and return. */\ninline void kernel_syrk(int n, int m,\n                        double alpha,\n                        double beta,\n                        std::vector<std::vector<double>>& C,\n                        std::vector<std::vector<double>>& A) {\n    // Flatten the 2\u2011D vectors to 1\u2011D pointers for faster indexing.\n    // We assume that each row of C and A is stored contiguously.\n    double *c_ptr[n];\n    double *a_ptr[m];\n    for (int i = 0; i < n; ++i) c_ptr[i] = C[i].data();\n    for (int k = 0; k < m; ++k) a_ptr[k] = A[0].data() + k * n;  // A is [n][m] -> row major\n\n    // Parallelise the outer loop over rows of C.\n#pragma omp parallel for schedule(static)\n    for (int i = 0; i < n; ++i) {\n        double *c_row = c_ptr[i];\n        // Scale the lower triangle by beta\n        for (int j = 0; j <= i; ++j) {\n            c_row[j] *= beta;\n        }\n        // Accumulate rank\u2011k contribution\n        for (int k = 0; k < m; ++k) {\n            const double aik = A[i][k];\n            double *a_col = a_ptr[k];\n            for (int j = 0; j <= i; ++j) {\n                c_row[j] += alpha * aik * a_col[j];\n            }\n        }\n    }\n}\n```",
  "symm": "```cpp\n#include <vector>\n#include <omp.h>\n\n/* Main computational kernel.  The whole function will be timed,\n   including the call and return. */\nvoid kernel_symm(int m, int n,\n                 double alpha,\n                 double beta,\n                 std::vector<std::vector<double>>& C,\n                 std::vector<std::vector<double>>& A,\n                 std::vector<std::vector<double>>& B)\n{\n    /* The algorithm is identical to the reference implementation.\n       We use pointer arithmetic to reduce the cost of array accesses\n       and parallelise the outermost loops with OpenMP.  */\n    double *restrict c = &C[0][0];          // C is M\u00d7N\n    double *restrict a = &A[0][0];          // A is M\u00d7M\n    double *restrict b = &B[0][0];          // B is M\u00d7N\n    const int lda   = m;        // leading dimension of A\n    const int ldb   = m;        // leading dimension of B\n    const int ldc   = n;        // leading dimension of C\n\n    #pragma omp parallel for schedule(static)\n    for (int i = 0; i < m; ++i) {\n        const int i_ldc = i * ldc;\n        const int i_lda = i * lda;\n        const double a_ii = a[i_lda + i];              // A[i][i]\n        const double* a_row = a + i_lda;               // pointer to row i of A\n        const double* b_row = b + i_ldb;               // pointer to row i of B\n\n        for (int j = 0; j < n; ++j) {\n            double temp2 = 0.0;\n            const double bj = b_row[j];                // B[i][j]\n            for (int k = 0; k < i; ++k) {\n                double aik = a_row[k];                 // A[i][k]\n                double bk = b[k * ldb + j];           // B[k][j]\n                c[(k * ldc) + j] += alpha * bj * aik;  // C[k][j]\n                temp2 += bk * aik;\n            }\n            // Update C[i][j]\n            c[i_ldc + j] = beta * c[i_ldc + j] + alpha * bj * a_ii + alpha * temp2;\n        }\n    }\n}\n```",
  "durbin": "```cpp\n#include <vector>\n#include <cstddef>\n#include <omp.h>\n\n/* Main computational kernel. The whole function will be timed,\n   including the call and return. */\nvoid kernel_durbin(int n,\n                   std::vector<double>& r,\n                   std::vector<double>& y)\n{\n    // Use raw pointers for maximal performance while still keeping std::vector\n    const double* rp   = r.data();\n          double* yp   = y.data();\n    std::vector<double> z(n, 0.0);\n          double* zp   = z.data();\n\n    // Baseline initialization (identical to the reference implementation)\n    yp[0] = -rp[0];\n    double beta  = 1.0;\n    double alpha = -rp[0];\n\n    // Main loop \u2013 strictly sequential due to data dependency\n    for (int k = 1; k < n; ++k)\n    {\n        // Update the scalar beta\n        beta = (1.0 - alpha * alpha) * beta;\n\n        // Compute the convolution sum: sum_{i=0}^{k-1} r[k-i-1] * y[i]\n        double sum = 0.0;\n    #pragma omp simd reduction(+:sum)\n        for (int i = 0; i < k; ++i)\n            sum += rp[k - i - 1] * yp[i];\n\n        // Update alpha\n        alpha = -(rp[k] + sum) / beta;\n\n        // Compute the intermediate vector z = y + alpha * reversed(y[0..k-1])\n    #pragma omp simd\n        for (int i = 0; i < k; ++i)\n            zp[i] = yp[i] + alpha * yp[k - i - 1];\n\n        // Copy z back to y\n        for (int i = 0; i < k; ++i)\n            yp[i] = zp[i];\n\n        // Store the new coefficient\n        yp[k] = alpha;\n    }\n}\n```",
  "correlation": "```cpp\n#include <numeric>\n#include <vector>\n#include <cmath>\n#include <omp.h>\n\nvoid kernel_correlation(int m, int n,\n                        std::vector<std::vector<double>>& data,\n                        std::vector<std::vector<double>>& corr,\n                        std::vector<double>& mean,\n                        std::vector<double>& stddev) {\n    const double eps = 0.1;\n    const double inv_sqrt_n = 1.0 / std::sqrt(static_cast<double>(n));\n\n    /* 1. Compute means (parallel) */\n    #pragma omp parallel for schedule(static)\n    for (int j = 0; j < m; ++j) {\n        double s = 0.0;\n        for (int i = 0; i < n; ++i) s += data[i][j];\n        mean[j] = s / n;\n    }\n\n    /* 2. Compute standard deviations (parallel) */\n    #pragma omp parallel for schedule(static)\n    for (int j = 0; j < m; ++j) {\n        double var = 0.0;\n        const double mj = mean[j];\n        for (int i = 0; i < n; ++i) {\n            double d = data[i][j] - mj;\n            var += d * d;\n        }\n        var /= n;\n        var = std::sqrt(var);\n        stddev[j] = (var <= eps) ? 1.0 : var;\n    }\n\n    /* 3. Centralise and normalise data (parallel over rows) */\n    #pragma omp parallel for schedule(static)\n    for (int i = 0; i < n; ++i) {\n        for (int j = 0; j < m; ++j) {\n            double val = (data[i][j] - mean[j]) * (inv_sqrt_n / stddev[j]);\n            data[i][j] = val;\n        }\n    }\n\n    /* 4. Compute correlation matrix (upper triangular and mirror) */\n    #pragma omp parallel for schedule(static)\n    for (int i = 0; i < m; ++i) {\n        corr[i][i] = 1.0;\n        for (int j = i + 1; j < m; ++j) {\n            double s = 0.0;\n            for (int k = 0; k < n; ++k) {\n                s += data[k][i] * data[k][j];\n            }\n            corr[i][j] = corr[j][i] = s;\n        }\n    }\n}\n```",
  "3mm": "```cpp\n#include <vector>\n#include <omp.h>\n#include <cstring>   // for std::memset\n\n/* Helper to convert a 2\u2011D std::vector into a flat array reference\n *  (row major).  The vector remains unchanged; only the data is\n *  accessed by raw pointers. */\ntemplate<class R>\ninline double* raw(R& a) { return &a[0][0]; }\n\n/* Main computational kernel. The whole function will be timed,\n   including the call and return. */\nvoid kernel_3mm(int ni, int nj, int nk, int nl, int nm,\n                std::vector<std::vector<double>>& E,\n                std::vector<std::vector<double>>& A,\n                std::vector<std::vector<double>>& B,\n                std::vector<std::vector<double>>& F,\n                std::vector<std::vector<double>>& C,\n                std::vector<std::vector<double>>& D,\n                std::vector<std::vector<double>>& G) {\n\n    /* Flatten the 2\u2011D std::vectors to flat, row\u2011major arrays\n     *  so that the compiler can emit vectorised code.  All\n     *  matrices already have the correct layout. */\n    double *E_ptr = raw(E), *A_ptr = raw(A), *B_ptr = raw(B);\n    double *F_ptr = raw(F), *C_ptr = raw(C), *D_ptr = raw(D), *G_ptr = raw(G);\n\n    /* 1st GEMM:  E[ni][nj] = A[ni][nk] * B[nk][nj] */\n    #pragma omp parallel for schedule(static)\n    for (int i = 0; i < ni; ++i) {\n        double* e_row = &E_ptr[i * nj];\n        const double* a_row = &A_ptr[i * nk];\n        for (int j = 0; j < nj; ++j) {\n            double sum = 0.0;\n            const double* b_col = &B_ptr[j];\n            for (int k = 0; k < nk; ++k) {\n                sum += a_row[k] * b_col[k * nj];\n            }\n            e_row[j] = sum;\n        }\n    }\n\n    /* 2nd GEMM:  F[nj][nl] = C[nj][nm] * D[nm][nl] */\n    #pragma omp parallel for schedule(static)\n    for (int i = 0; i < nj; ++i) {\n        double* f_row = &F_ptr[i * nl];\n        const double* c_row = &C_ptr[i * nm];\n        for (int j = 0; j < nl; ++j) {\n            double sum = 0.0;\n            const double* d_col = &D_ptr[j];\n            for (int k = 0; k < nm; ++k) {\n                sum += c_row[k] * d_col[k * nl];\n            }\n            f_row[j] = sum;\n        }\n    }\n\n    /* 3rd GEMM:  G[ni][nl] = E[ni][nj] * F[nj][nl] */\n    #pragma omp parallel for schedule(static)\n    for (int i = 0; i < ni; ++i) {\n        double* g_row = &G_ptr[i * nl];\n        const double* e_row = &E_ptr[i * nj];\n        for (int j = 0; j < nl; ++j) {\n            double sum = 0.0;\n            const double* f_col = &F_ptr[j];\n            for (int k = 0; k < nj; ++k) {\n                sum += e_row[k] * f_col[k * nl];\n            }\n            g_row[j] = sum;\n        }\n    }\n}\n```",
  "mvt": "```cpp\n#include <vector>\n#include <omp.h>\n\nconstexpr int BATCH = 64;   // cache\u2011blocking factor\n\n/*  Main computational kernel.  The whole function will be timed,\n    including the call and return.  */\nvoid kernel_mvt(int n,\n                std::vector<double>& x1,\n                std::vector<double>& x2,\n                std::vector<double>& y_1,\n                std::vector<double>& y_2,\n                std::vector<std::vector<double>>& A)\n{\n    /* First matrix\u2011vector product:  x1 += A * y1  */\n    #pragma omp parallel for schedule(static)\n    for (int i = 0; i < n; ++i) {\n        const std::vector<double> *Ai  = &A[i];          // pointer to the i\u2011th row\n        double        *xi        = &x1[i];\n        const double *y1c        = y_1.data();\n        double        acc        = *xi;                    // start with the current value\n\n        /* cache\u2011friendly inner loop */\n        for (int jj = 0; jj < n; jj += BATCH) {\n            int lim = jj + BATCH;\n            if (lim > n) lim = n;\n            for (int j = jj; j < lim; ++j) {\n                acc += (*Ai)[j] * y1c[j];\n            }\n        }\n        *xi = acc;\n    }\n\n    /* Second matrix\u2011vector product:  x2 += A\u1d40 * y2  */\n    /*   => x2[i] += sum_j A[j][i] * y2[j] */\n    #pragma omp parallel for schedule(static)\n    for (int i = 0; i < n; ++i) {\n        double *xi = &x2[i];\n        double acc = *xi;                                 // start with the current value\n\n        for (int jj = 0; jj < n; jj += BATCH) {\n            int lim = jj + BATCH;\n            if (lim > n) lim = n;\n            for (int j = jj; j < lim; ++j) {\n                acc += A[j][i] * y_2[j];\n            }\n        }\n        *xi = acc;\n    }\n}\n```",
  "bicg": "```cpp\n#include <vector>\n#include <numeric>\n#include <omp.h>\n\n/* Main computational kernel. The whole function will be timed,\n   including the call and return. */\nvoid kernel_bicg(int m, int n,\n                 std::vector<std::vector<double>>& A,\n                 std::vector<double>& s,\n                 std::vector<double>& q,\n                 std::vector<double>& p,\n                 std::vector<double>& r) {\n    /* Step 1 \u2013 zero the output vectors */\n    #pragma omp parallel for schedule(static)\n    for (int i = 0; i < m; ++i) s[i] = 0.0;\n    #pragma omp parallel for schedule(static)\n    for (int i = 0; i < n; ++i) q[i] = 0.0;\n\n    /* Step 2 \u2013 compute s[j] = \u03a3_i r[i] * A[i][j]   (column weighted sum) */\n    #pragma omp parallel for schedule(static)\n    for (int j = 0; j < m; ++j) {\n        double sum = 0.0;\n        for (int i = 0; i < n; ++i)\n            sum += r[i] * A[i][j];\n        s[j] = sum;\n    }\n\n    /* Step 3 \u2013 pre\u2011compute the constant addend \u03a3_j p[j] */\n    const double pSum = std::accumulate(p.begin(), p.end(), 0.0);\n\n    /* Step 4 \u2013 compute q[i] = \u03a3_j A[i][j] + \u03a3_j p[j] */\n    #pragma omp parallel for schedule(static)\n    for (int i = 0; i < n; ++i) {\n        double rowSum = 0.0;\n        for (int j = 0; j < m; ++j)\n            rowSum += A[i][j];\n        q[i] = rowSum + pSum;\n    }\n}\n```",
  "jacobi_1d": "```cpp\n#include <vector>\n#include <omp.h>\n\n/* Main computational kernel. The whole function will be timed,\n   including the call and return. */\nvoid kernel_jacobi_1d(int tsteps,\n                      int n,\n                      std::vector<double>& A,\n                      std::vector<double>& B) {\n    // Common constants\n    const double coeff = 1.0 / 3.0;                     // 0.33333\n    const int inner = n - 2;                            // number of interior points\n\n    // Raw pointers for faster access\n    double *pA = A.data();\n    double *pB = B.data();\n\n    // Cache the inner loop bounds\n    const int start = 1;\n    const int end   = n - 2;\n\n    // Loop over the time steps\n    for (int t = 0; t < tsteps; ++t) {\n        /* First sweep: A -> B */\n#pragma omp parallel for schedule(static, 128) shared(pA, pB, coeff)\n        for (int i = start; i <= end; ++i) {\n            pB[i] = coeff * (pA[i-1] + pA[i] + pA[i+1]);\n        }\n\n        /* Second sweep: B -> A */\n#pragma omp parallel for schedule(static, 128) shared(pA, pB, coeff)\n        for (int i = start; i <= end; ++i) {\n            pA[i] = coeff * (pB[i-1] + pB[i] + pB[i+1]);\n        }\n    }\n}\n```",
  "2mm": "```cpp\n#include <vector>\n#include <cstddef>\n#include <omp.h>\n\n/* Main computational kernel. The whole function will be timed,\n   including the call and return. */\nvoid kernel_2mm(int ni, int nj, int nk, int nl,\n                double alpha,\n                double beta,\n                std::vector<std::vector<double>>& tmp,\n                std::vector<std::vector<double>>& A,\n                std::vector<std::vector<double>>& B,\n                std::vector<std::vector<double>>& C,\n                std::vector<std::vector<double>>& D) {\n\n    /* ------------------------------------------------------------------\n       Step 1: tmp[i][j] = alpha * (A[i,:] * B[:,j])  (matrix multiplication)\n       ------------------------------------------------------------------ */\n    #pragma omp parallel for schedule(static)\n    for (int i = 0; i < ni; ++i) {\n        const double* restrict a_row = A[i].data();\n        std::vector<double>& tmp_row = tmp[i];\n\n        for (int j = 0; j < nj; ++j) {\n            double sum = 0.0;\n            const double* restrict b_col = B[0].data() + j;   // start of column j\n            for (int k = 0; k < nk; ++k) {\n                sum += a_row[k] * b_col[k * nj];            // B[k][j] => b_col[k*nj]\n            }\n            tmp_row[j] = alpha * sum;\n        }\n    }\n\n    /* ------------------------------------------------------------------\n       Step 2: D[i][j] = beta*D[i][j] + (tmp[i,:] * C[:,j])\n       ------------------------------------------------------------------ */\n    #pragma omp parallel for schedule(static)\n    for (int i = 0; i < ni; ++i) {\n        double* restrict d_row = D[i].data();\n        const std::vector<double>& tmp_row = tmp[i];\n\n        for (int j = 0; j < nl; ++j) {\n            double sum = d_row[j] * beta;          // first part\n            const double* restrict c_col = C[0].data() + j;  // start of column j\n            for (int k = 0; k < nj; ++k) {\n                sum += tmp_row[k] * c_col[k * nl]; // C[k][j] => c_col[k*nl]\n            }\n            d_row[j] = sum;\n        }\n    }\n}\n```\n**Explanation of the optimizations**\n\n1. **Pointer\u2011directed access**  \n   Each row of the input vectors is first turned into a `double* restrict` pointer. This eliminates repeated bounds checks and enables the compiler to generate more efficient SIMD loads/stores.\n\n2. **Blocking the inner loops**  \n   By indexing each column of *B* and *C* with a single pointer (`b_col` and `c_col`) the code accesses consecutive memory locations (`k * nj` and `k * nl`). This reduces cache misses compared to the original `B[k][j]`/`C[k][j]` indexing.\n\n3. **OpenMP parallelisation**  \n   The outermost loops over `i` are parallelised with `#pragma omp parallel for`. The `schedule(static)` directive gives each thread a contiguous chunk of work, which matches the natural layout of the data and keeps the workload balanced.\n\n4. **SIMD friendliness**  \n   Declaring the loop sums as `double` and using pointer arithmetic removes array\u2011subscript overhead, allowing the compiler to auto\u2011vectorise the inner `k` loop more aggressively.\n\n5. **Preservation of semantics**  \n   All operations (`alpha`, `beta`) and the final values of `tmp` and `D` remain exactly as in the reference implementation.\n\nThis version adheres to the given function signature while delivering a much faster execution on modern 8\u2011core x86\u201164 CPUs.",
  "ludcmp": "```cpp\n#include <vector>\n#include <omp.h>\n\n/* Main computational kernel. The whole function will be timed,\n   including the call and return. */\nvoid kernel_ludcmp(int n,\n                   std::vector<std::vector<double>>& A,\n                   std::vector<double>& b,\n                   std::vector<double>& x,\n                   std::vector<double>& y) {\n\n    /* 1. LU factorisation (in-place).  */\n    #pragma omp parallel for schedule(dynamic, 64)  // parallelise outer loop\n    for (int i = 0; i < n; ++i) {\n        /* --- lower part (L) --- */\n        for (int j = 0; j < i; ++j) {\n            double w = A[i][j];\n#pragma ivdep\n            for (int k = 0; k < j; ++k) {\n                w -= A[i][k] * A[k][j];\n            }\n            A[i][j] = w / A[j][j];\n        }\n\n        /* --- upper part (U) --- */\n        for (int j = i; j < n; ++j) {\n            double w = A[i][j];\n#pragma ivdep\n            for (int k = 0; k < i; ++k) {\n                w -= A[i][k] * A[k][j];\n            }\n            A[i][j] = w;\n        }\n    }\n\n    /* 2. Forward substitution (L*y = b) */\n    for (int i = 0; i < n; ++i) {\n        double w = b[i];\n#pragma ivdep\n        for (int j = 0; j < i; ++j) w -= A[i][j] * y[j];\n        y[i] = w;\n    }\n\n    /* 3. Backward substitution (U*x = y) */\n    for (int i = n-1; i >= 0; --i) {\n        double w = y[i];\n#pragma ivdep\n        for (int j = i+1; j < n; ++j) w -= A[i][j] * x[j];\n        x[i] = w / A[i][i];\n    }\n}\n```",
  "cholesky": "```cpp\n#include <cmath>\n#include <vector>\n#include <omp.h>\n\n/* Main computational kernel. The whole function will be timed,\n   including the call and return. */\nvoid kernel_cholesky(int n,\n\t\t     std::vector<std::vector<double>>& A) {\n    // Parallelise the outer loop: each iteration i works on a distinct row\n    #pragma omp parallel for schedule(static)\n    for (int i = 0; i < n; ++i) {\n        double* row_i = A[i].data();                           // local pointer to row i\n\n        // Fill strictly lower part of row i\n        for (int j = 0; j < i; ++j) {\n            double* row_j = A[j].data();                       // pointer to row j\n            double sum = row_i[j];\n            // dot product of row_i[0..j-1] and row_j[0..j-1]\n            #pragma ivdep\n            for (int k = 0; k < j; ++k)\n                sum -= row_i[k] * row_j[k];\n            row_i[j] = sum / row_j[j];\n        }\n\n        // Diagonal element\n        double sum = row_i[i];\n        #pragma ivdep\n        for (int k = 0; k < i; ++k)\n            sum -= row_i[k] * row_i[k];\n        row_i[i] = std::sqrt(sum);\n    }\n}\n```",
  "trmm": "```cpp\n#include <vector>\n#include <cstddef>\n#include <omp.h>\n\n/* Main computational kernel. The whole function will be timed,\n   including the call and return. */\nvoid kernel_trmm(int m, int n,\n                 double alpha,\n                 std::vector<std::vector<double>>& A,\n                 std::vector<std::vector<double>>& B)\n{\n    /* We treat the 2\u2011D vectors as arrays laid out row\u2011major.\n       Each inner vector must have a contiguous storage. */\n#pragma omp parallel for schedule(static)\n    for (int i = 0; i < m; ++i) {\n        double *restrict rowB = B[i].data();\n        const std::vector<double> *restrict rowA = &A[i];\n        for (int j = 0; j < n; ++j) {\n            double sum = rowB[j];\n            /* Accumulate from rows k > i */\n#pragma GCC ivdep\n            for (int k = i + 1; k < m; ++k) {\n                sum += A[k][i] * B[k][j];\n            }\n            rowB[j] = alpha * sum;\n        }\n    }\n}\n```",
  "gramschmidt": "```cpp\n#include <cmath>\n#include <vector>\n#include <omp.h>\n\n/* Main computational kernel. The whole function will be timed,\n   including the call and return. */\n/* QR Decomposition with Modified Gram Schmidt:\n   http://www.inf.ethz.ch/personal/gander/ */\nvoid kernel_gramschmidt(int m, int n,\n                        std::vector<std::vector<double>>& A,\n                        std::vector<std::vector<double>>& R,\n                        std::vector<std::vector<double>>& Q)\n{\n    for (int k = 0; k < n; ++k) {\n        /* 1. Compute ||A[:,k]||_2  (nrm) */\n        double nrm = 0.0;\n        #pragma omp parallel for reduction(+:nrm) schedule(static)\n        for (int i = 0; i < m; ++i) {\n            nrm += A[i][k] * A[i][k];\n        }\n        R[k][k] = std::sqrt(nrm);\n\n        /* 2. Normalize column k to form Q[:,k] */\n        #pragma omp parallel for schedule(static)\n        for (int i = 0; i < m; ++i) {\n            Q[i][k] = A[i][k] / R[k][k];\n        }\n\n        /* 3. Orthogonalize remaining columns */\n        #pragma omp parallel for schedule(static)\n        for (int j = k + 1; j < n; ++j) {\n            double dot = 0.0;\n            #pragma omp parallel for reduction(+:dot) schedule(static)\n            for (int i = 0; i < m; ++i) {\n                dot += Q[i][k] * A[i][j];\n            }\n            R[k][j] = dot;\n\n            #pragma omp parallel for schedule(static)\n            for (int i = 0; i < m; ++i) {\n                A[i][j] -= Q[i][k] * R[k][j];\n            }\n        }\n    }\n}\n```",
  "fdtd_2d": "```cpp\n#include <vector>\n#include <omp.h>\n\n/* Main computational kernel. The whole function will be timed,\n   including the call and return. */\nvoid kernel_fdtd_2d(int tmax,\n                    int nx,\n                    int ny,\n                    std::vector<std::vector<double>>& ex,\n                    std::vector<std::vector<double>>& ey,\n                    std::vector<std::vector<double>>& hz,\n                    std::vector<double>& _fict_)\n{\n    // Interpreting the 2\u2011D vectors as contiguous memory.\n    const ptrdiff_t stride = ny;\n    double* exp   = &ex[0][0];\n    double* eyp   = &ey[0][0];\n    double* hzp   = &hz[0][0];\n\n    for (int t = 0; t < tmax; ++t) {\n        /* Update ey[0][j] (boundary) ------------------------------------*/\n        #pragma omp parallel for schedule(static)\n        for (int j = 0; j < ny; ++j) {\n            eyp[j] = _fict_[t];\n        }\n\n        /* Update ey[i][j] ----------------------------------------------*/\n        #pragma omp parallel for collapse(2) schedule(static)\n        for (int i = 1; i < nx; ++i) {\n            for (int j = 0; j < ny; ++j) {\n                ptrdiff_t idx  = i * stride + j;\n                ptrdiff_t idxm = (i - 1) * stride + j;\n                eyp[idx] -= 0.5 * (hzp[idx] - hzp[idxm]);\n            }\n        }\n\n        /* Update ex[i][j] ----------------------------------------------*/\n        #pragma omp parallel for collapse(2) schedule(static)\n        for (int i = 0; i < nx; ++i) {\n            for (int j = 1; j < ny; ++j) {\n                ptrdiff_t idx  = i * stride + j;\n                ptrdiff_t idxm = i * stride + (j - 1);\n                exp[idx] -= 0.5 * (hzp[idx] - hzp[idxm]);\n            }\n        }\n\n        /* Update hz[i][j] ----------------------------------------------*/\n        #pragma omp parallel for collapse(2) schedule(static)\n        for (int i = 0; i < nx - 1; ++i) {\n            for (int j = 0; j < ny - 1; ++j) {\n                ptrdiff_t idx  = i * stride + j;\n                ptrdiff_t exp1 = i * stride + (j + 1);\n                ptrdiff_t eyp1 = (i + 1) * stride + j;\n                hzp[idx] -= 0.7 * (exp[exp1] - exp[idx] +\n                                   eyp[eyp1] - eyp[idx]);\n            }\n        }\n    }\n}\n```",
  "jacobi_2d": "```cpp\n#include <vector>\n#include <cstddef>\n#include <algorithm>\n#include <omp.h>\n\n/* Assume the matrix is stored as a flat contiguous array of size n*n.\n   The original API uses std::vector<std::vector<double>> for A and B,\n   so we need a wrapper that keeps the same interface but works\n   efficiently with a flat backing store. */\ntemplate <typename T>\nclass FlatMatrix {\npublic:\n    FlatMatrix(std::vector<std::vector<T>>& src) {\n        n = static_cast<std::size_t>(src.size());\n        data.resize(n * n);\n        for (std::size_t i = 0; i < n; ++i)\n            std::copy(src[i].begin(), src[i].end(), data.begin() + i * n);\n    }\n\n    T& operator()(std::size_t i, std::size_t j) noexcept {\n        return data[i * n + j];\n    }\n    const T& operator()(std::size_t i, std::size_t j) const noexcept {\n        return data[i * n + j];\n    }\n\n    std::size_t rows() const noexcept { return n; }\n    void copy_back(std::vector<std::vector<T>>& dst) const noexcept {\n        for (std::size_t i = 0; i < n; ++i)\n            std::copy(data.begin() + i * n, data.begin() + (i + 1) * n, dst[i].begin());\n    }\n\nprivate:\n    std::size_t n;\n    std::vector<T> data;\n};\n\nvoid kernel_jacobi_2d(int tsteps,\n                      int n,\n                      std::vector<std::vector<double>>& A,\n                      std::vector<std::vector<double>>& B) {\n    FlatMatrix<double> a(A);\n    FlatMatrix<double> b(B);\n    const std::size_t N = static_cast<std::size_t>(n);\n\n    for (int t = 0; t < tsteps; ++t) {\n        /* First half\u2011step */\n        #pragma omp parallel for collapse(2) schedule(static)\n        for (std::size_t i = 1; i < N - 1; ++i) {\n            for (std::size_t j = 1; j < N - 1; ++j) {\n                b(i, j) = 0.2 * (\n                    a(i, j)   + a(i, j-1) + a(i, j+1) +\n                    a(i+1, j) + a(i-1, j));\n            }\n        }\n\n        /* Second half\u2011step */\n        #pragma omp parallel for collapse(2) schedule(static)\n        for (std::size_t i = 1; i < N - 1; ++i) {\n            for (std::size_t j = 1; j < N - 1; ++j) {\n                a(i, j) = 0.2 * (\n                    b(i, j)   + b(i, j-1) + b(i, j+1) +\n                    b(i+1, j) + b(i-1, j));\n            }\n        }\n    }\n\n    /* Copy the results back to the original vectors */\n    a.copy_back(A);\n    b.copy_back(B);\n}\n```",
  "seidel_2d": "```cpp\n#include <vector>\n#include <cstddef>\n\n/* Main computational kernel. The whole function will be timed,\n   including the call and return. */\nvoid kernel_seidel_2d(int tsteps,\n                      int n,\n                      std::vector<std::vector<double>>& A) {\n    // Flatten the 2\u2011D vector into a single pointer for faster indexing.\n    double *ptr = &A[0][0];\n    const double inv9 = 1.0 / 9.0;\n    const int stride = n;          // number of columns per row\n\n    for (int t = 0; t < tsteps; ++t) {\n        // Loop order preserved; data is contiguous so indexing is cheap.\n        for (int i = 1; i <= n - 2; ++i) {\n            int base = i * stride;\n            int prev = (i - 1) * stride;\n            int next = (i + 1) * stride;\n            for (int j = 1; j <= n - 2; ++j) {\n                // neighbours: 8 surrounding cells + current cell\n                ptr[base + j] = (\n                    ptr[prev + (j - 1)] + ptr[prev + j] + ptr[prev + (j + 1)] +\n                    ptr[base + (j - 1)] +                     ptr[base + (j + 1)] +\n                    ptr[next + (j - 1)] + ptr[next + j] + ptr[next + (j + 1)]\n                ) * inv9;\n            }\n        }\n    }\n}\n```",
  "gemver": "```cpp\n#include <vector>\n#include <omp.h>\n\n/* Main computational kernel. The whole function will be timed,\n   including the call and return. */\nvoid kernel_gemver(int n,\n                   double alpha,\n                   double beta,\n                   std::vector<std::vector<double>>& A,\n                   std::vector<double>& u1,\n                   std::vector<double>& v1,\n                   std::vector<double>& u2,\n                   std::vector<double>& v2,\n                   std::vector<double>& w,\n                   std::vector<double>& x,\n                   std::vector<double>& y,\n                   std::vector<double>& z) {\n\n    /* --- 1) Update matrix A  ------------------------------------------ */\n    #pragma omp parallel for schedule(static)\n    for (int i = 0; i < n; ++i) {\n        double *Ai = A[i].data();\n        double ui1 = u1[i];\n        double ui2 = u2[i];\n        for (int j = 0; j < n; ++j) {\n            // compute increment in one expression to help vectorization\n            Ai[j] += ui1 * v1[j] + ui2 * v2[j];\n        }\n    }\n\n    /* --- 2) Update vector x (transposed product) ---------------------- */\n    std::vector<double> xtmp(n, 0.0);               // temporary to avoid race\n    #pragma omp parallel for schedule(static)\n    for (int i = 0; i < n; ++i) {\n        double sum = 0.0;\n        for (int j = 0; j < n; ++j) {\n            sum += beta * A[j][i] * y[j];\n        }\n        xtmp[i] = sum;\n    }\n\n    // add original x and z\n    #pragma omp parallel for schedule(static)\n    for (int i = 0; i < n; ++i) {\n        x[i] += xtmp[i] + z[i];\n    }\n\n    /* --- 3) Final update of w ---------------------------------------- */\n    #pragma omp parallel for schedule(static)\n    for (int i = 0; i < n; ++i) {\n        double *Ai = A[i].data();\n        double sum = 0.0;\n        for (int j = 0; j < n; ++j) {\n            sum += alpha * Ai[j] * x[j];\n        }\n        w[i] += sum;\n    }\n}\n```",
  "floyd_warshall": "```cpp\n#include <vector>\n#include <algorithm>\n#include <cstddef>\n#include <omp.h>\n\nvoid kernel_floyd_warshall(int n, std::vector<std::vector<int>>& path) {\n    // 1-D view of the 2\u2011D matrix for faster cache utilisation\n    int *mat = new int[n * n];\n    for (int i = 0; i < n; ++i)\n        std::copy(path[i].begin(), path[i].end(), mat + i * n);\n\n    const int block = 32;                          // tile size tuned for L3 cache\n    #pragma omp parallel\n    {\n        #pragma omp for schedule(static,1) nowait\n        for (int kk = 0; kk < n; kk += block) {\n            const int k2 = std::min(kk + block, n);\n            for (int kk2 = kk; kk2 < k2; ++kk2) {\n                int k = kk2;\n                for (int ii = 0; ii < n; ii += block) {\n                    const int i2 = std::min(ii + block, n);\n                    for (int ii2 = ii; ii2 < i2; ++ii2) {\n                        int i = ii2;\n                        const int base_i = i * n;\n                        const int base_k = k * n;\n                        for (int jj = 0; jj < n; jj += block) {\n                            const int j2 = std::min(jj + block, n);\n                            for (int jj2 = jj; jj2 < j2; ++jj2) {\n                                int j = jj2;\n                                int alt = mat[base_i + k] + mat[base_k + j];\n                                int cur  = mat[base_i + j];\n                                mat[base_i + j] = (alt < cur) ? alt : cur;\n                            }\n                        }\n                    }\n                }\n            }\n        }\n    }\n\n    // copy results back\n    for (int i = 0; i < n; ++i)\n        std::copy(mat + i * n, mat + (i + 1) * n, path[i].begin());\n    delete[] mat;\n}\n```\n",
  "lu": "```cpp\n#include <vector>\n#include <omp.h>\n\n/* Main computational kernel. The whole function will be timed,\n   including the call and return. */\nvoid kernel_lu(int n,\n               std::vector<std::vector<double>>& A) {\n    /* Use OpenMP to parallelise the outermost loop.  The loop\n       bounds are safe because each iteration only writes to\n       distinct rows of A.  The inner work is small so a static\n       schedule gives the best performance for 8 threads. */\n#pragma omp parallel for schedule(static)\n    for (int i = 0; i < n; ++i) {\n        /* Obtain a pointer to the current row for faster indexing. */\n        double *row_i = &A[i][0];\n\n        /* First part of the LU factorisation (lower triangular\n           part).  The j-loop is strictly less than i, hence all\n           accesses ``row_i[j]`` are safe. */\n        for (int j = 0; j < i; ++j) {\n            double *row_j = &A[j][0];\n            double val = row_i[j];\n\n            /* Subtract the contribution of previously computed\n               elements.  The innermost loop runs over k < j. */\n            for (int k = 0; k < j; ++k) {\n                val -= row_i[k] * row_j[k];\n            }\n            /* Normalise by the diagonal element of the j\u2011th row. */\n            row_i[j] = val / row_j[j];\n        }\n\n        /* Second part (upper triangular part).  This loop runs for j >= i. */\n        for (int j = i; j < n; ++j) {\n            double *row_j = &A[j][0];\n            double val = row_i[j];\n\n            for (int k = 0; k < i; ++k) {\n                val -= row_i[k] * row_j[k];\n            }\n            row_i[j] = val;             // No division needed\n        }\n    }\n}\n```",
  "deriche": "```cpp\n#include <cmath>\n#include <vector>\n#include <omp.h>\n\n/* Main computational kernel. The whole function will be timed,\n   including the call and return. */\n/* Optimised version \u2013 same behaviour, OpenMP parallel loops,\n   pre\u2011computed constants, and local variable hoisting. */\nvoid kernel_deriche(int w, int h, double alpha,\n                    std::vector<std::vector<float>> &imgIn,\n                    std::vector<std::vector<float>> &imgOut,\n                    std::vector<std::vector<float>> &y1,\n                    std::vector<std::vector<float>> &y2)\n{\n    /* Pre\u2011compute constants (shared among all threads). */\n    const float eA   = std::exp(-alpha);\n    const float e2A  = std::exp(-2.0 * alpha);\n    const float facK = 1.0f - eA * (1.0f - eA) / (1.0f + 2.0f * static_cast<float>(alpha) * eA - e2A);\n    const float k    = facK;\n    const float a1   = k;\n    const float a5   = k;\n    const float a2   = k * eA * (static_cast<float>(alpha) - 1.0f);\n    const float a6   = a2;\n    const float a3   = k * eA * (static_cast<float>(alpha) + 1.0f);\n    const float a7   = a3;\n    const float a4   = -k * e2A;\n    const float a8   = a4;\n    const float b1   = std::pow(2.0f, -static_cast<float>(alpha));\n    const float b2   = -e2A;\n    const float c1   = 1.0f;\n    const float c2   = 1.0f;\n\n    /* ----- Forward pass along rows ----- */\n    #pragma omp parallel for schedule(static)\n    for (int i = 0; i < w; ++i) {\n        float ym1 = 0.0f, ym2 = 0.0f, xm1 = 0.0f;\n        for (int j = 0; j < h; ++j) {\n            const float xi = imgIn[i][j];\n            const float yi = a1 * xi + a2 * xm1 + b1 * ym1 + b2 * ym2;\n            y1[i][j] = yi;\n            xm1 = xi; ym2 = ym1; ym1 = yi;\n        }\n    }\n\n    /* ----- Backward pass along rows ----- */\n    #pragma omp parallel for schedule(static)\n    for (int i = 0; i < w; ++i) {\n        float yp1 = 0.0f, yp2 = 0.0f, xp1 = 0.0f, xp2 = 0.0f;\n        for (int j = h - 1; j >= 0; --j) {\n            const float yi = a3 * xp1 + a4 * xp2 + b1 * yp1 + b2 * yp2;\n            y2[i][j] = yi;\n            xp2 = xp1; xp1 = imgIn[i][j];\n            yp2 = yp1; yp1 = yi;\n        }\n    }\n\n    /* ----- Combine rows into imgOut ----- */\n    #pragma omp parallel for collapse(2) schedule(static)\n    for (int i = 0; i < w; ++i) {\n        for (int j = 0; j < h; ++j) {\n            imgOut[i][j] = c1 * (y1[i][j] + y2[i][j]);\n        }\n    }\n\n    /* ----- Forward pass along columns ----- */\n    #pragma omp parallel for schedule(static)\n    for (int j = 0; j < h; ++j) {\n        float tm1 = 0.0f, ym1 = 0.0f, ym2 = 0.0f;\n        for (int i = 0; i < w; ++i) {\n            const float xo = imgOut[i][j];\n            const float yo = a5 * xo + a6 * tm1 + b1 * ym1 + b2 * ym2;\n            y1[i][j] = yo;\n            tm1 = xo; ym2 = ym1; ym1 = yo;\n        }\n    }\n\n    /* ----- Backward pass along columns ----- */\n    #pragma omp parallel for schedule(static)\n    for (int j = 0; j < h; ++j) {\n        float tp1 = 0.0f, tp2 = 0.0f, yp1 = 0.0f, yp2 = 0.0f;\n        for (int i = w - 1; i >= 0; --i) {\n            const float yo = a7 * tp1 + a8 * tp2 + b1 * yp1 + b2 * yp2;\n            y2[i][j] = yo;\n            tp2 = tp1; tp1 = imgOut[i][j];\n            yp2 = yp1; yp1 = yo;\n        }\n    }\n\n    /* ----- Combine into final output ----- */\n    #pragma omp parallel for collapse(2) schedule(static)\n    for (int i = 0; i < w; ++i) {\n        for (int j = 0; j < h; ++j) {\n            imgOut[i][j] = c2 * (y1[i][j] + y2[i][j]);\n        }\n    }\n}\n```\n"
}