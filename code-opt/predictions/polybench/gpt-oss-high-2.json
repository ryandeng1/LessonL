{
  "gesummv": "```cpp\n#include <vector>\n#include <omp.h>\n\nvoid kernel_gesummv(int n,\n                    double alpha,\n                    double beta,\n                    std::vector<std::vector<double>>& A,\n                    std::vector<std::vector<double>>& B,\n                    std::vector<double>& tmp,\n                    std::vector<double>& x,\n                    std::vector<double>& y)\n{\n    const double a = alpha;\n    const double b = beta;\n    const double* x_ptr = x.data();\n\n    #pragma omp parallel for schedule(static)\n    for (int i = 0; i < n; ++i) {\n        const double* Ai_ptr = A[i].data();\n        const double* Bi_ptr = B[i].data();\n\n        double tmp_sum = 0.0;\n        double y_sum   = 0.0;\n\n        for (int j = 0; j < n; ++j) {\n            tmp_sum += Ai_ptr[j] * x_ptr[j];\n            y_sum   += Bi_ptr[j] * x_ptr[j];\n        }\n\n        tmp[i] = tmp_sum;\n        y[i]   = a * tmp_sum + b * y_sum;\n    }\n}\n```\n",
  "gemm": "```cpp\n#include <vector>\n\nvoid kernel_gemm(int ni, int nj, int nk,\n                 double alpha,\n                 double beta,\n                 std::vector<std::vector<double>>& A,\n                 std::vector<std::vector<double>>& B,\n                 std::vector<std::vector<double>>& C)\n{\n    if (ni <= 0 || nj <= 0 || nk <= 0) return;\n\n    #pragma omp parallel for schedule(static)\n    for (int i = 0; i < ni; ++i) {\n        double* Crow = C[i].data();\n        const double* Arow = A[i].data();\n\n        // Scale C by beta\n        for (int j = 0; j < nj; ++j)\n            Crow[j] *= beta;\n\n        if (alpha == 0.0) continue;\n\n        for (int k = 0; k < nk; ++k) {\n            double aik = alpha * Arow[k];\n            const double* Brow = B[k].data();\n\n            #pragma omp simd\n            for (int j = 0; j < nj; ++j)\n                Crow[j] += aik * Brow[j];\n        }\n    }\n}\n```\n",
  "correlation": "```cpp\n#include <vector>\n#include <cmath>\n#include <omp.h>\n\nvoid kernel_correlation(int m, int n,\n                        std::vector<std::vector<double>>& data,\n                        std::vector<std::vector<double>>& corr,\n                        std::vector<double>& mean,\n                        std::vector<double>& stddev) {\n    const double eps = 0.1;\n\n    /* 1. Compute means */\n    for (int j = 0; j < m; ++j) {\n        double sum = 0.0;\n        for (int i = 0; i < n; ++i) {\n            sum += data[i][j];\n        }\n        mean[j] = sum / static_cast<double>(n);\n    }\n\n    /* 2. Compute standard deviations */\n    const double sqrt_n = std::sqrt(static_cast<double>(n));\n    for (int j = 0; j < m; ++j) {\n        double sumsq = 0.0;\n        double mu = mean[j];\n        for (int i = 0; i < n; ++i) {\n            double x = data[i][j] - mu;\n            sumsq += x * x;\n        }\n        double sigma = std::sqrt(sumsq / static_cast<double>(n));\n        if (sigma <= eps) sigma = 1.0;\n        stddev[j] = sigma;\n    }\n\n    /* 3. Normalise the data */\n    std::vector<double> denom(m);\n    for (int j = 0; j < m; ++j) {\n        denom[j] = sqrt_n * stddev[j];\n    }\n\n    #pragma omp parallel for schedule(static)\n    for (int i = 0; i < n; ++i) {\n        std::vector<double>& row = data[i];\n        for (int j = 0; j < m; ++j) {\n            row[j] -= mean[j];\n            row[j] /= denom[j];\n        }\n    }\n\n    /* 4. Compute the correlation matrix */\n    #pragma omp parallel for schedule(static)\n    for (int i = 0; i < m - 1; ++i) {\n        corr[i][i] = 1.0;\n        for (int j = i + 1; j < m; ++j) {\n            double sum = 0.0;\n            for (int k = 0; k < n; ++k) {\n                sum += data[k][i] * data[k][j];\n            }\n            corr[i][j] = sum;\n            corr[j][i] = sum;\n        }\n    }\n\n    if (m > 0) {\n        corr[m - 1][m - 1] = 1.0;\n    }\n}\n```",
  "covariance": "```cpp\n#include <vector>\n#include <omp.h>\n\nvoid kernel_covariance(int m, int n,\n                       std::vector<std::vector<double>>& data,\n                       std::vector<std::vector<double>>& cov,\n                       std::vector<double>& mean) {\n    const double inv_n   = 1.0 / static_cast<double>(n);\n    const double inv_n1  = 1.0 / (static_cast<double>(n) - 1.0);\n\n    /*--- 1. column means -------------------------------------------*/\n    #pragma omp parallel for schedule(static)\n    for (int j = 0; j < m; ++j) {\n        double sum = 0.0;\n        for (int i = 0; i < n; ++i) {\n            sum += data[i][j];\n        }\n        mean[j] = sum * inv_n;\n    }\n\n    /*--- 2. center the data ----------------------------------------*/\n    #pragma omp parallel for schedule(static)\n    for (int i = 0; i < n; ++i) {           // each row in its own cache line\n        double* row = data[i].data();\n        for (int j = 0; j < m; ++j) {\n            row[j] -= mean[j];\n        }\n    }\n\n    /*--- 3. row pointers for faster access ----------------------------*/\n    std::vector<const double*> rowptr(n);\n    for (int i = 0; i < n; ++i) rowptr[i] = data[i].data();\n\n    /*--- 4. covariance matrix ----------------------------------------*/\n    #pragma omp parallel for schedule(static)\n    for (int i = 0; i < m; ++i) {\n        const int i_col = i;                  // thread\u2011private copy of i\n        for (int j = i; j < m; ++j) {         // upper triangle\n            double sum = 0.0;\n            for (int k = 0; k < n; ++k) {\n                const double* row = rowptr[k];\n                sum += row[i_col] * row[j];\n            }\n            const double val = sum * inv_n1;\n            cov[i_col][j] = val;\n            cov[j][i_col] = val;              // symmetric entry\n        }\n    }\n}\n```\n",
  "trisolv": "```cpp\n#include <vector>\n\n/* Forward substitution for a unit lower\u2011triangular matrix L.\n   The implementation keeps the original algorithmic order\n   exactly, but uses raw pointers, restrict qualifiers, and simple\n   loop unrolling to reduce overhead and aid the compiler's\n   optimisations. */\nvoid kernel_trisolv(int n,\n                    std::vector<std::vector<double>>& L,\n                    std::vector<double>& x,\n                    std::vector<double>& b)\n{\n    /* Pointers to the underlying raw data.  The __restrict__\n       qualifier tells the compiler that these pointers do not alias\n       each other, which allows more aggressive vectorisation and\n       register reuse. */\n    double *__restrict xd   = x.data();\n    const double *__restrict bd = b.data();\n\n    for (int i = 0; i < n; ++i)\n    {\n        const double *__restrict Li = L[i].data();   // row i of L\n        double sum = bd[i];                         // start with RHS entry\n\n        /* Accumulate the inner dot product.  The loop is\n           manually unrolled by 4 to reduce loop\u2011overhead and\n           maintain the original sequential subtraction order. */\n        int j = 0;\n        for (; j + 3 < i; j += 4)\n        {\n            sum -= Li[j]   * xd[j];\n            sum -= Li[j+1] * xd[j+1];\n            sum -= Li[j+2] * xd[j+2];\n            sum -= Li[j+3] * xd[j+3];\n        }\n        for (; j < i; ++j)\n            sum -= Li[j] * xd[j];\n\n        /* Finish the forward substitution step. */\n        xd[i] = sum / Li[i];\n    }\n}\n```",
  "syrk": "```cpp\n#include <vector>\n#ifdef _OPENMP\n#include <omp.h>\n#endif\n\n// Computes C := alpha*A*A^T + beta*C for the lower triangular part of C.\n// A : n x m matrix, C : n x n matrix.\n// Only the lower triangular entries of C are updated, matching the original behavior.\nvoid kernel_syrk(int n, int m,\n                 double alpha,\n                 double beta,\n                 std::vector<std::vector<double>>& C,\n                 std::vector<std::vector<double>>& A)\n{\n#pragma omp parallel for schedule(static)\n    for (int i = 0; i < n; ++i)\n    {\n        double*      Ci = C[i].data();          // Pointer to row i of C\n        const double* Ai = A[i].data();         // Pointer to row i of A\n\n        // Scale the lower triangular part of C by beta\n        for (int j = 0; j <= i; ++j)\n            Ci[j] *= beta;\n\n        // Accumulate the rank-one updates\n        for (int k = 0; k < m; ++k)\n        {\n            double aik = Ai[k];\n            if (aik == 0.0)\n                continue;                     // Skip zero contributions for efficiency\n\n            double factor = alpha * aik;\n\n            for (int j = 0; j <= i; ++j)\n            {\n                const double* Aj = A[j].data();   // Pointer to row j of A\n                Ci[j] += factor * Aj[k];\n            }\n        }\n    }\n}\n```",
  "durbin": "```cpp\n#include <vector>\n\n#ifdef _OPENMP\n#include <omp.h>\n#endif\n\nvoid kernel_durbin(int n,\n                   std::vector<double>& r,\n                   std::vector<double>& y)\n{\n    if (n <= 0)\n        return;\n\n    const int PAR_THRESHOLD = 256;     // minimal loop size to justify a parallel region\n\n    const double * restrict rptr = r.data();\n    double * restrict yptr = y.data();\n\n    double beta  = 1.0;\n    double alpha = -rptr[0];   // y[0] = -r[0]\n    yptr[0] = alpha;\n\n    for (int k = 1; k < n; ++k) {\n        /* Step 1: Update beta with the previous reflection coefficient */\n        beta = (1.0 - alpha * alpha) * beta;\n\n        /* Step 2: Compute the dot\u2011product sum = \u03a3 r[k-1-i] * y[i]  (i=0..k-1) */\n        double sum = 0.0;\n        if (k > PAR_THRESHOLD) {\n#pragma omp parallel for reduction(+:sum) schedule(static)\n            for (int i = 0; i < k; ++i) {\n                sum += rptr[k - 1 - i] * yptr[i];\n            }\n        } else {\n            for (int i = 0; i < k; ++i) {\n                sum += rptr[k - 1 - i] * yptr[i];\n            }\n        }\n\n        /* Step 3: New reflection coefficient */\n        alpha = -(rptr[k] + sum) / beta;\n\n        /* Step 4: Update y[0..k-1] using the symmetric formula\n           y[i] = y[i] + alpha * y[k-1-i]  (i from 0 .. (k-1)/2) */\n        int limit = (k - 1) / 2;\n        if (k > PAR_THRESHOLD) {\n#pragma omp parallel for schedule(static)\n            for (int i = 0; i <= limit; ++i) {\n                int j = k - 1 - i;\n                double old_i = yptr[i];\n                double old_j = yptr[j];\n                yptr[i] = old_i + alpha * old_j;\n                yptr[j] = old_j + alpha * old_i;\n            }\n        } else {\n            for (int i = 0; i <= limit; ++i) {\n                int j = k - 1 - i;\n                double old_i = yptr[i];\n                double old_j = yptr[j];\n                yptr[i] = old_i + alpha * old_j;\n                yptr[j] = old_j + alpha * old_i;\n            }\n        }\n\n        /* Step 5: Store the newest coefficient */\n        yptr[k] = alpha;\n    }\n}\n```",
  "symm": "```cpp\n#include <vector>\n#include <omp.h>\n\nvoid kernel_symm(int m, int n,\n                 double alpha,\n                 double beta,\n                 std::vector<std::vector<double>>& C,\n                 std::vector<std::vector<double>>& A,\n                 std::vector<std::vector<double>>& B) {\n    // Parallelize the outer i\u2011loop by creating a team of threads.\n    // Each iteration of i is independent except for updates to C[j] with k<i.\n    // The j\u2011loop for a given i is parallelized over the threads.\n    #pragma omp parallel\n    {\n        for (int i = 0; i < m; ++i) {\n            const std::vector<double>& Ai   = A[i];          // row i of A\n            const double                Bii  = B[i][i];      // B[i][i] is not used directly but kept for readability\n            const double                alphaAii = alpha * Ai[i];  // alpha*A[i][i]\n\n            // Pre\u2011compute alpha*A[i][k] for k < i (read\u2011only for threads)\n            std::vector<double> alphaAik;\n            alphaAik.resize(i);\n            for (int k = 0; k < i; ++k) {\n                alphaAik[k] = alpha * Ai[k];\n            }\n\n            const double* Bi_row = B[i].data();  // pointer to row i of B for fast access\n\n            // Parallel over j for a fixed i\n            #pragma omp for schedule(static)\n            for (int j = 0; j < n; ++j) {\n                double temp2 = 0.0;\n                double B_i_j = Bi_row[j];\n\n                for (int k = 0; k < i; ++k) {\n                    C[k][j] += alphaAik[k] * B_i_j;\n                    temp2   += B[k][j] * Ai[k];\n                }\n\n                C[i][j] = beta * C[i][j] + alphaAii * B_i_j + alpha * temp2;\n            }\n        }\n    }\n}\n```\n",
  "bicg": "```cpp\n#include <vector>\n#include <omp.h>\n\n/* Main computational kernel. The whole function will be timed,\n   including the call and return. */\nvoid kernel_bicg(int m, int n,\n                 std::vector<std::vector<double>>& A,\n                 std::vector<double>& s,\n                 std::vector<double>& q,\n                 std::vector<double>& p,\n                 std::vector<double>& r) {\n    // If there is nothing to compute, simply return.\n    if (m <= 0 || n <= 0) {\n        return;\n    }\n\n    /* ------------------------------------------------------------\n     * Use a per\u2011thread local accumulation buffer for s to avoid\n     * false sharing and to keep the inner loop fully vectorizable.\n     * ------------------------------------------------------------ */\n    const int num_threads = omp_get_max_threads();\n    std::vector<double> local_s(num_threads * m, 0.0);   // contiguous buffer\n\n    #pragma omp parallel\n    {\n        const int tid    = omp_get_thread_num();\n        const double* p_ptr = p.data();          // pointer to p for fast access\n        const double* r_ptr = r.data();          // pointer to r for fast access\n        double*       s_local = local_s.data() + tid * m; // base of this thread's slice\n\n        // Divide the outer i\u2011loop among the threads\n        const int i_start = (n * tid) / num_threads;\n        const int i_end   = (n * (tid + 1)) / num_threads;\n\n        for (int i = i_start; i < i_end; ++i) {\n            const double* row  = A[i].data();    // pointer to row i\n            const double  r_val = r_ptr[i];       // r[i]\n            double sum_q = 0.0;\n\n            for (int j = 0; j < m; ++j) {\n                const double aij = row[j];\n                s_local[j] += r_val * aij;          // accumulation into local s\n                sum_q += aij + p_ptr[j];           // accumulate q\n            }\n            q[i] = sum_q;                          // store the result for this row\n        }\n    }  // end parallel region\n\n    /* ------------------------------------------------------------\n     * Reduce per\u2011thread accumulations into the global s vector.\n     * ------------------------------------------------------------ */\n    #pragma omp parallel for\n    for (int j = 0; j < m; ++j) {\n        double total = 0.0;\n        for (int t = 0; t < num_threads; ++t) {\n            total += local_s[t * m + j];\n        }\n        s[j] = total;\n    }\n}\n```",
  "2mm": "```cpp\n#include <vector>\n#include <algorithm>\n#include <omp.h>\n\n/* Main computational kernel. The whole function will be timed,\n   including the call and return. */\nvoid kernel_2mm(int ni, int nj, int nk, int nl,\n                double alpha,\n                double beta,\n                std::vector<std::vector<double>>& tmp,\n                std::vector<std::vector<double>>& A,\n                std::vector<std::vector<double>>& B,\n                std::vector<std::vector<double>>& C,\n                std::vector<std::vector<double>>& D) {\n\n    /* 1st phase \u2013 compute tmp = alpha * A * B   */\n    #pragma omp parallel for schedule(static)\n    for (int i = 0; i < ni; ++i) {\n        /* initialise the i\u2011th row of tmp */\n        std::fill(tmp[i].begin(), tmp[i].end(), 0.0);\n\n        const auto& Arow  = A[i];\n        double*       tmpRow = tmp[i].data();\n\n        for (int k = 0; k < nk; ++k) {\n            double aik = alpha * Arow[k];\n            if (aik == 0.0) continue;\n            const auto& Brow = B[k];\n            #pragma omp simd\n            for (int j = 0; j < nj; ++j) {\n                tmpRow[j] += aik * Brow[j];\n            }\n        }\n    }\n\n    /* 2nd phase \u2013 update D = beta * D + tmp * C   */\n    #pragma omp parallel for schedule(static)\n    for (int i = 0; i < ni; ++i) {\n        const auto& tmpRow = tmp[i];\n        double*       Drow  = D[i].data();\n\n        /* scale the row of D by beta */\n        #pragma omp simd\n        for (int j = 0; j < nl; ++j) {\n            Drow[j] *= beta;\n        }\n\n        /* accumulate tmp * C */\n        for (int k = 0; k < nj; ++k) {\n            double tkk = tmpRow[k];\n            if (tkk == 0.0) continue;\n            const auto& Crow = C[k];\n            #pragma omp simd\n            for (int j = 0; j < nl; ++j) {\n                Drow[j] += tkk * Crow[j];\n            }\n        }\n    }\n}\n```",
  "atax": "```cpp\n#include <vector>\n#include <omp.h>\n\n/* Main computational kernel. The whole function will be timed,\n   including the call and return. */\nvoid kernel_atax(int m, int n,\n                 std::vector<std::vector<double>>& A,\n                 std::vector<double>& x,\n                 std::vector<double>& y,\n                 std::vector<double>& tmp)\n{\n    const double* vec_x = x.data();          // cache the pointer to x\n\n    /* First stage: compute tmp = A * x */\n    #pragma omp parallel for schedule(static)\n    for (int i = 0; i < m; ++i) {\n        double prod = 0.0;\n        const double* row = A[i].data();\n\n        #pragma omp simd\n        for (int j = 0; j < n; ++j) {\n            prod += row[j] * vec_x[j];\n        }\n        tmp[i] = prod;\n    }\n\n    /* Second stage: compute y = A\u1d40 * tmp */\n    #pragma omp parallel for schedule(static)\n    for (int j = 0; j < n; ++j) {\n        double sum = 0.0;\n        for (int i = 0; i < m; ++i) {\n            sum += A[i][j] * tmp[i];\n        }\n        y[j] = sum;\n    }\n}\n```",
  "doitgen": "```cpp\n#include <vector>\n#include <algorithm>\n\n/* Main computational kernel. The whole function will be timed,\n   including the call and return. */\nvoid kernel_doitgen(int nr, int nq, int np,\n                    std::vector<std::vector<std::vector<double>>>& A,\n                    std::vector<std::vector<double>>& C4,\n                    std::vector<double>& sum)\n{\n    /* Handle degenerate sizes the same way as the original routine */\n    if (nr <= 0 || nq <= 0 || np <= 0)\n        return;\n\n    const int total = nr * nq;                 // number of rows to process\n\n    /* Parallelise across the (r,q) pairs.  Each thread owns its own\n       temporary buffer for the original row so that there is no\n       cross\u2011thread data race. */\n    #pragma omp parallel\n    {\n        std::vector<double> orig(np);          // temporary for the original row\n\n        #pragma omp for schedule(static)\n        for (int idx = 0; idx < total; ++idx)\n        {\n            const int r = idx / nq;\n            const int q = idx % nq;\n\n            double* dest = A[r][q].data();    // pointer to the row that will be overwritten\n\n            /* Copy the original row into the temporary buffer */\n            std::copy(dest, dest + np, orig.begin());\n\n            /* Start with a zeroed destination row */\n            std::fill(dest, dest + np, 0.0);\n\n            /* Perform the matrix\u2011vector multiplication:\n               dest[p] += orig[s] * C4[s][p]  for all s. */\n            for (int s = 0; s < np; ++s)\n            {\n                const double a_val = orig[s];\n                const double* c_row = C4[s].data();\n\n                #pragma omp simd\n                for (int p = 0; p < np; ++p)\n                {\n                    dest[p] += a_val * c_row[p];\n                }\n            }\n        }\n    }\n\n    /* Preserve original behaviour: after the loop 'sum' contains the\n       contents of the last processed row. */\n    std::copy(A[nr - 1][nq - 1].begin(),\n              A[nr - 1][nq - 1].end(),\n              sum.begin());\n}\n```",
  "jacobi_1d": "```cpp\n#include <vector>\n\nvoid kernel_jacobi_1d(int tsteps, int n,\n                      std::vector<double>& A, std::vector<double>& B)\n{\n    if (tsteps <= 0 || n <= 2) return;\n\n    const double coef = 0.33333;\n    double* a = A.data();\n    double* b = B.data();\n\n    #pragma omp parallel\n    {\n        for (int t = 0; t < tsteps; ++t) {\n            /* Update B from A */\n            #pragma omp for schedule(static)\n            for (int i = 1; i < n - 1; ++i) {\n                b[i] = coef * (a[i - 1] + a[i] + a[i + 1]);\n            }\n\n            /* Update A from B */\n            #pragma omp for schedule(static)\n            for (int i = 1; i < n - 1; ++i) {\n                a[i] = coef * (b[i - 1] + b[i] + b[i + 1]);\n            }\n        }\n    }\n}\n```",
  "adi": "```cpp\n#include <vector>\n#include <omp.h>\n\n/* Main computational kernel. The whole function will be timed,\n   including the call and return. */\n/* Based on a Fortran code fragment from Figure 5 of\n * \"Automatic Data and Computation Decomposition on Distributed Memory Parallel Computers\"\n * by Peizong Lee and Zvi Meir Kedem, TOPLAS, 2002\n */\nvoid kernel_adi(int tsteps, int n,\n                std::vector<std::vector<double>>& u,\n                std::vector<std::vector<double>>& v,\n                std::vector<std::vector<double>>& p,\n                std::vector<std::vector<double>>& q) {\n    /* Pre\u2011compute constants once */\n    const double DX = 1.0 / n;\n    const double DY = 1.0 / n;\n    const double DT = 1.0 / tsteps;\n    const double mul1 = 2.0 * DT / (DX * DX);\n    const double mul2 = 1.0 * DT / (DY * DY);\n    const double a   = -mul1 / 2.0;\n    const double b   = 1.0 + mul1;\n    const double c   = a;              /* c == a */\n    const double d   = -mul2 / 2.0;\n    const double e   = 1.0 + mul2;\n    const double f   = d;              /* f == d */\n\n    const double one_plus_2d = 1.0 + 2.0 * d;\n    const double one_plus_2a = 1.0 + 2.0 * a;\n\n    /* Parallelise over the spatial dimension i.  Both stages are\n       executed sequentially for each time step, but each stage\n       is parallelised with an OpenMP for.  Each thread works\n       on disjoint i's within a time step. */\n    #pragma omp parallel\n    {\n        for (int tstep = 1; tstep <= tsteps; ++tstep) {\n            /* ---------- Stage 1: update v from u ---------- */\n            #pragma omp for schedule(static)\n            for (int i = 1; i < n - 1; ++i) {\n                double *prow = p[i].data();   // p[i][*]\n                double *qrow = q[i].data();   // q[i][*]\n\n                /* Set boundary values for column i */\n                v[0][i] = 1.0;\n                prow[0] = 0.0;\n                qrow[0] = v[0][i];\n\n                /* Forward sweep in the j direction */\n                for (int j = 1; j < n - 1; ++j) {\n                    double denom = a * prow[j-1] + b;\n                    double pj    = -c / denom;\n                    prow[j] = pj;\n\n                    const double *urow = u[j].data();\n                    double qj = (-d * urow[i-1]\n                                 + one_plus_2d * urow[i]\n                                 - f * urow[i+1]\n                                 - a * qrow[j-1]) / denom;\n                    qrow[j] = qj;\n                }\n\n                v[n-1][i] = 1.0;\n\n                /* Backward sweep to form the solution column in v */\n                for (int j = n - 2; j >= 1; --j) {\n                    v[j][i] = prow[j] * v[j+1][i] + qrow[j];\n                }\n            }\n\n            /* ---------- Stage 2: update u from v ---------- */\n            #pragma omp for schedule(static)\n            for (int i = 1; i < n - 1; ++i) {\n                double *prow = p[i].data();   // p[i][*]\n                double *qrow = q[i].data();   // q[i][*]\n\n                u[i][0] = 1.0;\n                prow[0] = 0.0;\n                qrow[0] = u[i][0];\n\n                const double *vprev = v[i-1].data(); // row i\u20111\n                const double *vrow  = v[i].data();   // row i\n                const double *vnext = v[i+1].data(); // row i+1\n\n                /* Forward sweep in the j direction */\n                for (int j = 1; j < n - 1; ++j) {\n                    double denom = d * prow[j-1] + e;\n                    double pj    = -f / denom;\n                    prow[j] = pj;\n\n                    double qj = (-a * vprev[j]\n                                 + one_plus_2a * vrow[j]\n                                 - c * vnext[j]\n                                 - d * qrow[j-1]) / denom;\n                    qrow[j] = qj;\n                }\n\n                u[i][n-1] = 1.0;\n\n                /* Backward sweep to form the solution row in u */\n                for (int j = n - 2; j >= 1; --j) {\n                    u[i][j] = prow[j] * u[i][j+1] + qrow[j];\n                }\n            }\n        } // end time\u2011step loop\n    } // end parallel region\n}\n```",
  "3mm": "```cpp\n#include <vector>\n#include <omp.h>\n\nvoid kernel_3mm(int ni, int nj, int nk, int nl, int nm,\n                std::vector<std::vector<double>>& E,\n                std::vector<std::vector<double>>& A,\n                std::vector<std::vector<double>>& B,\n                std::vector<std::vector<double>>& F,\n                std::vector<std::vector<double>>& C,\n                std::vector<std::vector<double>>& D,\n                std::vector<std::vector<double>>& G)\n{\n    /* Pre\u2011compute pointers to the rows of B for fast access */\n    std::vector<const double*> B_rows(nk);\n    for (int k = 0; k < nk; ++k) B_rows[k] = B[k].data();\n\n    /* E = A * B  */\n    #pragma omp parallel for collapse(2) schedule(static)\n    for (int i = 0; i < ni; ++i) {\n        const double *a_row = A[i].data();\n        for (int j = 0; j < nj; ++j) {\n            double sum = 0.0;\n            for (int k = 0; k < nk; ++k) {\n                sum += a_row[k] * B_rows[k][j];\n            }\n            E[i][j] = sum;\n        }\n    }\n\n    /* Pre\u2011compute pointers to the rows of D for fast access */\n    std::vector<const double*> D_rows(nm);\n    for (int k = 0; k < nm; ++k) D_rows[k] = D[k].data();\n\n    /* F = C * D  */\n    #pragma omp parallel for collapse(2) schedule(static)\n    for (int i = 0; i < nj; ++i) {\n        const double *c_row = C[i].data();\n        for (int j = 0; j < nl; ++j) {\n            double sum = 0.0;\n            for (int k = 0; k < nm; ++k) {\n                sum += c_row[k] * D_rows[k][j];\n            }\n            F[i][j] = sum;\n        }\n    }\n\n    /* Pre\u2011compute pointers to the rows of F for fast access */\n    std::vector<const double*> F_rows(nj);\n    for (int k = 0; k < nj; ++k) F_rows[k] = F[k].data();\n\n    /* G = E * F  */\n    #pragma omp parallel for collapse(2) schedule(static)\n    for (int i = 0; i < ni; ++i) {\n        const double *e_row = E[i].data();\n        for (int j = 0; j < nl; ++j) {\n            double sum = 0.0;\n            for (int k = 0; k < nj; ++k) {\n                sum += e_row[k] * F_rows[k][j];\n            }\n            G[i][j] = sum;\n        }\n    }\n}\n```",
  "nussinov": "```cpp\n#include <vector>\n#include <algorithm>\n#ifdef _OPENMP\n#include <omp.h>\n#endif\n\n/* Optimised Nussinov kernel.\n * Uses a distance\u2011based DP order with OpenMP parallelisation\n * on the inner i\u2011loop. The algorithm is equivalent to the\n * original sequential implementation. */\nvoid kernel_nussinov(int n, std::vector<int>& seq,\n                     std::vector<std::vector<int>>& table)\n{\n    #pragma omp parallel\n    {\n        for (int dist = 1; dist < n; ++dist)\n        {\n            int max_i = n - dist;\n\n            #pragma omp for schedule(static)\n            for (int i = 0; i < max_i; ++i)\n            {\n                int j  = i + dist;\n                int best = table[i][j];\n                const int s_i = seq[i];\n                const int s_j = seq[j];\n\n                /* Unpaired j */\n                best = std::max(best, table[i][j - 1]);\n\n                /* Unpaired i */\n                best = std::max(best, table[i + 1][j]);\n\n                /* Pair i\u2011j */\n                if (i < j - 1)\n                {\n                    best = std::max(best,\n                        table[i + 1][j - 1] + ((s_i + s_j) == 3));\n                }\n                else\n                {\n                    best = std::max(best, table[i + 1][j - 1]);\n                }\n\n                /* Split */\n                for (int k = i + 1; k < j; ++k)\n                {\n                    best = std::max(best,\n                        table[i][k] + table[k + 1][j]);\n                }\n\n                table[i][j] = best;\n            }\n        }\n    }\n}\n```",
  "jacobi_2d": "```cpp\n#include <vector>\n#include <omp.h>\n\nvoid kernel_jacobi_2d(int tsteps,\n                      int n,\n                      std::vector<std::vector<double>>& A,\n                      std::vector<std::vector<double>>& B) {\n    const double coeff = 0.2;\n\n    /* Build raw row pointers for faster access. */\n    std::vector<double*> a_row(n);\n    std::vector<double*> b_row(n);\n    for (int i = 0; i < n; ++i) {\n        a_row[i] = A[i].data();\n        b_row[i] = B[i].data();\n    }\n\n    for (int t = 0; t < tsteps; ++t) {\n        /* Stage 1: compute B from A */\n#pragma omp parallel for schedule(static)\n        for (int i = 1; i < n - 1; ++i) {\n            double* restrict b = b_row[i];\n            const double* restrict a   = a_row[i];\n            const double* restrict a_up   = a_row[i - 1];\n            const double* restrict a_down = a_row[i + 1];\n#pragma omp simd\n            for (int j = 1; j < n - 1; ++j) {\n                double sum = a[j] + a[j - 1] + a[j + 1] + a_down[j] + a_up[j];\n                b[j] = coeff * sum;\n            }\n        }\n\n        /* Stage 2: compute A from B */\n#pragma omp parallel for schedule(static)\n        for (int i = 1; i < n - 1; ++i) {\n            double* restrict a = a_row[i];\n            const double* restrict b   = b_row[i];\n            const double* restrict b_up   = b_row[i - 1];\n            const double* restrict b_down = b_row[i + 1];\n#pragma omp simd\n            for (int j = 1; j < n - 1; ++j) {\n                double sum = b[j] + b[j - 1] + b[j + 1] + b_down[j] + b_up[j];\n                a[j] = coeff * sum;\n            }\n        }\n    }\n}\n```",
  "cholesky": "```cpp\n#include <cmath>\n#include <vector>\n\n/* Main computational kernel. The whole function will be timed,\n   including the call and return. */\nvoid kernel_cholesky(int n,\n\t\t     std::vector<std::vector<double>>& A) {\n    for (int i = 0; i < n; ++i) {\n        double* Ai = A[i].data();              // row i\n        for (int j = 0; j < i; ++j) {\n            double* Aj = A[j].data();          // row j\n            double tmp = Ai[j];\n            for (int k = 0; k < j; ++k) {\n                tmp -= Ai[k] * Aj[k];\n            }\n            Ai[j] = tmp / Aj[j];\n        }\n        double tmp = Ai[i];\n        for (int k = 0; k < i; ++k) {\n            tmp -= Ai[k] * Ai[k];\n        }\n        Ai[i] = std::sqrt(tmp);\n    }\n}\n```\n",
  "fdtd_2d": "```cpp\n#include <vector>\n#include <omp.h>\n\n/* Main computational kernel. The whole function will be timed,\n   including the call and return. */\nvoid kernel_fdtd_2d(int tmax,\n                    int nx,\n                    int ny,\n                    std::vector<std::vector<double>>& ex,\n                    std::vector<std::vector<double>>& ey,\n                    std::vector<std::vector<double>>& hz,\n                    std::vector<double>& _fict_) {\n    if (tmax <= 0 || nx <= 0 || ny <= 0) return;\n\n    const int NRows = nx;\n    const int NCols = ny;\n    const double half = 0.5;\n    const double coef = 0.7;\n\n    /* Pointers to each row for fast linear indexing */\n    std::vector<double*> exPtr(NRows);\n    std::vector<double*> eyPtr(NRows);\n    std::vector<double*> hzPtr(NRows);\n\n    for (int i = 0; i < NRows; ++i) {\n        exPtr[i] = ex[i].data();\n        eyPtr[i] = ey[i].data();\n        hzPtr[i] = hz[i].data();\n    }\n\n    for (int t = 0; t < tmax; ++t) {\n        /* 1. Source update on the first row of ey */\n        double* ey0 = eyPtr[0];\n#pragma omp simd\n        for (int j = 0; j < NCols; ++j) {\n            ey0[j] = _fict_[t];\n        }\n\n        /* 2. Update ey (excluding the first row) */\n#pragma omp parallel for schedule(static)\n        for (int i = 1; i < NRows; ++i) {\n            double* eyRow     = eyPtr[i];\n            double* hzRow     = hzPtr[i];\n            double* hzPrevRow = hzPtr[i - 1];\n#pragma omp simd\n            for (int j = 0; j < NCols; ++j) {\n                eyRow[j] -= half * (hzRow[j] - hzPrevRow[j]);\n            }\n        }\n\n        /* 3. Update ex (excluding the first column) */\n#pragma omp parallel for schedule(static)\n        for (int i = 0; i < NRows; ++i) {\n            double* exRow = exPtr[i];\n            double* hzRow = hzPtr[i];\n#pragma omp simd\n            for (int j = 1; j < NCols; ++j) {\n                exRow[j] -= half * (hzRow[j] - hzRow[j - 1]);\n            }\n        }\n\n        /* 4. Update hz */\n#pragma omp parallel for schedule(static)\n        for (int i = 0; i < NRows - 1; ++i) {\n            double* hzRow     = hzPtr[i];\n            const double* exRow     = exPtr[i];\n            const double* eyRow     = eyPtr[i];\n            const double* eyNextRow = eyPtr[i + 1];\n#pragma omp simd\n            for (int j = 0; j < NCols - 1; ++j) {\n                hzRow[j] -= coef * (exRow[j + 1] - exRow[j] + eyNextRow[j] - eyRow[j]);\n            }\n        }\n    }\n}\n```",
  "heat_3d": "```cpp\n#include <vector>\n#include <omp.h>\n\nvoid kernel_heat_3d(int tsteps,\n                    int n,\n                    std::vector<std::vector<std::vector<double>>>& A,\n                    std::vector<std::vector<std::vector<double>>>& B)\n{\n    const double coeff = 0.125;           // 1/8\n\n    #pragma omp parallel\n    {\n        for (int t = 1; t <= tsteps; ++t)\n        {\n            /* Update B from A */\n            #pragma omp for collapse(2) schedule(static)\n            for (int i = 1; i < n - 1; ++i)\n            {\n                const auto& A_ip = A[i + 1];   // A[i+1]\n                const auto& A_im = A[i - 1];   // A[i-1]\n                auto& B_i = B[i];             // B[i]\n\n                for (int j = 1; j < n - 1; ++j)\n                {\n                    const double* a_curr = A[i][j].data();        // A[i][j]\n                    const double* a_ip   = A_ip[j].data();        // A[i+1][j]\n                    const double* a_im   = A_im[j].data();        // A[i-1][j]\n                    const double* a_jp   = A[i][j + 1].data();    // A[i][j+1]\n                    const double* a_jm   = A[i][j - 1].data();    // A[i][j-1]\n                    double* b_curr = B_i[j].data();              // B[i][j]\n\n                    #pragma omp simd\n                    for (int k = 1; k < n - 1; ++k)\n                    {\n                        const double a0   = a_curr[k];\n                        const double neg2a = -2.0 * a0;     // -2*A[i][j][k]\n\n                        double t1 = (a_ip[k] + neg2a + a_im[k]) * coeff;\n                        double t2 = (a_jp[k] + neg2a + a_jm[k]) * coeff;\n                        double t3 = (a_curr[k + 1] + neg2a + a_curr[k - 1]) * coeff;\n\n                        b_curr[k] = ((t1 + t2) + t3) + a0;\n                    }\n                }\n            }\n\n            /* Update A from B */\n            #pragma omp for collapse(2) schedule(static)\n            for (int i = 1; i < n - 1; ++i)\n            {\n                const auto& B_ip = B[i + 1];   // B[i+1]\n                const auto& B_im = B[i - 1];   // B[i-1]\n                auto& A_i = A[i];             // A[i]\n\n                for (int j = 1; j < n - 1; ++j)\n                {\n                    const double* b_curr = B[i][j].data();        // B[i][j]\n                    const double* b_ip   = B_ip[j].data();        // B[i+1][j]\n                    const double* b_im   = B_im[j].data();        // B[i-1][j]\n                    const double* b_jp   = B[i][j + 1].data();    // B[i][j+1]\n                    const double* b_jm   = B[i][j - 1].data();    // B[i][j-1]\n                    double* a_curr = A_i[j].data();              // A[i][j]\n\n                    #pragma omp simd\n                    for (int k = 1; k < n - 1; ++k)\n                    {\n                        const double b0   = b_curr[k];\n                        const double neg2b = -2.0 * b0;     // -2*B[i][j][k]\n\n                        double t1 = (b_ip[k] + neg2b + b_im[k]) * coeff;\n                        double t2 = (b_jp[k] + neg2b + b_jm[k]) * coeff;\n                        double t3 = (b_curr[k + 1] + neg2b + b_curr[k - 1]) * coeff;\n\n                        a_curr[k] = ((t1 + t2) + t3) + b0;\n                    }\n                }\n            }\n        }\n    }\n}\n```",
  "syr2k": "```cpp\n#include <vector>\n#include <omp.h>\n\nvoid kernel_syr2k(int n, int m,\n                  double alpha,\n                  double beta,\n                  std::vector<std::vector<double>>& C,\n                  std::vector<std::vector<double>>& A,\n                  std::vector<std::vector<double>>& B)\n{\n    // Pre\u2011allocate row pointers to avoid repeated data() calls\n    std::vector<const double*> A_rows(n);\n    std::vector<const double*> B_rows(n);\n    for (int i = 0; i < n; ++i) {\n        A_rows[i] = A[i].data();\n        B_rows[i] = B[i].data();\n    }\n\n    const double a = alpha;\n    const double b = beta;\n\n    // Parallel over the outer index i\n    #pragma omp parallel for schedule(dynamic)\n    for (int i = 0; i < n; ++i) {\n        double* ci = C[i].data();\n        const double* ai = A_rows[i];\n        const double* bi = B_rows[i];\n\n        // Scale the lower triangular part of C\n        for (int j = 0; j <= i; ++j) {\n            ci[j] *= b;\n        }\n\n        for (int k = 0; k < m; ++k) {\n            const double B_ik = bi[k];\n            const double A_ik = ai[k];\n\n#pragma omp simd\n            for (int j = 0; j <= i; ++j) {\n                ci[j] += (A_rows[j][k] * a) * B_ik + (B_rows[j][k] * a) * A_ik;\n            }\n        }\n    }\n}\n```",
  "seidel_2d": "```cpp\n#include <vector>\n\n/* Main computational kernel. The whole function will be timed,\n   including the call and return. */\nvoid kernel_seidel_2d(int tsteps, int n, std::vector<std::vector<double>>& A) {\n    const double inv9 = 1.0 / 9.0;                     // precompute division factor\n\n    for (int t = 0; t < tsteps; ++t) {\n        for (int i = 1; i <= n - 2; ++i) {\n            /* Pointers to the three rows involved in the stencil. */\n            double* __restrict up   = A[i - 1].data();\n            double* __restrict cur  = A[i].data();\n            double* __restrict down = A[i + 1].data();\n\n            int j = 1;\n            /* Unroll the inner loop by a factor of 4 for better instruction throughput. */\n            for (; j <= n - 5; j += 4) {\n                const double s0 = up[j - 1] + up[j] + up[j + 1] +\n                                  cur[j - 1] + cur[j] + cur[j + 1] +\n                                  down[j - 1] + down[j] + down[j + 1];\n\n                const double s1 = up[j] + up[j + 1] + up[j + 2] +\n                                  cur[j] + cur[j + 1] + cur[j + 2] +\n                                  down[j] + down[j + 1] + down[j + 2];\n\n                const double s2 = up[j + 1] + up[j + 2] + up[j + 3] +\n                                  cur[j + 1] + cur[j + 2] + cur[j + 3] +\n                                  down[j + 1] + down[j + 2] + down[j + 3];\n\n                const double s3 = up[j + 2] + up[j + 3] + up[j + 4] +\n                                  cur[j + 2] + cur[j + 3] + cur[j + 4] +\n                                  down[j + 2] + down[j + 3] + down[j + 4];\n\n                cur[j]     = s0 * inv9;\n                cur[j + 1] = s1 * inv9;\n                cur[j + 2] = s2 * inv9;\n                cur[j + 3] = s3 * inv9;\n            }\n\n            /* Handle any remaining columns that weren't covered by the loop unrolling. */\n            for (; j <= n - 2; ++j) {\n                double s = up[j - 1] + up[j] + up[j + 1] +\n                           cur[j - 1] + cur[j] + cur[j + 1] +\n                           down[j - 1] + down[j] + down[j + 1];\n                cur[j] = s * inv9;\n            }\n        }\n    }\n}\n```\n",
  "lu": "```cpp\n#include <vector>\n#include <omp.h>\n\n/* Main computational kernel. The whole function will be timed,\n   including the call and return. */\nvoid kernel_lu(int n,\n               std::vector<std::vector<double>>& A) {\n    /* Create a pointer array to each row for faster indexing. */\n    std::vector<double*> rows(n);\n    for (int i = 0; i < n; ++i)\n        rows[i] = A[i].data();\n\n    for (int i = 0; i < n; ++i) {\n        double *row_i = rows[i];\n\n        /* --- Lower triangular part (L) --- */\n        for (int j = 0; j < i; ++j) {\n            for (int k = 0; k < j; ++k) {\n                row_i[j] -= row_i[k] * rows[k][j];\n            }\n            row_i[j] /= rows[j][j];\n        }\n\n        /* --- Upper triangular part (U) --- */\n#pragma omp parallel for schedule(static)\n        for (int j = i; j < n; ++j) {\n            for (int k = 0; k < i; ++k) {\n                row_i[j] -= row_i[k] * rows[k][j];\n            }\n        }\n    }\n}\n```",
  "mvt": "```cpp\n#include <vector>\n#include <omp.h>\n\nvoid kernel_mvt(int n,\n                std::vector<double>& x1,\n                std::vector<double>& x2,\n                std::vector<double>& y_1,\n                std::vector<double>& y_2,\n                std::vector<std::vector<double>>& A) {\n    const double* y1 = y_1.data();\n    const double* y2 = y_2.data();\n\n    /* First pass \u2013 x1 += A * y_1 */\n    #pragma omp parallel for schedule(static)\n    for (int i = 0; i < n; ++i) {\n        const double* row_i = A[i].data();\n        double sum = x1[i];\n        #pragma omp simd\n        for (int j = 0; j < n; ++j) {\n            sum += row_i[j] * y1[j];\n        }\n        x1[i] = sum;\n    }\n\n    /* Second pass \u2013 x2 += A\u1d40 * y_2 */\n    #pragma omp parallel for schedule(static)\n    for (int i = 0; i < n; ++i) {\n        double sum = x2[i];\n        #pragma omp simd\n        for (int j = 0; j < n; ++j) {\n            sum += A[j][i] * y2[j];\n        }\n        x2[i] = sum;\n    }\n}\n```",
  "gramschmidt": "```cpp\n#include <cmath>\n#include <vector>\n#include <omp.h>\n\n/* Main computational kernel. The whole function will be timed,\n   including the call and return. */\n/* QR Decomposition with Modified Gram Schmidt:\n   http://www.inf.ethz.ch/personal/gander/ */\nvoid kernel_gramschmidt(int m, int n,\n                        std::vector<std::vector<double>>& A,\n                        std::vector<std::vector<double>>& R,\n                        std::vector<std::vector<double>>& Q)\n{\n    /* Get raw pointers for faster access */\n    std::vector<double*> Ap(m);\n    std::vector<double*> Qp(m);\n    std::vector<double*> Rp(n);\n    for (int i = 0; i < m; ++i) {\n        Ap[i] = A[i].data();\n        Qp[i] = Q[i].data();\n    }\n    for (int i = 0; i < n; ++i) {\n        Rp[i] = R[i].data();\n    }\n\n    /* Temporary buffer for the current Q column */\n    std::vector<double> Qk(m);\n\n    for (int k = 0; k < n; ++k) {\n        /* Compute the norm of column k of A */\n        double nrm = 0.0;\n        for (int i = 0; i < m; ++i) {\n            double v = Ap[i][k];\n            nrm += v * v;\n        }\n        double rkk = std::sqrt(nrm);\n        Rp[k][k] = rkk;\n\n        /* Parallel work for this k */\n        #pragma omp parallel\n        {\n            /* Compute Q column and fill the Q matrix */\n            #pragma omp for schedule(static)\n            for (int i = 0; i < m; ++i) {\n                double q = Ap[i][k] / rkk;\n                Qk[i]   = q;\n                Qp[i][k] = q;\n            }\n\n            /* Compute remaining columns j > k */\n            #pragma omp for schedule(static)\n            for (int j = k + 1; j < n; ++j) {\n                double dot = 0.0;\n                for (int i = 0; i < m; ++i) {\n                    dot += Qk[i] * Ap[i][j];\n                }\n                Rp[k][j] = dot;\n                for (int i = 0; i < m; ++i) {\n                    Ap[i][j] -= Qk[i] * dot;\n                }\n            }\n        } /* end parallel region */\n    }\n}\n```",
  "ludcmp": "```cpp\n#include <vector>\n\n/* Main computational kernel. The whole function will be timed,\n   including the call and return. */\nvoid kernel_ludcmp(int n,\n                   std::vector<std::vector<double>>& A,\n                   std::vector<double>& b,\n                   std::vector<double>& x,\n                   std::vector<double>& y)\n{\n    /* Transpose of A for fast column access\n       (B[j][k] == A[k][j])                                       */\n    std::vector<std::vector<double>> B(n, std::vector<double>(n));\n    for (int i = 0; i < n; ++i)\n        for (int j = 0; j < n; ++j)\n            B[j][i] = A[i][j];\n\n    /* LU decomposition (Doolittle) */\n    for (int i = 0; i < n; ++i) {\n        double* Ai = A[i].data();           // row i\n        /* 1) Compute multipliers (lower\u2011triangular part) */\n        for (int j = 0; j < i; ++j) {\n            double w = Ai[j];\n            double* Bj = B[j].data();      // column j\n            for (int k = 0; k < j; ++k)\n                w -= Ai[k] * Bj[k];\n            double pivot = Bj[j];           // A[j][j]\n            w /= pivot;\n            Ai[j] = w;\n            Bj[i] = w;                     // keep B up to date\n        }\n        /* 2) Update upper\u2011triangular part */\n        for (int j = i; j < n; ++j) {\n            double w = Ai[j];\n            double* Bj = B[j].data();      // column j\n            for (int k = 0; k < i; ++k)\n                w -= Ai[k] * Bj[k];\n            Ai[j] = w;\n            Bj[i] = w;                     // keep B up to date\n        }\n    }\n\n    /* Forward substitution: L * y = b  (L has 1s on diagonal) */\n    for (int i = 0; i < n; ++i) {\n        double w = b[i];\n        double* Ai = A[i].data();\n        for (int j = 0; j < i; ++j)\n            w -= Ai[j] * y[j];\n        y[i] = w;\n    }\n\n    /* Backward substitution: U * x = y */\n    for (int i = n - 1; i >= 0; --i) {\n        double w = y[i];\n        double* Ai = A[i].data();\n        for (int j = i + 1; j < n; ++j)\n            w -= Ai[j] * x[j];\n        x[i] = w / Ai[i];\n    }\n}\n```",
  "gemver": "```cpp\n#include <vector>\n#include <omp.h>\n\n/* Main computational kernel. The whole function will be timed,\n   including the call and return. */\nvoid kernel_gemver(int n,\n                   double alpha,\n                   double beta,\n                   std::vector<std::vector<double>>& A,\n                   std::vector<double>& u1,\n                   std::vector<double>& v1,\n                   std::vector<double>& u2,\n                   std::vector<double>& v2,\n                   std::vector<double>& w,\n                   std::vector<double>& x,\n                   std::vector<double>& y,\n                   std::vector<double>& z)\n{\n    /* Nothing to do for empty matrices */\n    if (n <= 0) return;\n\n    /* Keep read\u2011only pointers to the input vectors once per parallel region */\n    const double* v1data = v1.data();\n    const double* v2data = v2.data();\n    const double* ydata = y.data();\n    const double* zdata = z.data();\n    const double* xdata = x.data();      // read\u2011only after the 2nd step\n\n    #pragma omp parallel\n    {\n        /* Step 1 : A += u1*v1\u1d40 + u2*v2\u1d40 */\n        #pragma omp for schedule(static)\n        for (int i = 0; i < n; ++i)\n        {\n            double u1i = u1[i];\n            double u2i = u2[i];\n            double* Ai = A[i].data();\n            #pragma omp simd\n            for (int j = 0; j < n; ++j)\n                Ai[j] += u1i * v1data[j] + u2i * v2data[j];\n        }\n\n        /* Step 2 + 3 : x += beta*A\u1d40*y ; x += z */\n        #pragma omp for schedule(static)\n        for (int i = 0; i < n; ++i)\n        {\n            double xi = x[i];\n            for (int j = 0; j < n; ++j)\n            {\n                double betay = beta * ydata[j];\n                const double* row = A[j].data();   // A[j][i]\n                xi += betay * row[i];\n            }\n            xi += zdata[i];\n            x[i] = xi;\n        }\n\n        /* Step 4 : w += alpha*A*x */\n        #pragma omp for schedule(static)\n        for (int i = 0; i < n; ++i)\n        {\n            double wi = w[i];\n            const double* Ai = A[i].data();\n            #pragma omp simd\n            for (int j = 0; j < n; ++j)\n                wi += alpha * Ai[j] * xdata[j];\n            w[i] = wi;\n        }\n    }\n}\n```",
  "deriche": "```cpp\n#include <cmath>\n#include <vector>\n#include <omp.h>\n\n/* Optimised Deriche filter kernel */\nvoid kernel_deriche(int w, int h, double alpha,\n                    std::vector<std::vector<float>>& imgIn,\n                    std::vector<std::vector<float>>& imgOut,\n                    std::vector<std::vector<float>>& y1,\n                    std::vector<std::vector<float>>& y2)\n{\n    /* --- Pre\u2011compute filter coefficients --- */\n    const double exp_a  = std::exp(-alpha);\n    const double exp_2a = exp_a * exp_a;\n    const double k_d    = 1.0 - exp_a * (1.0 - exp_a) /\n                          (1.0 + 2.0 * alpha * exp_a - exp_2a);\n    const float  k       = static_cast<float>(k_d);\n\n    const float a1 = k, a5 = k;\n\n    const double a2_d = static_cast<double>(k) * exp_a * (alpha - 1.0);\n    const float  a2   = static_cast<float>(a2_d);\n    const float  a6   = a2;\n\n    const double a3_d = static_cast<double>(k) * exp_a * (alpha + 1.0);\n    const float  a3   = static_cast<float>(a3_d);\n    const float  a7   = a3;\n\n    const double a4_d = -static_cast<double>(k) * exp_2a;\n    const float  a4   = static_cast<float>(a4_d);\n    const float  a8   = a4;\n\n    const float  b1 = static_cast<float>(std::pow(2.0, -alpha));\n    const float  b2 = static_cast<float>(-exp_2a);\n\n    /* --- Row\u2011wise forward pass: compute y1 --- */\n    #pragma omp parallel for schedule(static)\n    for (int i = 0; i < w; ++i) {\n        const float *in  = imgIn[i].data();\n        float *      y1r = y1[i].data();\n\n        float ym1 = 0.0f, ym2 = 0.0f, xm1 = 0.0f;\n        for (int j = 0; j < h; ++j) {\n            const float x = in[j];\n            const float y = a1 * x + a2 * xm1 + b1 * ym1 + b2 * ym2;\n            y1r[j] = y;\n            xm1 = x;\n            ym2 = ym1;\n            ym1 = y;\n        }\n    }\n\n    /* --- Row\u2011wise backward pass: compute y2 --- */\n    #pragma omp parallel for schedule(static)\n    for (int i = 0; i < w; ++i) {\n        const float *in  = imgIn[i].data();\n        float *      y2r = y2[i].data();\n\n        float yp1 = 0.0f, yp2 = 0.0f, xp1 = 0.0f, xp2 = 0.0f;\n        for (int j = h - 1; j >= 0; --j) {\n            const float y = a3 * xp1 + a4 * xp2 + b1 * yp1 + b2 * yp2;\n            y2r[j] = y;\n            xp2 = xp1;\n            xp1 = in[j];\n            yp2 = yp1;\n            yp1 = y;\n        }\n    }\n\n    /* --- Merge row results into imgOut (c1 = 1.0f) --- */\n    #pragma omp parallel for schedule(static)\n    for (int i = 0; i < w; ++i) {\n        const float *y1r = y1[i].data();\n        const float *y2r = y2[i].data();\n        float *      out = imgOut[i].data();\n        for (int j = 0; j < h; ++j)\n            out[j] = y1r[j] + y2r[j];   // c1 = 1.0f\n    }\n\n    /* --- Column\u2011wise forward pass on imgOut: compute new y1 --- */\n    #pragma omp parallel for schedule(static)\n    for (int j = 0; j < h; ++j) {\n        float tm1 = 0.0f, ym1 = 0.0f, ym2 = 0.0f;\n        for (int i = 0; i < w; ++i) {\n            const float x = imgOut[i][j];\n            const float y = a5 * x + a6 * tm1 + b1 * ym1 + b2 * ym2;\n            y1[i][j] = y;\n            tm1 = x;\n            ym2 = ym1;\n            ym1 = y;\n        }\n    }\n\n    /* --- Column\u2011wise backward pass on imgOut: compute new y2 --- */\n    #pragma omp parallel for schedule(static)\n    for (int j = 0; j < h; ++j) {\n        float tp1 = 0.0f, tp2 = 0.0f, yp1 = 0.0f, yp2 = 0.0f;\n        for (int i = w - 1; i >= 0; --i) {\n            const float y = a7 * tp1 + a8 * tp2 + b1 * yp1 + b2 * yp2;\n            y2[i][j] = y;\n            tp2 = tp1;\n            tp1 = imgOut[i][j];\n            yp2 = yp1;\n            yp1 = y;\n        }\n    }\n\n    /* --- Final merge: imgOut = y1 + y2 (c2 = 1.0f) --- */\n    #pragma omp parallel for schedule(static)\n    for (int i = 0; i < w; ++i) {\n        const float *y1r = y1[i].data();\n        const float *y2r = y2[i].data();\n        float *      out = imgOut[i].data();\n        for (int j = 0; j < h; ++j)\n            out[j] = y1r[j] + y2r[j];   // c2 = 1.0f\n    }\n}\n```",
  "trmm": "```cpp\n#include <vector>\n#include <algorithm>\n#include <omp.h>\n\n/* Main computational kernel. The whole function will be timed,\n   including the call and return. */\nvoid kernel_trmm(int m, int n,\n                 double alpha,\n                 std::vector<std::vector<double>>& A,\n                 std::vector<std::vector<double>>& B)\n{\n    if (m <= 0 || n <= 0) return;\n\n    /* Handle the trivial case without any work. */\n    if (alpha == 0.0) {\n        for (int i = 0; i < m; ++i)\n            std::fill(B[i].begin(), B[i].end(), 0.0);\n        return;\n    }\n\n    /* Preserve the original B values. */\n    std::vector<std::vector<double>> B_orig = B;\n\n    /* Row pointers to speed up indexing. */\n    std::vector<double*>          B_rows(m);\n    std::vector<double*>          B_orig_rows(m);\n    for (int i = 0; i < m; ++i) {\n        B_rows[i]      = B[i].data();\n        B_orig_rows[i] = B_orig[i].data();\n    }\n\n    std::vector<const double*> A_rows(m);\n    for (int i = 0; i < m; ++i)\n        A_rows[i] = A[i].data();\n\n    const int n_local = n;\n\n    /* Parallelise over rows of B (i).  Each thread works on disjoint rows. */\n#pragma omp parallel for schedule(static)\n    for (int i = 0; i < m; ++i) {\n        double* Bi = B_rows[i];\n        for (int k = i + 1; k < m; ++k) {\n            double a = A_rows[k][i];\n            const double* Bk = B_orig_rows[k];\n#pragma omp simd\n            for (int j = 0; j < n_local; ++j) {\n                Bi[j] += a * Bk[j];\n            }\n        }\n#pragma omp simd\n        for (int j = 0; j < n_local; ++j) {\n            Bi[j] *= alpha;\n        }\n    }\n}\n```",
  "floyd_warshall": "```cpp\n#include <vector>\n#include <omp.h>\n\nvoid kernel_floyd_warshall(int n, std::vector<std::vector<int>>& path) {\n    // Pointers to each row for fast access\n    std::vector<int*> rows(n);\n    for (int i = 0; i < n; ++i) {\n        rows[i] = path[i].data();\n    }\n\n    #pragma omp parallel\n    {\n        for (int k = 0; k < n; ++k) {\n            int* rk = rows[k];\n\n            // Update the row for vertex k first (serially)\n            #pragma omp single\n            {\n                int* ri = rk;\n                int valik = ri[k];\n                #pragma omp simd\n                for (int j = 0; j < n; ++j) {\n                    int alt = valik + rk[j];\n                    if (alt < ri[j]) ri[j] = alt;\n                }\n            }\n\n            // Update all other rows in parallel\n            #pragma omp for schedule(static)\n            for (int i = 0; i < n; ++i) {\n                if (i == k) continue;          // already updated\n                int* ri = rows[i];\n                int valik = ri[k];\n                #pragma omp simd\n                for (int j = 0; j < n; ++j) {\n                    int alt = valik + rk[j];\n                    if (alt < ri[j]) ri[j] = alt;\n                }\n            }\n        }\n    }\n}\n```\n"
}