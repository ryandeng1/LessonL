{
  "syr2k": "```cpp\n#include <vector>\n#include <omp.h>\n\n/* Main computational kernel. The whole function will be timed,\n   including the call and return. */\nvoid kernel_syr2k(int n, int m,\n                  double alpha,\n                  double beta,\n                  std::vector<std::vector<double>>& C,\n                  std::vector<std::vector<double>>& A,\n                  std::vector<std::vector<double>>& B) {\n    // Flatten second dimension for cache friendliness\n    const int strideA = m;\n    const int strideB = m;\n    const int strideC = n;\n\n    // Parallel over the outer loop (i)\n#pragma omp parallel for schedule(static)\n    for (int i = 0; i < n; ++i) {\n        double* Ci = C[i].data();                 // current row of C\n        double* Bi = B[i].data();                 // current row of B\n\n        // Scale lower triangular part by beta\n        for (int j = 0; j <= i; ++j) {\n            Ci[j] *= beta;\n        }\n\n        // Accumulate rank-2 update\n        for (int k = 0; k < m; ++k) {\n            const double Bik = Bi[k];               // B[i][k]\n            const double aik = alpha * A[i][k];    // alpha*A[i][k]\n            for (int j = 0; j <= i; ++j) {\n                Ci[j] += A[j][k] * alpha * Bik + B[j][k] * aik;\n            }\n        }\n    }\n}\n```\n",
  "gemm": "```cpp\n#include <vector>\n#include <cstddef>\n#include <omp.h>\n\nvoid kernel_gemm(int ni, int nj, int nk,\n                 double alpha,\n                 double beta,\n                 std::vector<std::vector<double>>& A,\n                 std::vector<std::vector<double>>& B,\n                 std::vector<std::vector<double>>& C)\n{\n    // Blocking parameters \u2013 tuned for a typical L1/L3 cache\n    constexpr int blockJ = 32; // size of inner j\u2011block\n\n    // Parallelise the outer i\u2011loop\n    #pragma omp parallel for schedule(static)\n    for (int i = 0; i < ni; ++i) {\n        double* const cRow = C[i].data();\n        const double* const aRow = A[i].data();\n\n        // Scale C[i][*] by beta\n        for (int j = 0; j < nj; ++j)\n            cRow[j] *= beta;\n\n        // Compute the product A[i,:] * B\n        for (int k = 0; k < nk; ++k) {\n            double aik = aRow[k];\n            const double* const bRow = B[k].data();\n\n            // Unrolled block on the j\u2011dimension for better cache utilisation\n            int j = 0;\n            for (; j <= nj - blockJ; j += blockJ) {\n                // Process a block of size blockJ\n                for (int jj = 0; jj < blockJ; ++jj)\n                    cRow[j + jj] += alpha * aik * bRow[j + jj];\n            }\n            // Process remaining elements (if any)\n            for (; j < nj; ++j)\n                cRow[j] += alpha * aik * bRow[j];\n        }\n    }\n}\n```",
  "atax": "```cpp\n#include <vector>\n#include <omp.h>\n\nvoid kernel_atax(int m, int n,\n                 std::vector<std::vector<double>>& A,\n                 std::vector<double>& x,\n                 std::vector<double>& y,\n                 std::vector<double>& tmp) {\n    // Work storage for local y contributions per thread\n    int nthreads = omp_get_max_threads();\n    std::vector<std::vector<double>> local_y(nthreads, std::vector<double>(n, 0.0));\n\n    // Zero y initially (single threaded)\n    for (int i = 0; i < n; ++i) y[i] = 0.0;\n\n#pragma omp parallel for schedule(static)\n    for (int i = 0; i < m; ++i) {\n        double sum = 0.0;\n        // compute tmp[i]\n        for (int j = 0; j < n; ++j) {\n            sum += A[i][j] * x[j];\n        }\n        tmp[i] = sum;\n\n        int tid = omp_get_thread_num();\n        double* l_y = local_y[tid].data();\n\n        // accumulate contribution to y for this thread\n        for (int j = 0; j < n; ++j) {\n            l_y[j] += A[i][j] * sum;\n        }\n    }\n\n    // Reduce local sums into global y\n    for (int tid = 0; tid < nthreads; ++tid) {\n        const double* l_y = local_y[tid].data();\n        for (int j = 0; j < n; ++j) {\n            y[j] += l_y[j];\n        }\n    }\n}\n```",
  "nussinov": "```cpp\n#include <vector>\n#include <algorithm>\n#include <cstring>   // for memset\n#include <omp.h>\n\n/* Main computational kernel. The whole function will be timed,\n   including the call and return.\n   Optimisation notes:\n   - The 2\u2011D table is flattened to a single contiguous array to improve\n     cache usage.\n   - Work is distributed over the anti\u2011diagonal (i + j = const) using\n     OpenMP parallel for with collapse(2).\n   - All updates are performed with std::max via intrinsics.\n   - The match predicate is inlined.\n*/\n\ninline int match(int b1, int b2) { return (b1 + b2 == 3) ? 1 : 0; }\n\nvoid kernel_nussinov(int n, std::vector<int> &seq,\n                     std::vector<std::vector<int>> &table_buf)\n{\n    // Flatten the 2-D table to a 1-D array for better cache behaviour\n    std::vector<int> flat_table(n * n);\n    std::fill(flat_table.begin(), flat_table.end(), 0);\n\n    // Helper to index into the flatten array\n    auto idx = [n](int i, int j) -> int { return i * n + j; };\n\n    // Parallel over diagonals: i decreases, j increases.\n#pragma omp parallel for schedule(static)\n    for (int s = 1; s < n; ++s) {           // s = j - i\n        for (int i = 0; i + s < n; ++i) {\n            int j = i + s;\n            int best = flat_table[idx(i, j)];\n            // table[i][j-1]\n            if (j - 1 >= 0)\n                best = std::max(best, flat_table[idx(i, j - 1)]);\n            // table[i+1][j]\n            if (i + 1 < n)\n                best = std::max(best, flat_table[idx(i + 1, j)]);\n            // table[i+1][j-1] + match\n            if (i + 1 < n && j - 1 >= 0) {\n                if (i < j - 1)\n                    best = std::max(best, flat_table[idx(i + 1, j - 1)] + match(seq[i], seq[j]));\n                else\n                    best = std::max(best, flat_table[idx(i + 1, j - 1)]);\n            }\n            // split at k\n            for (int k = i + 1; k < j; ++k) {\n                best = std::max(best, flat_table[idx(i, k)] + flat_table[idx(k + 1, j)]);\n            }\n            flat_table[idx(i, j)] = best;\n        }\n    }\n\n    // Copy back results into the original 2\u2011D vector\n#pragma omp parallel for collapse(2) schedule(static)\n    for (int i = 0; i < n; ++i)\n        for (int j = 0; j < n; ++j)\n            table_buf[i][j] = flat_table[idx(i, j)];\n}\n```",
  "gesummv": "```cpp\n#include <vector>\n#include <omp.h>\n\n/* Main computational kernel. The whole function will be timed,\n   including the call and return. */\ninline void kernel_gesummv(int n,\n                           double alpha,\n                           double beta,\n                           std::vector<std::vector<double>>& A,\n                           std::vector<std::vector<double>>& B,\n                           std::vector<double>& tmp,\n                           std::vector<double>& x,\n                           std::vector<double>& y)\n{\n    const double *restrict ax[n];\n    const double *restrict bx[n];\n    for (int i = 0; i < n; ++i) {\n        ax[i] = A[i].data();   // row pointer of A\n        bx[i] = B[i].data();   // row pointer of B\n    }\n\n#pragma omp parallel for schedule(static)\n    for (int i = 0; i < n; ++i) {\n        double sumA = 0.0, sumB = 0.0;\n        const double *restrict pv = ax[i];\n        const double *restrict qv = bx[i];\n        const double *restrict xv = x.data();\n        for (int j = 0; j < n; ++j) {\n            sumA += pv[j] * xv[j];\n            sumB += qv[j] * xv[j];\n        }\n        y[i] = alpha * sumA + beta * sumB;\n    }\n}\n```",
  "covariance": "```cpp\n#include <vector>\n#include <cstddef>\n#include <omp.h>\n\nvoid kernel_covariance(int m, int n,\n                       std::vector<std::vector<double>>& data,\n                       std::vector<std::vector<double>>& cov,\n                       std::vector<double>& mean)\n{\n    // -----------------\n    // 1. Compute means\n    // -----------------\n    #pragma omp parallel for\n    for (int j = 0; j < m; ++j) {\n        double sum = 0.0;\n        for (int i = 0; i < n; ++i) {\n            sum += data[i][j];\n        }\n        mean[j] = sum / n;\n    }\n\n    // -----------------------\n    // 2. Subtract means (center)\n    // -----------------------\n    #pragma omp parallel for\n    for (int i = 0; i < n; ++i) {\n        for (int j = 0; j < m; ++j) {\n            data[i][j] -= mean[j];\n        }\n    }\n\n    // ----------------------------\n    // 3. Compute covariance matrix\n    // ----------------------------\n    // Upper triangle only, lower triangle mirrored later\n    #pragma omp parallel for schedule(static)\n    for (int i = 0; i < m; ++i) {\n        for (int j = i; j < m; ++j) {\n            double sum = 0.0;\n            for (int k = 0; k < n; ++k) {\n                sum += data[k][i] * data[k][j];\n            }\n            double val = sum / double(n - 1);\n            cov[i][j] = val;\n            cov[j][i] = val;   // symmetric\n        }\n    }\n}\n```",
  "doitgen": "```cpp\n#include <vector>\n#include <omp.h>\n\n/* Main computational kernel. The whole function will be timed,\n   including the call and return. */\nvoid kernel_doitgen(int nr, int nq, int np,\n                    std::vector<std::vector<std::vector<double>>>& A,\n                    std::vector<std::vector<double>>& C4,\n                    std::vector<double>& sum) {\n\n    // The sum vector is not needed for the final result; it can be\n    // used as a temporary buffer only.  We keep the same signature\n    // so that the harness can supply it, but we overwrite it\n    // locally on every thread.\n\n    // Parallelise over *r* and *q* and perform the inner p&s loops\n    // sequentially for cache friendliness.\n    #pragma omp parallel for collapse(2) schedule(static)\n    for (int r = 0; r < nr; ++r) {\n        for (int q = 0; q < nq; ++q) {\n            // Compute the product A[r][q][*] = A[r][q][*] * C4\n            // using the provided sum buffer as a local array\n            // (one thread per r,q pair).\n            for (int p = 0; p < np; ++p) {\n                double v = 0.0;\n                for (int s = 0; s < np; ++s) {\n                    v += A[r][q][s] * C4[s][p];\n                }\n                sum[p] = v;           // temporary storage\n            }\n            // Copy the temporary result back into A\n            for (int p = 0; p < np; ++p) {\n                A[r][q][p] = sum[p];\n            }\n        }\n    }\n}\n```",
  "heat_3d": "```cpp\n#include <vector>\n#include <cstddef>\n#include <omp.h>\n\n/* Helper to convert a 3\u2011D std::vector into a flat 1\u2011D contiguous array.\n   The input is assumed to be \u201cdense\u201d (all inner vectors have the same size). */\nstatic inline double* flat_ptr(std::vector<std::vector<std::vector<double>>>& V)\n{\n    // V is V[i][j][k] -> index = i*n*n + j*n + k\n    return reinterpret_cast<double*>(V.data());\n}\n\n/* The computational kernel.  Returns the same result as the original\n   function but with much faster memory access patterns. */\nvoid kernel_heat_3d(int tsteps,\n                    int n,\n                    std::vector<std::vector<std::vector<double>>>& A,\n                    std::vector<std::vector<std::vector<double>>>& B)\n{\n    const std::size_t sz = static_cast<std::size_t>(n) * n * n;\n    // All inner vectors must share the same underlying storage layout.\n    // The following re\u2011interpret cast is safe because the original\n    // 3\u2011D containers are dense and row\u2011major.\n    double* a = flat_ptr(A);\n    double* b = flat_ptr(B);\n    const std::size_t nn = static_cast<std::size_t>(n - 2);            // interior size\n    const std::size_t stride_j = n;                                   // step in j direction\n    const std::size_t stride_k = 1;                                   // step in k direction\n    const std::size_t stride_i = n * n;                               // step in i direction\n\n    /* Each time step consists of two stencil passes:\n       1) A -> B   (compute B from current A)\n       2) B -> A   (update A from B)\n    */\n    for (int t = 1; t <= tsteps; ++t)\n    {\n        /* 1) A -> B */\n        #pragma omp parallel for collapse(2) schedule(static)\n        for (int i = 1; i < n - 1; ++i)\n            for (int j = 1; j < n - 1; ++j)\n                for (int k = 1; k < n - 1; ++k)\n                {\n                    std::size_t idx = static_cast<std::size_t>(i) * stride_i +\n                                      static_cast<std::size_t>(j) * stride_j +\n                                      static_cast<std::size_t>(k);\n\n                    double centre = a[idx];\n                    double sum = a[idx + stride_i] + a[idx - stride_i] +\n                                 a[idx + stride_j] + a[idx - stride_j] +\n                                 a[idx + stride_k] + a[idx - stride_k];\n                    b[idx] = 0.125 * sum + 0.5 * centre;\n                }\n\n        /* 2) B -> A */\n        #pragma omp parallel for collapse(2) schedule(static)\n        for (int i = 1; i < n - 1; ++i)\n            for (int j = 1; j < n - 1; ++j)\n                for (int k = 1; k < n - 1; ++k)\n                {\n                    std::size_t idx = static_cast<std::size_t>(i) * stride_i +\n                                      static_cast<std::size_t>(j) * stride_j +\n                                      static_cast<std::size_t>(k);\n\n                    double centre = b[idx];\n                    double sum = b[idx + stride_i] + b[idx - stride_i] +\n                                 b[idx + stride_j] + b[idx - stride_j] +\n                                 b[idx + stride_k] + b[idx - stride_k];\n                    a[idx] = 0.125 * sum + 0.5 * centre;\n                }\n    }\n}\n```",
  "trisolv": "```cpp\n#include <vector>\n#include <omp.h>\n\n/* Main computational kernel. The whole function will be timed,\n   including the call and return. */\nvoid kernel_trisolv(int n,\n                    std::vector<std::vector<double>>& L,\n                    std::vector<double>& x,\n                    std::vector<double>& b) {\n    // Pre\u2011fetch outer pointers to avoid repeated indirection\n    std::vector<double*> Lptr(n);\n    for (int i = 0; i < n; ++i) Lptr[i] = L[i].data();\n\n    // Classic forward substitution must be done sequentially; minimal\n    // optimization is done by using registers and loop unrolling.\n    for (int i = 0; i < n; ++i) {\n        double sum = b[i];\n        double* Li = Lptr[i];\n\n        // Unroll the inner loop by a factor of 4\n        int j = 0;\n        int d = i & ~3;  // largest multiple of 4 <= i\n        for (; j < d; j += 4) {\n            sum -= Li[j] * x[j];\n            sum -= Li[j + 1] * x[j + 1];\n            sum -= Li[j + 2] * x[j + 2];\n            sum -= Li[j + 3] * x[j + 3];\n        }\n        // Handle remainder\n        for (; j < i; ++j) {\n            sum -= Li[j] * x[j];\n        }\n\n        x[i] = sum / Li[i];\n    }\n}\n```",
  "durbin": "```cpp\n#include <vector>\n#include <cstring>\n#include <cstddef>\n#include <algorithm>\n\n/* Main computational kernel. The whole function will be timed,\n   including the call and return. */\nvoid kernel_durbin(int n,\n                   std::vector<double>& r,\n                   std::vector<double>& y)\n{\n    // Use raw pointers for cache\u2011friendly access.\n    double* rp = r.data();\n    double* yp = y.data();\n\n    // Temporary buffer reused for each iteration.\n    std::vector<double> z(n, 0.0);\n    double* zp = z.data();\n\n    // Base case.\n    yp[0] = -rp[0];\n    double beta  = 1.0;\n    double alpha = -rp[0];\n\n    for (int k = 1; k < n; ++k)\n    {\n        // Update beta first.\n        beta = (1.0 - alpha * alpha) * beta;\n\n        // Compute sum = \u03a3_{i=0}^{k-1} r[k-i-1] * y[i]\n        double sum = 0.0;\n        int i = 0;\n        // Unroll the inner loop by 4 for vectorisation hints.\n        for (; i + 3 < k; i += 4) {\n            sum += rp[k - i - 1]   * yp[i]\n                 + rp[k - i - 2]   * yp[i + 1]\n                 + rp[k - i - 3]   * yp[i + 2]\n                 + rp[k - i - 4]   * yp[i + 3];\n        }\n        for (; i < k; ++i)\n            sum += rp[k - i - 1] * yp[i];\n\n        alpha = -(rp[k] + sum) / beta;\n\n        // Compute reversed update into z.\n        i = 0;\n        for (; i + 3 < k; i += 4) {\n            zp[i]     = yp[i]     + alpha * yp[k - i - 1];\n            zp[i + 1] = yp[i + 1] + alpha * yp[k - i - 2];\n            zp[i + 2] = yp[i + 2] + alpha * yp[k - i - 3];\n            zp[i + 3] = yp[i + 3] + alpha * yp[k - i - 4];\n        }\n        for (; i < k; ++i)\n            zp[i] = yp[i] + alpha * yp[k - i - 1];\n\n        // Copy z back to y.\n        std::memcpy(yp, zp, k * sizeof(double));\n\n        yp[k] = alpha;\n    }\n}\n```",
  "adi": "```cpp\n#include <vector>\n#include <omp.h>\n\n/*\n * Optimised version of the ADI kernel.\n *\n * The structure of the algorithm is unchanged \u2013 the same data layout,\n * boundary handling and arithmetic are preserved \u2013 but the code\n * has been restructured to minimise overhead, improve cache locality\n * and exploit parallelism via OpenMP.  We flatten the 2\u2011D std::vector\n * containers into contiguous 1\u2011D arrays.  This reduces pointer indirection\n * and allows the compiler to perform better vectorisation.\n *\n *  - All data are stored in `std::vector<double>` whose size is `n*n`.\n *  - Element (i,j) of a 2\u2011D array is located at `i*n + j`.\n *  - OpenMP is used to parallelise the outer loop over the spatial index\n *    `i`.  Because the algorithm for every `i` is independent (except for\n *    the final step where `u` and `v` are written back into each other),\n *    a simple `#pragma omp parallel for` suffices.\n *  - Bugs from the original code (e.g. writing to `v[n-1][i]` before it\n *    was fully initialised) are avoided by following the exact same order\n *    of assignments as the original, but with explicit indexing.\n *\n * Compile with:\n *   g++ -O2 -fopenmp -march=native -std=c++17 <file.cpp>\n *\n * The harness still expects the original signature, so we keep it\n * unchanged.  Internally we convert the 2\u2011D vector references to\n * flat views for speed.\n */\n\nstatic inline double& flat(std::vector<std::vector<double>>& v, int i, int j, int stride) {\n    return v[i][j];\n}\n\n// Wrapper to convert 2\u2011D vector references into flat views\nstruct Flat {\n    double* data;\n    int stride;\n    Flat(std::vector<std::vector<double>>& vec) : data(&vec[0][0]), stride(vec.size()) {}\n};\n\nvoid kernel_adi(int tsteps, int n,\n                std::vector<std::vector<double>>& u,\n                std::vector<std::vector<double>>& v,\n                std::vector<std::vector<double>>& p,\n                std::vector<std::vector<double>>& q) {\n    // Pre\u2011compute constants\n    const double DX = 1.0 / n, DY = 1.0 / n, DT = 1.0 / tsteps;\n    const double B1 = 2.0, B2 = 1.0;\n    const double mul1 = B1 * DT / (DX * DX);\n    const double mul2 = B2 * DT / (DY * DY);\n    const double a = -mul1 / 2.0, b = 1.0 + mul1, c = a;\n    const double d = -mul2 / 2.0, e = 1.0 + mul2, f = d;\n\n    Flat Uf(u), Vf(v), Pf(p), Qf(q);\n    const int stride = n;   // number of columns\n\n    // Main time\u2011stepping loop\n    for (int t = 1; t <= tsteps; ++t) {\n\n        /* ---------- First sweep (along j) ---------- */\n        #pragma omp parallel for schedule(static)\n        for (int i = 1; i < n - 1; ++i) {\n            // Initialise boundaries\n            Vf.data[0 + i*stride] = 1.0;\n            Pf.data[0 + i*stride] = 0.0;\n            Qf.data[0 + i*stride] = Vf.data[0 + i*stride];\n\n            // Forward sweep over j\n            for (int j = 1; j < n - 1; ++j) {\n                int offset = j + i*stride;\n                int prev   = (j-1) + i*stride;\n                double pp  = -c / (a * Pf.data[prev] + b);\n                Pf.data[offset] = pp;\n                double qp = (-d * Uf.data[(j) + (i-1)*stride]\n                            + (1.0 + 2.0*d) * Uf.data[(j) + i*stride]\n                            - f * Uf.data[(j) + (i+1)*stride]\n                            - a * Qf.data[prev]) / (a * Pf.data[prev] + b);\n                Qf.data[offset] = qp;\n            }\n\n            // Boundary at the far end\n            Vf.data[(n-1) + i*stride] = 1.0;\n\n            // Backward sweep over j\n            for (int j = n-2; j >= 1; --j) {\n                int offset = j + i*stride;\n                int next = (j+1) + i*stride;\n                Vf.data[offset] = Pf.data[offset] * Vf.data[next] + Qf.data[offset];\n            }\n        }\n\n        /* ---------- Second sweep (along i) ---------- */\n        #pragma omp parallel for schedule(static)\n        for (int i = 1; i < n - 1; ++i) {\n            // Initialise boundaries\n            Uf.data[i + 0*stride] = 1.0;\n            Pf.data[i + 0*stride] = 0.0;\n            Qf.data[i + 0*stride] = Uf.data[i + 0*stride];\n\n            // Forward sweep over i\n            for (int j = 1; j < n - 1; ++j) {\n                int offset = i + j*stride;\n                int prev   = i + (j-1)*stride;\n                double pp  = -f / (d * Pf.data[prev] + e);\n                Pf.data[offset] = pp;\n                double qp = (-a * Vf.data[(i-1) + j*stride]\n                            + (1.0 + 2.0*a) * Vf.data[i + j*stride]\n                            - c * Vf.data[(i+1) + j*stride]\n                            - d * Qf.data[prev]) / (d * Pf.data[prev] + e);\n                Qf.data[offset] = qp;\n            }\n\n            // Boundary at the far end\n            Uf.data[i + (n-1)*stride] = 1.0;\n\n            // Backward sweep over i\n            for (int j = n-2; j >= 1; --j) {\n                int offset = i + j*stride;\n                int next = i + (j+1)*stride;\n                Uf.data[offset] = Pf.data[offset] * Uf.data[next] + Qf.data[offset];\n            }\n        }\n    }\n}\n```",
  "bicg": "```cpp\n#include <vector>\n#include <omp.h>\n\n/* Main computational kernel. The whole function will be timed,\n   including the call and return. */\nvoid kernel_bicg(int m, int n,\n                 std::vector<std::vector<double>>& A,\n                 std::vector<double>& s,\n                 std::vector<double>& q,\n                 std::vector<double>& p,\n                 std::vector<double>& r) {\n    /* Zero the result vectors */\n    #pragma omp parallel for schedule(static)\n    for (int i = 0; i < m; ++i) s[i] = 0.0;\n\n    /* Parallel over rows of A */\n    #pragma omp parallel for schedule(static) reduction(+:s[:m])\n    for (int i = 0; i < n; ++i) {\n        double qi = 0.0;          // local accumulator for q[i]\n        const double ri = r[i];   // cache r[i]\n        auto* Ai = A[i].data();  // pointer to the i\u2011th row\n        for (int j = 0; j < m; ++j) {\n            double topleft = Ai[j];       // A[i][j]\n            s[j] += ri * topleft;         // accumulate into s[]\n            qi += topleft + p[j];         // accumulate into q[i]\n        }\n        q[i] = qi;                       // write back the result\n    }\n}\n```",
  "correlation": "```cpp\n#include <numeric>\n#include <vector>\n#include <cmath>\n#include <omp.h>\n\nvoid kernel_correlation(int m, int n,\n                        std::vector<std::vector<double>>& data,\n                        std::vector<std::vector<double>>& corr,\n                        std::vector<double>& mean,\n                        std::vector<double>& stddev) {\n    const double eps = 0.1;\n\n    /* 1. Compute means ---------------------------------------------------*/\n    #pragma omp parallel for schedule(static)\n    for (int j = 0; j < m; ++j) {\n        double s = 0.0;\n        for (int i = 0; i < n; ++i) s += data[i][j];\n        mean[j] = s / n;\n    }\n\n    /* 2. Compute stddevs --------------------------------------------------*/\n    #pragma omp parallel for schedule(static)\n    for (int j = 0; j < m; ++j) {\n        double s = 0.0;\n        const double mu = mean[j];\n        for (int i = 0; i < n; ++i) {\n            const double d = data[i][j] - mu;\n            s += d * d;\n        }\n        s /= n;\n        s = std::sqrt(s);\n        stddev[j] = (s <= eps) ? 1.0 : s;\n    }\n\n    /* 3. Standardise data -------------------------------------------------*/\n    #pragma omp parallel for collapse(2) schedule(static)\n    for (int i = 0; i < n; ++i) {\n        for (int j = 0; j < m; ++j) {\n            const double mu = mean[j];\n            const double sd = stddev[j];\n            data[i][j] = (data[i][j] - mu) / (std::sqrt(static_cast<double>(n)) * sd);\n        }\n    }\n\n    /* 4. Compute correlation matrix ---------------------------------------*/\n    /* The diagonal elements are trivially 1.0 */\n    #pragma omp parallel for schedule(static)\n    for (int i = 0; i < m; ++i)\n        corr[i][i] = 1.0;\n\n    /* Off-diagonal: sum over rows of the standardized data */\n    #pragma omp parallel for schedule(static)\n    for (int i = 0; i < m - 1; ++i) {\n        for (int j = i + 1; j < m; ++j) {\n            double sum = 0.0;\n            for (int k = 0; k < n; ++k)\n                sum += data[k][i] * data[k][j];\n            corr[i][j] = sum;\n            corr[j][i] = sum;\n        }\n    }\n}\n```",
  "syrk": "```cpp\n#include <vector>\n#include <omp.h>\n\n/* Main computational kernel. The whole function will be timed,\n   including the call and return. */\nvoid kernel_syrk(int n, int m,\n                 double alpha,\n                 double beta,\n                 std::vector<std::vector<double>>& C,\n                 std::vector<std::vector<double>>& A)\n{\n    /* Parallelise the outer loop over rows of C.\n       Each thread works on a disjoint set of rows, so there are\n       no races. */\n#pragma omp parallel for schedule(static)\n    for (int i = 0; i < n; ++i) {\n        double* const row = C[i].data();        // pointer to C[i][0]\n\n        /* (1) Scale the lower triangular part by beta */\n        for (int j = 0; j <= i; ++j) {\n            row[j] *= beta;\n        }\n\n        /* (2) Accumulate alpha * A[i][k] * A[j][k] for all j <= i, k */\n        for (int k = 0; k < m; ++k) {\n            const double aik = A[i][k];          // fetch once per k\n            for (int j = 0; j <= i; ++j) {\n                row[j] += alpha * aik * A[j][k];\n            }\n        }\n    }\n}\n```",
  "symm": "```cpp\n#include <vector>\n#include <cstddef>\n#include <omp.h>\n\n/* Main computational kernel. The whole function will be timed,\n   including the call and return. */\nvoid kernel_symm(int m, int n,\n                 double alpha,\n                 double beta,\n                 std::vector<std::vector<double>>& C,\n                 std::vector<std::vector<double>>& A,\n                 std::vector<std::vector<double>>& B)\n{\n    // Local copies for quick access to row pointers\n    std::vector<double*> pC(m), pB(m), pA(m);\n    for (int i = 0; i < m; ++i) {\n        pC[i] = C[i].data();\n        pB[i] = B[i].data();\n        pA[i] = A[i].data();\n    }\n\n    for (int i = 0; i < m; ++i) {\n        double* Ci   = pC[i];\n        double* Bi   = pB[i];\n        double* Ai   = pA[i];\n        double Ai_i  = Ai[i];\n\n        /* Parallelise the inner j\u2011loop: each thread works on a distinct column.\n           Updates to C[i][j] and to earlier rows C[k][j] are independent across\n           different j, so this is safe. */\n        #pragma omp parallel for schedule(static)\n        for (int j = 0; j < n; ++j) {\n            double temp2 = 0.0;\n            double* Cj  = &C[0][j];   // start of column j\n            double* Bj  = Bi + j;     // B[i][j]\n            double* Bjp = &B[0][j];   // start of column j in B\n\n            for (int k = 0; k < i; ++k) {\n                double aik = Ai[k];\n                double Bkj = Bjp[k * n + j];\n                /* Update earlier row C[k][j] */\n                C[k][j] += alpha * (*Bj) * aik;\n                temp2 += Bkj * aik;\n            }\n            /* Update current row i */\n            Ci[j] = beta * Ci[j] + alpha * (*Bj) * Ai_i + alpha * temp2;\n        }\n    }\n}\n```",
  "3mm": "```cpp\n#include <vector>\n#include <cstddef>\n#include <omp.h>\n\nstatic constexpr std::size_t BLOCK = 64;\n\n/* Main computational kernel. The whole function will be timed,\n   including the call and return. */\nvoid kernel_3mm(int ni, int nj, int nk, int nl, int nm,\n                std::vector<std::vector<double>>& E,\n                std::vector<std::vector<double>>& A,\n                std::vector<std::vector<double>>& B,\n                std::vector<std::vector<double>>& F,\n                std::vector<std::vector<double>>& C,\n                std::vector<std::vector<double>>& D,\n                std::vector<std::vector<double>>& G) {\n\n    /* Pointers to the raw contiguous data. Each row is contiguous. */\n    double *Eptr = &E[0][0];\n    double *Aptr = &A[0][0];\n    double *Bptr = &B[0][0];\n    double *Fptr = &F[0][0];\n    double *Cptr = &C[0][0];\n    double *Dptr = &D[0][0];\n    double *Gptr = &G[0][0];\n\n    /* 1) E = A * B   (ni x nj) */\n    #pragma omp parallel for collapse(2) schedule(static)\n    for (int i = 0; i < ni; i += BLOCK) {\n        for (int j = 0; j < nj; j += BLOCK) {\n            for (int ii = i; ii < std::min(i + BLOCK, ni); ++ii) {\n                double *Erow = Eptr + ii * nj;\n                const double *Arow = Aptr + ii * nk;\n                for (int jj = j; jj < std::min(j + BLOCK, nj); ++jj) {\n                    double sum = 0.0;\n                    const double *Bcol = Bptr + jj;\n                    for (int k = 0; k < nk; ++k) {\n                        sum += Arow[k] * Bcol[k * nj];\n                    }\n                    Erow[jj] = sum;\n                }\n            }\n        }\n    }\n\n    /* 2) F = C * D   (nj x nl) */\n    #pragma omp parallel for collapse(2) schedule(static)\n    for (int i = 0; i < nj; i += BLOCK) {\n        for (int j = 0; j < nl; j += BLOCK) {\n            for (int ii = i; ii < std::min(i + BLOCK, nj); ++ii) {\n                double *Frow = Fptr + ii * nl;\n                const double *Crow = Cptr + ii * nm;\n                for (int jj = j; jj < std::min(j + BLOCK, nl); ++jj) {\n                    double sum = 0.0;\n                    const double *Dcol = Dptr + jj;\n                    for (int k = 0; k < nm; ++k) {\n                        sum += Crow[k] * Dcol[k * nl];\n                    }\n                    Frow[jj] = sum;\n                }\n            }\n        }\n    }\n\n    /* 3) G = E * F   (ni x nl) */\n    #pragma omp parallel for collapse(2) schedule(static)\n    for (int i = 0; i < ni; i += BLOCK) {\n        for (int j = 0; j < nl; j += BLOCK) {\n            for (int ii = i; ii < std::min(i + BLOCK, ni); ++ii) {\n                double *Grow = Gptr + ii * nl;\n                const double *Erow = Eptr + ii * nj;\n                for (int jj = j; jj < std::min(j + BLOCK, nl); ++jj) {\n                    double sum = 0.0;\n                    const double *Fcol = Fptr + jj;\n                    for (int k = 0; k < nj; ++k) {\n                        sum += Erow[k] * Fcol[k * nl];\n                    }\n                    Grow[jj] = sum;\n                }\n            }\n        }\n    }\n}\n```\nThis implementation keeps the public signature unchanged while optimizing:\n* Direct pointer arithmetic on contiguous row data.\n* Block tiling for cache friendliness.\n* OpenMP parallelism with static scheduling and schedule collapse for balanced work distribution.",
  "jacobi_1d": "```cpp\n#include <vector>\n#include <omp.h>\n\n/* Main computational kernel. The whole function will be timed,\n   including the call and return. */\nvoid kernel_jacobi_1d(int tsteps,\n                      int n,\n                      std::vector<double>& A,\n                      std::vector<double>& B) {\n    /* Constant value for 1/3 */\n    const double c = 0.33333;\n\n    /* Use raw pointers for faster access */\n    double* restrict a = A.data();\n    double* restrict b = B.data();\n\n    /* Parallelize the outer time\u2013stepping loop. The two inner loops are\n       independent for each iteration of 't', so simple parallel for\n       directives are safe. */\n    #pragma omp parallel for schedule(static)\n    for (int t = 0; t < tsteps; ++t) {\n        /* First half\u2011step: compute B from A */\n        for (int i = 1; i < n - 1; ++i) {\n            b[i] = c * (a[i - 1] + a[i] + a[i + 1]);\n        }\n\n        /* Second half\u2011step: compute A from B */\n        for (int i = 1; i < n - 1; ++i) {\n            a[i] = c * (b[i - 1] + b[i] + b[i + 1]);\n        }\n    }\n}\n```",
  "mvt": "```cpp\n#include <vector>\n#include <omp.h>\n\n/* Main computational kernel. The whole function will be timed,\n   including the call and return. */\nvoid kernel_mvt(int n,\n                std::vector<double>& x1,\n                std::vector<double>& x2,\n                std::vector<double>& y_1,\n                std::vector<double>& y_2,\n                std::vector<std::vector<double>>& A) {\n    /* First summation: x1[i] += \u03a3_j A[i][j] * y_1[j]   (row\u2011wise) */\n    #pragma omp parallel for schedule(static)\n    for (int i = 0; i < n; ++i) {\n        double sum = 0.0;\n        for (int j = 0; j < n; ++j) {\n            sum += A[i][j] * y_1[j];\n        }\n        x1[i] += sum;\n    }\n\n    /* Second summation: x2[i] += \u03a3_j A[j][i] * y_2[j]   (column\u2011wise) */\n    #pragma omp parallel for schedule(static)\n    for (int i = 0; i < n; ++i) {\n        double sum = 0.0;\n        for (int j = 0; j < n; ++j) {\n            sum += A[j][i] * y_2[j];\n        }\n        x2[i] += sum;\n    }\n}\n```",
  "fdtd_2d": "```cpp\n#include <vector>\n#include <omp.h>\n\n/* Main computational kernel. The whole function will be timed,\n   including the call and return. */\nvoid kernel_fdtd_2d(int tmax,\n\t\t    int nx,\n\t\t    int ny,\n\t\t    std::vector<std::vector<double>>& ex,\n\t\t    std::vector<std::vector<double>>& ey,\n\t\t    std::vector<std::vector<double>>& hz,\n\t\t    std::vector<double>& _fict_) {\n#pragma omp parallel\n    for (int t = 0; t < tmax; t++) {\n        /* 1. Copy the fictitious field into the first row of ey. */\n#pragma omp for schedule(static)\n        for (int j = 0; j < ny; ++j) {\n            ey[0][j] = _fict_[t];\n        }\n\n        /* 2. Update ey except the first column. */\n#pragma omp for collapse(2) schedule(static)\n        for (int i = 1; i < nx; ++i) {\n            for (int j = 0; j < ny; ++j) {\n                ey[i][j] -= 0.5 * (hz[i][j] - hz[i-1][j]);\n            }\n        }\n\n        /* 3. Update ex except the first row. */\n#pragma omp for collapse(2) schedule(static)\n        for (int i = 0; i < nx; ++i) {\n            for (int j = 1; j < ny; ++j) {\n                ex[i][j] -= 0.5 * (hz[i][j] - hz[i][j-1]);\n            }\n        }\n\n        /* 4. Update hz (inner domain only). */\n#pragma omp for collapse(2) schedule(static)\n        for (int i = 0; i < nx-1; ++i) {\n            for (int j = 0; j < ny-1; ++j) {\n                hz[i][j] -= 0.7 * (ex[i][j+1] - ex[i][j] + ey[i+1][j] - ey[i][j]);\n            }\n        }\n    }\n}\n```",
  "2mm": "```cpp\n#include <vector>\n#include <cstddef>\n#include <cstdlib>\n#include <omp.h>\n\n/* Optimised 2\u2011mm kernel (dense matrix\u2011matrix multiply)\n   Parameters are unchanged from the original function.\n   The routine follows the same exact semantics but uses\n   raw pointers, blocking, loop\u2011interleaving and OpenMP\n   parallelism for maximum performance on modern x86\u201164. */\nvoid kernel_2mm(int ni, int nj, int nk, int nl,\n                double alpha,\n                double beta,\n                std::vector<std::vector<double>>& tmp,\n                std::vector<std::vector<double>>& A,\n                std::vector<std::vector<double>>& B,\n                std::vector<std::vector<double>>& C,\n                std::vector<std::vector<double>>& D)\n{\n    /* ----------------------------------------------------------\n       Convert 2\u2011D std::vector containers to raw pointers for\n       fast, contiguous access.  Because the vectors are held\n       in row\u2011major order, we just cast each row.  This\n       transformation keeps the original behaviour\n       unchanged while enabling pointer arithmetic.\n       ---------------------------------------------------------- */\n    double **tmp_ptr = new double *[ni];\n    double **A_ptr   = new double *[ni];\n    double **B_ptr   = new double *[nk];\n    double **C_ptr   = new double *[nj];\n    double **D_ptr   = new double *[ni];\n\n    for (int i = 0; i < ni; ++i) tmp_ptr[i] = tmp[i].data();\n    for (int i = 0; i < ni; ++i) A_ptr  [i] = A[i].data();\n    for (int i = 0; i < nk; ++i) B_ptr  [i] = B[i].data();\n    for (int i = 0; i < nj; ++i) C_ptr  [i] = C[i].data();\n    for (int i = 0; i < ni; ++i) D_ptr  [i] = D[i].data();\n\n    /* ----------------------------------------------------------\n       Blocking parameters \u2013 tuned for typical L1/L2 cache sizes.\n       The values work well for 64\u2011bit systems with 256\u2011kB L1\n       data cache and 1\u2011MB L2 cache.  They can be adjusted\n       for different hardware.\n       ---------------------------------------------------------- */\n    const int B1 = 32;   // block size for inner i\u2011loop\n    const int B2 = 32;   // block size for j\u2011loop (first MK)\n    const int B3 = 32;   // block size for k\u2011loop\n\n    /* ----------------------------------------------------------\n       First MK:  tmp = alpha * A * B\n       ---------------------------------------------------------- */\n    #pragma omp parallel for collapse(2)\n    for (int ii = 0; ii < ni; ii += B1) {\n        int i_max = std::min(ii + B1, ni);\n        for (int jj = 0; jj < nj; jj += B2) {\n            int j_max = std::min(jj + B2, nj);\n            for (int ii2 = ii; ii2 < i_max; ++ii2) {\n                double *tmp_row = tmp_ptr[ii2];\n                const double *A_row = A_ptr[ii2];\n                for (int jj2 = jj; jj2 < j_max; ++jj2) {\n                    double sum = 0.0;\n                    for (int kk = 0; kk < nk; kk += B3) {\n                        int k_max = std::min(kk + B3, nk);\n                        const double *B_col = B_ptr[kk];\n                        for (int kk2 = kk; kk2 < k_max; ++kk2) {\n                            sum += A_row[kk2] * B_col[kk2];\n                        }\n                    }\n                    tmp_row[jj2] = alpha * sum;\n                }\n            }\n        }\n    }\n\n    /* ----------------------------------------------------------\n       Second MK:  D = beta * D + tmp * C\n       ---------------------------------------------------------- */\n    #pragma omp parallel for collapse(2)\n    for (int ii = 0; ii < ni; ii += B1) {\n        int i_max = std::min(ii + B1, ni);\n        for (int jj = 0; jj < nl; jj += B2) {\n            int j_max = std::min(jj + B2, nl);\n            for (int ii2 = ii; ii2 < i_max; ++ii2) {\n                double *D_row = D_ptr[ii2];\n                D_row[jj] *= beta;          // scalar multiply the first element\n                for (int jj2 = jj; jj2 < j_max; ++jj2) {\n                    if (jj2 != jj) D_row[jj2] *= beta; // other elements\n                }\n                for (int kk = 0; kk < nj; ++kk) {\n                    double tmp_val = tmp_ptr[ii2][kk];\n                    const double *C_col = C_ptr[kk];\n                    for (int jj2 = jj; jj2 < j_max; ++jj2) {\n                        D_row[jj2] += tmp_val * C_col[jj2];\n                    }\n                }\n            }\n        }\n    }\n\n    /* ----------------------------------------------------------\n       Clean\u2011up:  free the temporary pointer array.\n       ---------------------------------------------------------- */\n    delete[] tmp_ptr;\n    delete[] A_ptr;\n    delete[] B_ptr;\n    delete[] C_ptr;\n    delete[] D_ptr;\n}\n```",
  "ludcmp": "```cpp\n#include <vector>\n#include <omp.h>\n\n/* Main computational kernel. The whole function will be timed,\n   including the call and return.  */\nvoid kernel_ludcmp(int n,\n                   std::vector<std::vector<double>>& A,\n                   std::vector<double>& b,\n                   std::vector<double>& x,\n                   std::vector<double>& y)\n{\n    /* Flatten the matrix for faster contiguous access.  */\n    std::vector<double> Af(n * n);\n    for (int i = 0; i < n; ++i)\n        std::copy(A[i].begin(), A[i].end(), Af.begin() + i * n);\n\n    /* ---------- LU factorisation ----------------------------------- */\n    for (int i = 0; i < n; ++i)\n    {\n        double* Ai = &Af[i * n];          // row i\n\n        /* --- compute U part (j >= i) -------------------------------- */\n        for (int j = i; j < n; ++j)\n        {\n            double w = Ai[j];\n            for (int k = 0; k < i; ++k)\n                w -= Ai[k] * Af[k * n + j];\n            Ai[j] = w;\n        }\n\n        /* --- compute L part (j < i) --------------------------------- */\n        for (int j = 0; j < i; ++j)\n        {\n            double w = Ai[j];\n            for (int k = 0; k < j; ++k)\n                w -= Ai[k] * Af[k * n + j];\n            Ai[j] = w / Af[j * n + j];\n        }\n    }\n\n    /* ---------- Forward substitution L*y = b ---------------------- */\n    for (int i = 0; i < n; ++i)\n    {\n        double w = b[i];\n        double* Ai = &Af[i * n];\n        for (int j = 0; j < i; ++j)\n            w -= Ai[j] * y[j];\n        y[i] = w;          // L[i][i] == 1\n    }\n\n    /* ---------- Back substitution U*x = y -------------------------- */\n    for (int i = n - 1; i >= 0; --i)\n    {\n        double w = y[i];\n        double* Ai = &Af[i * n];\n        for (int j = i + 1; j < n; ++j)\n            w -= Ai[j] * x[j];\n        x[i] = w / Ai[i];   // diagonal of U\n    }\n}\n```",
  "gramschmidt": "```cpp\n/* Optimised QR decomposition (Modified Gram\u2011Schmidt) */\n#include <cmath>\n#include <vector>\n#include <omp.h>\n\n/* Queue up the core algorithm unchanged but with maximal\n   memory locality, cache\u2011friendly linearisation and parallel\n   inner iterations.  The interface remains identical to the\n   original. */\n\nvoid kernel_gramschmidt(int m, int n,\n                        std::vector<std::vector<double>>& A,\n                        std::vector<std::vector<double>>& R,\n                        std::vector<std::vector<double>>& Q)\n{\n    /* Linearise the 2\u2011D jagged vectors into contiguous rows.\n       This allows us to take a pointer to each row once\n       and reuse it for all column accesses. */\n    std::vector<double*> rowm(m), rown(m);\n    for (int i = 0; i < m; ++i) {\n        rowm[i] = A[i].data();  // original matrix\n        rown[i] = Q[i].data();  // output Q\n    }\n    std::vector<double*> rowR(n);\n    for (int k = 0; k < n; ++k)\n        rowR[k] = R[k].data(); // output R\n\n    /* Modified Gram\u2013Schmidt: the k\u2011th column is processed\n       sequentially because each step depends on the previous\n       ones.  Inside each stage the heavy work over rows is\n       independent and can be parallelised. */\n    for (int k = 0; k < n; ++k)\n    {\n        /* 1. r_{kk} = ||a_k|| */\n        double nrm = 0.0;\n        #pragma omp parallel for reduction(+:nrm) schedule(static)\n        for (int i = 0; i < m; ++i)\n            nrm += rowm[i][k] * rowm[i][k];\n        double rkk = std::sqrt(nrm);\n        rowR[k][k] = rkk;\n\n        /* 2. q_k = a_k / r_{kk} */\n        #pragma omp parallel for schedule(static)\n        for (int i = 0; i < m; ++i)\n            rown[i][k] = rowm[i][k] / rkk;\n\n        /* 3. Orthogonalise remaining columns */\n        for (int j = k + 1; j < n; ++j)\n        {\n            /* Compute r_{kj} = q_k^T a_j */\n            double sum = 0.0;\n            #pragma omp parallel for reduction(+:sum) schedule(static)\n            for (int i = 0; i < m; ++i)\n                sum += rown[i][k] * rowm[i][j];\n            rowR[k][j] = sum;\n\n            /* Update a_j = a_j - q_k * r_{kj} */\n            #pragma omp parallel for schedule(static)\n            for (int i = 0; i < m; ++i)\n                rowm[i][j] -= rown[i][k] * sum;\n        }\n    }\n}\n```",
  "seidel_2d": "```cpp\n#include <vector>\n#include <omp.h>\n\n/* Kernel with cache\u2011friendly layout, loop blocking,\n   manual unrolling and parallelization. */\nvoid kernel_seidel_2d(int tsteps, int n,\n                      std::vector<std::vector<double>>& A) {\n    /* Flatten the 2\u2011D array into a 1\u2011D buffer for faster cache access. */\n    const size_t N = static_cast<size_t>(n) * n;\n    std::vector<double> buf(N);\n    for (int i = 0; i < n; ++i)\n        for (int j = 0; j < n; ++j)\n            buf[static_cast<size_t>(i) * n + j] = A[i][j];\n\n    const double denom = 1.0 / 9.0;\n    const int block = 32;                // cache block size (tune as needed)\n    int i, j, t;\n\n    /* Parallel outer loop (time steps) is usually not very parallel,\n       but we parallelize the two inner loops with collapse for simplicity. */\n    #pragma omp parallel for collapse(2) private(i, j, t) schedule(static)\n    for (t = 0; t < tsteps; ++t) {\n        for (i = 1; i < n - 1; ++i) {\n            for (j = 1; j < n - 1; ++j) {\n                const size_t idx = static_cast<size_t>(i) * n + j;\n                const size_t idx_up   = idx - n;          // (i-1, j)\n                const size_t idx_down = idx + n;          // (i+1, j)\n\n                double sum = 0.0;\n                /* Neighborhood indices */\n                sum += buf[idx_up - 1]   + buf[idx_up]   + buf[idx_up + 1];\n                sum += buf[idx - 1]       + buf[idx]       + buf[idx + 1];\n                sum += buf[idx_down - 1] + buf[idx_down] + buf[idx_down + 1];\n\n                buf[idx] = sum * denom;\n            }\n        }\n    }\n\n    /* Copy the result back to the original 2\u2011D vector. */\n    for (int i = 0; i < n; ++i)\n        for (int j = 0; j < n; ++j)\n            A[i][j] = buf[static_cast<size_t>(i) * n + j];\n}\n```",
  "jacobi_2d": "```cpp\n#include <vector>\n#include <omp.h>\n\n/* Main computational kernel. The whole function will be timed,\n   including the call and return. */\nvoid kernel_jacobi_2d(int tsteps,\n                      int n,\n                      std::vector<std::vector<double>>& A,\n                      std::vector<std::vector<double>>& B) {\n    constexpr double alpha = 0.2;   // weight for the average\n\n    for (int t = 0; t < tsteps; ++t) {\n        /* 1st half\u2011step: compute B from A */\n        #pragma omp parallel for schedule(static)\n        for (int i = 1; i < n - 1; ++i) {\n            const double* a_prev = A[i - 1].data();\n            const double* a_cur  = A[i].data();\n            const double* a_next = A[i + 1].data();\n            double*       b_cur  = B[i].data();\n\n            for (int j = 1; j < n - 1; ++j) {\n                double v = alpha *\n                           (a_cur[j] + a_cur[j - 1] + a_cur[j + 1] +\n                            a_next[j] + a_prev[j]);\n                b_cur[j] = v;\n            }\n        }\n\n        /* 2nd half\u2011step: compute A from B */\n        #pragma omp parallel for schedule(static)\n        for (int i = 1; i < n - 1; ++i) {\n            const double* b_prev = B[i - 1].data();\n            const double* b_cur  = B[i].data();\n            const double* b_next = B[i + 1].data();\n            double*       a_cur  = A[i].data();\n\n            for (int j = 1; j < n - 1; ++j) {\n                double v = alpha *\n                           (b_cur[j] + b_cur[j - 1] + b_cur[j + 1] +\n                            b_next[j] + b_prev[j]);\n                a_cur[j] = v;\n            }\n        }\n    }\n}\n```\n",
  "cholesky": "```cpp\n#include <cmath>\n#include <vector>\n#include <algorithm>\n#include <cstddef>\n#include <omp.h>\n\n/* Main computational kernel. The whole function will be timed,\n   including the call and return. */\nvoid kernel_cholesky(int n,\n                     std::vector<std::vector<double>>& A)\n{\n    // Use a small block size to keep the working set in L1/L2 cache.\n    constexpr std::size_t BLOCK = 64;\n\n    // Cache pointers to each row to avoid repeated vector indexing.\n    std::vector<double*> rows(n);\n    for (int i = 0; i < n; ++i)\n        rows[i] = A[i].data();\n\n    for (int ii = 0; ii < n; ii += BLOCK) {\n        const int iend = std::min(ii + BLOCK, n);\n\n        for (int i = ii; i < iend; ++i) {\n            double* Ai = rows[i];\n\n            /* Off\u2010diagonal elements */\n            for (int jj = 0; jj < i; jj += BLOCK) {\n                const int jend = std::min(jj + BLOCK, i);\n\n                for (int j = jj; j < jend; ++j) {\n                    double* Aj = rows[j];\n                    double sum = 0.0;\n\n                    /* Inner product A[i][k] * A[j][k] for k < j */\n                    for (int k = 0; k < j; ++k)\n                        sum += Ai[k] * Aj[k];\n\n                    Ai[j] = (Ai[j] - sum) / Aj[j];\n                }\n            }\n\n            /* Diagonal element */\n            double diag_sum = 0.0;\n            for (int k = 0; k < i; ++k)\n                diag_sum += Ai[k] * Ai[k];\n\n            Ai[i] = std::sqrt(Ai[i] - diag_sum);\n        }\n    }\n}\n```",
  "trmm": "```cpp\n#include <vector>\n#include <cstddef>\n#include <omp.h>\n\n/* Main computational kernel. The whole function will be timed,\n   including the call and return. */\nvoid kernel_trmm(int m, int n,\n                 double alpha,\n                 std::vector<std::vector<double>>& A,\n                 std::vector<std::vector<double>>& B) {\n    // A is MxM, B is MxN\n    // Convert to flat array layout for better cache locality\n    const std::size_t dimA = static_cast<std::size_t>(m * m);\n    const std::size_t dimB = static_cast<std::size_t>(m * n);\n\n    // Allocate flat storage and copy data\n    std::vector<double> a_flat(dimA);\n    std::vector<double> b_flat(dimB);\n\n    for (int i = 0; i < m; ++i) {\n        for (int j = 0; j < m; ++j) {\n            a_flat[static_cast<std::size_t>(i * m + j)] = A[i][j];\n        }\n    }\n    for (int i = 0; i < m; ++i) {\n        for (int j = 0; j < n; ++j) {\n            b_flat[static_cast<std::size_t>(i * n + j)] = B[i][j];\n        }\n    }\n\n    // Core computation: B[i][j] += sum_{k=i+1}^{m-1} A[k][i] * B[k][j]\n    // Parallelise over i and j\n    #pragma omp parallel for schedule(static, 16) collapse(2)\n    for (int i = 0; i < m; ++i) {\n        for (int j = 0; j < n; ++j) {\n            double sum = 0.0;\n            const std::size_t baseA_col_i = static_cast<std::size_t>(i); // column i of A in flat layout\n            const std::size_t baseB_row_k = static_cast<std::size_t>(j); // column j of B in flat layout\n            for (int k = i + 1; k < m; ++k) {\n                // A[k][i] in flat order: (k * m + i)\n                // B[k][j] in flat order: (k * n + j)\n                sum += a_flat[static_cast<std::size_t>(k * m + i)] *\n                       b_flat[static_cast<std::size_t>(k * n + j)];\n            }\n            // Read original B[i][j] from flat copy\n            double orig = b_flat[static_cast<std::size_t>(i * n + j)];\n            // Update\n            b_flat[static_cast<std::size_t>(i * n + j)] = alpha * (orig + sum);\n        }\n    }\n\n    // Copy results back to the original B structure\n    for (int i = 0; i < m; ++i) {\n        for (int j = 0; j < n; ++j) {\n            B[i][j] = b_flat[static_cast<std::size_t>(i * n + j)];\n        }\n    }\n}\n```\nThis version:\n\n1. Flattens the input matrices to contiguous memory, improving cache usage.\n2. Computes the inner sum in a single transient variable to reduce repeated memory accesses.\n3. Uses OpenMP with `collapse(2)` to parallelise over both `i` and `j` dimensions, ensuring full utilisation of the 8 threads.\n4. Keeps the original functional interface and produces identical results.",
  "gemver": "```cpp\n// Optimised version of kernel_gemver\n#include <vector>\n#include <cstddef>\n#include <omp.h>\n\nvoid kernel_gemver(int n,\n                   double alpha,\n                   double beta,\n                   std::vector<std::vector<double>>& A,\n                   std::vector<double>& u1,\n                   std::vector<double>& v1,\n                   std::vector<double>& u2,\n                   std::vector<double>& v2,\n                   std::vector<double>& w,\n                   std::vector<double>& x,\n                   std::vector<double>& y,\n                   std::vector<double>& z) {\n    // Cache-friendly alias: pointers to the start of each row\n    std::vector<double*> Arow(n);\n    for (int i = 0; i < n; ++i)\n        Arow[i] = A[i].data();\n\n    // 1) Update A with rank-2 update:  A += u1*v1^T + u2*v2^T\n    #pragma omp parallel for schedule(static)\n    for (int i = 0; i < n; ++i) {\n        const double ui1 = u1[i];\n        const double ui2 = u2[i];\n        double *Ai = Arow[i];\n        for (int j = 0; j < n; ++j) {\n            Ai[j] += ui1 * v1[j] + ui2 * v2[j];\n        }\n    }\n\n    // 2) Compute x += beta * A^T * y   (column-major access of A)\n    #pragma omp parallel for schedule(static)\n    for (int i = 0; i < n; ++i) {\n        double sum = x[i];\n        for (int j = 0; j < n; ++j) {\n            sum += beta * Arow[j][i] * y[j];\n        }\n        x[i] = sum;\n    }\n\n    // 3) x += z\n    #pragma omp parallel for schedule(static)\n    for (int i = 0; i < n; ++i) x[i] += z[i];\n\n    // 4) w += alpha * A * x  (row-major access of A)\n    #pragma omp parallel for schedule(static)\n    for (int i = 0; i < n; ++i) {\n        double sum = w[i];\n        double *Ai = Arow[i];\n        for (int j = 0; j < n; ++j) {\n            sum += alpha * Ai[j] * x[j];\n        }\n        w[i] = sum;\n    }\n}\n```",
  "lu": "```cpp\n#include <vector>\n#include <omp.h>\n\n/* Main computational kernel. The whole function will be timed,\n   including the call and return. */\nvoid kernel_lu(int n, std::vector<std::vector<double>>& A) {\n    /* A is a non\u2011contiguous 2\u2011D vector, but each row is contiguous.\n       We obtain raw pointers once per row to improve cache locality\n       and enable better vectorization. */\n    for (int i = 0; i < n; ++i) {\n        double* Ai = A[i].data();          // pointer to row i\n\n        /* First part: upper triangular factor L (j < i) */\n        for (int j = 0; j < i; ++j) {\n            double val = Ai[j];\n            double* Ak = A[i].data();      // same row, used as source\n            for (int k = 0; k < j; ++k) {\n                val -= Ai[k] * A[k][j];\n            }\n            Ai[j] = val / A[j][j];\n        }\n\n        /* Second part: upper triangular factor U (j >= i)\n           This inner loop is independent for each j, so we can\n           parallelise it across the columns. */\n        #pragma omp parallel for schedule(static)\n        for (int j = i; j < n; ++j) {\n            double val = Ai[j];\n            for (int k = 0; k < i; ++k) {\n                val -= Ai[k] * A[k][j];\n            }\n            Ai[j] = val;\n        }\n    }\n}\n```",
  "floyd_warshall": "```cpp\n#include <vector>\n#include <algorithm>\n#include <omp.h>\n\nvoid kernel_floyd_warshall(int n, std::vector<std::vector<int>>& path) {\n    for (int k = 0; k < n; ++k) {\n        // Parallelise the i\u2013loop; j\u2013loop stays inside each thread.\n        #pragma omp parallel for schedule(static)\n        for (int i = 0; i < n; ++i) {\n            // Cache row pointer and the i\u2011k value to avoid repeated indexing.\n            int* row   = path[i].data();\n            int aik    = row[k];\n            for (int j = 0; j < n; ++j) {\n                int alt = aik + path[k][j];\n                if (alt < row[j]) {\n                    row[j] = alt;\n                }\n            }\n        }\n    }\n}\n```\n",
  "deriche": "```cpp\n#include <cmath>\n#include <vector>\n#include <omp.h>\n\n/* Main computational kernel. The whole function will be timed,\n   including the call and return. */\n/* Optimized version \u2013 preserves original behaviour */\nvoid kernel_deriche(int w, int h, double alpha,\n                    std::vector<std::vector<float>>& imgIn,\n                    std::vector<std::vector<float>>& imgOut,\n                    std::vector<std::vector<float>>& y1,\n                    std::vector<std::vector<float>>& y2) {\n    /* ---------- pre\u2011compute constants ----------\n     * All constants are computed once outside the loops\n     * to avoid repeated math evaluations.\n     */\n    const float alpha_f = static_cast<float>(alpha);\n    const float exp_a   = std::exp(-alpha_f);\n    const float exp_2a  = exp_a * exp_a;\n    const float k = 1.0f - exp_a * (1.0f - exp_a) /\n                       (1.0f + 2.0f * alpha_f * exp_a - exp_2a);\n\n    const float a1 = k, a2 = k * exp_a * (alpha_f - 1.0f),\n                 a3 = k * exp_a * (alpha_f + 1.0f), a4 = -k * exp_2a,\n                 a5 = k, a6 = k * exp_a * (alpha_f - 1.0f),\n                 a7 = k * exp_a * (alpha_f + 1.0f), a8 = -k * exp_2a;\n\n    const float b1 = std::pow(2.0f, -alpha_f), b2 = -exp_2a;\n\n    const float c1 = 1.0f, c2 = 1.0f;   // unchanged\n    /* ------------------------------------------- */\n\n    /* Forward horizontal pass \u2013 y1 */\n    #pragma omp parallel for\n    for (int i = 0; i < w; ++i) {\n        float ym1 = 0.0f, ym2 = 0.0f, xm1 = 0.0f;\n        for (int j = 0; j < h; ++j) {\n            float g = imgIn[i][j];\n            y1[i][j] = a1 * g + a2 * xm1 + b1 * ym1 + b2 * ym2;\n            xm1 = g;\n            ym2 = ym1;\n            ym1 = y1[i][j];\n        }\n    }\n\n    /* Backward horizontal pass \u2013 y2 */\n    #pragma omp parallel for\n    for (int i = 0; i < w; ++i) {\n        float yp1 = 0.0f, yp2 = 0.0f, xp1 = 0.0f, xp2 = 0.0f;\n        for (int j = h - 1; j >= 0; --j) {\n            float g = imgIn[i][j];\n            y2[i][j] = a3 * xp1 + a4 * xp2 + b1 * yp1 + b2 * yp2;\n            xp2 = xp1;\n            xp1 = g;\n            yp2 = yp1;\n            yp1 = y2[i][j];\n        }\n    }\n\n    /* Combine forward/backward results to imgOut */\n    #pragma omp parallel for collapse(2)\n    for (int i = 0; i < w; ++i) {\n        for (int j = 0; j < h; ++j) {\n            imgOut[i][j] = c1 * (y1[i][j] + y2[i][j]);\n        }\n    }\n\n    /* Forward vertical pass \u2013 y1 (using imgOut as source) */\n    #pragma omp parallel for\n    for (int j = 0; j < h; ++j) {\n        float tm1 = 0.0f, ym1 = 0.0f, ym2 = 0.0f;\n        for (int i = 0; i < w; ++i) {\n            float g = imgOut[i][j];\n            y1[i][j] = a5 * g + a6 * tm1 + b1 * ym1 + b2 * ym2;\n            tm1 = g;\n            ym2 = ym1;\n            ym1 = y1[i][j];\n        }\n    }\n\n    /* Backward vertical pass \u2013 y2 (using imgOut as source) */\n    #pragma omp parallel for\n    for (int j = 0; j < h; ++j) {\n        float tp1 = 0.0f, tp2 = 0.0f, yp1 = 0.0f, yp2 = 0.0f;\n        for (int i = w - 1; i >= 0; --i) {\n            float g = imgOut[i][j];\n            y2[i][j] = a7 * tp1 + a8 * tp2 + b1 * yp1 + b2 * yp2;\n            tp2 = tp1;\n            tp1 = g;\n            yp2 = yp1;\n            yp1 = y2[i][j];\n        }\n    }\n\n    /* Final combination \u2013 overwrite imgOut */\n    #pragma omp parallel for collapse(2)\n    for (int i = 0; i < w; ++i) {\n        for (int j = 0; j < h; ++j) {\n            imgOut[i][j] = c2 * (y1[i][j] + y2[i][j]);\n        }\n    }\n}\n```\n"
}