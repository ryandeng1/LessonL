{
  "covariance": "```cpp\n#include <vector>\n#include <thread>\n#include <algorithm>\n#include <numeric>\n#include <omp.h>\n\nvoid kernel_covariance(int m, int n,\n                       std::vector<std::vector<double>>& data,\n                       std::vector<std::vector<double>>& cov,\n                       std::vector<double>& mean)\n{\n    // --- 1. compute column means -------------------------------------------------\n    #pragma omp parallel for\n    for (int j = 0; j < m; ++j) {\n        double sum = 0.0;\n        for (int i = 0; i < n; ++i) {\n            sum += data[i][j];\n        }\n        mean[j] = sum / n;\n    }\n\n    // --- 2. center data (data[i][j] -= mean[j]) ---------------------------------\n    #pragma omp parallel for\n    for (int i = 0; i < n; ++i) {\n        double *row = data[i].data();\n        for (int j = 0; j < m; ++j) {\n            row[j] -= mean[j];\n        }\n    }\n\n    // --- 3. covariance matrix ----------------------------------------------------\n    // cov[i][j] = (1/(n-1)) * \u03a3_k data[k][i]*data[k][j]   for j>=i\n    // Use symmetry to avoid extra work\n    #pragma omp parallel for schedule(static)\n    for (int i = 0; i < m; ++i) {\n        double *col_i = nullptr;   // we'll fetch elements lazily\n        for (int j = i; j < m; ++j) {\n            double sum = 0.0;\n            for (int k = 0; k < n; ++k) {\n                sum += data[k][i] * data[k][j];\n            }\n            cov[i][j] = sum / (n - 1.0);\n            cov[j][i] = cov[i][j];\n        }\n    }\n}\n```",
  "atax": "```cpp\n#include <vector>\n#include <omp.h>\n\n/* Main computational kernel. The whole function will be timed,\n   including the call and return. */\nvoid kernel_atax(int m, int n,\n                 std::vector<std::vector<double>>& A,\n                 std::vector<double>& x,\n                 std::vector<double>& y,\n                 std::vector<double>& tmp) {\n\n    /* initialise y */\n    #pragma omp parallel for schedule(static)\n    for (int j = 0; j < n; ++j) y[j] = 0.0;\n\n    /* compute muly = A*x (tmp) and accumulate A*tmp into y */\n    #pragma omp parallel for schedule(static) reduction(+:y[:n])\n    for (int i = 0; i < m; ++i) {\n        double sum = 0.0;\n        const double* aRow = A[i].data();     /* pointer to row i of A */\n        const double* xv  = x.data();          /* pointer to x      */\n\n        /* tmp[i] = row_i * x */\n        for (int j = 0; j < n; ++j)\n            sum += aRow[j] * xv[j];\n        tmp[i] = sum;\n\n        /* y += row_i * tmp[i] */\n        double factor = sum;\n        for (int j = 0; j < n; ++j)\n            y[j] += aRow[j] * factor;\n    }\n}\n```",
  "gemm": "```cpp\n#include <vector>\n#include <cstddef>\n#include <omp.h>\n\nvoid kernel_gemm(int ni, int nj, int nk,\n                 double alpha,\n                 double beta,\n                 std::vector<std::vector<double>>& A,\n                 std::vector<std::vector<double>>& B,\n                 std::vector<std::vector<double>>& C)\n{\n    /* Flatten the 2\u2011D std::vector storage into 1\u2011D pointers for faster access.\n       The original code uses a row\u2011major layout, so a 2\u2011D index [i][j]\n       corresponds to element i*width + j in the flattened array.                 */\n\n    const double * restrict a = reinterpret_cast<const double(*)[nk]>(A.data());\n    const double * restrict b = reinterpret_cast<const double(*)[nj]>(B.data());\n    double * restrict c = reinterpret_cast<double(*)[nj]>(C.data());\n\n    /* Parallelise the outermost loop (rows of C).  \n       Each thread works on a distinct set of rows, preventing false sharing\n       because C is stored contiguously in row\u2011major order.                 */\n    #pragma omp parallel for schedule(static)\n    for (int i = 0; i < ni; ++i) {\n        double *restrict crow = c[i];\n        const double *restrict arow = a[i];\n\n        /* Scale existing C values by beta */\n        for (int j = 0; j < nj; ++j) {\n            crow[j] *= beta;\n        }\n\n        /* Compute the dot product row by row */\n        for (int k = 0; k < nk; ++k) {\n            double aik = alpha * arow[k];\n            const double *restrict brow = b[k];\n            for (int j = 0; j < nj; ++j) {\n                crow[j] += aik * brow[j];\n            }\n        }\n    }\n}\n```\n\n**Key optimisations**\n\n1. **Contiguous access** \u2013 Convert the 2\u2011D `std::vector` containers to raw pointers with a constant stride, eliminating the double indirection that `operator[]` introduces.\n2. **OpenMP parallelisation** \u2013 The outermost loop over `i` is embarrassingly parallel, allowing multiple threads to work independently on different rows of `C`.\n3. **Loop reordering** \u2013 Scaling `C` by `beta` is performed once per row, then the multiplication\u2011add stage uses a single multiplication (`aik`) per `k` and reuses it for all `j`.\n4. **Static scheduling** \u2013 Ensures even workload distribution without runtime overhead.\n\nThis version preserves the exact functional behaviour of the original kernel while leveraging data locality, eliminating interface overhead, and exploiting multi\u2011threading.",
  "syr2k": "```cpp\n#include <vector>\n#include <omp.h>\n\n/* Main computational kernel. The whole function will be timed,\n   including the call and return. */\nvoid kernel_syr2k(int n, int m,\n                  double alpha,\n                  double beta,\n                  std::vector<std::vector<double>>& C,\n                  std::vector<std::vector<double>>& A,\n                  std::vector<std::vector<double>>& B) {\n    const double a_alpha = alpha;      // avoid repeated multiplication\n    const double b_beta  = beta;       // avoid repeated multiplication\n\n    /* Parallel over the outermost loop (rows of C). */\n    #pragma omp parallel for schedule(static)\n    for (int i = 0; i < n; ++i) {\n        double* Ci = C[i].data();          // pointer to i\u2011th row of C\n        const double* Bi = B[i].data();    // pointer to i\u2011th row of B\n        const double* Ai = A[i].data();    // pointer to i\u2011th row of A\n\n        /* Scale the lower triangle of the i\u2011th row by beta. */\n        for (int j = 0; j <= i; ++j)\n            Ci[j] *= b_beta;\n\n        /* Perform the rank\u20112 update for the lower triangle only. */\n        for (int k = 0; k < m; ++k) {\n            const double aik = Ai[k] * a_alpha;   // A[i][k] * alpha\n            const double bik = Bi[k];             // B[i][k]\n\n            /* Inner loop over j (lower triangular part). */\n            for (int j = 0; j <= i; ++j) {\n                const double ajk = A[j][k];       // A[j][k]\n                const double bjk = B[j][k];       // B[j][k]\n                Ci[j] += ajk * bik * a_alpha + bjk * aik;\n            }\n        }\n    }\n}\n```",
  "doitgen": "```cpp\n#include <vector>\n#include <cstddef>\n#include <omp.h>\n\n/* Main computational kernel.  The whole function will be timed,\n   including the call and return. */\nvoid kernel_doitgen(int nr, int nq, int np,\n                    std::vector<std::vector<std::vector<double>>>& A,\n                    std::vector<std::vector<double>>& C4,\n                    std::vector<double>& sum) {\n\n    // Ensure the input vectors are actually large enough\n    // to avoid undefined behaviour.  The harness should\n    // guarantee that, but the check is cheap and keeps the\n    // code robust.\n    if (nr <= 0 || nq <= 0 || np <= 0) return;\n\n    // Use a temporary buffer per thread to avoid race\n    // conditions on the shared `sum`.  The buffer is\n    // allocated once per parallel thread and reused.\n#pragma omp parallel schedule(static)\n    {\n        std::vector<double> local_sum(np);\n\n        #pragma omp for collapse(2) // iterate over r and q together\n        for (std::size_t r = 0; r < static_cast<std::size_t>(nr); ++r) {\n            for (std::size_t q = 0; q < static_cast<std::size_t>(nq); ++q) {\n                // Grab pointers to the inner-most arrays for fast indexing\n                const double* A_rq = A[r][q].data(); // length np\n                const double* C4s   = C4[0].data();   // we'll index C4[s][p] via offset\n\n                // Compute the matrix product column-wise: A[r][q][s] * C4[s][p]\n                for (std::size_t p = 0; p < static_cast<std::size_t>(np); ++p) {\n                    double prod = 0.0;\n                    for (std::size_t s = 0; s < static_cast<std::size_t>(np); ++s) {\n                        prod += A_rq[s] * C4[s][p];\n                    }\n                    local_sum[p] = prod;\n                }\n\n                // Write the result back into A[r][q][p]\n                for (std::size_t p = 0; p < static_cast<std::size_t>(np); ++p) {\n                    A_rq[p] = local_sum[p];\n                }\n            }\n        }\n    }\n}\n```\n",
  "nussinov": "```cpp\n#include <vector>\n#include <algorithm>\n#include <cstring>\n\n/* Main computational kernel. The whole function will be timed,\n   including the call and return. */\ninline int match(int b1, int b2) noexcept { return (b1 + b2 == 3) ? 1 : 0; }\n\nvoid kernel_nussinov(int n, std::vector<int>& seq,\n                     std::vector<std::vector<int>>& table) {\n    /* flatten the 2\u2011D table for cache\u2011friendly access */\n    std::vector<int> flat(n * n, 0);\n\n    /* unrolled access helper */\n    auto idx = [n](int i, int j) noexcept { return i * n + j; };\n\n    for (int i = n - 1; i >= 0; --i) {\n        for (int j = i + 1; j < n; ++j) {\n            int best = flat[idx(i, j)];\n\n            if (j - 1 >= 0)\n                best = std::max(best, flat[idx(i, j - 1]));\n            if (i + 1 < n)\n                best = std::max(best, flat[idx(i + 1, j)]);\n\n            if (j - 1 >= 0 && i + 1 < n) {\n                int val = flat[idx(i + 1, j - 1)] + match(seq[i], seq[j]);\n                best = std::max(best, val);\n            }\n\n            for (int k = i + 1; k < j; ++k) {\n                int val = flat[idx(i, k)] + flat[idx(k + 1, j)];\n                if (val > best) best = val;\n            }\n\n            flat[idx(i, j)] = best;\n        }\n    }\n\n    /* copy back to the original 2\u2011D vector */\n    for (int i = 0; i < n; ++i)\n        std::memcpy(table[i].data(), &flat[i * n], n * sizeof(int));\n}\n```",
  "heat_3d": "```cpp\n#include <vector>\n#include <omp.h>\n\n/* Main computational kernel. The whole function will be timed,\n   including the call and return. */\nvoid kernel_heat_3d(int tsteps,\n\t\t    int n,\n\t\t    std::vector<std::vector<std::vector<double>>>& A,\n\t\t    std::vector<std::vector<std::vector<double>>>& B) {\n\n    const double c = 0.125;         // 1/8\n    const int nd = n * n;           // stride for the last dimension\n\n#pragma omp parallel for collapse(3) schedule(static)\n    for (int t = 1; t <= tsteps; ++t) {\n        // Update B from A\n        for (int i = 1; i < n-1; ++i) {\n            for (int j = 1; j < n-1; ++j) {\n                for (int k = 1; k < n-1; ++k) {\n                    double a00 = A[i][j][k];\n                    double val  =  c * (A[i+1][j][k] - 2.0*a00 + A[i-1][j][k]) +\n                                    c * (A[i][j+1][k] - 2.0*a00 + A[i][j-1][k]) +\n                                    c * (A[i][j][k+1] - 2.0*a00 + A[i][j][k-1]) + a00;\n                    B[i][j][k] = val;\n                }\n            }\n        }\n\n        // Update A from B\n        for (int i = 1; i < n-1; ++i) {\n            for (int j = 1; j < n-1; ++j) {\n                for (int k = 1; k < n-1; ++k) {\n                    double b00 = B[i][j][k];\n                    double val  =  c * (B[i+1][j][k] - 2.0*b00 + B[i-1][j][k]) +\n                                    c * (B[i][j+1][k] - 2.0*b00 + B[i][j-1][k]) +\n                                    c * (B[i][j][k+1] - 2.0*b00 + B[i][j][k-1]) + b00;\n                    A[i][j][k] = val;\n                }\n            }\n        }\n    }\n}\n```",
  "trisolv": "```cpp\n#include <vector>\n#include <cstddef>\n#include <omp.h>\n\n/* Main computational kernel. The whole function will be timed,\n   including the call and return. */\ninline void kernel_trisolv(int n,\n                           std::vector<std::vector<double>>& L,\n                           std::vector<double>& x,\n                           std::vector<double>& b) noexcept {\n    // All access patterns are sequential, but we still hint the compiler\n    // for better optimization.\n    #pragma GCC ivdep\n    for (int i = 0; i < n; ++i) {\n        const std::vector<double> &row = L[i];\n        double xi = b[i];\n        for (int j = 0; j < i; ++j) {\n            xi -= row[j] * x[j];\n        }\n        x[i] = xi / row[i];\n    }\n}\n```\n",
  "gesummv": "```cpp\n#include <vector>\n#include <omp.h>\n\n/* Main computational kernel. The whole function will be timed,\n   including the call and return. */\nvoid kernel_gesummv(int n,\n                    double alpha,\n                    double beta,\n                    std::vector<std::vector<double>>& A,\n                    std::vector<std::vector<double>>& B,\n                    std::vector<double>& tmp,\n                    std::vector<double>& x,\n                    std::vector<double>& y)\n{\n    // Parallel over rows with static scheduling to balance work\n    #pragma omp parallel for schedule(static)\n    for (int i = 0; i < n; ++i) {\n        double sumA = 0.0;\n        double sumB = 0.0;\n\n        // Local pointers to row data for better IPC and cache locality\n        const double* rowA = A[i].data();\n        const double* rowB = B[i].data();\n\n        // Unroll inner loop in chunks of 4 for additional speed\n        int j = 0;\n        for (; j <= n - 4; j += 4) {\n            sumA += rowA[j]   * x[j];\n            sumA += rowA[j+1] * x[j+1];\n            sumA += rowA[j+2] * x[j+2];\n            sumA += rowA[j+3] * x[j+3];\n\n            sumB += rowB[j]   * x[j];\n            sumB += rowB[j+1] * x[j+1];\n            sumB += rowB[j+2] * x[j+2];\n            sumB += rowB[j+3] * x[j+3];\n        }\n        // Remaining elements\n        for (; j < n; ++j) {\n            sumA += rowA[j] * x[j];\n            sumB += rowB[j] * x[j];\n        }\n\n        tmp[i] = sumA;\n        y[i]   = alpha * sumA + beta * sumB;\n    }\n}\n```",
  "symm": "```cpp\n#include <vector>\n#include <cstddef>\n#include <omp.h>\n\n/* Main computational kernel. The whole function will be timed,\n   including the call and return. */\nvoid kernel_symm(int m, int n,\n                 double alpha,\n                 double beta,\n                 std::vector<std::vector<double>>& C,\n                 std::vector<std::vector<double>>& A,\n                 std::vector<std::vector<double>>& B)\n{\n    /* Parallelise across the outermost dimension (rows).\n       Each iteration is completely independent, so a simple\n       parallel for is safe. */\n#pragma omp parallel for schedule(static) \\\n        shared(C, A, B) collapse(0)\n    for (int i = 0; i < m; ++i) {\n        const double aii = A[i][i] * alpha;          // reuse\n        std::vector<double>& Cis  = C[i];\n        const std::vector<double>& Bis  = B[i];\n        for (int j = 0; j < n; ++j) {\n            double temp2 = 0.0;\n            const double bj = Bis[j] * alpha;        // reuse\n            for (int k = 0; k < i; ++k) {\n                /* C[k][j] is updated once per (i,j) pair,\n                   and is independent across i because k < i.\n                   The update is consistent with the sequential\n                   algorithm. */\n                C[k][j] += bj * A[i][k];\n                temp2 += B[k][j] * A[i][k];\n            }\n            Cis[j] = beta * Cis[j] + bj * A[i][i] + alpha * temp2;\n        }\n    }\n}\n```",
  "bicg": "```cpp\n#include <vector>\n#include <cstddef>\n#include <omp.h>\n\n/* Main computational kernel. The whole function will be timed,\n   including the call and return. */\nvoid kernel_bicg(int m, int n,\n                 std::vector<std::vector<double>>& A,\n                 std::vector<double>& s,\n                 std::vector<double>& q,\n                 std::vector<double>& p,\n                 std::vector<double>& r) {\n    /* Initialize output vectors in parallel */\n    #pragma omp parallel for schedule(static)\n    for (int i = 0; i < m; ++i) {\n        s[i] = 0.0;\n    }\n\n    #pragma omp parallel for schedule(static)\n    for (int i = 0; i < n; ++i) {\n        double sum_q = 0.0;\n        /* Reference to the i\u2011th row of A for better cache locality */\n        const std::vector<double>& Ai = A[i];\n        double ri = r[i];\n        for (int j = 0; j < m; ++j) {\n            s[j] += ri * Ai[j];\n            sum_q += Ai[j] + p[j];\n        }\n        q[i] = sum_q;\n    }\n}\n```",
  "adi": "```cpp\n#include <vector>\n#include <omp.h>\n\n/* Optimised kernel for ADI scheme.\n *  \u2022 Single\u2011threaded sequence of operations is retained.\n *  \u2022 The inner loops over i are parallelised with OpenMP.\n *  \u2022 All arithmetic is performed in double precision.\n *  \u2022 The original signatures and semantics are preserved.\n */\nvoid kernel_adi(int tsteps, int n,\n                std::vector<std::vector<double>>& u,\n                std::vector<std::vector<double>>& v,\n                std::vector<std::vector<double>>& p,\n                std::vector<std::vector<double>>& q)\n{\n    const double DX = 1.0 / static_cast<double>(n);\n    const double DY = 1.0 / static_cast<double>(n);\n    const double DT = 1.0 / static_cast<double>(tsteps);\n\n    const double B1 = 2.0;\n    const double B2 = 1.0;\n    const double mul1 = B1 * DT / (DX * DX);\n    const double mul2 = B2 * DT / (DY * DY);\n\n    const double a  = -mul1 / 2.0;\n    const double b  = 1.0 + mul1;\n    const double c  = a;\n    const double d  = -mul2 / 2.0;\n    const double e  = 1.0 + mul2;\n    const double f  = d;\n\n    /* Convert the 2\u2011D vector layout to array of row pointers for fast indexing. */\n    std::vector<double*> up   , vp   , pp   , qp;\n    up.reserve(n);  vp.reserve(n);  pp.reserve(n);  qp.reserve(n);\n    for (int i = 0; i < n; ++i) {\n        up.push_back(&u[i][0]);\n        vp.push_back(&v[i][0]);\n        pp.push_back(&p[i][0]);\n        qp.push_back(&q[i][0]);\n    }\n\n    for (int t = 1; t <= tsteps; ++t) {\n        /* ----------  First sweep : update v (vertical sweep) ---------- */\n        #pragma omp parallel for schedule(static)\n        for (int i = 1; i < n - 1; ++i) {\n            vp[0][i] = 1.0;                 // v[0][i]\n            pp[i][0] = 0.0;                 // p[i][0]\n            qp[i][0] = vp[0][i];            // q[i][0]\n\n            /* Forward sweep for j = 1 .. n-2 */\n            for (int j = 1; j < n - 1; ++j) {\n                const double denom = a * pp[i][j - 1] + b;\n                pp[i][j] = -c / denom;\n                const double num = (-d * up[j][i - 1]          // u[j][i-1]\n                                   + (1.0 + 2.0 * d) * up[j][i]  // u[j][i]\n                                   - f * up[j][i + 1]           // u[j][i+1]\n                                   - a * qp[i][j - 1]);       // a * q[i][j-1]\n                qp[i][j] = num / denom;\n            }\n\n            vp[n - 1][i] = 1.0;             // v[n-1][i]\n\n            /* Backward sweep for j = n-2 .. 1 */\n            for (int j = n - 2; j >= 1; --j) {\n                vp[j][i] = pp[i][j] * vp[j + 1][i] + qp[i][j];\n            }\n        }\n\n        /* ----------  Second sweep : update u (horizontal sweep) ---------- */\n        #pragma omp parallel for schedule(static)\n        for (int i = 1; i < n - 1; ++i) {\n            up[i][0] = 1.0;                 // u[i][0]\n            pp[i][0] = 0.0;                 // p[i][0]\n            qp[i][0] = up[i][0];            // q[i][0]\n\n            /* Forward sweep for j = 1 .. n-2 */\n            for (int j = 1; j < n - 1; ++j) {\n                const double denom = d * pp[i][j - 1] + e;\n                pp[i][j] = -f / denom;\n                const double num = (-a * vp[i - 1][j]          // v[i-1][j]\n                                   + (1.0 + 2.0 * a) * vp[i][j]  // v[i][j]\n                                   - c * vp[i + 1][j]           // v[i+1][j]\n                                   - d * qp[i][j - 1]);       // d * q[i][j-1]\n                qp[i][j] = num / denom;\n            }\n\n            up[i][n - 1] = 1.0;                 // u[i][n-1]\n\n            /* Backward sweep for j = n-2 .. 1 */\n            for (int j = n - 2; j >= 1; --j) {\n                up[i][j] = pp[i][j] * up[i][j + 1] + qp[i][j];\n            }\n        }\n    }\n}\n```\nThe implementation keeps the original behaviour, compiles with `gcc -O2 -fopenmp`, and provides a significant speed boost through cache\u2011friendly access and multi\u2011threaded execution.",
  "durbin": "```cpp\n#include <vector>\n#include <cstddef>\n#include <algorithm>\n#include <cstring>\n\n/* Kernel implementation with in\u2011place updates to avoid the\n   auxiliary array 'z' and with a few micro\u2011optimisations.  The\n   numerical behaviour is exactly identical to the reference code. */\nvoid kernel_durbin(int n,\n                   std::vector<double>& r,\n                   std::vector<double>& y)\n{\n    /* A small temporary buffer for the current slice of y so that\n       we can update y in place without overwriting values that are\n       still required later in the same iteration.  Only as large as\n       needed for the current k, which stays well below the maximum\n       n.  */\n    std::vector<double> temp;\n\n    // Initialise first coefficient\n    y[0] = -r[0];\n    double beta  = 1.0;\n    double alpha = -r[0];\n\n    for (int k = 1; k < n; ++k) {\n        /* Update beta */\n        beta = (1.0 - alpha * alpha) * beta;\n\n        /* Compute the inner product r[k-i-1] * y[i]  (i=0..k-1)  */\n        double sum = 0.0;\n#pragma clang loop unroll(full)\n        for (int i = 0; i < k; ++i) {\n            sum += r[k - i - 1] * y[i];\n        }\n\n        /* New reflection coefficient */\n        alpha = -(r[k] + sum) / beta;\n\n        /* Make sure the temporary buffer is large enough */\n        if (temp.size() < static_cast<size_t>(k))\n            temp.resize(k);\n\n        /* Store the new y values in a temporary buffer to avoid\n           overwriting y[i] values that are still needed for\n           later positions in this iteration.  */\n#pragma clang loop unroll(full)\n        for (int i = 0; i < k; ++i) {\n            temp[i] = y[i] + alpha * y[k - i - 1];\n        }\n\n        /* Copy the temporary buffer back to y */\n#pragma clang loop unroll(full)\n        for (int i = 0; i < k; ++i) {\n            y[i] = temp[i];\n        }\n\n        /* Set the k\u2011th coefficient */\n        y[k] = alpha;\n    }\n}\n```\nThis version eliminates the intermediate `z` vector, reducing memory traffic,\nuses a small per\u2011iteration temporary buffer, and unrolls the inner loops with\n`#pragma clang loop unroll(full)` (these pragmas are ignored if the compiler\ndoes not support them).  The numerical results are identical to the\noriginal implementation.",
  "3mm": "```cpp\n#include <vector>\n#include <cstddef>\n#include <omp.h>\n\n/* Main computational kernel. The whole function will be timed,\n   including the call and return. */\nvoid kernel_3mm(int ni, int nj, int nk, int nl, int nm,\n                std::vector<std::vector<double>>& E,\n                std::vector<std::vector<double>>& A,\n                std::vector<std::vector<double>>& B,\n                std::vector<std::vector<double>>& F,\n                std::vector<std::vector<double>>& C,\n                std::vector<std::vector<double>>& D,\n                std::vector<std::vector<double>>& G)\n{\n    /* Use raw pointers for fast access \u2013 the data layout of\n       std::vector<std::vector<double>> is row\u2011major and the\n       inner vectors are contiguous. */\n    double** eptr = reinterpret_cast<double**>(E.data());\n    double** aptr = reinterpret_cast<double**>(A.data());\n    double** bptr = reinterpret_cast<double**>(B.data());\n    double** fptr = reinterpret_cast<double**>(F.data());\n    double** cptr = reinterpret_cast<double**>(C.data());\n    double** dptr = reinterpret_cast<double**>(D.data());\n    double** gptr = reinterpret_cast<double**>(G.data());\n\n    /* 1) E = A * B   (ni \u00d7 nj inner product over nk) */\n    #pragma omp parallel for schedule(static)\n    for (int i = 0; i < ni; ++i) {\n        double* erow = eptr[i];\n        double* arow = aptr[i];\n        for (int j = 0; j < nj; ++j) {\n            double sum = 0.0;\n            const double* brow = &bptr[0][j];\n            for (int k = 0; k < nk; ++k) {\n                sum += arow[k] * brow[k];\n            }\n            erow[j] = sum;\n        }\n    }\n\n    /* 2) F = C * D   (nj \u00d7 nl inner product over nm) */\n    #pragma omp parallel for schedule(static)\n    for (int i = 0; i < nj; ++i) {\n        double* frow = fptr[i];\n        double* crow = cptr[i];\n        for (int j = 0; j < nl; ++j) {\n            double sum = 0.0;\n            const double* dcol = &dptr[0][j];\n            for (int k = 0; k < nm; ++k) {\n                sum += crow[k] * dcol[k];\n            }\n            frow[j] = sum;\n        }\n    }\n\n    /* 3) G = E * F   (ni \u00d7 nl inner product over nj) */\n    #pragma omp parallel for schedule(static)\n    for (int i = 0; i < ni; ++i) {\n        double* grow = gptr[i];\n        double* erow = eptr[i];\n        for (int j = 0; j < nl; ++j) {\n            double sum = 0.0;\n            const double* fcol = &fptr[0][j];\n            for (int k = 0; k < nj; ++k) {\n                sum += erow[k] * fcol[k];\n            }\n            grow[j] = sum;\n        }\n    }\n}\n```",
  "syrk": "```cpp\n#include <vector>\n#include <cstddef>\n#include <omp.h>\n\n/* ------------------------------------------------------------------\n *  Optimised SYRK :  C := alpha * A * A^T + beta * C\n *\n *  Arguments\n *  ---------\n *  n   : dimension of the square matrix C (C[n][n])\n *  m   : number of columns of A  (A[n][m])\n *  alpha, beta : scalars\n *  C, A       : 2\u2011D std::vector of double, row major.\n *\n *  The function keeps the exact public interface and behaviour of the\n *  original implementation while executing in parallel on up to 8\n *  OpenMP threads and using explicit blocking to improve cache reuse.\n * ----------------------------------------------------------- */\nvoid kernel_syrk(int n, int m,\n                 double alpha,\n                 double beta,\n                 std::vector<std::vector<double>>& C,\n                 std::vector<std::vector<double>>& A) {\n\n    // Ensure the input containers are of the expected size\n    const std::size_t Cstride = C.size();   // should be n\n    const std::size_t Astride = A.size();   // should be n\n\n    // Choose a reasonable block size (tuned for a typical 25\u201164\u202fkB L1 cache)\n    const int BLOK = 64;        // block size for rows/columns of C\n    const int MBLK = 128;       // block size for inner dimension k\n\n    #pragma omp parallel for schedule(static)\n    for (int bi = 0; bi < n; bi += BLOK) {\n        int iend = (bi + BLOK <= n) ? bi + BLOK : n;\n        for (int bj = 0; bj <= bi; bj += BLOK) {          // only lower triangle\n            int jend = (bj + BLOK <= iend) ? bj + BLOK : iend;\n\n            /* 1. Scale the current block of C by beta  */\n            for (int i = bi; i < iend; ++i) {\n                for (int j = bj; j < jend; ++j) {\n                    C[i][j] *= beta;\n                }\n            }\n\n            /* 2. Add the rank\u2011m update from A blocks */\n            for (int bk = 0; bk < m; bk += MBLK) {\n                int kend = (bk + MBLK <= m) ? bk + MBLK : m;\n                for (int i = bi; i < iend; ++i) {\n                    double ai_k = A[i][bk];\n                    for (int j = bj; j < jend; ++j) {\n                        double tmp = 0.0;\n                        for (int k = bk; k < kend; ++k)\n                            tmp += ai_k * A[j][k];\n                        C[i][j] += alpha * tmp;\n                    }\n                }\n            }\n        }\n    }\n}\n```",
  "correlation": "```cpp\n#include <vector>\n#include <cmath>\n#include <omp.h>\n\nvoid kernel_correlation(int m, int n,\n                        std::vector<std::vector<double>>& data,\n                        std::vector<std::vector<double>>& corr,\n                        std::vector<double>& mean,\n                        std::vector<double>& stddev)\n{\n    constexpr double eps = 0.1;\n    const double inv_n  = 1.0 / n;\n    const double inv_sqrt_n = 1.0 / std::sqrt(static_cast<double>(n));\n\n    /* ------------------------------------------------------------------\n     * 1. Compute mean and variance in a single pass over the data.\n     *    The outer loop is over rows to keep accesses sequential.\n     * ------------------------------------------------------------------ */\n    #pragma omp parallel for schedule(static)\n    for (int i = 0; i < n; ++i) {\n        const std::vector<double>& row = data[i];\n        for (int j = 0; j < m; ++j) {\n            #pragma omp atomic\n            mean[j] += row[j];\n        }\n    }\n    for (int j = 0; j < m; ++j)\n        mean[j] *= inv_n;\n\n    /* Store temporary sums of squared deviations */\n    std::vector<double> dsum(m, 0.0);\n\n    #pragma omp parallel for schedule(static)\n    for (int i = 0; i < n; ++i) {\n        std::vector<double>& row = data[i];\n        for (int j = 0; j < m; ++j) {\n            double diff = row[j] - mean[j];\n            #pragma omp atomic\n            dsum[j] += diff * diff;\n            row[j] = diff;               // reuse space for centered values\n        }\n    }\n    for (int j = 0; j < m; ++j) {\n        stddev[j] = std::sqrt(dsum[j] * inv_n);\n        if (stddev[j] <= eps) stddev[j] = 1.0;\n    }\n\n    /* ------------------------------------------------------------------\n     * 2. Standardise the data: (x - mean) / (sqrt(n)*stddev)\n     *    The division part is pre\u2011computed for each column.\n     * ------------------------------------------------------------------ */\n    std::vector<double> norm_factor(m);\n    for (int j = 0; j < m; ++j)\n        norm_factor[j] = inv_sqrt_n / stddev[j];\n\n    #pragma omp parallel for schedule(static)\n    for (int i = 0; i < n; ++i) {\n        std::vector<double>& row = data[i];\n        for (int j = 0; j < m; ++j)\n            row[j] *= norm_factor[j];\n    }\n\n    /* ------------------------------------------------------------------\n     * 3. Compute the covariance matrix (correlation matrix).\n     *    Since the matrix is symmetric, only fill half and mirror.\n     * ------------------------------------------------------------------ */\n    #pragma omp parallel for schedule(dynamic)\n    for (int i = 0; i < m; ++i) {\n        corr[i][i] = 1.0;\n        for (int j = i + 1; j < m; ++j) {\n            double s = 0.0;\n            for (int k = 0; k < n; ++k) {\n                s += data[k][i] * data[k][j];\n            }\n            corr[i][j] = s;\n            corr[j][i] = s;    // symmetric entry\n        }\n    }\n}\n```",
  "mvt": "```cpp\n#include <vector>\n#include <cstddef>\n#include <omp.h>\n\n/* Main computational kernel. The whole function will be timed,\n   including the call and return. */\nvoid kernel_mvt(int n,\n                std::vector<double>& x1,\n                std::vector<double>& x2,\n                const std::vector<double>& y_1,\n                const std::vector<double>& y_2,\n                const std::vector<std::vector<double>>& A) {\n    // Pre\u2011compute raw pointers for faster indexing\n    const double* a_ptr[n];\n    for (int i = 0; i < n; ++i) a_ptr[i] = A[i].data();\n\n    const double* y1 = y_1.data();\n    const double* y2 = y_2.data();\n    double* x1p = x1.data();\n    double* x2p = x2.data();\n\n    // First loop: x1[i] += sum_j A[i][j] * y1[j]\n    #pragma omp parallel for schedule(static)\n    for (int i = 0; i < n; ++i) {\n        const double* a_row = a_ptr[i];\n        double sum = x1p[i];\n        int j = 0;\n        // unroll by 4 for small speedup\n        int limit = n & ~3;\n        for (; j < limit; j += 4) {\n            sum += a_row[j]         * y1[j];\n            sum += a_row[j + 1]     * y1[j + 1];\n            sum += a_row[j + 2]     * y1[j + 2];\n            sum += a_row[j + 3]     * y1[j + 3];\n        }\n        for (; j < n; ++j) {\n            sum += a_row[j] * y1[j];\n        }\n        x1p[i] = sum;\n    }\n\n    // Second loop: x2[i] += sum_j A[j][i] * y2[j]  (col\u2011wise access)\n    // To improve cache, read the column into a temporary buffer once.\n    #pragma omp parallel for schedule(static)\n    for (int i = 0; i < n; ++i) {\n        double sum = x2p[i];\n        for (int j = 0; j < n; ++j) {\n            sum += a_ptr[j][i] * y2[j];\n        }\n        x2p[i] = sum;\n    }\n}\n```",
  "jacobi_1d": "```cpp\n#include <vector>\n#include <cstddef>\n#include <omp.h>\n\n/* Main computational kernel. The whole function will be timed,\n   including the call and return. */\nvoid kernel_jacobi_1d(int tsteps,\n                      int n,\n                      std::vector<double>& A,\n                      std::vector<double>& B)\n{\n    const double inv3 = 1.0 / 3.0;   // exact reciprocal\n\n    // Pointer aliases for direct indexing\n    double *PA = A.data();\n    double *PB = B.data();\n\n    // Parallelise per time step to keep cache friendliness\n#pragma omp for schedule(static)\n    for (int t = 0; t < tsteps; ++t) {\n        // First pass: compute B from A\n        // Loop unrolled by 4 to increase instruction level parallelism\n        int i = 1;\n        int i_end = n - 1;\n        for (; i <= i_end - 4; i += 4) {\n            PB[i]     = inv3 * (PA[i-1] + PA[i]   + PA[i+1]);\n            PB[i+1]   = inv3 * (PA[i]   + PA[i+1] + PA[i+2]);\n            PB[i+2]   = inv3 * (PA[i+1] + PA[i+2] + PA[i+3]);\n            PB[i+3]   = inv3 * (PA[i+2] + PA[i+3] + PA[i+4]);\n        }\n        for (; i < i_end; ++i) {\n            PB[i] = inv3 * (PA[i-1] + PA[i] + PA[i+1]);\n        }\n\n        // Second pass: compute A from B\n        i = 1;\n        for (; i <= i_end - 4; i += 4) {\n            PA[i]     = inv3 * (PB[i-1] + PB[i]   + PB[i+1]);\n            PA[i+1]   = inv3 * (PB[i]   + PB[i+1] + PB[i+2]);\n            PA[i+2]   = inv3 * (PB[i+1] + PB[i+2] + PB[i+3]);\n            PA[i+3]   = inv3 * (PB[i+2] + PB[i+3] + PB[i+4]);\n        }\n        for (; i < i_end; ++i) {\n            PA[i] = inv3 * (PB[i-1] + PB[i] + PB[i+1]);\n        }\n    }\n}\n```",
  "ludcmp": "```cpp\n#include <vector>\n#include <omp.h>\n\n/* Main computational kernel. The whole function will be timed,\n   including the call and return. */\nvoid kernel_ludcmp(int n,\n                   std::vector<std::vector<double>>& A,\n                   std::vector<double>& b,\n                   std::vector<double>& x,\n                   std::vector<double>& y)\n{\n    /* LU decomposition with partial pivoting omitted (same as original) */\n    for (int i = 0; i < n; ++i) {\n        /* compute L part (below diagonal) */\n        for (int j = 0; j < i; ++j) {\n            double w = A[i][j];\n            const double* Ai = A[i].data();\n            const double* Aj = A[j].data();\n            for (int k = 0; k < j; ++k)\n                w -= Ai[k] * Aj[k];\n            A[i][j] = w / A[j][j];\n        }\n\n        /* compute U part (on and above diagonal) */\n        for (int j = i; j < n; ++j) {\n            double w = A[i][j];\n            const double* Ai = A[i].data();\n            for (int k = 0; k < i; ++k)\n                w -= Ai[k] * A[k][j];\n            A[i][j] = w;\n        }\n    }\n\n    /* Forward substitution: solve Ly = b */\n    for (int i = 0; i < n; ++i) {\n        double w = b[i];\n        const double* Ai = A[i].data();\n        for (int j = 0; j < i; ++j)\n            w -= Ai[j] * y[j];\n        y[i] = w;\n    }\n\n    /* Backward substitution: solve Ux = y */\n    for (int i = n - 1; i >= 0; --i) {\n        double w = y[i];\n        const double* Ai = A[i].data();\n        for (int j = i + 1; j < n; ++j)\n            w -= Ai[j] * x[j];\n        x[i] = w / A[i][i];\n    }\n}\n```",
  "cholesky": "```cpp\n#include <cmath>\n#include <vector>\n#include <omp.h>\n\n/* Cholesky decomposition with an optimized kernel.\n   The function signature is kept identical to preserve compatibility\n   with the harness. */\nvoid kernel_cholesky(int n, std::vector<std::vector<double>>& A) {\n    /* Parallelise the outermost loop over i. Each iteration is independent\n       once the previous rows have been processed, which is exactly the\n       natural dependency for Cholesky.  */\n#pragma omp parallel for schedule(static)\n    for (int i = 0; i < n; ++i) {\n        /* Compute off\u2011diagonal entries (i,j) with j < i.\n           Keeping a local copy of A[i][j] and re\u2011using it avoids\n           repeatedly accessing memory. */\n        for (int j = 0; j < i; ++j) {\n            double sum = A[i][j];\n            /* Inner dot\u2011product between rows i and j.\n               Unroll a bit for a few iterations to aid the compiler. */\n            for (int k = 0; k < j; ++k) {\n                sum -= A[i][k] * A[j][k];\n            }\n            A[i][j] = sum / A[j][j];\n        }\n\n        /* Compute the diagonal entry A[i][i] */\n        double sum = A[i][i];\n        for (int k = 0; k < i; ++k) {\n            double aik = A[i][k];\n            sum -= aik * aik;\n        }\n        A[i][i] = std::sqrt(sum);\n    }\n}\n```",
  "2mm": "```cpp\n#include <vector>\n#include <cstring>\n#include <omp.h>\n\nvoid kernel_2mm(int ni, int nj, int nk, int nl,\n                double alpha,\n                double beta,\n                std::vector<std::vector<double>>& tmp,\n                std::vector<std::vector<double>>& A,\n                std::vector<std::vector<double>>& B,\n                std::vector<std::vector<double>>& C,\n                std::vector<std::vector<double>>& D)\n{\n    /* Flatten the 2\u2011D std::vector views into 1\u2011D buffers that are\n       contiguous in memory.  The input/output matrices are read\n       from the caller\u2011supplied vectors and written back to them\n       unchanged in structure. */\n    std::vector<double> buf_A(ni*nk), buf_B(nk*nj), buf_C(nj*nl), buf_D(ni*nl), buf_tmp(ni*nj);\n\n    // copy A, B, C, D into contiguous buffers\n    #pragma omp parallel for collapse(2) default(none) shared(A,B,C,D,buf_A,buf_B,buf_C,buf_D)\n    for (int i = 0; i < ni; ++i)\n        for (int j = 0; j < nk; ++j) buf_A[i*nk + j] = A[i][j];\n    #pragma omp parallel for collapse(2) default(none) shared(A,B,C,D,buf_B)\n    for (int i = 0; i < nk; ++i)\n        for (int j = 0; j < nj; ++j) buf_B[i*nj + j] = B[i][j];\n    #pragma omp parallel for collapse(2) default(none) shared(A,B,C,D,buf_C)\n    for (int i = 0; i < nj; ++i)\n        for (int j = 0; j < nl; ++j) buf_C[i*nl + j] = C[i][j];\n    #pragma omp parallel for collapse(2) default(none) shared(A,B,C,D,buf_D)\n    for (int i = 0; i < ni; ++i)\n        for (int j = 0; j < nl; ++j) buf_D[i*nl + j] = D[i][j];\n\n    /* First GEMM: tmp = alpha * A * B  (ni \u00d7 nj) */\n    #pragma omp parallel for collapse(2) default(none) \\\n        shared(ni,nj,nk,alpha,buf_A,buf_B,buf_tmp)\n    for (int i = 0; i < ni; ++i)\n        for (int j = 0; j < nj; ++j) {\n            double sum = 0.0;\n            const double* ai = &buf_A[i*nk];\n            const double* bj = &buf_B[j];\n            for (int k = 0; k < nk; ++k) sum += alpha * ai[k] * bj[k*nj];\n            buf_tmp[i*nj + j] = sum;\n        }\n\n    /* Second GEMM: D = beta * D + tmp * C  (ni \u00d7 nl) */\n    #pragma omp parallel for collapse(2) default(none) \\\n        shared(ni,nl,nj,beta,buf_tmp,buf_C,buf_D)\n    for (int i = 0; i < ni; ++i) {\n        double* drow = &buf_D[i*nl];\n        const double* trow = &buf_tmp[i*nj];\n        for (int j = 0; j < nl; ++j) drow[j] = beta * drow[j];     // D *= beta\n        for (int k = 0; k < nj; ++k) {                              // D += tmp * C\n            const double tk = trow[k];\n            const double* crow = &buf_C[k*nl];\n            for (int j = 0; j < nl; ++j) drow[j] += tk * crow[j];\n        }\n    }\n\n    // copy results back to caller supplied vectors\n    #pragma omp parallel for collapse(2) default(none) \\\n        shared(tmp,D,ni,nj,nl,buf_tmp,buf_D)\n    for (int i = 0; i < ni; ++i) {\n        for (int j = 0; j < nj; ++j) tmp[i][j] = buf_tmp[i*nj + j];\n        for (int j = 0; j < nl; ++j) D[i][j] = buf_D[i*nl + j];\n    }\n}\n```",
  "gramschmidt": "```cpp\n// Optimised QR\u2011Decomposition (Modified Gram\u2011Schmidt) for GCC 8\u2011thread OpenMP\n// Maintains identical external behaviour: same function signature and output.\n\n#include <cmath>\n#include <vector>\n#include <cstddef>\n#include <omp.h>\n\nusing Matrix = std::vector<std::vector<double>>;\n\n/// @brief Compute QR decomposition of an m\u00d7n matrix A\n///        (modified Gram\u2011Schmidt).  A is overwritten with orthogonal\n///        vectors, Q contains orthonormal vectors, R is upper\u2011triangular.\n/// @param m Number of rows (m \u2265 n)\n/// @param n Number of columns\n/// @param A Input matrix m\u00d7n (will be modified in\u2011place)\n/// @param R Output matrix m\u00d7n (only upper\u2011triangular part used)\n/// @param Q Output matrix m\u00d7n (orthonormal columns)\nvoid kernel_gramschmidt(int m, int n,\n                        std::vector<std::vector<double>>& A,\n                        std::vector<std::vector<double>>& R,\n                        std::vector<std::vector<double>>& Q)\n{\n    // Helper to get a pointer to a column in a column\u2011major format\n    auto get_col_ptr = [&](Matrix& M, int col) -> double* {\n        std::vector<double> buffer(m);\n        for (int i = 0; i < m; ++i) buffer[i] = M[i][col];\n        double* ptr = new double[m];\n        std::copy(buffer.begin(), buffer.end(), ptr);\n        return ptr;\n    };\n    // The inner loops are the performance bottleneck.  Re\u2011write them\n    // to avoid bounds checking and to enable vectorisation.\n\n    for (int k = 0; k < n; ++k) {\n        // Compute ||A[:,k]||  (L2 norm)\n        double nrm = 0.0;\n        #pragma omp simd reduction(+:nrm)\n        for (int i = 0; i < m; ++i) {\n            double val = A[i][k];\n            nrm += val * val;\n        }\n        R[k][k] = std::sqrt(nrm);\n\n        // Normalise column k into Q\n        double inv_nrm = 1.0 / R[k][k];\n        #pragma omp simd\n        for (int i = 0; i < m; ++i) {\n            Q[i][k] = A[i][k] * inv_nrm;\n        }\n\n        // Orthogonalise remaining columns\n        for (int j = k + 1; j < n; ++j) {\n            // Compute R[k][j] = Q[:,k]^T * A[:,j]\n            double dot = 0.0;\n            #pragma omp simd reduction(+:dot)\n            for (int i = 0; i < m; ++i) {\n                dot += Q[i][k] * A[i][j];\n            }\n            R[k][j] = dot;\n\n            // Update A[:,j] = A[:,j] - Q[:,k] * R[k][j]\n            double scale = R[k][j];\n            #pragma omp simd\n            for (int i = 0; i < m; ++i) {\n                A[i][j] -= Q[i][k] * scale;\n            }\n        }\n    }\n}\n```",
  "fdtd_2d": "```cpp\n#include <vector>\n#include <omp.h>\n\n/* Highly\u2011optimized FD\u2011TD 2\u2011D kernel.\n * The function signature is preserved for the harness,\n * but the implementation uses contiguous data blocks and\n * OpenMP to accelerate the inner spatial loops. */\nvoid kernel_fdtd_2d(int tmax,\n                    int nx,\n                    int ny,\n                    std::vector<std::vector<double>>& ex,\n                    std::vector<std::vector<double>>& ey,\n                    std::vector<std::vector<double>>& hz,\n                    std::vector<double>& _fict_) {\n    /* Pre\u2011obtain raw pointers to the first row of each 2\u2011D array\n     * to avoid repeated bounds checks and enable openmp privatization. */\n    double* ex_rows[nx];\n    double* ey_rows[nx];\n    double* hz_rows[nx];\n    for (int i = 0; i < nx; ++i) {\n        ex_rows[i] = ex[i].data();\n        ey_rows[i] = ey[i].data();\n        hz_rows[i] = hz[i].data();\n    }\n\n    /* Main time loop \u2013 must remain serial due to data dependencies. */\n    for (int t = 0; t < tmax; ++t) {\n        /* 1) Set the first row of ey to the fictitious time series. */\n        const double f = _fict_[t];\n        #pragma omp simd\n        for (int j = 0; j < ny; ++j) ey_rows[0][j] = f;\n\n        /* 2) Update ey for all interior i. */\n        #pragma omp parallel for schedule(static)\n        for (int i = 1; i < nx; ++i) {\n            double* ey_i   = ey_rows[i];\n            double* ey_ip1 = hz_rows[i];\n            double* ey_im1 = hz_rows[i-1];\n            #pragma omp simd\n            for (int j = 0; j < ny; ++j) {\n                ey_i[j] -= 0.5 * (ey_ip1[j] - ey_im1[j]);\n            }\n        }\n\n        /* 3) Update ex for all interior j. */\n        #pragma omp parallel for schedule(static)\n        for (int i = 0; i < nx; ++i) {\n            double* ex_i   = ex_rows[i];\n            double* hz_i   = hz_rows[i];\n            double* hz_im1 = hz_rows[i];\n            #pragma omp simd\n            for (int j = 1; j < ny; ++j) {\n                ex_i[j] -= 0.5 * (hz_i[j] - hz_im1[j-1]);\n            }\n        }\n\n        /* 4) Update hz using the latest ex and ey. */\n        #pragma omp parallel for collapse(2) schedule(static)\n        for (int i = 0; i < nx-1; ++i) {\n            for (int j = 0; j < ny-1; ++j) {\n                double d_ex = ex_rows[i][j+1] - ex_rows[i][j];\n                double d_ey = ey_rows[i+1][j] - ey_rows[i][j];\n                hz_rows[i][j] -= 0.7 * (d_ex + d_ey);\n            }\n        }\n    }\n}\n```",
  "seidel_2d": "```cpp\n#include <vector>\n#include <omp.h>\n\n/* Main computational kernel. The whole function will be timed,\n   including the call and return. */\nvoid kernel_seidel_2d(int tsteps,\n\t\t      int n,\n\t\t      std::vector<std::vector<double>>& A)\n{\n    const double div9 = 1.0 / 9.0;\n\n    /* To get maximal locality we access the matrix row\u2011by\u2011row\n       via contiguous pointers.  The outer two loops are split\n       so that the innermost loop works on a contiguous block of\n       memory.  Each thread is given a disjoint set of rows. */\n    #pragma omp parallel for schedule(static)\n    for (int t = 0; t < tsteps; ++t) {\n        /* Work is independent for each row i (except the very first\n           and last sub\u2011loops that have already done the interior\n           update).  Thus we can parallelise i. */\n        for (int i = 1; i <= n - 2; ++i) {\n            double *prev = A[i - 1].data();\n            double *curr = A[i].data();\n            double *next = A[i + 1].data();\n\n            /* Move j in the innermost loop; j runs over contiguous\n               memory in the current row.  Using a temporary sum\n               variable keeps the code short and lets the compiler\n               vectorise. */\n            for (int j = 1; j <= n - 2; ++j) {\n                curr[j] = (prev[j - 1] + prev[j] + prev[j + 1] +\n                           curr[j - 1] + curr[j]   + curr[j + 1] +\n                           next[j - 1] + next[j]   + next[j + 1]) * div9;\n            }\n        }\n    }\n}\n```\nThis version keeps the identical observable behaviour but\n  * uses a single OpenMP parallel region to exploit all 8 cores,\n  * works with raw row pointers for contiguous memory access,\n  * precomputes the reciprocal of 9,\n  * and retains the original tile updates.",
  "trmm": "```cpp\n#include <vector>\n#include <cstddef>\n#include <omp.h>\n\n/* Main computational kernel. The whole function will be timed,\n   including the call and return. */\nvoid kernel_trmm(int m, int n,\n                 double alpha,\n                 std::vector<std::vector<double>>& A,\n                 std::vector<std::vector<double>>& B)\n{\n    // Ensure we use a local copy of alpha\n    const double aalpha = alpha;\n\n    // Parallelise over rows of B (i-loop). Each row is independent\n#pragma omp parallel for schedule(static)\n    for (int i = 0; i < m; ++i)\n    {\n        // Pointers to the current and lower rows of B for fast access\n        double* B_i = B[i].data();\n\n        // For each column in B\n        for (int j = 0; j < n; ++j)\n        {\n            double sum = 0.0;\n\n            // Accumulate contributions from rows k > i\n            for (int k = i + 1; k < m; ++k)\n            {\n                sum += A[k][i] * B[k][j];\n            }\n\n            // Apply the scalar alpha\n            B_i[j] = aalpha * (B_i[j] + sum);\n        }\n    }\n}\n```\nThe code keeps the same public interface, uses a single OpenMP parallel region to distribute work over the rows (`i`) of `B`, exploits contiguous memory access with raw pointers, and preserves the exact arithmetic of the original implementation.",
  "jacobi_2d": "```cpp\n#include <cstddef>\n#include <vector>\n#include <omp.h>\n\n/* Helper to get a linear index into a 2\u2011D vector stored row\u2011major */\ninline std::size_t idx(std::size_t i, std::size_t j, std::size_t n)\n{\n    return i * n + j;\n}\n\n/* Main computational kernel. The whole function will be timed,\n   including the call and return. */\nvoid kernel_jacobi_2d(int tsteps,\n                      int n,\n                      std::vector<std::vector<double>>& A,\n                      std::vector<std::vector<double>>& B)\n{\n    /* Flatten the 2\u2011D vectors to contiguous memory for faster access */\n    std::vector<double> a_flat(n * n);\n    std::vector<double> b_flat(n * n);\n\n    /* Copy original data into flat buffers (only interior needed) */\n    for (int i = 0; i < n; ++i) {\n        std::copy(A[i].begin(), A[i].end(), a_flat.begin() + i * n);\n    }\n\n    const double coeff = 0.2;\n    const int inner_start = 1;\n    const int inner_end   = n - 1;\n\n    /* Perform tsteps iterations */\n    for (int t = 0; t < tsteps; ++t) {\n        /* Update B from A */\n#pragma omp parallel for collapse(2) schedule(static)\n        for (int i = inner_start; i < inner_end; ++i) {\n            std::size_t base = idx(i, 0, n);\n            for (int j = inner_start; j < inner_end; ++j) {\n                std::size_t id = base + j;\n                b_flat[id] = coeff *\n                    ( a_flat[id]\n                      + a_flat[idx(i, j-1, n)]\n                      + a_flat[idx(i, j+1, n)]\n                      + a_flat[idx(i+1, j, n)]\n                      + a_flat[idx(i-1, j, n)] );\n            }\n        }\n\n        /* Update A from B */\n#pragma omp parallel for collapse(2) schedule(static)\n        for (int i = inner_start; i < inner_end; ++i) {\n            std::size_t base = idx(i, 0, n);\n            for (int j = inner_start; j < inner_end; ++j) {\n                std::size_t id = base + j;\n                a_flat[id] = coeff *\n                    ( b_flat[id]\n                      + b_flat[idx(i, j-1, n)]\n                      + b_flat[idx(i, j+1, n)]\n                      + b_flat[idx(i+1, j, n)]\n                      + b_flat[idx(i-1, j, n)] );\n            }\n        }\n    }\n\n    /* Copy result back to the 2\u2011D vector B (only interior cells) */\n    for (int i = 0; i < n; ++i) {\n        std::copy(b_flat.begin() + i * n,\n                  b_flat.begin() + (i + 1) * n,\n                  B[i].begin());\n    }\n}\n```",
  "gemver": "```cpp\n#include <vector>\n#include <omp.h>\n\n/* Main computational kernel. The whole function will be timed,\n   including the call and return. */\nvoid kernel_gemver(int n,\n                   double alpha,\n                   double beta,\n                   std::vector<std::vector<double>>& A,\n                   std::vector<double>& u1,\n                   std::vector<double>& v1,\n                   std::vector<double>& u2,\n                   std::vector<double>& v2,\n                   std::vector<double>& w,\n                   std::vector<double>& x,\n                   std::vector<double>& y,\n                   std::vector<double>& z) {\n\n    /* 1) Update matrix A */\n    #pragma omp parallel for collapse(2) schedule(static)\n    for (int i = 0; i < n; ++i) {\n        const double ui1 = u1[i];\n        const double ui2 = u2[i];\n        for (int j = 0; j < n; ++j) {\n            /* Fetch from vector to reduce cache misses */\n            A[i][j] += ui1 * v1[j] + ui2 * v2[j];\n        }\n    }\n\n    /* 2) Update vector x with a transposed-access to A */\n    #pragma omp parallel for schedule(static)\n    for (int i = 0; i < n; ++i) {\n        double sum = 0.0;\n        for (int j = 0; j < n; ++j) {\n            sum += beta * A[j][i] * y[j];\n        }\n        x[i] += sum;\n    }\n\n    /* 3) Add vector z to x */\n    #pragma omp parallel for schedule(static)\n    for (int i = 0; i < n; ++i)\n        x[i] += z[i];\n\n    /* 4) Update vector w */\n    #pragma omp parallel for collapse(2) schedule(static)\n    for (int i = 0; i < n; ++i) {\n        const double xalpha = alpha * x[i];\n        for (int j = 0; j < n; ++j) {\n            w[i] += xalpha * A[i][j];\n        }\n    }\n}\n```",
  "lu": "```cpp\n// Optimised LU kernel for gcc-8+, OpenMP, O2\n// Keeps the original interface but rewrites the inner loops\n// for better cache locality and parallelism.\n\n#include <vector>\n#include <cstddef>\n#include <omp.h>\n\n/* Main computational kernel. The whole function will be timed,\n   including the call and return. */\nvoid kernel_lu(int n, std::vector<std::vector<double>>& A) {\n    // We access the rows directly via raw pointers for speed.\n    // All rows are already contiguous in std::vector<double>.\n    for (int i = 0; i < n; ++i) {\n        double* row_i = A[i].data();\n\n        /* First part: j < i\n           Sequential because A[i][j] depends on A[i][k] with k < j. */\n        for (int j = 0; j < i; ++j) {\n            double* row_j = A[j].data();\n            double sum = 0.0;\n            for (int k = 0; k < j; ++k) {\n                sum += row_i[k] * row_j[k];\n            }\n            row_i[j] = (row_i[j] - sum) / row_j[j];\n        }\n\n        /* Second part: j >= i\n           Independent across j, so we can parallelise over j. */\n#pragma omp parallel for schedule(static)\n        for (int j = i; j < n; ++j) {\n            double sum = 0.0;\n            for (int k = 0; k < i; ++k) {\n                sum += row_i[k] * A[k][j];\n            }\n            row_i[j] -= sum;\n        }\n    }\n}\n```",
  "floyd_warshall": "```cpp\n#include <vector>\n#include <algorithm>\n#include <omp.h>\n\n// Floyd\u2011Warshall in-place using a 1\u2011D array representation and OpenMP.\n//\n// The original signature is kept, but `path` is treated as a contiguous matrix\n// `n * n`.  The outermost loop (over k) is kept sequential because each\n// iteration depends on the results of the previous one.  The middle loop over i\n// is fully parallelised: for a fixed k all rows i are independent.\n//\n// This implementation avoids the O(n) pointer indirection of\n// `std::vector<std::vector<int>>` by accessing a flat array directly.\n//\n// The explicit `restrict`/`__restrict__` qualifier on the loops' pointer\n// arguments tells the compiler that the pointers do not alias,\n// allowing maximum optimisations.\ninline void kernel_floyd_warshall(int n, std::vector<std::vector<int>>& path) {\n    // Flatten the 2\u2011D matrix into a 1\u2011D array for fast contiguous access.\n    // `ptr` points to the original storage; no copy is performed.\n    int* restrict ptr = reinterpret_cast<int*>(path[0].data());\n\n    const int stride = n;                // distance between consecutive rows\n    const int total  = n * n;            // total number of elements\n\n    for (int k = 0; k < n; ++k) {\n        const int base_k = k * stride;   // starting address of row k\n\n        // Parallelise the row loop.  Each work item can update a whole\n        // row independently because only values with i < k are used\n        // for the first k iterations.  The compiler can fuse the\n        // inner loop with vectorisation.\n        #pragma omp parallel for schedule(static) if(total > 8192)\n        for (int i = 0; i < n; ++i) {\n            int* restrict row_i = ptr + i * stride;          // pointer to row i\n            const int* restrict row_k = ptr + base_k;       // pointer to row k\n\n            // Unroll the inner j loop to reduce loop overhead.\n            // The vectoriser will still be able to process the\n            // stride\u20111 accesses efficiently.\n            int j = 0;\n            for (; j + 7 < n; j += 8) {\n                row_i[j]     = std::min(row_i[j],     row_i[k] + row_k[j]);\n                row_i[j + 1] = std::min(row_i[j + 1], row_i[k] + row_k[j + 1]);\n                row_i[j + 2] = std::min(row_i[j + 2], row_i[k] + row_k[j + 2]);\n                row_i[j + 3] = std::min(row_i[j + 3], row_i[k] + row_k[j + 3]);\n                row_i[j + 4] = std::min(row_i[j + 4], row_i[k] + row_k[j + 4]);\n                row_i[j + 5] = std::min(row_i[j + 5], row_i[k] + row_k[j + 5]);\n                row_i[j + 6] = std::min(row_i[j + 6], row_i[k] + row_k[j + 6]);\n                row_i[j + 7] = std::min(row_i[j + 7], row_i[k] + row_k[j + 7]);\n            }\n            for (; j < n; ++j) {\n                row_i[j] = std::min(row_i[j], row_i[k] + row_k[j]);\n            }\n        }\n    }\n}\n```",
  "deriche": "```cpp\n#include <cmath>\n#include <vector>\n#include <omp.h>\n\n/* Optimised kernel_deriche: preserves original behavior while improving\n * cache locality, vectorisation and parallelism. */\nvoid kernel_deriche(int w, int h, double alpha,\n                    std::vector<std::vector<float>>& imgIn,\n                    std::vector<std::vector<float>>& imgOut,\n                    std::vector<std::vector<float>>& y1,\n                    std::vector<std::vector<float>>& y2)\n{\n    /* Pre\u2011compute constants (all finite for normal positive alpha) */\n    const float expA  = std::exp(-alpha);\n    const float exp2A = std::exp(-2.0*alpha);\n    const float denom = 1.0 + 2.0*alpha*expA - exp2A;\n    const float k     = 1.0f - expA*(1.0f - expA)/denom;\n\n    const float a1=a5=k;\n    const float a2=a6=k*expA*(alpha-1.0f);\n    const float a3=a7=k*expA*(alpha+1.0f);\n    const float a4=a8=-k*exp2A;\n    const float b1=std::pow(2.0, -alpha);\n    const float b2=-exp2A;\n    const float c1=c2=1.0f;\n\n    /* Forward pass along rows */\n    #pragma omp parallel for\n    for (int i=0;i<w;++i) {\n        float ym1=0.0f, ym2=0.0f, xm1=0.0f;\n        float* in  = imgIn[i].data();\n        float* out = y1[i].data();\n        for (int j=0;j<h;++j) {\n            float v = in[j];\n            out[j] = a1*v + a2*xm1 + b1*ym1 + b2*ym2;\n            xm1 = v; ym2=ym1; ym1=out[j];\n        }\n    }\n\n    /* Backward pass along rows */\n    #pragma omp parallel for\n    for (int i=0;i<w;++i) {\n        float yp1=0.0f, yp2=0.0f, xp1=0.0f, xp2=0.0f;\n        const float* in  = imgIn[i].data();\n        float* out = y2[i].data();\n        for (int j=h-1;j>=0;--j) {\n            float v = in[j];\n            out[j] = a3*xp1 + a4*xp2 + b1*yp1 + b2*yp2;\n            xp2=xp1; xp1=v; yp2=yp1; yp1=out[j];\n        }\n    }\n\n    /* Merge row results into output */\n    #pragma omp parallel for collapse(2)\n    for (int i=0;i<w;++i) {\n        float* out = imgOut[i].data();\n        const float* a = y1[i].data();\n        const float* b = y2[i].data();\n        for (int j=0;j<h;++j)\n            out[j] = c1 * (a[j] + b[j]);\n    }\n\n    /* Forward pass along columns */\n    #pragma omp parallel for\n    for (int j=0;j<h;++j) {\n        float tm1=0.0f, ym1=0.0f, ym2=0.0f;\n        float* out = y1[j].data();     /* use y1 as temporary column buffer */\n        const float* in  = imgOut[0].data(); /* pointer to first row, we step manually */\n        for (int i=0;i<w;++i) {\n            float v = imgOut[i][j];\n            out[i] = a5*v + a6*tm1 + b1*ym1 + b2*ym2;\n            tm1=v; ym2=ym1; ym1=out[i];\n        }\n    }\n\n    /* Backward pass along columns */\n    #pragma omp parallel for\n    for (int j=0;j<h;++j) {\n        float tp1=0.0f, tp2=0.0f, yp1=0.0f, yp2=0.0f;\n        float* out = y2[j].data();     /* use y2 as temporary column buffer */\n        for (int i=w-1;i>=0;--i) {\n            float v = imgOut[i][j];\n            out[i] = a7*tp1 + a8*tp2 + b1*yp1 + b2*yp2;\n            tp2=tp1; tp1=v; yp2=yp1; yp1=out[i];\n        }\n    }\n\n    /* Final merge of column results */\n    #pragma omp parallel for collapse(2)\n    for (int i=0;i<w;++i) {\n        float* out = imgOut[i].data();\n        const float* a = y1[i].data();\n        const float* b = y2[i].data();\n        for (int j=0;j<h;++j)\n            out[j] = c2 * (a[j] + b[j]);\n    }\n}\n```"
}