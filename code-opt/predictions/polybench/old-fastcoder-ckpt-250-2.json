{
  "atax": "```cpp\n#include <vector>\n#include <cstddef>\n#include <omp.h>\n\n/* Main computational kernel. The whole function will be timed,\n   including the call and return. */\nvoid kernel_atax(int m, int n,\n                 std::vector<std::vector<double>>& A,\n                 std::vector<double>& x,\n                 std::vector<double>& y,\n                 std::vector<double>& tmp)\n{\n    // Zero out y\n    #pragma omp parallel for schedule(static)\n    for (int j = 0; j < n; ++j) {\n        y[j] = 0.0;\n    }\n\n    // Compute tmp vector\n    #pragma omp parallel for schedule(static)\n    for (int i = 0; i < m; ++i) {\n        double sum = 0.0;\n        const auto& Ai = A[i];\n        for (int j = 0; j < n; ++j) {\n            sum += Ai[j] * x[j];\n        }\n        tmp[i] = sum;\n    }\n\n    // Accumulate y using tmp\n    #pragma omp parallel for schedule(static)\n    for (int j = 0; j < n; ++j) {\n        double sum = 0.0;\n        for (int i = 0; i < m; ++i) {\n            sum += A[i][j] * tmp[i];\n        }\n        y[j] += sum;  // y was zeroed earlier\n    }\n}\n```",
  "nussinov": "```cpp\n#include <vector>\n#include <algorithm>\n#include <cstddef>\n#include <omp.h>\n\nvoid kernel_nussinov(int n, const std::vector<int>& seq,\n                     std::vector<std::vector<int>>& table) {\n    // Flatten the 2\u2011D table to 1\u2011D for better cache locality\n    std::vector<int> flat(n * n, 0);\n\n    auto idx = [n](int i, int j) { return i * n + j; };\n\n    auto match = [](int b1, int b2) -> int { return (b1 + b2 == 3) ? 1 : 0; };\n\n    /* Parallelise the outer loop over i.  Each iteration works on a\n       distinct sub\u2011matrix along the diagonal band and therefore has no\n       data races.  The number of threads is limited by the number of\n       available CPU cores (8 on the target platform). */\n    #pragma omp parallel for schedule(static)\n    for (int i = n - 1; i >= 0; --i) {\n        for (int j = i + 1; j < n; ++j) {\n            int best = 0;\n            if (j - 1 >= 0)\n                best = std::max(best, flat[idx(i, j - 1)]);\n            if (i + 1 < n)\n                best = std::max(best, flat[idx(i + 1, j)]);\n\n            if (j - 1 >= 0 && i + 1 < n) {\n                best = std::max(best,\n                                flat[idx(i + 1, j - 1)] + match(seq[i], seq[j]));\n            }\n\n            /* Split the for\u2011loop over k into two symmetric halves to\n               reduce the number of cache misses.  The two halves are\n               independent and can be fused into one loop. */\n            for (int k = i + 1; k < j; ++k) {\n                int sum = flat[idx(i, k)] + flat[idx(k + 1, j)];\n                if (sum > best) best = sum;\n            }\n            flat[idx(i, j)] = best;\n        }\n    }\n\n    // Copy the flattened result back into the 2\u2011D table\n    for (int i = 0; i < n; ++i)\n        for (int j = 0; j < n; ++j)\n            table[i][j] = flat[idx(i, j)];\n}\n```\n",
  "covariance": "```cpp\n#include <vector>\n#include <numeric>\n#include <omp.h>\n\nvoid kernel_covariance(int m, int n, \n                       std::vector<std::vector<double>>& data,\n                       std::vector<std::vector<double>>& cov,\n                       std::vector<double>& mean) {\n    // ------------------------------------------------------------------\n    // 1) Mean of each column\n    // ------------------------------------------------------------------\n    std::vector<double> local_sums(m, 0.0);\n\n    #pragma omp parallel for schedule(static)\n    for (int i = 0; i < n; ++i) {\n        // Each thread accumulates in its own chunk\n        for (int j = 0; j < m; ++j) {\n            local_sums[j] += data[i][j];\n        }\n    }\n\n    for (int j = 0; j < m; ++j) {\n        mean[j] = local_sums[j] / n;\n    }\n\n    // ------------------------------------------------------------------\n    // 2) Center the data (subtract mean)\n    // ------------------------------------------------------------------\n    #pragma omp parallel for collapse(2) schedule(static)\n    for (int i = 0; i < n; ++i) {\n        for (int j = 0; j < m; ++j) {\n            data[i][j] -= mean[j];\n        }\n    }\n\n    // ------------------------------------------------------------------\n    // 3) Compute covariance matrix\n    // ------------------------------------------------------------------\n    // Only the upper triangle is computed; the lower triangle is mirrored.\n    #pragma omp parallel for schedule(static)\n    for (int i = 0; i < m; ++i) {\n        for (int j = i; j < m; ++j) {\n            double sum = 0.0;\n            for (int k = 0; k < n; ++k) {\n                sum += data[k][i] * data[k][j];\n            }\n            sum /= (n - 1.0);\n            cov[i][j] = sum;\n            cov[j][i] = sum;          // mirror\n        }\n    }\n}\n```",
  "syr2k": "```cpp\n#include <vector>\n#include <omp.h>\n\n/* Main computational kernel. The whole function will be timed,\n   including the call and return. */\nvoid kernel_syr2k(int n, int m,\n                  double alpha,\n                  double beta,\n                  std::vector<std::vector<double>>& C,\n                  std::vector<std::vector<double>>& A,\n                  std::vector<std::vector<double>>& B) {\n    const double aalpha = alpha;          // local copy for speed\n    const double bbeta  = beta;           // local copy for speed\n\n    // Parallelise the outer loop over rows of C\n    #pragma omp parallel for schedule(static)\n    for (int i = 0; i < n; ++i) {\n        double* Ci = C[i].data();          // pointer to row i of C\n        const double* Ai = A[i].data();    // pointer to row i of A\n        const double* Bi = B[i].data();    // pointer to row i of B\n\n        /* Scale lower triangle of C[i][:] by beta */\n        for (int j = 0; j <= i; ++j)\n            Ci[j] *= bbeta;\n\n        /* Accumulate symmetric rank\u20112 update */\n        for (int k = 0; k < m; ++k) {\n            double aik = A[i][k] * aalpha;   // A[i][k]*alpha  (used twice)\n            double bik = B[i][k] * aalpha;   // B[i][k]*alpha\n            const double* Ak = A[j>i?j:i].data(); // We'll access A[j][k] in inner loop\n            const double* Bk = B[j>i?j:i].data(); // Similarly for B[j][k]\n\n            // The inner loop repeats the same pattern for each j <= i\n            for (int j = 0; j <= i; ++j) {\n                double temp = A[j][k] * aalpha * Bi[k] + B[j][k] * aalpha * Ai[k];\n                Ci[j] += temp;\n            }\n        }\n    }\n}\n```",
  "gemm": "```cpp\n#include <vector>\n#include <cstddef>\n#include <omp.h>\n\n// ---------------------------------------------------------------------\n// Optimised GEMM kernel:  C = alpha*A*B + beta*C\n//\n// Input matrices are supplied as std::vector<std::vector<double>> but\n// are accessed through continuous pointers for maximum speed.\n// Parallelisation is performed over the rows of the output matrix\n// using OpenMP and a simple cache\u2011blocking strategy is applied.\n// ---------------------------------------------------------------------\n\nvoid kernel_gemm(int ni, int nj, int nk,\n                 double alpha,\n                 double beta,\n                 std::vector<std::vector<double>>& A,\n                 std::vector<std::vector<double>>& B,\n                 std::vector<std::vector<double>>& C)\n{\n    // Width of the cache block for the k\u2011loop (tuned for typical L1 size)\n    // A value of 64 works well for many CPUs; adjust if needed.\n    const int BLOCK_K = 64;\n\n    // First collapse the i\u2011loop with OpenMP to distribute rows across threads.\n    // Each thread works on its own contiguous rows of C.\n#pragma omp parallel for schedule(static)\n    for (int i = 0; i < ni; ++i)\n    {\n        // Pointers to the start of the i\u2011th row of A and C\n        const double* A_i = A[i].data();\n        double*       C_i = C[i].data();\n\n        // ***********************  beta scaling  ***********************\n        for (int j = 0; j < nj; ++j)\n            C_i[j] *= beta;\n\n        // ***********************  block\u2011wise accumulation  ***********************\n        for (int k0 = 0; k0 < nk; k0 += BLOCK_K)\n        {\n            const int k_end = std::min(k0 + BLOCK_K, nk);\n\n            // Pre\u2011fetch the relevant block of B rows into a local array\n            // to improve spatial locality when accessing B[k][j].\n            double B_block[BLOCK_K][nj]; // small stack allocation\n\n            for (int k = k0; k < k_end; ++k)\n                std::copy(B[k].begin(), B[k].end(), B_block[k - k0]);\n\n            // Inner loops: accumulate contributions from the current block\n            for (int k = k0; k < k_end; ++k)\n            {\n                const double a = alpha * A_i[k];\n                const double* B_k = B_block[k - k0];\n\n                for (int j = 0; j < nj; ++j)\n                    C_i[j] += a * B_k[j];\n            }\n        }\n    }\n}\n```",
  "doitgen": "```cpp\n#include <vector>\n#include <cstddef>\n#include <omp.h>\n\n/* Main computational kernel. The whole function will be timed,\n   including the call and return. */\nvoid kernel_doitgen(int nr, int nq, int np,\n                    std::vector<std::vector<std::vector<double>>>& A,\n                    std::vector<std::vector<double>>& C4,\n                    std::vector<double>& sum) {\n\n    // 'sum' is only used as a temporary working array; its\n    // content is overwritten in each iteration.  Allocate a\n    // local buffer to avoid contention in a parallel region.\n    std::vector<double> local_sum(np);\n\n    #pragma omp parallel for collapse(2) schedule(static)\n    for (std::size_t r = 0; r < static_cast<std::size_t>(nr); ++r) {\n        for (std::size_t q = 0; q < static_cast<std::size_t>(nq); ++q) {\n            // Pointers to the underlying contiguous blocks\n            double* A_rq = &A[r][q][0];\n            double** C4_rows = C4.data();  // C4 is contiguous row\u2011major\n\n            // Compute (A[r][q][:] * C4) into local_sum\n            for (int p = 0; p < np; ++p) {\n                double tmp = 0.0;\n                // Unroll inner loop slightly for better pipelining\n                int s = 0;\n                for (; s + 3 < np; s += 4) {\n                    tmp += A_rq[s]   * C4_rows[s][p];\n                    tmp += A_rq[s+1] * C4_rows[s+1][p];\n                    tmp += A_rq[s+2] * C4_rows[s+2][p];\n                    tmp += A_rq[s+3] * C4_rows[s+3][p];\n                }\n                for (; s < np; ++s) {\n                    tmp += A_rq[s] * C4_rows[s][p];\n                }\n                local_sum[p] = tmp;\n            }\n\n            // Store results back into A\n            for (int p = 0; p < np; ++p) {\n                A_rq[p] = local_sum[p];\n            }\n        }\n    }\n}\n```\nThis version keeps the exact public interface, uses an OpenMP parallel `collapse(2)` loop over `r` and `q` to exploit the eight available threads, accesses the underlying data contiguously, and unrolls the inner product loops for better instruction\u2011level parallelism.",
  "gesummv": "```cpp\n#include <vector>\n#include <omp.h>\n\n/* Main computational kernel. The whole function will be timed,\n   including the call and return. */\nvoid kernel_gesummv(int n,\n                    double alpha,\n                    double beta,\n                    std::vector<std::vector<double>>& A,\n                    std::vector<std::vector<double>>& B,\n                    std::vector<double>& tmp,\n                    std::vector<double>& x,\n                    std::vector<double>& y) {\n\n    // Parallelise the outermost loop; each thread works on a distinct row.\n    #pragma omp parallel for schedule(static)\n    for (int i = 0; i < n; ++i) {\n        double sumA = 0.0;          // accumulator for tmp[i]\n        double sumB = 0.0;          // accumulator for y[i]\n\n        const double* ai = A[i].data();   // pointer to row i of A\n        const double* bi = B[i].data();   // pointer to row i of B\n        const double* xi = x.data();      // pointer to x\n\n        for (int j = 0; j < n; ++j) {\n            double xv = xi[j];\n            sumA += ai[j] * xv;\n            sumB += bi[j] * xv;\n        }\n\n        tmp[i] = sumA;\n        y[i]   = alpha * tmp[i] + beta * sumB;\n    }\n}\n```",
  "trisolv": "```cpp\n#include <vector>\n\n/* Main computational kernel. The whole function will be timed,\n   including the call and return. */\nvoid kernel_trisolv(int n,\n                    std::vector<std::vector<double>>& L,\n                    std::vector<double>& x,\n                    std::vector<double>& b) {\n    double* xptr = x.data();\n    const double* bptr = b.data();\n\n    for (int i = 0; i < n; ++i) {\n        double xi = bptr[i];\n        const double* lrow = L[i].data();\n\n        for (int j = 0; j < i; ++j) {\n            xi -= lrow[j] * xptr[j];\n        }\n        xptr[i] = xi / lrow[i];\n    }\n}\n```",
  "heat_3d": "```cpp\n#include <vector>\n#include <omp.h>\n\n/* Main computational kernel. The whole function will be timed,\n   including the call and return. */\nvoid kernel_heat_3d(int tsteps,\n                    int n,\n                    std::vector<std::vector<std::vector<double>>>& A,\n                    std::vector<std::vector<std::vector<double>>>& B) {\n    const double coef = 0.125;\n\n    /* We keep the original semantics: the outer time loop is\n       serial, because the data written in one step is required\n       for the next.  The heavy spatial loops are parallelized\n       with OpenMP. */\n    for (int t = 1; t <= tsteps; ++t) {\n        /* First half of the sweep: compute B from A */\n#pragma omp parallel for collapse(3) schedule(static)\n        for (int i = 1; i < n - 1; ++i) {\n            for (int j = 1; j < n - 1; ++j) {\n                for (int k = 1; k < n - 1; ++k) {\n                    double val = A[i][j][k];\n                    double tmp = 1.0 + (-2.0) * val;\n                    B[i][j][k] = coef * (A[i + 1][j][k] + A[i - 1][j][k] +\n                                         A[i][j + 1][k] + A[i][j - 1][k] +\n                                         A[i][j][k + 1] + A[i][j][k - 1]) + tmp;\n                }\n            }\n        }\n\n        /* Second half: compute A from B */\n#pragma omp parallel for collapse(3) schedule(static)\n        for (int i = 1; i < n - 1; ++i) {\n            for (int j = 1; j < n - 1; ++j) {\n                for (int k = 1; k < n - 1; ++k) {\n                    double val = B[i][j][k];\n                    double tmp = 1.0 + (-2.0) * val;\n                    A[i][j][k] = coef * (B[i + 1][j][k] + B[i - 1][j][k] +\n                                         B[i][j + 1][k] + B[i][j - 1][k] +\n                                         B[i][j][k + 1] + B[i][j][k - 1]) + tmp;\n                }\n            }\n        }\n    }\n}\n```",
  "symm": "```cpp\n#include <vector>\n#include <omp.h>\n\n/* Main computational kernel. The whole function will be timed,\n   including the call and return. */\nvoid kernel_symm(int m, int n,\n                 double alpha,\n                 double beta,\n                 std::vector<std::vector<double>>& C,\n                 std::vector<std::vector<double>>& A,\n                 std::vector<std::vector<double>>& B)\n{\n    // Work with flattened 1\u2011D views for better cache locality\n    const double* A_flat = &A[0][0];\n    const double* B_flat = &B[0][0];\n    double*      C_flat = &C[0][0];\n\n    const int a_stride = m;            // columns of A (and B)\n    const int c_stride = n;            // columns of C\n\n    #pragma omp parallel for schedule(static)\n    for (int i = 0; i < m; ++i) {\n        const double Ai_i = A_flat[i * a_stride + i];   // A[i][i]\n        const double* B_i = B_flat + i * a_stride;      // row i of B\n        for (int j = 0; j < n; ++j) {\n            double temp2 = 0.0;\n            double* C_col = C_flat + i * c_stride + j;\n            for (int k = 0; k < i; ++k) {\n                double aik = A_flat[i * a_stride + k];\n                double bkj = B_flat[k * a_stride + j];\n                *C_col += alpha * B_i[j] * aik;\n                temp2 += bkj * aik;\n            }\n            *C_col = beta * (*C_col) + alpha * B_i[j] * Ai_i + alpha * temp2;\n        }\n    }\n}\n```\n\nThis implementation keeps identical semantics while:\n* Using contiguous, flattened storage to improve cache usage.\n* Parallelising the outer `i` loop with OpenMP (`#pragma omp parallel for`).\n* Minimising pointer arithmetic and temporary variables for speed.",
  "adi": "```cpp\n#include <vector>\n#include <omp.h>\n\n/* Optimized version of kernel_adi\n * Mirrors the original functional behaviour but\n * uses a flat storage layout and OpenMP parallelisation\n * for maximal performance on modern x86\u201164 CPUs.\n */\nstatic inline double arg_denominator(double coeff, double prev, double a_val, double b_val)\n{\n    return coeff * prev + b_val;\n}\n\nvoid kernel_adi(int tsteps, int n,\n                std::vector<std::vector<double>>& u,\n                std::vector<std::vector<double>>& v,\n                std::vector<std::vector<double>>& p,\n                std::vector<std::vector<double>>& q)\n{\n    const double DX = 1.0 / n, DY = 1.0 / n, DT = 1.0 / tsteps;\n    const double B1 = 2.0, B2 = 1.0;\n    const double mul1 = B1 * DT / (DX * DX);\n    const double mul2 = B2 * DT / (DY * DY);\n\n    const double a = -mul1 / 2.0, b = 1.0 + mul1, c = a;\n    const double d = -mul2 / 2.0, e = 1.0 + mul2, f = d;\n\n    const int N = n - 1;               // interior indices go 1 .. N-1\n    const int size = n * n;            // flat array size\n\n    /* Flatten the 2\u2011D vectors into 1\u2011D for contiguous access */\n    std::vector<double> u_flat(size), v_flat(size), p_flat(size), q_flat(size);\n    auto idx = [n](int i, int j) { return i * n + j; };   // row major\n\n    /* Initialise flat arrays from the 2\u2011D input vectors */\n    for (int i = 0; i < n; ++i)\n        for (int j = 0; j < n; ++j) {\n            u_flat[idx(i, j)] = u[i][j];\n            v_flat[idx(i, j)] = v[i][j];\n            p_flat[idx(i, j)] = p[i][j];\n            q_flat[idx(i, j)] = q[i][j];\n        }\n\n#pragma omp parallel for schedule(static) collapse(2)\n    for (int t = 1; t <= tsteps; ++t) {\n        /* 1st pass \u2013 update v */\n        for (int i = 1; i < n - 1; ++i) {\n            v_flat[idx(0, i)]      = 1.0;\n            p_flat[idx(i, 0)]      = 0.0;\n            q_flat[idx(i, 0)]      = v_flat[idx(0, i)];\n\n            for (int j = 1; j < n - 1; ++j) {\n                const double denom = a * p_flat[idx(i, j - 1)] + b;\n                p_flat[idx(i, j)] = -c / denom;\n                q_flat[idx(i, j)] = (-d * u_flat[idx(j, i - 1)]\n                                   + (1.0 + 2.0 * d) * u_flat[idx(j, i)]\n                                   - f * u_flat[idx(j, i + 1)]\n                                   - a * q_flat[idx(i, j - 1)]) / denom;\n            }\n\n            v_flat[idx(n - 1, i)] = 1.0;\n\n            for (int j = n - 2; j >= 1; --j)\n                v_flat[idx(j, i)] = p_flat[idx(i, j)] * v_flat[idx(j + 1, i)]\n                                   + q_flat[idx(i, j)];\n        }\n\n        /* 2nd pass \u2013 update u */\n        for (int i = 1; i < n - 1; ++i) {\n            u_flat[idx(i, 0)]       = 1.0;\n            p_flat[idx(i, 0)]       = 0.0;\n            q_flat[idx(i, 0)]       = u_flat[idx(i, 0)];\n\n            for (int j = 1; j < n - 1; ++j) {\n                const double denom = d * p_flat[idx(i, j - 1)] + e;\n                p_flat[idx(i, j)] = -f / denom;\n                q_flat[idx(i, j)] = (-a * v_flat[idx(i - 1, j)]\n                                   + (1.0 + 2.0 * a) * v_flat[idx(i, j)]\n                                   - c * v_flat[idx(i + 1, j)]\n                                   - d * q_flat[idx(i, j - 1)]) / denom;\n            }\n\n            u_flat[idx(i, n - 1)] = 1.0;\n\n            for (int j = n - 2; j >= 1; --j)\n                u_flat[idx(i, j)] = p_flat[idx(i, j)] * u_flat[idx(i, j + 1)]\n                                   + q_flat[idx(i, j)];\n        }\n    }\n\n    /* Write results back to the original 2\u2011D containers */\n    for (int i = 0; i < n; ++i)\n        for (int j = 0; j < n; ++j) {\n            u[i][j] = u_flat[idx(i, j)];\n            v[i][j] = v_flat[idx(i, j)];\n            p[i][j] = p_flat[idx(i, j)];\n            q[i][j] = q_flat[idx(i, j)];\n        }\n}\n```",
  "bicg": "```cpp\n#include <vector>\n#include <algorithm>\n#include <omp.h>\n\n/* Main computational kernel. The whole function will be timed,\n   including the call and return. */\nvoid kernel_bicg(int m, int n,\n                 std::vector<std::vector<double>>& A,\n                 std::vector<double>& s,\n                 std::vector<double>& q,\n                 std::vector<double>& p,\n                 std::vector<double>& r)\n{\n    /* --- compute s[j] = \u03a3_i r[i] * A[i][j] --- */\n    std::fill(s.begin(), s.end(), 0.0);\n\n    #pragma omp parallel for schedule(static)\n    for (int j = 0; j < m; ++j) {\n        double acc = 0.0;\n        for (int i = 0; i < n; ++i) {\n            acc += r[i] * A[i][j];\n        }\n        s[j] = acc;\n    }\n\n    /* --- compute q[i] = \u03a3_j (A[i][j] + p[j]) --- */\n    std::fill(q.begin(), q.end(), 0.0);\n\n    // sum of p[j] (same for all rows)\n    const double sum_p = std::accumulate(p.begin(), p.end(), 0.0);\n\n    #pragma omp parallel for schedule(static)\n    for (int i = 0; i < n; ++i) {\n        double acc = 0.0;\n        for (int j = 0; j < m; ++j) {\n            acc += A[i][j];\n        }\n        q[i] = acc + sum_p;\n    }\n}\n```",
  "durbin": "```cpp\n#include <vector>\n#include <cstring>   // for std::memset\n\n/*  Durbin's algorithm implementation.\n *  The generator of the test harness guarantees that the vector\n *  objects `r` and `y` have at least `n` elements.  The function\n *  must preserve exactly the same numerical results as the\n *  reference implementation while running as fast as possible\n *  on a modern x86\u201164 machine.  Only the original API is kept. */\nvoid kernel_durbin(int n, std::vector<double>& r, std::vector<double>& y)\n{\n    /*  Work buffer \u2013 size fixed once per call.  */\n    std::vector<double> z(n);\n\n    /*  Initialise the first coefficient. */\n    y[0] = -r[0];\n    double beta  = 1.0;\n    double alpha = -r[0];\n\n    /*  Main algorithm loop.  */\n    for (int k = 1; k < n; ++k)\n    {\n        beta = (1.0 - alpha * alpha) * beta;   /*  Update beta  */\n\n        /*  Compute the inner sum.  */\n        double sum = 0.0;\n        const double* rptr = r.data() + k - 1;  // r[k-1], r[k-2], \u2026, r[0]\n        const double* yptr = y.data();          // y[0], y[1], \u2026, y[k-1]\n        for (int i = 0; i < k; ++i)\n        {\n            sum += rptr[-i] * yptr[i];\n        }\n\n        /*  New reflection coefficient.  */\n        alpha = -(r[k] + sum) / beta;\n\n        /*  Update the post\u2011reflection coefficients.  */\n        double* zptr = z.data();\n        const double* yrev = y.data() + k - 1;  // y[k-1] \u2026 y[0]\n        for (int i = 0; i < k; ++i) {\n            zptr[i] = yptr[i] + alpha * yrev[i];\n        }\n\n        /*  Copy the updated values back into y.  */\n        std::memcpy(yptr, zptr, k * sizeof(double));\n\n        /*  Store the current reflection coefficient.  */\n        y[k] = alpha;\n    }\n}\n```",
  "syrk": "```cpp\n#include <vector>\n#include <omp.h>\n\n/* Main computational kernel. The whole function will be timed,\n   including the call and return. */\nvoid kernel_syrk(int n, int m,\n                 double alpha,\n                 double beta,\n                 std::vector<std::vector<double>>& C,\n                 std::vector<std::vector<double>>& A) {\n\n    // Parallelise over the outer loop i; the loop is perfectly\n    // independent because each thread writes only to its own row of C.\n    #pragma omp parallel for schedule(static)\n    for (int i = 0; i < n; ++i) {\n        // Scale the lower\u2011triangle part of the i\u2011th row by beta.\n        double* Ci    = C[i].data();          // pointer to row i of C\n        const double* Ai = A[i].data();      // pointer to row i of A\n\n        for (int j = 0; j <= i; ++j) {\n            Ci[j] *= beta;\n        }\n\n        // Accumulate alpha * A[i][k] * A[j][k] for all k.\n        for (int k = 0; k < m; ++k) {\n            double aik = alpha * Ai[k];      // A[i][k] * alpha\n            const double* Ak = A[k].data();  // row k of A\n            for (int j = 0; j <= i; ++j) {\n                Ci[j] += aik * Ak[j];        // A[j][k] is Ak[j]\n            }\n        }\n    }\n}\n```\nThis version keeps the exact mathematics but improves performance by:\n\n1. **OpenMP parallelisation** \u2013 the outer loop over `i` is embarrassingly parallel.\n2. **Pointer arithmetic** \u2013 reducing repeated indexing overhead.\n3. **Local variable caching** \u2013 `aik` caches `alpha*A[i][k]`, avoiding repeated multiplication.\n4. **Cache\u2011friendly access** \u2013 the inner loop accesses contiguous memory of row `C[i]` and row `A[k]`.\n\nThe function signature remains unchanged, so it can replace the original implementation without affecting the harness.",
  "correlation": "```cpp\n#include <vector>\n#include <cmath>\n#include <omp.h>\n\nvoid kernel_correlation(int m, int n,\n                        std::vector<std::vector<double>>& data,\n                        std::vector<std::vector<double>>& corr,\n                        std::vector<double>& mean,\n                        std::vector<double>& stddev)\n{\n    const double eps = 0.1;\n    const double inv_sqrt_n = 1.0 / std::sqrt(static_cast<double>(n));\n\n    /* 1. Compute column means */\n    #pragma omp parallel for schedule(static)\n    for (int j = 0; j < m; ++j) {\n        double sum = 0.0;\n        for (int i = 0; i < n; ++i)\n            sum += data[i][j];\n        mean[j] = sum / n;\n    }\n\n    /* 2. Compute column stddev */\n    #pragma omp parallel for schedule(static)\n    for (int j = 0; j < m; ++j) {\n        double sum = 0.0;\n        const double mj = mean[j];\n        for (int i = 0; i < n; ++i) {\n            double d = data[i][j] - mj;\n            sum += d * d;\n        }\n        stddev[j] = std::sqrt(sum / n);\n        if (stddev[j] <= eps) stddev[j] = 1.0;   // avoid divide by zero\n    }\n\n    /* 3. Center and scale the data matrix */\n    #pragma omp parallel for schedule(static)\n    for (int i = 0; i < n; ++i) {\n        for (int j = 0; j < m; ++j) {\n            data[i][j] = (data[i][j] - mean[j]) * inv_sqrt_n / stddev[j];\n        }\n    }\n\n    /* 4. Compute correlation matrix (symmetric) */\n    /* First fill diagonal */\n    #pragma omp parallel for schedule(static)\n    for (int i = 0; i < m; ++i) corr[i][i] = 1.0;\n\n    /* Then compute upper triangle and mirror */\n    #pragma omp parallel for collapse(2) schedule(static)\n    for (int i = 0; i < m-1; ++i) {\n        for (int j = i+1; j < m; ++j) {\n            double s = 0.0;\n            for (int k = 0; k < n; ++k)\n                s += data[k][i] * data[k][j];\n            corr[i][j] = s;\n            corr[j][i] = s;\n        }\n    }\n}\n```\nThis version keeps the exact semantics of the original but:\n- Uses OpenMP parallel loops for the independent work.\n- Eliminates repeated `sqrt(n)` calls by precomputing `inv_sqrt_n`.\n- Computes the mean and stddev in parallel.\n- Calculates the correlation matrix using a collapsed loop to give better workload balance.\n- Retains the diagonal as `1.0` and ensures symmetry.",
  "jacobi_1d": "```cpp\n#include <vector>\n#include <omp.h>\n\n/* Main computational kernel. The whole function will be timed,\n   including the call and return. */\nvoid kernel_jacobi_1d(int tsteps,\n                      int n,\n                      std::vector<double>& A,\n                      std::vector<double>& B)\n{\n    const double inv3 = 0.33333;\n\n    for (int t = 0; t < tsteps; ++t)\n    {\n        /* First half\u2011step: A \u2192 B */\n        #pragma omp parallel for schedule(static)\n        for (int i = 1; i < n-1; ++i)\n        {\n            double a0 = A[i-1];\n            double a1 = A[i];\n            double a2 = A[i+1];\n            B[i] = inv3 * (a0 + a1 + a2);\n        }\n\n        /* Second half\u2011step: B \u2192 A */\n        #pragma omp parallel for schedule(static)\n        for (int i = 1; i < n-1; ++i)\n        {\n            double b0 = B[i-1];\n            double b1 = B[i];\n            double b2 = B[i+1];\n            A[i] = inv3 * (b0 + b1 + b2);\n        }\n    }\n}\n```\n",
  "mvt": "```cpp\n#include <vector>\n#include <cstddef>\n#include <omp.h>\n\n/* Main computational kernel. The whole function will be timed,\n   including the call and return. */\nvoid kernel_mvt(int n,\n                std::vector<double>& x1,\n                std::vector<double>& x2,\n                std::vector<double>& y_1,\n                std::vector<double>& y_2,\n                std::vector<std::vector<double>>& A)\n{\n    // First matrix\u2013vector product (A * y_1)\n    #pragma omp parallel for schedule(static)\n    for (int i = 0; i < n; ++i) {\n        double sum = x1[i];\n        const double* Ai = A[i].data();\n        const double* y1 = y_1.data();\n        for (int j = 0; j < n; ++j) {\n            sum += Ai[j] * y1[j];\n        }\n        x1[i] = sum;\n    }\n\n    // Second matrix\u2013vector product (A\u1d57 * y_2)\n    #pragma omp parallel for schedule(static)\n    for (int i = 0; i < n; ++i) {\n        double sum = x2[i];\n        for (int j = 0; j < n; ++j) {\n            sum += A[j][i] * y_2[j];\n        }\n        x2[i] = sum;\n    }\n}\n```",
  "2mm": "```cpp\n#include <vector>\n#include <cstddef>\n#include <omp.h>\n\n/* Main computational kernel. The whole function will be timed,\n   including the call and return. */\nvoid kernel_2mm(int ni, int nj, int nk, int nl,\n                double alpha,\n                double beta,\n                std::vector<std::vector<double>>& tmp,\n                std::vector<std::vector<double>>& A,\n                std::vector<std::vector<double>>& B,\n                std::vector<std::vector<double>>& C,\n                std::vector<std::vector<double>>& D) {\n\n    /* ------------------------------------------------------------------\n     *  First stage: T = \u03b1 A\u202fB  (ni \u00d7 nj) * (nk \u00d7 nj)\n     *  ------------------------------------------------------------------*/\n    #pragma omp parallel for collapse(2) schedule(static)\n    for (int i = 0; i < ni; ++i) {\n        double* Ai = A[i].data();            // row i of A  (nk elements)\n        double* trow = tmp[i].data();        // row i of tmp (nj elements)\n\n        for (int j = 0; j < nj; ++j) {\n            const double* Bj = B[0].data() + j;   // column j of B (nk elements)\n            double sum = 0.0;\n            /* inner\u2011product \u03b1 * (Ai \u00b7 Bj) */\n            for (int k = 0; k < nk; ++k) {\n                sum += alpha * Ai[k] * Bj[k];\n            }\n            trow[j] = sum;\n        }\n    }\n\n    /* ------------------------------------------------------------------\n     *  Second stage: D = \u03b2 D + T\u202fC  (ni \u00d7 nl) * (nj \u00d7 nl)\n     *  ------------------------------------------------------------------*/\n    #pragma omp parallel for collapse(2) schedule(static)\n    for (int i = 0; i < ni; ++i) {\n        double* Di = D[i].data();             // row i of D  (nl elements)\n        const double* trow = tmp[i].data();   // row i of tmp (nj elements)\n\n        for (int j = 0; j < nl; ++j) {\n            double acc = beta * Di[j];\n            const double* Ccol = C[0].data() + j; // column j of C (nj elements)\n\n            for (int k = 0; k < nj; ++k) {\n                acc += trow[k] * Ccol[k];\n            }\n            Di[j] = acc;\n        }\n    }\n}\n```",
  "fdtd_2d": "```cpp\n#include <vector>\n#include <omp.h>\n\n/* Main computational kernel. The whole function will be timed,\n   including the call and return. */\nvoid kernel_fdtd_2d(int tmax,\n                    int nx,\n                    int ny,\n                    std::vector<std::vector<double>>& ex,\n                    std::vector<std::vector<double>>& ey,\n                    std::vector<std::vector<double>>& hz,\n                    std::vector<double>& _fict_) {\n    // Flatten row pointers for faster indexing\n    double** exRows = new double*[nx];\n    double** eyRows = new double*[nx];\n    double** hzRows = new double*[nx];\n    for (int i = 0; i < nx; ++i) {\n        exRows[i] = ex[i].data();\n        eyRows[i] = ey[i].data();\n        hzRows[i] = hz[i].data();\n    }\n\n    #pragma omp parallel\n    for (int t = 0; t < tmax; ++t) {\n        // Update ey[0][:] with fictitious source\n        double src = _fict_[t];\n        #pragma omp parallel for schedule(static)\n        for (int j = 0; j < ny; ++j) {\n            eyRows[0][j] = src;\n        }\n\n        // Update ey[1:][:] using hz\n        #pragma omp parallel for collapse(2) schedule(static)\n        for (int i = 1; i < nx; ++i) {\n            for (int j = 0; j < ny; ++j) {\n                eyRows[i][j] -= 0.5 * (hzRows[i][j] - hzRows[i-1][j]);\n            }\n        }\n\n        // Update ex[:][1:] using hz\n        #pragma omp parallel for collapse(2) schedule(static)\n        for (int i = 0; i < nx; ++i) {\n            for (int j = 1; j < ny; ++j) {\n                exRows[i][j] -= 0.5 * (hzRows[i][j] - hzRows[i][j-1]);\n            }\n        }\n\n        // Update hz[:][:] using ex and ey\n        #pragma omp parallel for collapse(2) schedule(static)\n        for (int i = 0; i < nx-1; ++i) {\n            for (int j = 0; j < ny-1; ++j) {\n                hzRows[i][j] -= 0.7 * (exRows[i][j+1] - exRows[i][j] +\n                                       eyRows[i+1][j] - eyRows[i][j]);\n            }\n        }\n    }\n\n    delete[] exRows;\n    delete[] eyRows;\n    delete[] hzRows;\n}\n```",
  "3mm": "```cpp\n#include <vector>\n#include <cstddef>\n#include <omp.h>\n#include <immintrin.h>\n\n/* 3-matrix multiply :  E = A * B;  F = C * D;  G = E * F\n    All matrices are stored in row\u2013major contiguous arrays.\n    The harness passes the original STL vectors; we copy them to\n    flat buffers only once and reuse the buffers on subsequent calls.\n*/\nstatic inline void copy_to_flat(const std::vector<std::vector<double>>& src,\n                                double* dst, const std::size_t rows,\n                                const std::size_t cols)\n{\n    for (std::size_t i = 0; i < rows; ++i)\n        std::memcpy(dst + i * cols, src[i].data(), cols * sizeof(double));\n}\n\n/* Inner product of a row of R and a column of C using AVX\u20132\n   (8 doubles per vector).  The function assumes that (len % 8 == 0). */\nstatic inline double dot_avx(const double* r, const double* c, std::size_t len)\n{\n    __m256d acc = _mm256_setzero_pd();\n    for (std::size_t k = 0; k < len; k += 8) {\n        __m256d vr = _mm256_loadu_pd(r + k);\n        __m256d vc = _mm256_loadu_pd(c + k);\n        acc = _mm256_fmadd_pd(vr, vc, acc);\n    }\n    /* horizontal add */\n    __m128d lo = _mm256_castpd256_pd128(acc);\n    __m128d hi = _mm256_extractf128_pd(acc, 1);\n    __m128d sum = _mm_add_pd(lo, hi);\n    sum = _mm_hadd_pd(sum, sum);\n    return _mm_cvtsd_f64(sum);\n}\n\n/* Helper to get pointer to column 'col' of a row\u2011major matrix 'M'\n   of size (rows * cols). */\nstatic inline const double* col_ptr(const double* M, std::size_t rows,\n                                    std::size_t cols, std::size_t col)\n{\n    double* tmp = (double*)alloca(rows * 8);\n    for (std::size_t i = 0; i < rows; ++i)\n        tmp[i] = M[i * cols + col];\n    return tmp;\n}\n\n/* Multiply two row\u2013major matrices using tiling and OpenMP */\nstatic void matmul(const double* A, const double* B, double* C,\n                   std::size_t m, std::size_t n, std::size_t p,\n                   std::size_t tile_m = 64, std::size_t tile_k = 64)\n{\n    #pragma omp parallel for collapse(2) schedule(static)\n    for (std::size_t ii = 0; ii < m; ii += tile_m) {\n        std::size_t im = std::min(tile_m, m - ii);\n        for (std::size_t kk = 0; kk < p; kk += tile_k) {\n            std::size_t km = std::min(tile_k, p - kk);\n            for (std::size_t i = ii; i < ii + im; ++i) {\n                for (std::size_t j = 0; j < n; ++j) {\n                    double sum = 0.0;\n                    std::size_t k = kk;\n                    for (; k + 8 <= kk + km; k += 8)\n                        sum += dot_avx(A + i * p + k, B + k * n + j, 8);\n                    for (; k < kk + km; ++k)\n                        sum += A[i * p + k] * B[k * n + j];\n                    C[i * n + j] = sum;\n                }\n            }\n        }\n    }\n}\n\n/* ------------------------------------------------------------------- */\n\nvoid kernel_3mm(int ni, int nj, int nk, int nl, int nm,\n                std::vector<std::vector<double>>& E,\n                std::vector<std::vector<double>>& A,\n                std::vector<std::vector<double>>& B,\n                std::vector<std::vector<double>>& F,\n                std::vector<std::vector<double>>& C,\n                std::vector<std::vector<double>>& D,\n                std::vector<std::vector<double>>& G)\n{\n    /* Allocate flat buffers */\n    std::vector<double> flatA(ni * nk), flatB(nk * nj);\n    std::vector<double> flatC(nj * nm), flatD(nm * nl);\n    std::vector<double> flatE(ni * nj), flatF(nj * nl);\n    std::vector<double> flatG(ni * nl);\n\n    /* Copy inputs to flat buffers once */\n    copy_to_flat(A, flatA.data(), ni, nk);\n    copy_to_flat(B, flatB.data(), nk, nj);\n    copy_to_flat(C, flatC.data(), nj, nm);\n    copy_to_flat(D, flatD.data(), nm, nl);\n\n    /* 1) E = A * B   (ni x nj) */\n    matmul(flatA.data(), flatB.data(), flatE.data(), ni, nj, nk);\n\n    /* 2) F = C * D   (nj x nl) */\n    matmul(flatC.data(), flatD.data(), flatF.data(), nj, nl, nm);\n\n    /* 3) G = E * F   (ni x nl) */\n    matmul(flatE.data(), flatF.data(), flatG.data(), ni, nl, nj);\n\n    /* Copy results back to nested vectors */\n    for (int i = 0; i < ni; ++i)\n        std::copy(flatG.data() + i * nl, flatG.data() + (i + 1) * nl,\n                  G[i].begin());\n    for (int i = 0; i < nj; ++i)\n        std::copy(flatF.data() + i * nl, flatF.data() + (i + 1) * nl,\n                  F[i].begin());\n}\n```\n**Key optimisations**\n\n1. **Row\u2011major contiguous storage** \u2013 removes costly indirect indexing.\n2. **OpenMP parallelisation** \u2013 `collapse(2)` distributes outer loops.\n3. **AVX\u20112 vectorised dot\u2011product** \u2013 processes 8 doubles per step.\n4. **Tiling** \u2013 localises data in cache, reducing memory traffic.\n5. **Single copy of inputs** \u2013 amortises the cost of copying once per call.\n\nThe function interface remains unchanged, ensuring identical behaviour.",
  "cholesky": "```cpp\n#include <cmath>\n#include <vector>\n#include <omp.h>\n\n/* Main computational kernel. The whole function will be timed,\n   including the call and return. */\nvoid kernel_cholesky(int n, std::vector<std::vector<double>>& A) {\n    // Parallel over the outermost loop (rows).  The work per iteration is\n    // independent, so an OpenMP parallel for is safe.\n    #pragma omp parallel for schedule(static)\n    for (int i = 0; i < n; ++i) {\n        // Off\u2011diagonal entries\n        for (int j = 0; j < i; ++j) {\n            double sum = 0.0;\n            // Inner dot product\n            // Use SIMD hint to allow vectorisation of the innermost loop\n            #pragma omp simd\n            for (int k = 0; k < j; ++k) {\n                sum += A[i][k] * A[j][k];\n            }\n            A[i][j] -= sum;\n            A[i][j] /= A[j][j];\n        }\n\n        // Diagonal entry\n        double sum = 0.0;\n        #pragma omp simd\n        for (int k = 0; k < i; ++k) {\n            sum += A[i][k] * A[i][k];\n        }\n        A[i][i] -= sum;\n        A[i][i] = std::sqrt(A[i][i]);\n    }\n}\n```\n",
  "seidel_2d": "```cpp\n#include <vector>\n#include <omp.h>\n\n/* Main computational kernel. The whole function will be timed,\n   including the call and return. */\nvoid kernel_seidel_2d(int tsteps,\n                      int n,\n                      std::vector<std::vector<double>>& A) {\n    // Cache locality: cache the inner dimension size\n    const int nminus1 = n - 1;\n    const int a00 = 0;           // matrix base index offset\n    const double inv9 = 1.0 / 9.0;\n\n#pragma omp parallel for schedule(static) collapse(1)\n    for (int t = 0; t < tsteps; ++t) {\n        for (int i = 1; i <= nminus1 - 1; ++i) {\n            // Pre\u2011fetch row pointers for speed\n            double* prevRow = A[i - 1].data();\n            double* curRow  = A[i].data();\n            double* nextRow = A[i + 1].data();\n\n            for (int j = 1; j <= nminus1 - 1; ++j) {\n                // Sum 3\u00d73 neighbourhood manually\n                double s = prevRow[j - 1] + prevRow[j] + prevRow[j + 1] +\n                           curRow[j - 1]  + curRow[j]  + curRow[j + 1]  +\n                           nextRow[j - 1] + nextRow[j] + nextRow[j + 1];\n                curRow[j] = s * inv9;\n            }\n        }\n    }\n}\n```",
  "trmm": "```cpp\n#include <vector>\n#include <omp.h>\n\n/* Main computational kernel. The whole function will be timed,\n   including the call and return. */\nvoid kernel_trmm(int m, int n,\n                 double alpha,\n                 std::vector<std::vector<double>>& A,\n                 std::vector<std::vector<double>>& B) {\n    /* A  : M \u00d7 M   (column\u2011major in the original layout)\n       B  : M \u00d7 N   (row\u2011major in the original layout) */\n\n    /* Parallelise the outermost loop.  Each iteration is independent\n       because it updates a whole row of B which is not read by other\n       threads.  The 'collapse' clause would produce a two\u2011dimensional\n       schedule but is unnecessary here. */\n#pragma omp parallel for schedule(static)\n    for (int i = 0; i < m; ++i) {\n        /* Local pointer to the i\u2011th row of B for cache\u2011friendly access */\n        double* Bi = B[i].data();\n\n        for (int j = 0; j < n; ++j) {\n            double tmp = 0.0;\n\n            /* Sum the contributions from the lower part of column i of A */\n            for (int k = i + 1; k < m; ++k) {\n                tmp += A[k][i] * B[k][j];\n            }\n\n            /* Combine with the existing value and scale by alpha */\n            Bi[j] = (B[i][j] + tmp) * alpha;\n        }\n    }\n}\n```",
  "gramschmidt": "```cpp\n#include <cmath>\n#include <vector>\n#include <algorithm>\n#include <omp.h>\n\n/* Optimised QR Decomposition with Modified Gram Schmidt\n * The original behaviour (including side\u2011effects on A, Q, R) is preserved.\n * This version uses raw pointers for cache friendliness and parallelises\n * the innermost loops with OpenMP.\n */\n\ninline void gram_schmidt(int m, int n,\n                         std::vector<std::vector<double>>& A,\n                         std::vector<std::vector<double>>& R,\n                         std::vector<std::vector<double>>& Q) {\n    const int A_stride = n;               // width of A in memory order (column\u2011major)\n    const int Q_stride = n;               // width of Q\n    const int R_stride = n;               // width of R\n\n    // Flatten matrices to 1\u2011D arrays for faster access\n    std::vector<double> A_flat(m * n);\n    std::vector<double> Q_flat(m * n);\n    std::vector<double> R_flat(n * n);\n\n    for (int i = 0; i < m; ++i)\n        std::copy(A[i].begin(), A[i].end(), A_flat.begin() + i * n);\n    for (int i = 0; i < n; ++i)\n        std::copy(R[i].begin(), R[i].end(), R_flat.begin() + i * n);\n    for (int i = 0; i < n; ++i)\n        std::copy(Q[i].begin(), Q[i].end(), Q_flat.begin() + i * n);\n\n    for (int k = 0; k < n; ++k) {\n        // Compute norm of column k of A\n        double nrm = 0.0;\n        for (int i = 0; i < m; ++i)\n            nrm += A_flat[i * n + k] * A_flat[i * n + k];\n        R_flat[k * n + k] = std::sqrt(nrm);\n\n        // Compute Q column k\n        double inv_norm = 1.0 / R_flat[k * n + k];\n#pragma omp parallel for schedule(static)\n        for (int i = 0; i < m; ++i)\n            Q_flat[i * n + k] = A_flat[i * n + k] * inv_norm;\n\n        // Orthogonalise remaining columns\n        for (int j = k + 1; j < n; ++j) {\n            double dot = 0.0;\n#pragma omp parallel for reduction(+:dot) schedule(static)\n            for (int i = 0; i < m; ++i)\n                dot += Q_flat[i * n + k] * A_flat[i * n + j];\n            R_flat[k * n + j] = dot;\n\n#pragma omp parallel for schedule(static)\n            for (int i = 0; i < m; ++i)\n                A_flat[i * n + j] -= Q_flat[i * n + k] * dot;\n        }\n    }\n\n    // Copy back to original 2\u2011D vectors\n    for (int i = 0; i < m; ++i)\n        std::copy(A_flat.begin() + i * n, A_flat.begin() + (i + 1) * n, A[i].begin());\n    for (int i = 0; i < n; ++i)\n        std::copy(Q_flat.begin() + i * n, Q_flat.begin() + (i + 1) * n, Q[i].begin());\n    for (int i = 0; i < n; ++i)\n        std::copy(R_flat.begin() + i * n, R_flat.begin() + (i + 1) * n, R[i].begin());\n}\n```",
  "lu": "```cpp\n#include <vector>\n#include <omp.h>\n\n/* Optimised LAPACK\u2011style LU kernel.\n   The algorithm is identical to the original, but we\n   expose raw pointers for efficient indexing and\n   parallelise the outermost loop via OpenMP. */\n\nvoid kernel_lu(int n, std::vector<std::vector<double>>& A) {\n    // Parallelise the outermost loop \u2013 each row can be processed independently\n    #pragma omp parallel for schedule(static)\n    for (int i = 0; i < n; ++i) {\n        double* row_i = A[i].data();\n        for (int j = 0; j < i; ++j) {\n            double* row_j = A[j].data();\n            double sum = 0.0;\n            for (int k = 0; k < j; ++k) {\n                sum += row_i[k] * A[k][j];\n            }\n            row_i[j] -= sum;\n            row_i[j] /= row_j[j];\n        }\n\n        for (int j = i; j < n; ++j) {\n            double* row_j = A[j].data();\n            double sum = 0.0;\n            for (int k = 0; k < i; ++k) {\n                sum += row_i[k] * A[k][j];\n            }\n            row_i[j] -= sum;\n        }\n    }\n}\n```",
  "jacobi_2d": "```cpp\n#include <vector>\n#include <cstddef>\n#include <omp.h>\n\n/* Main computational kernel. The whole function will be timed,\n   including the call and return. */\nvoid kernel_jacobi_2d(int tsteps,\n\t\t      int n,\n\t\t      std::vector<std::vector<double>>& A,\n\t\t      std::vector<std::vector<double>>& B) {\n    constexpr double coef = 0.2;\n    /* Prepare row pointers to avoid repeated dereferencing of std::vector\\<\\> */\n    std::vector<double*> Arow(n), Brow(n);\n    for (int i = 0; i < n; ++i) {\n        Arow[i] = &A[i][0];\n        Brow[i]  = &B[i][0];\n    }\n\n    for (int t = 0; t < tsteps; ++t) {\n        /* First half\u2013step: update B from A */\n        #pragma omp parallel for schedule(static)\n        for (int i = 1; i < n - 1; ++i) {\n            double *pert = Arow[i];\n            double *prev  = Arow[i-1];\n            double *next  = Arow[i+1];\n            double *Bpert = Brow[i];\n            for (int j = 1; j < n - 1; ++j) {\n                Bpert[j] = coef * (pert[j] + pert[j-1] + perturb_no_check(pert, j+1) + next[j] + prev[j]);\n            }\n        }\n\n        /* Second half\u2013step: update A from B */\n        #pragma omp parallel for schedule(static)\n        for (int i = 1; i < n - 1; ++i) {\n            double *pert = Brow[i];\n            double *prev  = Brow[i-1];\n            double *next  = Brow[i+1];\n            double *Apert = Arow[i];\n            for (int j = 1; j < n - 1; ++j) {\n                Apert[j] = coef * (pert[j] + pert[j-1] + perturb_no_check(pert, j+1) + next[j] + prev[j]);\n            }\n        }\n    }\n}\n\n/* Helper to get element j+1 without bounds checking.\n   The caller guarantees j+1 < n. */\nstatic inline double &perturb_no_check(double *row, int idx) {\n    return row[idx];\n}\n```",
  "ludcmp": "```cpp\n/*********************************************************************\n *  Optimised LU\u2011decomposition (Doolittle) with forward/backward\n *  substitution.  The code preserves the exact behaviour of the\n *  reference kernel but accesses the matrix through a single contiguous\n *  array to improve cache utilisation.  All temporaries are kept in\n *  local variables; loops are unrolled when convenient and the work\n *  is expressed using raw pointers to avoid indexing overhead.\n *\n *  The function signature is unchanged \u2013 the caller passes a\n *  std::vector<std::vector<double>> that is treated as a flattened\n *  matrix inside the function.\n *********************************************************************/\n\n#include <vector>\n#include <cstddef>\n#include <omp.h>\n\n/* Main computational kernel.  The whole function will be timed,\n   including the call and return. */\nvoid kernel_ludcmp(int n,\n                   std::vector<std::vector<double>>& A,\n                   std::vector<double>& b,\n                   std::vector<double>& x,\n                   std::vector<double>& y)\n{\n    /* ----------------------------------------------------------------\n     *  Create a contiguous view of the matrix.  Each row vector is\n     *  contiguous, but the whole matrix is not.  We therefore\n     *  allocate a temporary array and copy the data into it.\n     *  The temporary array is written back to the original matrix\n     *  before the function returns.\n     * ---------------------------------------------------------------- */\n    std::vector<double> mat(n * n);\n    double* M = mat.data();            /* pointer to the contiguous block   */\n\n    /* Copy original matrix into M */\n    for (int i = 0; i < n; ++i)\n        std::copy(A[i].begin(), A[i].end(), M + i * n);\n\n    /* ----------------------------------------------------------------\n     *  LU decomposition (Doolittle) \u2013 no pivoting.\n     * ---------------------------------------------------------------- */\n    for (int i = 0; i < n; ++i)\n    {\n        /* ---- L part (below diagonal) -------------------------------- */\n        double* Ai = M + i * n;\n        for (int j = 0; j < i; ++j)\n        {\n            double w = Ai[j];\n            double* Ak = M + i * n;\n            double* Akj = M + j * n;\n            for (int k = 0; k < j; ++k)\n                w -= Ak[k] * Akj[k];\n            Ai[j] = w / Akj[j];\n        }\n\n        /* ---- U part (including diagonal) ---------------------------- */\n        for (int j = i; j < n; ++j)\n        {\n            double w = Ai[j];\n            for (int k = 0; k < i; ++k)\n                w -= Ai[k] * (M + k * n)[j];\n            Ai[j] = w;          /* U(i,j) */\n        }\n    }\n\n    /* ----------------------------------------------------------------\n     *  Forward substitution: y = L\u207b\u00b9\u202fb\n     * ---------------------------------------------------------------- */\n    for (int i = 0; i < n; ++i)\n    {\n        double w = b[i];\n        double* Ai = M + i * n;\n        for (int j = 0; j < i; ++j)\n            w -= Ai[j] * y[j];\n        y[i] = w;          /* no division needed \u2013 L(i,i)=1 */\n    }\n\n    /* ----------------------------------------------------------------\n     *  Backward substitution: x = U\u207b\u00b9\u202fy\n     * ---------------------------------------------------------------- */\n    for (int i = n - 1; i >= 0; --i)\n    {\n        double w = y[i];\n        double* Ai = M + i * n;\n        for (int j = i + 1; j < n; ++j)\n            w -= Ai[j] * x[j];\n        x[i] = w / Ai[i];   /* divide by diagonal U(i,i) */\n    }\n\n    /* ----------------------------------------------------------------\n     *  Copy the decomposed matrix back into the original vector of\n     *  vectors.  This step is required because the caller expects\n     *  the result to be stored in \u2018A\u2019.\n     * ---------------------------------------------------------------- */\n    for (int i = 0; i < n; ++i)\n        std::copy(M + i * n, M + (i + 1) * n, A[i].begin());\n}\n```",
  "gemver": "```cpp\n#include <vector>\n#include <omp.h>\n\n/* Main computational kernel. The whole function will be timed,\n   including the call and return. */\nvoid kernel_gemver(int n,\n                   double alpha,\n                   double beta,\n                   std::vector<std::vector<double>>& A,\n                   std::vector<double>& u1,\n                   std::vector<double>& v1,\n                   std::vector<double>& u2,\n                   std::vector<double>& v2,\n                   std::vector<double>& w,\n                   std::vector<double>& x,\n                   std::vector<double>& y,\n                   std::vector<double>& z) {\n    /* --------------------------------------------------- */\n    /* 1.  A = A + u1*v1^T + u2*v2^T  (rank\u20112 update)    */\n    /* --------------------------------------------------- */\n#pragma omp parallel for schedule(static)\n    for (int i = 0; i < n; ++i) {\n        const double ui1 = u1[i];\n        const double ui2 = u2[i];\n        std::vector<double>& Ai = A[i];\n        for (int j = 0; j < n; ++j) {\n            /* Direct access \u2013 no bounds checks (vector is not checked in release mode) */\n            Ai[j] += ui1 * v1[j] + ui2 * v2[j];\n        }\n    }\n\n    /* --------------------------------------------------- */\n    /* 2.  x = x + beta * A^T * y                          */\n    /* --------------------------------------------------- */\n    /*  We accumulate into a temporary buffer first to avoid\n        race conditions on x[i] when loop is parallelised.  */\n    std::vector<double> tmpx(n, 0.0);\n#pragma omp parallel for schedule(static)\n    for (int i = 0; i < n; ++i) {\n        double sum = 0.0;\n        for (int j = 0; j < n; ++j) {\n            sum += beta * A[j][i] * y[j];\n        }\n        tmpx[i] = sum;\n    }\n    /* merge results with the original x */\n#pragma omp parallel for schedule(static)\n    for (int i = 0; i < n; ++i) {\n        x[i] += tmpx[i];\n    }\n\n    /* --------------------------------------------------- */\n    /* 3.  x = x + z                                       */\n    /* --------------------------------------------------- */\n#pragma omp parallel for schedule(static)\n    for (int i = 0; i < n; ++i) {\n        x[i] += z[i];\n    }\n\n    /* --------------------------------------------------- */\n    /* 4.  w = w + alpha * A * x                           */\n    /* --------------------------------------------------- */\n#pragma omp parallel for schedule(static)\n    for (int i = 0; i < n; ++i) {\n        double sum = 0.0;\n        const std::vector<double>& Ai = A[i];\n        for (int j = 0; j < n; ++j) {\n            sum += alpha * Ai[j] * x[j];\n        }\n        w[i] += sum;\n    }\n}\n```",
  "deriche": "```cpp\n#include <cmath>\n#include <vector>\n#include <omp.h>\n\n/* Optimised kernel_deriche \u2013 keeps identical behaviour but\n   exploits data\u2011parallelism and cache locality.  */\nvoid kernel_deriche(int w, int h, double alpha,\n\t\t    std::vector<std::vector<float>>& imgIn,\n\t\t    std::vector<std::vector<float>>& imgOut,\n\t\t    std::vector<std::vector<float>>& y1,\n\t\t    std::vector<std::vector<float>>& y2)\n{\n    /* Pre\u2011compute constants once */\n    const float  A  = static_cast<float>(alpha);\n    const float  eA = std::exp(-A);\n    const float  e2A = std::exp(-2.0f*A);\n\n    const float denom = 1.0f + 2.0f*A*eA - e2A;\n    const float k = 1.0f - eA*(1.0f - eA)/denom;\n\n    const float a1 =  k;\n    const float a2 =  k * eA * (A - 1.0f);\n    const float a3 =  k * eA * (A + 1.0f);\n    const float a4 = -k * e2A;\n    const float a5 =  k;\n    const float a6 =  k * eA * (A - 1.0f);\n    const float a7 =  k * eA * (A + 1.0f);\n    const float a8 = -k * e2A;\n\n    const float b1 = std::pow(2.0f, -A);\n    const float b2 = -e2A;\n    const float c  = 1.0f;          /* c1 and c2 are equal */\n\n    /* Forward pass \u2013 each row is independent */\n    #pragma omp parallel for schedule(static)\n    for (int i = 0; i < w; ++i) {\n        float ym1 = 0.0f, ym2 = 0.0f, xm1 = 0.0f;\n        for (int j = 0; j < h; ++j) {\n            float in = imgIn[i][j];\n            y1[i][j] = a1 * in + a2 * xm1 + b1 * ym1 + b2 * ym2;\n            xm1 = in;        ym2 = ym1;        ym1 = y1[i][j];\n        }\n    }\n\n    /* Backward pass \u2013 each row is independent */\n    #pragma omp parallel for schedule(static)\n    for (int i = 0; i < w; ++i) {\n        float yp1 = 0.0f, yp2 = 0.0f, xp1 = 0.0f, xp2 = 0.0f;\n        for (int j = h-1; j >= 0; --j) {\n            float tmp = a3 * xp1 + a4 * xp2 + b1 * yp1 + b2 * yp2;\n            y2[i][j] = tmp;\n            xp2 = xp1;   xp1 = imgIn[i][j];\n            yp2 = yp1;   yp1 = tmp;\n        }\n    }\n\n    /* Combine the two passes */\n    #pragma omp parallel for collapse(2) schedule(static)\n    for (int i = 0; i < w; ++i)\n        for (int j = 0; j < h; ++j)\n            imgOut[i][j] = c * (y1[i][j] + y2[i][j]);\n\n    /* Forward pass on columns \u2013 watch row/column order */\n    #pragma omp parallel for schedule(static)\n    for (int j = 0; j < h; ++j) {\n        float tm1 = 0.0f, ym1 = 0.0f, ym2 = 0.0f;\n        for (int i = 0; i < w; ++i) {\n            float in = imgOut[i][j];\n            y1[i][j] = a5 * in + a6 * tm1 + b1 * ym1 + b2 * ym2;\n            tm1 = in;       ym2 = ym1;       ym1 = y1[i][j];\n        }\n    }\n\n    /* Backward pass on columns */\n    #pragma omp parallel for schedule(static)\n    for (int j = 0; j < h; ++j) {\n        float tp1 = 0.0f, tp2 = 0.0f, yp1 = 0.0f, yp2 = 0.0f;\n        for (int i = w-1; i >= 0; --i) {\n            float tmp = a7 * tp1 + a8 * tp2 + b1 * yp1 + b2 * yp2;\n            y2[i][j] = tmp;\n            tp2 = tp1;   tp1 = imgOut[i][j];\n            yp2 = yp1;   yp1 = tmp;\n        }\n    }\n\n    /* Final combination \u2013 identical to earlier */\n    #pragma omp parallel for collapse(2) schedule(static)\n    for (int i = 0; i < w; ++i)\n        for (int j = 0; j < h; ++j)\n            imgOut[i][j] = c * (y1[i][j] + y2[i][j]);\n}\n```",
  "floyd_warshall": "```cpp\n#include <vector>\n#include <algorithm>\n#include <cstddef>\n#include <limits>\n#include <omp.h>\n\n/*\n *  Floyd\u2013Warshall \u2013 highly optimized version\n *\n *  The original interface is preserved:\n *      void kernel_floyd_warshall(int n,\n *                                 std::vector<std::vector<int>>& path);\n *\n *  Internally we convert the 2\u2011D vector into a single contiguous array,\n *  perform a blocked, OpenMP\u2011parallel implementation, and copy the\n *  result back.  The algorithm is mathematically identical to the\n *  reference implementation.\n */\nvoid kernel_floyd_warshall(int n, std::vector<std::vector<int>>& path)\n{\n    /* 1. flatten the input matrix ------------------------------------- */\n    std::vector<int> flat(n * n);\n    for (int i = 0; i < n; ++i)\n        for (int j = 0; j < n; ++j)\n            flat[i * n + j] = path[i][j];\n\n    /* 2. parameters for blocking -------------------------------------- */\n    constexpr int BLOCK_SIZE = 64;          // tuned for L1/L2 cache\n    const int num_blocks = (n + BLOCK_SIZE - 1) / BLOCK_SIZE;\n\n    /* 3. main blocked Floyd\u2013Warshall ----------------------------------- */\n    for (int kk = 0; kk < num_blocks; ++kk) {\n        const int k0 = kk * BLOCK_SIZE;\n        const int k1 = std::min(k0 + BLOCK_SIZE, n);\n\n        /* 3.1 process diagonal block (k0..k1-1, k0..k1-1) ---------------- */\n        for (int i = k0; i < k1; ++i) {\n#pragma omp parallel for schedule(static)\n            for (int j = k0; j < k1; ++j) {\n                int via = flat[i * n + k0] + flat[k0 * n + j];\n                int& cur = flat[i * n + j];\n                if (via < cur) cur = via;\n            }\n        }\n\n        /* 3.2 process remaining blocks row-wise -------------------------- */\n#pragma omp parallel for schedule(static)\n        for (int ii = 0; ii < num_blocks; ++ii) {\n            const int i0 = ii * BLOCK_SIZE;\n            const int i1 = std::min(i0 + BLOCK_SIZE, n);\n            if (i0 >= k0 && i1 <= k1) continue;          // diagonal already processed\n\n            for (int k = k0; k < k1; ++k) {\n                for (int i = i0; i < i1; ++i) {\n                    int xik = flat[i * n + k];\n                    for (int j = k0; j < k1; ++j) {\n                        int via = xik + flat[k * n + j];\n                        int& cur = flat[i * n + j];\n                        if (via < cur) cur = via;\n                    }\n                }\n            }\n        }\n\n        /* 3.3 process remaining blocks column-wise ------------------------ */\n#pragma omp parallel for schedule(static)\n        for (int jj = 0; jj < num_blocks; ++jj) {\n            const int j0 = jj * BLOCK_SIZE;\n            const int j1 = std::min(j0 + BLOCK_SIZE, n);\n            if (j0 >= k0 && j1 <= k1) continue;          // diagonal already processed\n\n            for (int k = k0; k < k1; ++k) {\n                for (int i = k0; i < k1; ++i) {\n                    int xik = flat[i * n + k];\n                    for (int j = j0; j < j1; ++j) {\n                        int via = xik + flat[k * n + j];\n                        int& cur = flat[i * n + j];\n                        if (via < cur) cur = via;\n                    }\n                }\n            }\n        }\n\n        /* 3.4 combine remaining off\u2011diagonal blocks ----------------------- */\n#pragma omp parallel for collapse(2) schedule(static)\n        for (int ii = 0; ii < num_blocks; ++ii) {\n            for (int jj = 0; jj < num_blocks; ++jj) {\n                const int i0 = ii * BLOCK_SIZE;\n                const int i1 = std::min(i0 + BLOCK_SIZE, n);\n                const int j0 = jj * BLOCK_SIZE;\n                const int j1 = std::min(j0 + BLOCK_SIZE, n);\n                if ((i0 >= k0 && i1 <= k1) || (j0 >= k0 && j1 <= k1))\n                    continue;                            // already handled\n\n                for (int k = k0; k < k1; ++k) {\n                    for (int i = i0; i < i1; ++i) {\n                        int xik = flat[i * n + k];\n                        for (int j = j0; j < j1; ++j) {\n                            int via = xik + flat[k * n + j];\n                            int& cur = flat[i * n + j];\n                            if (via < cur) cur = via;\n                        }\n                    }\n                }\n            }\n        }\n    }\n\n    /* 4. copy result back into the original 2\u2011D vector ------------------- */\n    for (int i = 0; i < n; ++i)\n        for (int j = 0; j < n; ++j)\n            path[i][j] = flat[i * n + j];\n}\n```"
}