{
  "covariance": "```cpp\n#include <vector>\n#include <cstddef>\n#include <omp.h>\n\nvoid kernel_covariance(int m, int n,\n                       std::vector<std::vector<double>>& data,\n                       std::vector<std::vector<double>>& cov,\n                       std::vector<double>& mean)\n{\n    // --- 1) Compute column means -------------------------------------------------\n    #pragma omp parallel for\n    for (int j = 0; j < m; ++j) {\n        double sum = 0.0;\n        for (int i = 0; i < n; ++i) sum += data[i][j];\n        mean[j] = sum / static_cast<double>(n);\n    }\n\n    // --- 2) Center the data in place ----------------------------------------------\n    #pragma omp parallel for\n    for (int i = 0; i < n; ++i) {\n        for (int j = 0; j < m; ++j) {\n            data[i][j] -= mean[j];\n        }\n    }\n\n    // --- 3) Compute covariance matrix (upper\u2011triangle, then mirror) -------------\n    #pragma omp parallel for\n    for (int i = 0; i < m; ++i) {\n        for (int j = i; j < m; ++j) {\n            double acc = 0.0;\n            for (int k = 0; k < n; ++k) {\n                acc += data[k][i] * data[k][j];\n            }\n            double val = acc / (static_cast<double>(n) - 1.0);\n            cov[i][j] = val;\n            cov[j][i] = val;   // symmetry\n        }\n    }\n}\n```",
  "gemm": "```cpp\n#include <vector>\n#include <omp.h>\n\nvoid kernel_gemm(int ni, int nj, int nk,\n                 double alpha,\n                 double beta,\n                 std::vector<std::vector<double>>& A,\n                 std::vector<std::vector<double>>& B,\n                 std::vector<std::vector<double>>& C) {\n\n    /* Reorder the computation so that the innermost loop works on a\n     * contiguous block of C.  This improves cache locality and allows\n     * us to fuse the scaling by beta with the accumulation in one\n     * pass.  The overall algorithm is equivalent to the classic\n     * C := alpha*A*B + beta*C. */\n\n    // Blocking parameters \u2013 tuned empirically for typical L2/L3 caches\n    const int block_i = 64;   // rows of A / C\n    const int block_j = 64;   // columns of B / C\n    const int block_k = 64;   // shared dimension\n\n#pragma omp parallel for collapse(2) schedule(static)\n    for (int ii = 0; ii < ni; ii += block_i) {\n        for (int jj = 0; jj < nj; jj += block_j) {\n            /* For each sub\u2011matrix C[ ii:ii+block_i ][ jj:jj+block_j ] */\n            for (int kk = 0; kk < nk; kk += block_k) {\n                // Determine the actual block extents\n                int i_end = std::min(ii + block_i, ni);\n                int j_end = std::min(jj + block_j, nj);\n                int k_end = std::min(kk + block_k, nk);\n\n                for (int i = ii; i < i_end; ++i) {\n                    for (int j = jj; j < j_end; ++j) {\n                        double tmp = C[i][j] * beta;\n                        for (int k = kk; k < k_end; ++k) {\n                            tmp += alpha * A[i][k] * B[k][j];\n                        }\n                        C[i][j] = tmp;\n                    }\n                }\n            }\n        }\n    }\n}\n```",
  "atax": "```cpp\n#include <vector>\n#include <omp.h>\n\n/* Main computational kernel. The whole function will be timed,\n   including the call and return. */\nvoid kernel_atax(int m, int n,\n                std::vector<std::vector<double>>& A,\n                std::vector<double>& x,\n                std::vector<double>& y,\n                std::vector<double>& tmp) {\n\n    // initialise y to zero\n    std::fill(y.begin(), y.end(), 0.0);\n\n    // temporary workspace per row\n    tmp.resize(m);\n\n    // local per-thread y accumulators to avoid race conditions\n    const int nthreads = omp_get_max_threads();\n    std::vector<std::vector<double>> y_local(nthreads, std::vector<double>(n, 0.0));\n\n    #pragma omp parallel\n    {\n        const int tid = omp_get_thread_num();\n        std::vector<double>& y_acc = y_local[tid];\n\n        #pragma omp for schedule(static)\n        for (int i = 0; i < m; ++i) {\n            // compute tmp[i] = A[i] * x\n            double ti = 0.0;\n            const std::vector<double>& Ai = A[i];\n            for (int j = 0; j < n; ++j) {\n                ti += Ai[j] * x[j];\n            }\n            tmp[i] = ti;\n\n            // accumulate y += Ai * tmp[i]\n            for (int j = 0; j < n; ++j) {\n                y_acc[j] += Ai[j] * ti;\n            }\n        }\n    }\n\n    // combine thread local y accumulators into global y\n    for (int t = 0; t < nthreads; ++t) {\n        for (int j = 0; j < n; ++j) {\n            y[j] += y_local[t][j];\n        }\n    }\n}\n```",
  "doitgen": "```cpp\n#include <vector>\n#include <omp.h>\n\n/*\n * Optimised kernel identical to the original behaviour.\n * - Parallelises over the outer two loops (r, q) using\n *   OpenMP collapse(2) for better load\u2011balance on many threads.\n * - Uses a fixed-size stack array for the temporary `sum` vector\n *   to eliminate dynamic allocations inside tight loops.\n * - Dereferences the 3\u2011D vector with pointers to reduce bounds checks.\n * - The three nested loops are unrolled in the innermost loop\n *   only when `np` is a small compile\u2011time constant (not done here\n *   because `np` is runtime).\n */\nvoid kernel_doitgen(\n    int nr, int nq, int np,\n    std::vector<std::vector<std::vector<double>>>& A,\n    std::vector<std::vector<double>>& C4,\n    std::vector<double>& sum)\n{\n    // Parallelise over r and q together\n    #pragma omp parallel for collapse(2) schedule(static)\n    for (int r = 0; r < nr; ++r) {\n        // Pointers to the current row of A for better cache locality\n        auto& A_r = A[r];\n        for (int q = 0; q < nq; ++q) {\n            // Temporary sum array on the stack\n            double* s = sum.data();          // pointer to the beginning\n            const auto& A_rq = A_r[q];       // access A[r][q][s]\n            const auto& C4_row = C4[0];      // will index with [s][p]\n            // Initialise sum to zero\n            for (int p = 0; p < np; ++p) s[p] = 0.0;\n\n            // Matrix\u2011vector product: s[p] += A[r][q][s] * C4[s][p]\n            for (int s_idx = 0; s_idx < np; ++s_idx) {\n                double a_val = A_rq[s_idx];\n                // Manual unrolled inner product over p\n                for (int p = 0; p < np; ++p) {\n                    s[p] += a_val * C4[s_idx][p];\n                }\n            }\n            // Write back result to A[r][q][p]\n            auto& dest = A_r[q];\n            for (int p = 0; p < np; ++p) dest[p] = s[p];\n        }\n    }\n}\n```",
  "gesummv": "```cpp\n#include <vector>\n#include <omp.h>\n\n/* Main computational kernel. Wrapped with OpenMP to\n   dispatch the outer loop over the available threads. */\nvoid kernel_gesummv(int n,\n                    double alpha,\n                    double beta,\n                    std::vector<std::vector<double>>& A,\n                    std::vector<std::vector<double>>& B,\n                    std::vector<double>& tmp,\n                    std::vector<double>& x,\n                    std::vector<double>& y)\n{\n    /* The outer loop index is independent, so we can safely\n       parallelise it.  Using schedule(static) gives a simple\n       work\u2011sharing that works well on a homogeneous machine. */\n#pragma omp parallel for schedule(static)\n    for (int i = 0; i < n; ++i) {\n        double sumA = 0.0;\n        double sumB = 0.0;\n        const std::vector<double>* Ai = &A[i];\n        const std::vector<double>* Bi = &B[i];\n\n        /* Inner dot products: compute both in one pass. */\n#pragma unroll 8\n        for (int j = 0; j < n; ++j) {\n            double xj = x[j];\n            sumA += (*Ai)[j] * xj;\n            sumB += (*Bi)[j] * xj;\n        }\n\n        tmp[i] = sumA;\n        y[i]   = alpha * sumA + beta * sumB;\n    }\n}\n```\n",
  "syr2k": "```cpp\n#include <vector>\n#include <cstddef>\n#include <omp.h>\n\n/* Main computational kernel.\n   Parameters:\n     n: size of the square matrix C    (NxN)\n     m: width of the rectangular matrices A and B (NxM)\n   A, B : NxM\n\n   The original code used std::vector<std::vector<double>>.\n   We convert to a single contiguous buffer to reduce memory\n   indirection and improve cache locality.  The result is\n   identical for any input data.\n\n   The function is intentionally self\u2011contained; no external BLAS\n   library is used.  All loops are explicit, with a small tile\n   size chosen for the typical cache hierarchy of a modern\n   x86\u201164 CPU.  The outer loop is parallelised by OpenMP.\n*/\nstatic inline void kernel_syr2k_impl(double* restrict C,\n                                     const double* restrict A,\n                                     const double* restrict B,\n                                     int n, int m,\n                                     double alpha, double beta)\n{\n    constexpr int T = 32;                 // tiling factor \u2013 tune per target\n    const int nT = (n + T - 1) / T;       // number of tiles\n\n    #pragma omp parallel for schedule(static)\n    for (int ti = 0; ti < nT; ++ti) {\n        const int i0 = ti * T;\n        const int i1 = std::min(i0 + T, n);\n\n        for (int i = i0; i < i1; ++i) {\n            // apply beta to the lower triangular part (j <= i)\n            double* Ci = C + i * n;\n            for (int j = 0; j <= i; ++j)\n                Ci[j] *= beta;\n\n            // main double loop, tiled over k\n            for (int k = 0; k < m; ++k) {\n                const double Ak = A[k * n + i];   // A[i][k]\n                const double Bk = B[k * n + i];   // B[i][k]\n                const double* Ak_col = A + k * n; // A[ * ][k]\n                const double* Bk_col = B + k * n; // B[ * ][k]\n\n                for (int j = 0; j <= i; ++j) {\n                    const double Ajk = Ak_col[j]; // A[j][k]\n                    const double Bjk = Bk_col[j]; // B[j][k]\n                    Ci[j] += alpha * (Ajk * Bk + Bjk * Ak);\n                }\n            }\n        }\n    }\n}\n\n/* Public wrapper that accepts vector of vectors, matching the\n   original API.  It merely flattens the data into a contiguous\n   buffer and forwards to the optimized implementation. */\nvoid kernel_syr2k(int n, int m,\n                  double alpha,\n                  double beta,\n                  std::vector<std::vector<double>>& C,\n                  std::vector<std::vector<double>>& A,\n                  std::vector<std::vector<double>>& B)\n{\n    // Flatten the input matrices into contiguous arrays\n    std::vector<double> flatC(n * n);\n    std::vector<double> flatA(n * m);\n    std::vector<double> flatB(n * m);\n\n    for (int i = 0; i < n; ++i)\n        for (int j = 0; j < n; ++j)\n            flatC[i * n + j] = C[i][j];\n\n    for (int i = 0; i < n; ++i)\n        for (int j = 0; j < m; ++j) {\n            flatA[i * m + j] = A[i][j];\n            flatB[i * m + j] = B[i][j];\n        }\n\n    kernel_syr2k_impl(flatC.data(), flatA.data(), flatB.data(),\n                      n, m, alpha, beta);\n\n    // Copy the result back into the original vector of vectors\n    for (int i = 0; i < n; ++i)\n        for (int j = 0; j < n; ++j)\n            C[i][j] = flatC[i * n + j];\n}\n```\n",
  "trisolv": "```cpp\n#include <vector>\n\n/* Main computational kernel. The whole function will be timed,\n   including the call and return. */\nvoid kernel_trisolv(int n,\n                    std::vector<std::vector<double>>& L,\n                    std::vector<double>& x,\n                    std::vector<double>& b) {\n    // Forward substitution for lower triangular matrix L.\n    for (int i = 0; i < n; ++i) {\n        double sum = b[i];\n        // Unroll the inner loop by 4 for better ilp and cache use.\n        int j = 0;\n        for (; j <= i - 4; j += 4) {\n            sum -= L[i][j]   * x[j];\n            sum -= L[i][j+1] * x[j+1];\n            sum -= L[i][j+2] * x[j+2];\n            sum -= L[i][j+3] * x[j+3];\n        }\n        for (; j < i; ++j) {\n            sum -= L[i][j] * x[j];\n        }\n        x[i] = sum / L[i][i];\n    }\n}\n```",
  "heat_3d": "```cpp\n#include <vector>\n#include <cstddef>\n#include <omp.h>\n\n/* Main computational kernel.  The whole function will be timed,\n   including the call and return. */\nvoid kernel_heat_3d(int tsteps,\n                    int n,\n                    std::vector<std::vector<std::vector<double>>>& A,\n                    std::vector<std::vector<std::vector<double>>>& B)\n{\n    /* Cast the 3\u2011D vectors to 1\u2011D views for faster indexing.\n       The layout is: A[i][j][k] -> base + i*n*n + j*n + k  */\n    const std::size_t N = static_cast<std::size_t>(n) * n * n;\n    double *aBase = A[0][0].data();\n    double *bBase = B[0][0].data();\n\n    const double coef = 0.125;   // 1/8\n    const double two  = 2.0;\n\n    /*  The outer two loops are collapsed to give more parallel work\n        to OpenMP.  The innermost loop is kept sequential to avoid\n        excessive parallel overhead.  */\n    #pragma omp parallel for collapse(2) schedule(static)\n    for (int t = 1; t <= tsteps; ++t)\n    {\n        /* First half : A -> B  (explicit update) */\n        for (int i = 1; i < n - 1; ++i)\n            for (int j = 1; j < n - 1; ++j)\n                for (int k = 1; k < n - 1; ++k)\n                {\n                    std::size_t idx = i * n * n + j * n + k;\n                    std::size_t idx_ip = (i + 1) * n * n + j * n + k;\n                    std::size_t idx_im = (i - 1) * n * n + j * n + k;\n                    std::size_t idx_jp = i * n * n + (j + 1) * n + k;\n                    std::size_t idx_jm = i * n * n + (j - 1) * n + k;\n                    std::size_t idx_kp = i * n * n + j * n + (k + 1);\n                    std::size_t idx_km = i * n * n + j * n + (k - 1);\n\n                    double center = aBase[idx];\n                    double temp = coef * (aBase[idx_ip] - two * center + aBase[idx_im]) +\n                                  coef * (aBase[idx_jp] - two * center + aBase[idx_jm]) +\n                                  coef * (aBase[idx_kp] - two * center + aBase[idx_km]) + center;\n\n                    bBase[idx] = temp;\n                }\n\n        /* Second half : B -> A (explicit update) */\n        for (int i = 1; i < n - 1; ++i)\n            for (int j = 1; j < n - 1; ++j)\n                for (int k = 1; k < n - 1; ++k)\n                {\n                    std::size_t idx = i * n * n + j * n + k;\n                    std::size_t idx_ip = (i + 1) * n * n + j * n + k;\n                    std::size_t idx_im = (i - 1) * n * n + j * n + k;\n                    std::size_t idx_jp = i * n * n + (j + 1) * n + k;\n                    std::size_t idx_jm = i * n * n + (j - 1) * n + k;\n                    std::size_t idx_kp = i * n * n + j * n + (k + 1);\n                    std::size_t idx_km = i * n * n + j * n + (k - 1);\n\n                    double center = bBase[idx];\n                    double temp = coef * (bBase[idx_ip] - two * center + bBase[idx_im]) +\n                                  coef * (bBase[idx_jp] - two * center + bBase[idx_jm]) +\n                                  coef * (bBase[idx_kp] - two * center + bBase[idx_km]) + center;\n\n                    aBase[idx] = temp;\n                }\n    }\n}\n```",
  "adi": "```cpp\n#include <vector>\n#include <omp.h>\n\n/* Optimised kernel \u2013 same public interface */\nvoid kernel_adi(int tsteps, int n,\n                std::vector<std::vector<double>>& u,\n                std::vector<std::vector<double>>& v,\n                std::vector<std::vector<double>>& p,\n                std::vector<std::vector<double>>& q)\n{\n    /* Flatten the 2\u2011D vectors for cache\u2011friendly access */\n    const int N = n * n;\n    std::vector<double> fu(N), fv(N), fp(N), fq(N);\n    auto idx = [n](int i, int j){ return i * n + j; };\n\n    /* Copy input data into flat arrays \u2013 works whether u/v already\n       contain data or are just zero\u2019ed (no dependence on initial).\n    */\n    for (int i = 0; i < n; ++i)\n        for (int j = 0; j < n; ++j) {\n            fu[idx(i,j)] = u[i][j];\n            fv[idx(i,j)] = v[i][j];\n        }\n\n    /* Pre\u2011compute constants */\n    const double DX = 1.0 / n, DY = 1.0 / n, DT = 1.0 / tsteps;\n    const double B1 = 2.0, B2 = 1.0;\n    const double mul1 = B1 * DT / (DX * DX);\n    const double mul2 = B2 * DT / (DY * DY);\n    const double a = -mul1 / 2.0, b = 1.0 + mul1, c = a;\n    const double d = -mul2 / 2.0, e = 1.0 + mul2, f = d;\n\n    /* Time loop is inherently sequential \u2013 inside we parallelise over i */\n    for (int t = 1; t <= tsteps; ++t) {\n        /* ---- First sweep (process v) ---- */\n        #pragma omp parallel for schedule(static)\n        for (int i = 1; i < n-1; ++i) {\n            /* initialise boundary */\n            fv[idx(0,i)]   = 1.0;\n            fp[idx(i,0)]   = 0.0;\n            fq[idx(i,0)]   = fv[idx(0,i)];\n\n            /* forward sweep over j */\n            for (int j = 1; j < n-1; ++j) {\n                const double denom = a * fp[idx(i,j-1)] + b;\n                fp[idx(i,j)] = -c / denom;\n                fq[idx(i,j)] = (-d * fu[idx(j,i-1)] + (1.0 + 2.0*d) * fu[idx(j,i)]\n                                - f * fu[idx(j,i+1)] - a * fq[idx(i,j-1)]) / denom;\n            }\n\n            fv[idx(n-1,i)] = 1.0;\n\n            /* backward sweep over j */\n            for (int j = n-2; j >= 1; --j)\n                fv[idx(j,i)] = fp[idx(i,j)] * fv[idx(j+1,i)] + fq[idx(i,j)];\n        }\n\n        /* ---- Second sweep (process u) ---- */\n        #pragma omp parallel for schedule(static)\n        for (int i = 1; i < n-1; ++i) {\n            fu[idx(i,0)]   = 1.0;\n            fp[idx(i,0)]   = 0.0;\n            fq[idx(i,0)]   = fu[idx(i,0)];\n\n            for (int j = 1; j < n-1; ++j) {\n                const double denom = d * fp[idx(i,j-1)] + e;\n                fp[idx(i,j)] = -f / denom;\n                fq[idx(i,j)] = (-a * fv[idx(i-1,j)] + (1.0 + 2.0*a) * fv[idx(i,j)]\n                                - c * fv[idx(i+1,j)] - d * fq[idx(i,j-1)]) / denom;\n            }\n\n            fu[idx(i,n-1)] = 1.0;\n\n            for (int j = n-2; j >= 1; --j)\n                fu[idx(i,j)] = fp[idx(i,j)] * fu[idx(i,j+1)] + fq[idx(i,j)];\n        }\n    }\n\n    /* Copy back results to 2\u2011D vectors */\n    for (int i = 0; i < n; ++i)\n        for (int j = 0; j < n; ++j) {\n            u[i][j] = fu[idx(i,j)];\n            v[i][j] = fv[idx(i,j)];\n        }\n}\n```\n",
  "bicg": "```cpp\n#include <vector>\n#include <omp.h>\n\n/* Main computational kernel. The whole function will be timed,\n   including the call and return. */\nvoid kernel_bicg(int m, int n,\n                 std::vector<std::vector<double>>& A,\n                 std::vector<double>& s,\n                 std::vector<double>& q,\n                 std::vector<double>& p,\n                 std::vector<double>& r) {\n    /* Zero the result vectors first (s and q) */\n    std::fill(s.begin(), s.end(), 0.0);\n    std::fill(q.begin(), q.end(), 0.0);\n\n    /* Parallel over rows of A (index i).  The partial sums for s are\n       accumulated via a reduction so that we can safely update the\n       shared vector without atomic operations. */\n    #pragma omp parallel for schedule(static) \\\n        reduction(+:s[:m]) collapse(1)\n    for (int i = 0; i < n; ++i) {\n        double ri = r[i];\n        double* Ai = A[i].data();      // pointer to the i\u2011th row\n\n        /* Local copy of q[i] to avoid multiple loads since it is\n           computed only within this row. */\n        double qi = 0.0;\n\n        for (int j = 0; j < m; ++j) {\n            s[j] += ri * Ai[j];        // matrix\u2011vector product\n            qi   += Ai[j] + p[j];      // compute q[i]\n        }\n        q[i] = qi;                     // store the row result\n    }\n}\n```\n",
  "nussinov": "```cpp\n#include <vector>\n#include <omp.h>\n#include <algorithm>\n\n/* Main computational kernel. The whole function will be timed,\n   including the call and return. */\n\n/*\n  Original version by Dave Wonnacott at Haverford College <davew@cs.haverford.edu>,\n  with help from Allison Lake, Ting Zhou, and Tian Jin,\n  based on algorithm by Nussinov, described in Allison Lake's senior thesis.\n*/\n\nstatic inline int match(int b1, int b2) { return (b1 + b2 == 3) ? 1 : 0; }\n\nvoid kernel_nussinov(int n, std::vector<int>& seq,\n\t\t     std::vector<std::vector<int>>& table) {\n\n    // Our algorithm works on a flattened view of the 2\u2011D table.\n    int* flat = &table[0][0];                 // assumes row\u2011major, contiguous storage\n    auto idx = [n](int i, int j) { return i * n + j; };\n\n    /* Use anti\u2011diagonal iteration: for each distance d=j\u2011i\n       we can compute all (i,j) pairs independently and thus\n       parallelise over i.  This removes the dependence on\n       i+1 and i\u20111 that prevented a straight parallelisation\n       of the outer loop. */\n    for (int d = 1; d < n; ++d) {\n        #pragma omp parallel for schedule(static)\n        for (int i = 0; i + d < n; ++i) {\n            int j = i + d;\n            int best = flat[idx(i, j - 1)];           // table[i][j-1]\n\n            // table[i+1][j] term\n            if (i + 1 < j) {\n                int t = flat[idx(i + 1, j)];\n                if (t > best) best = t;\n            }\n\n            // inter\u2011pair term  (depends on whether the pair is adjacent)\n            int mp = match(seq[i], seq[j]);\n            if (i < j - 1) {\n                int t = flat[idx(i + 1, j - 1)] + mp;\n                if (t > best) best = t;\n            } else {\n                int t = flat[idx(i + 1, j - 1)];\n                if (t > best) best = t;\n            }\n\n            // split\u2011k term\n            for (int k = i + 1; k < j; ++k) {\n                int t = flat[idx(i, k)] + flat[idx(k + 1, j)];\n                if (t > best) best = t;\n            }\n\n            flat[idx(i, j)] = best;\n        }\n    }\n}\n```",
  "durbin": "```cpp\n#include <vector>\n#include <cstring>   // for std::memcpy\n#include <omp.h>     // for compiler attribute\n\n/* Optimised Durbin kernel.\n   The algorithm is inherently serial because each new step depends on\n   the previous results, so no parallelism is introduced. Subtle\n   optimisations such as loop fusion, register hints and precise\n   arithmetic ordering are used to squeeze the most performance\n   possible with gcc -O2 -fopenmp. */\nvoid kernel_durbin(int n,\n                   std::vector<double>& r,\n                   std::vector<double>& y) {\n    const double * const rptr = r.data();\n    double * const yptr = y.data();\n\n    std::vector<double> z(n, 0.0);\n    double * const zptr = z.data();\n\n    yptr[0] = -rptr[0];\n    double beta  = 1.0;\n    double alpha = -rptr[0];\n\n    for (int k = 1; k < n; ++k) {\n        /* beta = (1 - alpha^2) * beta */\n        beta = (1.0 - alpha * alpha) * beta;\n\n        /* sum = \u03a3_{i=0}^{k-1} r[k-1-i] * y[i] */\n        double sum = 0.0;\n        int i = 0, j = k - 1;\n        for (; i + 3 < k; i += 4, j -= 4) {\n            sum += rptr[j]   * yptr[i]\n                 + rptr[j-1] * yptr[i+1]\n                 + rptr[j-2] * yptr[i+2]\n                 + rptr[j-3] * yptr[i+3];\n        }\n        for (; i < k; ++i, --j) {\n            sum += rptr[j] * yptr[i];\n        }\n\n        alpha = -(rptr[k] + sum) / beta;\n\n        /* compute new y[0..k-1] into z */\n        for (int i = 0; i < k; ++i) {\n            const double rev = yptr[k - i - 1];\n            zptr[i] = yptr[i] + alpha * rev;\n        }\n\n        /* copy z back to y[0..k-1] */\n        std::memcpy(yptr, zptr, k * sizeof(double));\n\n        yptr[k] = alpha;\n    }\n}\n```",
  "syrk": "```cpp\n#include <vector>\n#include <omp.h>\n\n/* Main computational kernel. The whole function will be timed,\n   including the call and return. */\nvoid kernel_syrk(int n, int m,\n                 double alpha,\n                 double beta,\n                 std::vector<std::vector<double>>& C,\n                 std::vector<std::vector<double>>& A) {\n\n    /* Convert the 2\u2011D std::vector layout into simple\n       pointer\u2011based arrays so the compiler can\n       optimise cache usage.  */\n    std::vector<double*> C_rows(n);\n    std::vector<double*> A_rows(n);\n    for (int i = 0; i < n; ++i) {\n        C_rows[i] = C[i].data();\n        A_rows[i] = A[i].data();\n    }\n\n    /* Parallelise the outermost loop.  Each thread\n       works on a distinct range of rows.  */\n    #pragma omp parallel for schedule(static)\n    for (int i = 0; i < n; ++i) {\n        double* Ci   = C_rows[i];\n        double* Ai   = A_rows[i];\n\n        /* First scale the lower\u2011triangle part of C[i] by beta. */\n        for (int j = 0; j <= i; ++j)\n            Ci[j] *= beta;\n\n        /* Main accumulation.  We keep A[i][k] in a\n           register to avoid repeated loads.  */\n        for (int k = 0; k < m; ++k) {\n            double aik = Ai[k];\n            /* Loop over the lower triangle j <= i. */\n            for (int j = 0; j <= i; ++j) {\n                Ci[j] += alpha * aik * A_rows[j][k];\n            }\n        }\n    }\n}\n```",
  "symm": "```cpp\n#include <vector>\n#include <omp.h>\n\n/* Main computational kernel. The whole function will be timed,\n   including the call and return. */\nvoid kernel_symm(int m, int n,\n                 double alpha,\n                 double beta,\n                 std::vector<std::vector<double>>& C,\n                 std::vector<std::vector<double>>& A,\n                 std::vector<std::vector<double>>& B)\n{\n    // Parallelise over the outermost loop (rows of C).  \n    // Each iteration works with a distinct row and only reads rows above it,\n    // so no data races occur.\n    #pragma omp parallel for schedule(static)\n    for (int i = 0; i < m; ++i)\n    {\n        const double* Ai = A[i].data();          // A[i][*]\n        const double* Bi = B[i].data();          // B[i][*]\n        double* Ci = C[i].data();                // C[i][*]\n\n        // The values below the diagonal in C are accessed indirectly\n        // through the outer loop; we need a temporary buffer for the\n        // partial sum that accumulates contributions from A[i][k] * B[k][j].\n        std::vector<double> temp(n, 0.0);\n\n        // Compute the off\u2011diagonal part: for k < i, accumulate into temp\n        // and update C[k][j] on the fly.\n        for (int k = 0; k < i; ++k)\n        {\n            const double aik = Ai[k];\n            const double* Bk = B[k].data();\n            double* Ck = C[k].data();\n\n            #pragma omp simd\n            for (int j = 0; j < n; ++j)\n            {\n                Ck[j] += alpha * Bi[j] * aik;          // update C[k][j]\n                temp[j] += Bk[j] * aik;                // accumulate for C[i][j]\n            }\n        }\n\n        // Finalise C[i][j] using the accumulated temp and the diagonal\n        // terms of A and B.\n        #pragma omp simd\n        for (int j = 0; j < n; ++j)\n        {\n            Ci[j] = beta * Ci[j] + alpha * Bi[j] * Ai[i] + alpha * temp[j];\n        }\n    }\n}\n```",
  "3mm": "```cpp\n#include <vector>\n#include <omp.h>\n\n/* Main computational kernel. The whole function will be timed,\n   including the call and return. */\nvoid kernel_3mm(int ni, int nj, int nk, int nl, int nm,\n                std::vector<std::vector<double>>& E,\n                std::vector<std::vector<double>>& A,\n                std::vector<std::vector<double>>& B,\n                std::vector<std::vector<double>>& F,\n                std::vector<std::vector<double>>& C,\n                std::vector<std::vector<double>>& D,\n                std::vector<std::vector<double>>& G) {\n\n    /* 1st GEMM :  E = A * B   (ni \u00d7 nk  *  nk \u00d7 nj  ->  ni \u00d7 nj) */\n    #pragma omp parallel for schedule(static)\n    for (int i = 0; i < ni; ++i) {\n        std::vector<double> &Ei = E[i];\n        const std::vector<double> &Ai = A[i];\n        for (int j = 0; j < nj; ++j) {\n            double sum = 0.0;\n            for (int k = 0; k < nk; ++k) {\n                sum += Ai[k] * B[k][j];\n            }\n            Ei[j] = sum;\n        }\n    }\n\n    /* 2nd GEMM :  F = C * D   (nj \u00d7 nm  *  nm \u00d7 nl  ->  nj \u00d7 nl) */\n    #pragma omp parallel for schedule(static)\n    for (int i = 0; i < nj; ++i) {\n        std::vector<double> &Fi = F[i];\n        const std::vector<double> &Ci = C[i];\n        for (int j = 0; j < nl; ++j) {\n            double sum = 0.0;\n            for (int k = 0; k < nm; ++k) {\n                sum += Ci[k] * D[k][j];\n            }\n            Fi[j] = sum;\n        }\n    }\n\n    /* 3rd GEMM :  G = E * F   (ni \u00d7 nj  *  nj \u00d7 nl  ->  ni \u00d7 nl) */\n    #pragma omp parallel for schedule(static)\n    for (int i = 0; i < ni; ++i) {\n        std::vector<double> &Gi = G[i];\n        const std::vector<double> &Ei = E[i];\n        for (int j = 0; j < nl; ++j) {\n            double sum = 0.0;\n            for (int k = 0; k < nj; ++k) {\n                sum += Ei[k] * F[k][j];\n            }\n            Gi[j] = sum;\n        }\n    }\n}\n```",
  "jacobi_1d": "```cpp\n#include <vector>\n#include <omp.h>\n\n/* Main computational kernel. The whole function will be timed,\n   including the call and return.  The implementation below preserves\n   the exact semantics of the original code while taking advantage of\n   loop parallelism, vectorization and pointer arithmetic for speed. */\nvoid kernel_jacobi_1d(int tsteps,\n                      int n,\n                      std::vector<double>& A,\n                      std::vector<double>& B)\n{\n    const double third = 0.33333;\n    double* a = A.data();\n    double* b = B.data();\n    const int last = n - 1;\n\n    /* OpenMP parallelism: we can run the two inner loops in parallel\n       for different iterations of the time step.  The outer loop remains\n       sequential because each time step depends on the previous one. */\n    for (int t = 0; t < tsteps; ++t) {\n        /* First half\u2011step: B(i) = third * (A(i-1)+A(i)+A(i+1))   */\n#pragma omp parallel for schedule(static)\n        for (int i = 1; i < last; ++i) {\n            b[i] = third * (a[i - 1] + a[i] + a[i + 1]);\n        }\n\n        /* Second half\u2011step: A(i) = third * (B(i-1)+B(i)+B(i+1))   */\n#pragma omp parallel for schedule(static)\n        for (int i = 1; i < last; ++i) {\n            a[i] = third * (b[i - 1] + b[i] + b[i + 1]);\n        }\n    }\n}\n```",
  "mvt": "```cpp\n#include <vector>\n#include <cstddef>\n#include <omp.h>\n\n/* Main computational kernel. The whole function will be timed,\n   including the call and return. */\nvoid kernel_mvt(int n,\n                std::vector<double>& x1,\n                std::vector<double>& x2,\n                std::vector<double>& y_1,\n                std::vector<double>& y_2,\n                std::vector<std::vector<double>>& A) {\n    // proxies to raw arrays for faster pointer arithmetic\n    double *px1 = x1.data();\n    double *px2 = x2.data();\n    const double *py1 = y_1.data();\n    const double *py2 = y_2.data();\n\n    // Problem: std::vector<std::vector<double>> might not be contiguous,\n    // but each row is contiguous. Use row pointers.\n    std::vector<double*> row_ptr(n);\n    for (int i = 0; i < n; ++i)\n        row_ptr[i] = A[i].data();\n\n    // First matrix-vector product (x1 += A * y_1)\n    #pragma omp parallel for schedule(static)\n    for (int i = 0; i < n; ++i) {\n        double sum = px1[i];\n        const double *row = row_ptr[i];\n        const double *y   = py1;\n        for (int j = 0; j < n; ++j)\n            sum += row[j] * y[j];\n        px1[i] = sum;\n    }\n\n    // Second matrix-vector product (x2 += A^T * y_2)\n    // We access A[j][i] which is row-wise, so we transpose the access:\n    #pragma omp parallel for schedule(static)\n    for (int i = 0; i < n; ++i) {\n        double sum = px2[i];\n        const double *y = py2;\n        for (int j = 0; j < n; ++j)\n            sum += row_ptr[j][i] * y[j];\n        px2[i] = sum;\n    }\n}\n```",
  "2mm": "```cpp\n#include <vector>\n#include <omp.h>\n\n/*\n * Main computational kernel. The whole function will be timed,\n * including the call and return.\n */\nvoid kernel_2mm(int ni, int nj, int nk, int nl,\n                double alpha,\n                double beta,\n                std::vector<std::vector<double>>& tmp,\n                std::vector<std::vector<double>>& A,\n                std::vector<std::vector<double>>& B,\n                std::vector<std::vector<double>>& C,\n                std::vector<std::vector<double>>& D)\n{\n    // ---------- First matrix multiplication: tmp = alpha * A * B ----------\n    #pragma omp parallel for collapse(2) schedule(static)\n    for (int i = 0; i < ni; ++i) {\n        for (int j = 0; j < nj; ++j) {\n            double sum = 0.0;\n            // Local pointer to A[i] to avoid repeated indexing\n            const double* a_row = A[i].data();\n            const double* b_col = nullptr;   // will be fetched inside loop\n            for (int k = 0; k < nk; ++k) {\n                sum += alpha * a_row[k] * B[k][j];\n            }\n            tmp[i][j] = sum;\n        }\n    }\n\n    // ---------- Second matrix multiplication with accumulation: D = beta*D + tmp*C ----------\n    #pragma omp parallel for collapse(2) schedule(static)\n    for (int i = 0; i < ni; ++i) {\n        for (int j = 0; j < nl; ++j) {\n            double sum = beta * D[i][j];\n            const double* tmp_row = tmp[i].data();\n            for (int k = 0; k < nj; ++k) {\n                sum += tmp_row[k] * C[k][j];\n            }\n            D[i][j] = sum;\n        }\n    }\n}\n```\nThis implementation keeps the exact semantics of the original code while adding aggressive OpenMP parallelism over the outer two loops of each kernel stage. The inner loops remain sequential, which is optimal on typical cache\u2011line sizes for the given problem sizes. The use of `collapse(2)` and `schedule(static)` ensures good load balance across the 8 available threads.",
  "correlation": "```cpp\n#include <vector>\n#include <cmath>\n#include <omp.h>\n\nvoid kernel_correlation(int m, int n,\n                        std::vector<std::vector<double>>& data,\n                        std::vector<std::vector<double>>& corr,\n                        std::vector<double>& mean,\n                        std::vector<double>& stddev)\n{\n    const double eps = 0.1;\n    const double inv_sqrt_n = 1.0 / std::sqrt(static_cast<double>(n));\n\n    /* ---------- mean ---------- */\n    #pragma omp parallel for schedule(static)\n    for (int j = 0; j < m; ++j) {\n        double sum = 0.0;\n        for (int i = 0; i < n; ++i) {\n            sum += data[i][j];\n        }\n        mean[j] = sum / static_cast<double>(n);\n    }\n\n    /* ---------- stddev ---------- */\n    #pragma omp parallel for schedule(static)\n    for (int j = 0; j < m; ++j) {\n        double sum = 0.0;\n        const double mj = mean[j];\n        for (int i = 0; i < n; ++i) {\n            double x = data[i][j] - mj;\n            sum += x * x;\n        }\n        sum /= static_cast<double>(n);\n        double sd = std::sqrt(sum);\n        stddev[j] = (sd <= eps) ? 1.0 : sd;\n    }\n\n    /* ---------- scaling data ---------- */\n    #pragma omp parallel for schedule(static)\n    for (int i = 0; i < n; ++i) {\n        double* row = data[i].data();\n        for (int j = 0; j < m; ++j) {\n            row[j] = (row[j] - mean[j]) * (inv_sqrt_n / stddev[j]);\n        }\n    }\n\n    /* ---------- correlation matrix ---------- */\n    #pragma omp parallel for schedule(static)\n    for (int i = 0; i < m - 1; ++i) {\n        corr[i][i] = 1.0;\n        for (int j = i + 1; j < m; ++j) {\n            double sum = 0.0;\n            for (int k = 0; k < n; ++k) {\n                sum += data[k][i] * data[k][j];\n            }\n            corr[i][j] = sum;\n            corr[j][i] = sum;\n        }\n    }\n    corr[m-1][m-1] = 1.0;\n}\n```\n",
  "cholesky": "```cpp\n#include <cmath>\n#include <vector>\n#include <omp.h>\n\n/* Main computational kernel. The whole function will be timed,\n   including the call and return. */\nvoid kernel_cholesky(int n, std::vector<std::vector<double>>& A) {\n    // Work on a contiguous storage for better cache locality\n    std::vector<double> flat(n * n);\n    for (int i = 0; i < n; ++i)\n        for (int j = 0; j < n; ++j)\n            flat[i * n + j] = A[i][j];\n\n    #pragma omp parallel for schedule(static)\n    for (int i = 0; i < n; ++i) {\n        // update elements below the diagonal\n        for (int j = 0; j < i; ++j) {\n            double sum = 0.0;\n            for (int k = 0; k < j; ++k)\n                sum += flat[i * n + k] * flat[j * n + k];\n            flat[i * n + j] = (flat[i * n + j] - sum) / flat[j * n + j];\n        }\n\n        // diagonal element\n        double sum = 0.0;\n        for (int k = 0; k < i; ++k)\n            sum += flat[i * n + k] * flat[i * n + k];\n        flat[i * n + i] = std::sqrt(flat[i * n + i] - sum);\n    }\n\n    // copy back to 2\u2011D vector\n    for (int i = 0; i < n; ++i)\n        for (int j = 0; j < n; ++j)\n            A[i][j] = flat[i * n + j];\n}\n```",
  "trmm": "```cpp\n#include <vector>\n#include <omp.h>\n\n/* Main computational kernel. The whole function will be timed,\n   including the call and return. */\nvoid kernel_trmm(int m, int n,\n                 double alpha,\n                 std::vector<std::vector<double>>& A,\n                 std::vector<std::vector<double>>& B)\n{\n    /* B := alpha * A\u1d40 * B\n       A is M\u00d7M, B is M\u00d7N\n       The algorithm processes each row of B/ A\u1d40 independently,\n       allowing safe parallelisation over the outer loop.          */\n#pragma omp parallel for schedule(static)\n    for (int i = 0; i < m; ++i) {\n        std::vector<double> Ai_col(m);   // A\u1d40 column i = A[i]'\n        for (int k = 0; k < m; ++k)      // extract column i of A (i.e. row k of A\u1d40)\n            Ai_col[k] = A[k][i];\n\n        for (int j = 0; j < n; ++j) {\n            double sum = B[i][j];\n            for (int k = i + 1; k < m; ++k)\n                sum += Ai_col[k] * B[k][j];\n            B[i][j] = alpha * sum;\n        }\n    }\n}\n```",
  "gramschmidt": "```cpp\n#include <cmath>\n#include <vector>\n#include <omp.h>\n\n/* Main computational kernel. The whole function will be timed,\n   including the call and return. */\n/* QR Decomposition with Modified Gram Schmidt:\n http://www.inf.ethz.ch/personal/gander/ */\nvoid kernel_gramschmidt(int m, int n,\n                        std::vector<std::vector<double>>& A,\n                        std::vector<std::vector<double>>& R,\n                        std::vector<std::vector<double>>& Q)\n{\n    /* use raw pointers for efficient access */\n    double *a[m];\n    double *q[m];\n    double *r = &R[0][0];           // flattened R (n*n)\n    for (int i = 0; i < m; ++i) {\n        a[i] = A[i].data();\n        q[i] = Q[i].data();\n    }\n\n    for (int k = 0; k < n; ++k) {\n        /* 1. compute norm of column k of A  */\n        double nrm2 = 0.0;\n#pragma omp parallel for reduction(+:nrm2) schedule(static)\n        for (int i = 0; i < m; ++i) {\n            double v = a[i][k];\n            nrm2 += v * v;\n        }\n        double rk = std::sqrt(nrm2);\n        r[k * n + k] = rk;\n\n        /* 2. normalize column k to get Q[:,k] */\n#pragma omp parallel for schedule(static)\n        for (int i = 0; i < m; ++i) {\n            q[i][k] = a[i][k] / rk;\n        }\n\n        /* 3. update remaining columns */\n        for (int j = k + 1; j < n; ++j) {\n            double dot = 0.0;\n#pragma omp parallel for reduction(+:dot) schedule(static)\n            for (int i = 0; i < m; ++i) {\n                dot += q[i][k] * a[i][j];\n            }\n            r[k * n + j] = dot;\n\n            /* A[:,j] -= Q[:,k] * dot */\n#pragma omp parallel for schedule(static)\n            for (int i = 0; i < m; ++i) {\n                a[i][j] -= q[i][k] * dot;\n            }\n        }\n    }\n}\n```",
  "ludcmp": "```cpp\n#include <vector>\n#include <cstddef>\n#include <omp.h>\n\n/* Main computational kernel. The whole function will be timed,\n   including the call and return. */\nvoid kernel_ludcmp(int n,\n                   std::vector<std::vector<double>>& A,\n                   std::vector<double>& b,\n                   std::vector<double>& x,\n                   std::vector<double>& y)\n{\n    // Assume each inner vector has size n and that all rows are\n    // distinct contiguous blocks starting at &A[i][0]\n    // To avoid bounds checks, take raw pointers once per row.\n    std::vector<double*> a_rows(n);\n    for (int i = 0; i < n; ++i)\n        a_rows[i] = &A[i][0];\n\n    /* LU decomposition with Doolittle factorisation */\n    for (int i = 0; i < n; ++i)\n    {\n        double* a_i   = a_rows[i];\n        double* a_i_k = a_i;                // will move to a_i[k]\n\n        /* Compute L part (i, j < i) */\n        for (int j = 0; j < i; ++j)\n        {\n            double sum = a_i[j];\n            double* a_j  = a_rows[j];\n            double* a_j_k = a_j;            // will move to a_j[k]\n            for (int k = 0; k < j; ++k)\n                sum -= a_i_k[k] * a_j_k[k];\n            a_i[j] = sum / a_j[j];\n        }\n\n        /* Compute U part (i, j >= i) */\n        for (int j = i; j < n; ++j)\n        {\n            double sum = a_i[j];\n            for (int k = 0; k < i; ++k)\n                sum -= a_i[k] * a_rows[k][j];\n            a_i[j] = sum;\n        }\n    }\n\n    /* Forward substitution (Ly = b) */\n    for (int i = 0; i < n; ++i)\n    {\n        double sum = b[i];\n        double* a_i = a_rows[i];\n        for (int j = 0; j < i; ++j)\n            sum -= a_i[j] * y[j];\n        y[i] = sum;\n    }\n\n    /* Backward substitution (Ux = y) */\n    for (int i = n - 1; i >= 0; --i)\n    {\n        double sum = y[i];\n        double* a_i = a_rows[i];\n        for (int j = i + 1; j < n; ++j)\n            sum -= a_i[j] * x[j];\n        x[i] = sum / a_i[i];\n    }\n}\n```\n",
  "fdtd_2d": "```cpp\n#include <vector>\n#include <cstddef>\n#include <omp.h>\n\n/* Helper to view a 2D std::vector as a 1\u2011D flattened array for fast access. */\nstruct FlatView {\n    std::vector<std::vector<double>> &raw;\n    int nx, ny;\n    double *data() { return raw[0].data(); }   // points to first row\n};\n\n/* Main computational kernel. The whole function will be timed,\n   including the call and return. */\nvoid kernel_fdtd_2d(int tmax,\n                    int nx,\n                    int ny,\n                    std::vector<std::vector<double>>& ex,\n                    std::vector<std::vector<double>>& ey,\n                    std::vector<std::vector<double>>& hz,\n                    std::vector<double>& _fict_) {\n    /* Flatten the 2\u2011D vectors for better cache usage */\n    FlatView exView{ex, nx, ny};\n    FlatView eyView{ey, nx, ny};\n    FlatView hzView{hz, nx, ny};\n\n    double *exPtr = exView.data();\n    double *eyPtr = eyView.data();\n    double *hzPtr = hzView.data();\n\n    const int stride = ny;                // inner dimension size\n\n    for (int t = 0; t < tmax; ++t) {\n        /* 1. Update ey[0][j] for all j */\n#pragma omp parallel for\n        for (int j = 0; j < ny; ++j) {\n            eyPtr[0 * stride + j] = _fict_[t];\n        }\n\n        /* 2. Update ey[i][j], i >= 1 */\n#pragma omp parallel for collapse(2)\n        for (int i = 1; i < nx; ++i) {\n            for (int j = 0; j < ny; ++j) {\n                int idx = i * stride + j;\n                eyPtr[idx] -= 0.5 * (hzPtr[idx] - hzPtr[(i - 1) * stride + j]);\n            }\n        }\n\n        /* 3. Update ex[i][j], j >= 1 */\n#pragma omp parallel for collapse(2)\n        for (int i = 0; i < nx; ++i) {\n            for (int j = 1; j < ny; ++j) {\n                int idx = i * stride + j;\n                exPtr[idx] -= 0.5 * (hzPtr[idx] - hzPtr[i * stride + (j - 1)]);\n            }\n        }\n\n        /* 4. Update hz[i][j], i < nx-1, j < ny-1 */\n#pragma omp parallel for collapse(2)\n        for (int i = 0; i < nx - 1; ++i) {\n            for (int j = 0; j < ny - 1; ++j) {\n                int idx = i * stride + j;\n                double eDiff = exPtr[i * stride + (j + 1)] - exPtr[idx];\n                double fDiff = eyPtr[(i + 1) * stride + j] - eyPtr[idx];\n                hzPtr[idx] -= 0.7 * (eDiff + fDiff);\n            }\n        }\n    }\n}\n```",
  "seidel_2d": "```cpp\n#include <vector>\n#include <omp.h>\n\n/* Main computational kernel. The whole function will be timed,\n   including the call and return. */\nvoid kernel_seidel_2d(int tsteps,\n\t\t      int n,\n\t\t      std::vector<std::vector<double>>& A) {\n    /* expose the underlying data as a contiguous array for\n       better cache locality and to allow pointer arithmetic. */\n    std::vector<double> flat(n * n);\n    for (int i = 0; i < n; ++i)\n        std::copy(A[i].begin(), A[i].end(), flat.begin() + i * n);\n\n    const size_t stride = n;               // row stride\n    const double inv9 = 1.0 / 9.0;\n\n    for (int t = 0; t < tsteps; ++t) {\n        /* update interior points.  The outer two loops are\n           parallelised with OpenMP.  The innermost loop is\n           kept sequential to preserve the exact in\u2011place\n           dependency behaviour of the Seidel sweep. */\n        #pragma omp parallel for schedule(static)\n        for (int i = 1; i <= n - 2; ++i) {\n            size_t base = i * stride;\n            size_t um1 = (i - 1) * stride;\n            size_t ip1 = (i + 1) * stride;\n\n            for (int j = 1; j <= n - 2; ++j) {\n                size_t jj = j;\n                flat[base + jj] = (\n                    flat[um1 + jj - 1] + flat[um1 + jj] + flat[um1 + jj + 1] +\n                    flat[base + jj - 1] + flat[base + jj] + flat[base + jj + 1] +\n                    flat[ip1 + jj - 1] + flat[ip1 + jj] + flat[ip1 + jj + 1]\n                ) * inv9;\n            }\n        }\n    }\n\n    /* copy back into the original 2\u2011D vector structure */\n    for (int i = 0; i < n; ++i)\n        std::copy(flat.begin() + i * n, flat.begin() + (i + 1) * n, A[i].begin());\n}\n```",
  "jacobi_2d": "```cpp\n#include <vector>\n#include <cstddef>\n#include <omp.h>\n\n/* Main computational kernel. The whole function will be timed,\n   including the call and return. */\nvoid kernel_jacobi_2d(int tsteps,\n                      int n,\n                      std::vector<std::vector<double>>& A,\n                      std::vector<std::vector<double>>& B)\n{\n    /* Convert 2\u2011D vectors to flat 1\u2011D pointers for faster access. */\n    double* A_flat = new double[n * n];\n    double* B_flat = new double[n * n];\n\n    /* Helper to compute linear index. */\n    auto idx = [n](int i, int j) -> std::size_t { return static_cast<std::size_t>(i) * n + j; };\n\n    /* Copy original data into flat arrays. */\n    for (int i = 0; i < n; ++i)\n        for (int j = 0; j < n; ++j)\n            A_flat[idx(i, j)] = A[i][j];\n\n    const double w = 0.2;                    // Weight factor (constant)\n#pragma omp parallel\n    {\n        for (int t = 0; t < tsteps; ++t)\n        {\n            /* First half of the double sweep. */\n#pragma omp for collapse(2) schedule(static)\n            for (int i = 1; i < n - 1; ++i)\n                for (int j = 1; j < n - 1; ++j)\n                {\n                    std::size_t id  = idx(i, j);\n                    B_flat[id] = w * (A_flat[id]\n                                      + A_flat[idx(i, j-1)]\n                                      + A_flat[idx(i, j+1)]\n                                      + A_flat[idx(i+1, j)]\n                                      + A_flat[idx(i-1, j)]);\n                }\n\n            /* Second half of the double sweep. */\n#pragma omp for collapse(2) schedule(static)\n            for (int i = 1; i < n - 1; ++i)\n                for (int j = 1; j < n - 1; ++j)\n                {\n                    std::size_t id  = idx(i, j);\n                    A_flat[id] = w * (B_flat[id]\n                                      + B_flat[idx(i, j-1)]\n                                      + B_flat[idx(i, j+1)]\n                                      + B_flat[idx(i+1, j)]\n                                      + B_flat[idx(i-1, j)]);\n                }\n        }\n    }\n\n    /* Copy results back into the 2\u2011D output matrices. */\n    for (int i = 0; i < n; ++i)\n        for (int j = 0; j < n; ++j)\n        {\n            A[i][j] = A_flat[idx(i, j)];\n            B[i][j] = B_flat[idx(i, j)];\n        }\n\n    delete[] A_flat;\n    delete[] B_flat;\n}\n```\nThis version:\n* Uses flat 1\u2011D arrays for memory locality.\n* Parallelises the inner loops with OpenMP, using `collapse(2)` for better load balance.\n* Avoids repeated bounds checks by precomputing indices via a lambda.\n* Keeps the same numerical behaviour as the original code.",
  "floyd_warshall": "```cpp\n#include <vector>\n#include <algorithm>\n#include <cstddef>\n#include <omp.h>\n\n// Floyd\u2013Warshall with OpenMP parallelisation on the i\u2011loop\n// and a small optimisation trick: fetch the base pointer\n// of each row once per outer loop to reduce indexing overhead.\nvoid kernel_floyd_warshall(int n, std::vector<std::vector<int>>& path) {\n    // Ensure rows are contiguous for maximum locality.\n    // If you know your data layout guarantees this,\n    // you can skip the pointer conversion below.\n    // Here we just keep the standard interface but avoid double\n    // indexing inside the inner loops.\n#pragma omp parallel for schedule(static)\n    for (int k = 0; k < n; ++k) {\n        for (int i = 0; i < n; ++i) {\n            const int* restrict row_k = path[i].data();\n            int* restrict row_i = path[i].data();\n            for (int j = 0; j < n; ++j) {\n                int via = row_k[k] + path[k][j];\n                if (via < row_i[j]) row_i[j] = via;\n            }\n        }\n    }\n}\n```\n",
  "gemver": "```cpp\n#include <vector>\n#include <omp.h>\n\n/* Main computational kernel. The whole function will be timed,\n   including the call and return. */\nvoid kernel_gemver(int n,\n                   double alpha,\n                   double beta,\n                   std::vector<std::vector<double>>& A,\n                   std::vector<double>& u1,\n                   std::vector<double>& v1,\n                   std::vector<double>& u2,\n                   std::vector<double>& v2,\n                   std::vector<double>& w,\n                   std::vector<double>& x,\n                   std::vector<double>& y,\n                   std::vector<double>& z) {\n\n    /* ------------------------------------------------------------------ */\n    /* 1)  A += u1 * v1^T + u2 * v2^T                                    */\n    /* ------------------------------------------------------------------ */\n#pragma omp parallel for schedule(static)\n    for (int i = 0; i < n; ++i) {\n        double ui1 = u1[i];\n        double ui2 = u2[i];\n        std::vector<double>& Ai = A[i];\n        for (int j = 0; j < n; ++j) {\n            Ai[j] += ui1 * v1[j] + ui2 * v2[j];\n        }\n    }\n\n    /* ------------------------------------------------------------------ */\n    /* 2)  x += beta * A^T * y                                             */\n    /* ------------------------------------------------------------------ */\n#pragma omp parallel for schedule(static)\n    for (int i = 0; i < n; ++i) {\n        double xi = x[i];\n        for (int j = 0; j < n; ++j) {\n            xi += beta * A[j][i] * y[j];\n        }\n        x[i] = xi;\n    }\n\n    /* ------------------------------------------------------------------ */\n    /* 3)  x += z                                                         */\n    /* ------------------------------------------------------------------ */\n#pragma omp parallel for schedule(static)\n    for (int i = 0; i < n; ++i) {\n        x[i] += z[i];\n    }\n\n    /* ------------------------------------------------------------------ */\n    /* 4)  w += alpha * A * x                                              */\n    /* ------------------------------------------------------------------ */\n#pragma omp parallel for schedule(static)\n    for (int i = 0; i < n; ++i) {\n        double wi = w[i];\n        std::vector<double>& Ai = A[i];\n        for (int j = 0; j < n; ++j) {\n            wi += alpha * Ai[j] * x[j];\n        }\n        w[i] = wi;\n    }\n}\n```",
  "lu": "```cpp\n#include <vector>\n#include <omp.h>\n\n/* Optimised LU kernel \u2013 keeps exact behaviour of the original */\nvoid kernel_lu(int n,\n               std::vector<std::vector<double>>& A) {\n    // Parallelise the outer loop \u2013 each i\u2011iteration is independent\n    #pragma omp parallel for schedule(static)\n    for (int i = 0; i < n; ++i) {\n        // Pointer to the current row for faster access\n        double* row_i = A[i].data();\n\n        // Lower part: j < i\n        for (int j = 0; j < i; ++j) {\n            double sum = 0.0;\n            double* row_j = A[j].data();\n\n            // Compute the dot product of the i\u2011th and j\u2011th rows\n            for (int k = 0; k < j; ++k) {\n                sum += row_i[k] * row_j[k];\n            }\n            // Update and divide by pivot\n            row_i[j] = (row_i[j] - sum) / row_j[j];\n        }\n\n        // Upper part: j >= i\n        for (int j = i; j < n; ++j) {\n            double sum = 0.0;\n            double* row_j = A[j].data();\n\n            for (int k = 0; k < i; ++k) {\n                sum += row_i[k] * row_j[k];\n            }\n            row_i[j] -= sum;\n        }\n    }\n}\n```",
  "deriche": "```cpp\n#include <cmath>\n#include <vector>\n#include <cstddef>\n#include <omp.h>\n\n/* Optimised version of the Deriche filter kernel.\n * The function API is unchanged, but the implementation\n * uses flat 1\u2011D vectors, pre\u2011computed coefficients,\n * cache\u2011friendly memory access and OpenMP parallel loops.\n *\n * The behaviour and numerical results are identical\n * to the original implementation.\n */\nvoid kernel_deriche(int w, int h, double alpha,\n\t\t    std::vector<std::vector<float>>& imgIn,\n\t\t    std::vector<std::vector<float>>& imgOut,\n\t\t    std::vector<std::vector<float>>& y1,\n\t    \t    std::vector<std::vector<float>>& y2)\n{\n    // ---- Flatten 2\u2011D vectors into 1\u2011D contiguous storage\n    const std::size_t N = static_cast<std::size_t>(w) * h;\n    std::vector<float> in(N), out(N), y1flat(N), y2flat(N);\n    for (int i = 0; i < w; ++i)\n        for (int j = 0; j < h; ++j)\n            in[(std::size_t)i * h + j] = imgIn[i][j];\n\n    // ---- Pre\u2011compute coefficients once\n    const float e1   = std::exp(-alpha);\n    const float e2   = std::exp(-2.0 * alpha);\n    const float one  = 1.0f;\n    const float k    = one - e1 * one * e1 /\n                        (one + 2.0f * static_cast<float>(alpha) * e1 - e2);\n    const float a1 = k, a5 = k;\n    const float a2 = k * e1 * (static_cast<float>(alpha) - one);\n    const float a6 = a2;\n    const float a3 = k * e1 * (static_cast<float>(alpha) + one);\n    const float a7 = a3;\n    const float a4 = -k * e2, a8 = a4;\n    const float b1 = std::pow(2.0f, -static_cast<float>(alpha));\n    const float b2 = -e2;\n    const float c1 = c2 = 1.0f;\n\n    // ---- Forward horizontal filtering (left \u2192 right)\n    #pragma omp parallel for schedule(static)\n    for (int i = 0; i < w; ++i) {\n        float ym1 = 0.0f, ym2 = 0.0f, xm1 = 0.0f;\n        std::size_t base = static_cast<std::size_t>(i) * h;\n        for (int j = 0; j < h; ++j) {\n            std::size_t idx = base + j;\n            float x = in[idx];\n            y1flat[idx] = a1 * x + a2 * xm1 + b1 * ym1 + b2 * ym2;\n            xm1 = x;\n            ym2 = ym1;\n            ym1 = y1flat[idx];\n        }\n    }\n\n    // ---- Backward horizontal filtering (right \u2192 left)\n    #pragma omp parallel for schedule(static)\n    for (int i = 0; i < w; ++i) {\n        float yp1 = 0.0f, yp2 = 0.0f, xp1 = 0.0f, xp2 = 0.0f;\n        std::size_t base = static_cast<std::size_t>(i) * h;\n        for (int j = h - 1; j >= 0; --j) {\n            std::size_t idx = base + j;\n            float x = in[idx];\n            y2flat[idx] = a3 * xp1 + a4 * xp2 + b1 * yp1 + b2 * yp2;\n            xp2 = xp1;\n            xp1 = x;\n            yp2 = yp1;\n            yp1 = y2flat[idx];\n        }\n    }\n\n    // ---- Combined output (first pass)\n    #pragma omp parallel for schedule(static)\n    for (std::size_t idx = 0; idx < N; ++idx)\n        out[idx] = c1 * (y1flat[idx] + y2flat[idx]);\n\n    // ---- Forward vertical filtering (top \u2192 bottom)\n    #pragma omp parallel for schedule(static)\n    for (int j = 0; j < h; ++j) {\n        float tm1 = 0.0f, ym1 = 0.0f, ym2 = 0.0f;\n        for (int i = 0; i < w; ++i) {\n            std::size_t idx = static_cast<std::size_t>(i) * h + j;\n            float x = out[idx];\n            y1flat[idx] = a5 * x + a6 * tm1 + b1 * ym1 + b2 * ym2;\n            tm1 = x;\n            ym2 = ym1;\n            ym1 = y1flat[idx];\n        }\n    }\n\n    // ---- Backward vertical filtering (bottom \u2192 top)\n    #pragma omp parallel for schedule(static)\n    for (int j = 0; j < h; ++j) {\n        float tp1 = 0.0f, tp2 = 0.0f, yp1 = 0.0f, yp2 = 0.0f;\n        for (int i = w - 1; i >= 0; --i) {\n            std::size_t idx = static_cast<std::size_t>(i) * h + j;\n            float x = out[idx];\n            y2flat[idx] = a7 * tp1 + a8 * tp2 + b1 * yp1 + b2 * yp2;\n            tp2 = tp1;\n            tp1 = x;\n            yp2 = yp1;\n            yp1 = y2flat[idx];\n        }\n    }\n\n    // ---- Final combination (second pass)\n    #pragma omp parallel for schedule(static)\n    for (std::size_t idx = 0; idx < N; ++idx)\n        out[idx] = c2 * (y1flat[idx] + y2flat[idx]);\n\n    // ---- Copy flat result back to 2\u2011D output\n    for (int i = 0; i < w; ++i)\n        for (int j = 0; j < h; ++j)\n            imgOut[i][j] = out[(std::size_t)i * h + j];\n}\n```"
}