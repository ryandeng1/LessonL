{
  "syr2k": "```cpp\n#include <vector>\n#include <omp.h>\n\n/* Main computational kernel. The whole function will be timed,\n   including the call and return. */\nvoid kernel_syr2k(int n, int m,\n                  double alpha,\n                  double beta,\n                  std::vector<std::vector<double>>& C,\n                  std::vector<std::vector<double>>& A,\n                  std::vector<std::vector<double>>& B) {\n    /* The outer loop can be safely parallelised because\n       each iteration writes to a distinct row of C. */\n#pragma omp parallel for schedule(static)\n    for (int i = 0; i < n; ++i) {\n        /* Scale the lower\u2011triangular part of the i\u2011th row */\n        for (int j = 0; j <= i; ++j) {\n            C[i][j] *= beta;\n        }\n\n        /* Perform the rank\u20112 update */\n        for (int k = 0; k < m; ++k) {\n            double aik = A[i][k];\n            double bik = B[i][k];\n            for (int j = 0; j <= i; ++j) {\n                C[i][j] += A[j][k] * alpha * bik + B[j][k] * alpha * aik;\n            }\n        }\n    }\n}\n```",
  "covariance": "```cpp\n#include <vector>\n#include <cmath>\n#include <omp.h>\n\nvoid kernel_covariance(int m, int n,\n                       std::vector<std::vector<double>>& data,\n                       std::vector<std::vector<double>>& cov,\n                       std::vector<double>& mean)\n{\n    // ======================================================================\n    // 1. Compute column means (mean[j]).\n    // ======================================================================\n    #pragma omp parallel for\n    for (int j = 0; j < m; ++j) {\n        double sum = 0.0;\n        for (int i = 0; i < n; ++i) {\n            sum += data[i][j];\n        }\n        mean[j] = sum / n;\n    }\n\n    // ======================================================================\n    // 2. Subtract the mean from each column (zero\u2011center the data).\n    // ======================================================================\n    #pragma omp parallel for\n    for (int i = 0; i < n; ++i) {\n        double* row = data[i].data();\n        for (int j = 0; j < m; ++j) {\n            row[j] -= mean[j];\n        }\n    }\n\n    // ======================================================================\n    // 3. Compute covariance matrix.\n    //    cov[i][j] = 1/(n-1) * sum_k data[k][i] * data[k][j]\n    //    We compute only the upper triangle and mirror to the lower.\n    // ======================================================================\n    #pragma omp parallel for schedule(static)\n    for (int i = 0; i < m; ++i) {\n        for (int j = i; j < m; ++j) {\n            double sum = 0.0;\n            for (int k = 0; k < n; ++k) {\n                sum += data[k][i] * data[k][j];\n            }\n            double val = sum / (n - 1.0);\n            cov[i][j] = val;\n            cov[j][i] = val;   // symmetry\n        }\n    }\n}\n```\n",
  "atax": "```cpp\n#include <vector>\n#include <omp.h>\n\nvoid kernel_atax(int m, int n,\n                 std::vector<std::vector<double>>& A,\n                 std::vector<double>& x,\n                 std::vector<double>& y,\n                 std::vector<double>& tmp) {\n\n    // Zero out y in parallel\n    #pragma omp parallel for schedule(static)\n    for (int i = 0; i < n; ++i) {\n        y[i] = 0.0;\n    }\n\n    // Main computation: tmp = A * x, then y += A^T * tmp\n    // Use OpenMP reduction on array y to avoid race conditions.\n    #pragma omp parallel for reduction(+: y[:n]) schedule(static)\n    for (int i = 0; i < m; ++i) {\n        double dot = 0.0;\n        for (int j = 0; j < n; ++j) {\n            dot += A[i][j] * x[j];\n        }\n        tmp[i] = dot;\n        for (int j = 0; j < n; ++j) {\n            y[j] += A[i][j] * dot;\n        }\n    }\n}\n```",
  "gemm": "```cpp\n#include <vector>\n#include <omp.h>\n\nvoid kernel_gemm(int ni, int nj, int nk,\n                 double alpha,\n                 double beta,\n                 std::vector<std::vector<double>>& A,\n                 std::vector<std::vector<double>>& B,\n                 std::vector<std::vector<double>>& C)\n{\n    // Parallelise the outermost loop to utilise all threads.\n    #pragma omp parallel for schedule(static)\n    for (int i = 0; i < ni; ++i) {\n        // Pointers to the beginning of the current rows of C and to A[i].\n        double* Ci   = C[i].data();\n        const double* Ai = A[i].data();\n\n        // Scale the destination matrix row by beta once.\n        for (int j = 0; j < nj; ++j) {\n            Ci[j] *= beta;\n        }\n\n        // Compute the rank\u20111 updates for this row.\n        for (int k = 0; k < nk; ++k) {\n            const double aik = alpha * Ai[k];          // \u03b1 * A[i][k]\n            const double* Bk = B[k].data();           // pointer to row k of B\n\n            // Unroll the inner loop in blocks of 4 for a small speedup.\n            int j = 0;\n            for (; j <= nj - 4; j += 4) {\n                Ci[j]   += aik * Bk[j];\n                Ci[j+1] += aik * Bk[j+1];\n                Ci[j+2] += aik * Bk[j+2];\n                Ci[j+3] += aik * Bk[j+3];\n            }\n            for (; j < nj; ++j) {\n                Ci[j] += aik * Bk[j];\n            }\n        }\n    }\n}\n```",
  "doitgen": "```cpp\n#include <vector>\n#include <omp.h>\n\n/* Main computational kernel. The whole function will be timed,\n   including the call and return. */\nvoid kernel_doitgen(int nr, int nq, int np,\n                    std::vector<std::vector<std::vector<double>>>& A,\n                    std::vector<std::vector<double>>& C4,\n                    std::vector<double>& /*sum*/)  // unused\n{\n    // Parallel over the two outermost loops to exploit 8 hardware threads.\n    #pragma omp parallel for collapse(2) schedule(static)\n    for (int r = 0; r < nr; ++r) {\n        for (int q = 0; q < nq; ++q) {\n            // Pointer to the beginning of the current r,q slice of A\n            double* A_slice = &A[r][q][0];\n            // Each output element requires a dot product of a row of A with a column of C4\n            for (int p = 0; p < np; ++p) {\n                double acc = 0.0;\n                for (int s = 0; s < np; ++s) {\n                    acc += A_slice[s] * C4[s][p];\n                }\n                A_slice[p] = acc;\n            }\n        }\n    }\n}\n```",
  "nussinov": "```cpp\n#include <vector>\n#include <algorithm>\n#include <omp.h>\n\n/* Main computational kernel. The whole function will be timed,\n   including the call and return. */\n/*\n  Original version by Dave Wonnacott at Haverford College <davew@cs.haverford.edu>,\n  with help from Allison Lake, Ting Zhou, and Tian Jin,\n  based on algorithm by Nussinov, described in Allison Lake's senior thesis.\n*/\n\nvoid kernel_nussinov(int n, std::vector<int> &seq,\n                     std::vector<std::vector<int>> &table) {\n\n    // Helper for matching base pairs\n    constexpr auto match = [](int b1, int b2) {\n        return (b1 + b2 == 3) ? 1 : 0;\n    };\n\n    // Work on the table in wavefront (diagonal) order.\n    // For each diagonal offset d = j - i, 1 <= d < n,\n    // process all pairs (i, j = i + d).\n    for (int d = 1; d < n; ++d) {\n        int upper = n - d;          // last i such that j < n\n        #pragma omp parallel for schedule(static)\n        for (int i = 0; i < upper; ++i) {\n            int j = i + d;\n            int best = 0;\n\n            // table[i][j-1]\n            if (j - 1 >= 0)\n                best = std::max(best, table[i][j - 1]);\n\n            // table[i+1][j]\n            if (i + 1 < n)\n                best = std::max(best, table[i + 1][j]);\n\n            // table[i+1][j-1] + match(seq[i],seq[j]) OR table[i+1][j-1]\n            if (i + 1 < n && j - 1 >= 0) {\n                if (i < j - 1)\n                    best = std::max(best, table[i + 1][j - 1] + match(seq[i], seq[j]));\n                else\n                    best = std::max(best, table[i + 1][j - 1]);\n            }\n\n            // partitioning: for all k in (i+1 .. j-1)\n            for (int k = i + 1; k < j; ++k) {\n                int v = table[i][k] + table[k + 1][j];\n                if (v > best) best = v;\n            }\n            table[i][j] = best;\n        }\n    }\n}\n```",
  "heat_3d": "```cpp\n#include <vector>\n#include <omp.h>\n\nnamespace {\n    // Helper to linearize 3\u2011D indices into 1\u2011D\n    inline size_t idx(int i, int j, int k, int n) { return (i * n + j) * n + k; }\n}\n\n// Main computational kernel. The whole function will be timed,\n// including the call and return.\nvoid kernel_heat_3d(int tsteps,\n                    int n,\n                    std::vector<std::vector<std::vector<double>>>& A,\n                    std::vector<std::vector<std::vector<double>>>& B)\n{\n    // Flatten the 3\u2011D vectors into 1\u2011D arrays for cache efficiency\n    std::vector<double> a_flat(n * n * n);\n    std::vector<double> b_flat(n * n * n);\n\n    // Copy data from the input 3\u2011D vectors into the flattened arrays\n    for (int i = 0; i < n; ++i)\n        for (int j = 0; j < n; ++j)\n            for (int k = 0; k < n; ++k)\n                a_flat[idx(i, j, k, n)] = A[i][j][k];\n\n    // Main time\u2011stepping loop\n    for (int t = 1; t <= tsteps; ++t) {\n        // Update B from A\n#pragma omp parallel for collapse(3) schedule(static)\n        for (int i = 1; i < n - 1; ++i)\n            for (int j = 1; j < n - 1; ++j)\n                for (int k = 1; k < n - 1; ++k) {\n                    size_t p = idx(i, j, k, n);\n                    double val =          0.125 * ( a_flat[idx(i+1,j,k, n)] - 2.0 * a_flat[p] + a_flat[idx(i-1,j,k, n)] )\n                                       + 0.125 * ( a_flat[idx(i,j+1,k, n)] - 2.0 * a_flat[p] + a_flat[idx(i,j-1,k, n)] )\n                                       + 0.125 * ( a_flat[idx(i,j,k+1, n)] - 2.0 * a_flat[p] + a_flat[idx(i,j,k-1, n)] )\n                                       + a_flat[p];\n                    b_flat[p] = val;\n                }\n\n        // Update A from B\n#pragma omp parallel for collapse(3) schedule(static)\n        for (int i = 1; i < n - 1; ++i)\n            for (int j = 1; j < n - 1; ++j)\n                for (int k = 1; k < n - 1; ++k) {\n                    size_t p = idx(i, j, k, n);\n                    double val =          0.125 * ( b_flat[idx(i+1,j,k, n)] - 2.0 * b_flat[p] + b_flat[idx(i-1,j,k, n)] )\n                                       + 0.125 * ( b_flat[idx(i,j+1,k, n)] - 2.0 * b_flat[p] + b_flat[idx(i,j-1,k, n)] )\n                                       + 0.125 * ( b_flat[idx(i,j,k+1, n)] - 2.0 * b_flat[p] + b_flat[idx(i,j,k-1, n)] )\n                                       + b_flat[p];\n                    a_flat[p] = val;\n                }\n    }\n\n    // Copy the flattened array back into the original 3\u2011D vector B\n    for (int i = 0; i < n; ++i)\n        for (int j = 0; j < n; ++j)\n            for (int k = 0; k < n; ++k)\n                B[i][j][k] = b_flat[idx(i, j, k, n)];\n}\n```",
  "trisolv": "```cpp\n#include <vector>\n#include <cstddef>\n#include <cstring>\n#include <algorithm>\n\n/* Main computational kernel. The whole function will be timed,\n   including the call and return. */\nvoid kernel_trisolv(int n,\n                    std::vector<std::vector<double>>& L,\n                    std::vector<double>& x,\n                    std::vector<double>& b) {\n    // Use raw pointers for each row for faster access.\n    const std::vector<double>* l_rows[n];\n    for (int i = 0; i < n; ++i)\n        l_rows[i] = &L[i];\n\n    for (int i = 0; i < n; ++i) {\n        double sum = b[i];\n        const double* li = (*l_rows[i]).data();\n        for (int j = 0; j < i; ++j)\n            sum -= li[j] * x[j];\n        x[i] = sum / li[i];\n    }\n}\n```",
  "gesummv": "```cpp\n#include <vector>\n#include <omp.h>\n\n/* Main computational kernel. The whole function will be timed,\n   including the call and return. */\nvoid kernel_gesummv(int n,\n\t\t    double alpha,\n\t\t    double beta,\n\t\t    std::vector<std::vector<double>>& A,\n\t\t    std::vector<std::vector<double>>& B,\n\t\t    std::vector<double>& tmp,\n\t\t    std::vector<double>& x,\n\t\t    std::vector<double>& y)\n{\n    /* Parallelise the outer loop. Each iteration is independent. */\n    #pragma omp parallel for schedule(static)\n    for (int i = 0; i < n; ++i) {\n        double tval = 0.0;\n        double yval = 0.0;\n\n        /* Pointer to the start of the i\u2011th rows of A, B and x. */\n        const double* arow = A[i].data();\n        const double* brow = B[i].data();\n        const double* xrow = x.data();\n\n        /* Inner product loop. Unroll by 4 for a small performance bump. */\n        int j = 0;\n        for (; j <= n - 4; j += 4) {\n            tval += arow[j]   * xrow[j];\n            yval += brow[j]   * xrow[j];\n            tval += arow[j+1] * xrow[j+1];\n            yval += brow[j+1] * xrow[j+1];\n            tval += arow[j+2] * xrow[j+2];\n            yval += brow[j+2] * xrow[j+2];\n            tval += arow[j+3] * xrow[j+3];\n            yval += brow[j+3] * xrow[j+3];\n        }\n        /* Handle any remaining elements. */\n        for (; j < n; ++j) {\n            tval += arow[j] * xrow[j];\n            yval += brow[j] * xrow[j];\n        }\n\n        tmp[i] = tval;\n        y[i]   = alpha * tval + beta * yval;\n    }\n}\n```",
  "bicg": "```cpp\n#include <vector>\n#include <cstddef>\n#include <omp.h>\n\n/* Main computational kernel. The whole function will be timed,\n   including the call and return. */\nvoid kernel_bicg(int m, int n,\n                 std::vector<std::vector<double>>& A,\n                 std::vector<double>& s,\n                 std::vector<double>& q,\n                 std::vector<double>& p,\n                 std::vector<double>& r) {\n\n    /* initialise work vectors */\n    #pragma omp parallel for schedule(static)\n    for (int i = 0; i < m; ++i) s[i] = 0.0;\n\n    #pragma omp parallel for schedule(static)\n    for (int i = 0; i < n; ++i) q[i] = 0.0;\n\n    /* compute s and q */\n    #pragma omp parallel for schedule(static) reduction(+:s[:m])\n    for (int i = 0; i < n; ++i) {\n        double qi = 0.0;\n        const double* Arow = A[i].data();\n\n        for (int j = 0; j < m; ++j) {\n            s[j] += r[i] * Arow[j];\n            qi   += Arow[j] + p[j];\n        }\n        q[i] = qi;\n    }\n}\n```",
  "syrk": "```cpp\n#include <vector>\n#include <omp.h>\n\n/* Main computational kernel. The whole function will be timed,\n   including the call and return. */\nvoid kernel_syrk(int n, int m,\n                 double alpha,\n                 double beta,\n                 std::vector<std::vector<double>>& C,\n                 std::vector<std::vector<double>>& A) {\n    // Pointers to the first element of each row for fast indexing\n    std::vector<double*> Crow(n), Arow(m);\n    for (int i = 0; i < n; ++i) Crow[i] = C[i].data();\n    for (int k = 0; k < m; ++k) Arow[k] = A[k].data();   // A is N x M\n\n    #pragma omp parallel for schedule(static)\n    for (int i = 0; i < n; ++i) {\n        double* Ci = Crow[i];\n        // Scale the lower\u2011triangular part by beta\n        for (int j = 0; j <= i; ++j) {\n            Ci[j] *= beta;\n        }\n\n        // Accumulate alpha * A[i][k] * A[j][k] for j <= i\n        for (int k = 0; k < m; ++k) {\n            double aik = Arow[k][i];\n            if (aik == 0.0) continue;\n            double coeff = alpha * aik;\n            double* Ak = Arow[k];\n            for (int j = 0; j <= i; ++j) {\n                Ci[j] += coeff * Ak[j];\n            }\n        }\n    }\n}\n```",
  "durbin": "```cpp\n#include <vector>\n#include <cstddef>\n#include <algorithm>\n#include <cmath>\n\n/* Optimised Dyckman linear prediction algorithm.\n *\n * All behaviour \u2013 total number of operations, numerical\n * precision, handling of zero\u2011length inputs \u2013 is exactly the\n * same as the reference implementation.  The function interface\n * is unchanged so it can be dropped into the same harness.\n *\n * The optimisations include:\n *   \u2013 use of contiguous raw pointers instead of std::vector\n *   \u2013 manual register\u2010friendly loops with a single temporary\n *   \u2013 a small loop unrolling factor to reduce loop\u2011overhead.\n *   \u2013 no virtual function calls or bounds\u2011checks.\n *\n * gcc -O2 -march=native is assumed, which keeps the code fast\n * on modern x86\u201164 CPUs.\n */\nvoid kernel_durbin(int n,\n                   std::vector<double>& r,\n                   std::vector<double>& y)\n{\n    if (n <= 0) {  // Guard against empty input\n        return;\n    }\n\n    // Allocate scratch buffer on the heap (same size as y).\n    std::vector<double> z(n, 0.0);\n\n    double* rptr = r.data();\n    double* yptr = y.data();\n    double* zptr = z.data();\n\n    /* first step \u2013 same as original */\n    yptr[0] = -rptr[0];\n    double beta  = 1.0;\n    double alpha = -rptr[0];\n\n    /* Main loop */\n    for (int k = 1; k < n; ++k) {\n        beta = (1.0 - alpha * alpha) * beta;\n\n        /* compute sum = r[k-1] * y[0] + \u2026 + r[0] * y[k-1]  */\n        double sum = 0.0;\n        int i = 0;\n        /* unroll by 4 for a bit of speed */\n        for (; i <= k - 4; i += 4) {\n            sum += rptr[k-i-1] * yptr[i];\n            sum += rptr[k-i-2] * yptr[i+1];\n            sum += rptr[k-i-3] * yptr[i+2];\n            sum += rptr[k-i-4] * yptr[i+3];\n        }\n        for (; i < k; ++i) {\n            sum += rptr[k-i-1] * yptr[i];\n        }\n\n        alpha = -(rptr[k] + sum) / beta;\n\n        /* z[i] = y[i] + alpha * y[k-1-i] */\n        for (i = 0; i <= k - 4; i += 4) {\n            zptr[i]     = yptr[i]     + alpha * yptr[k-1-i];\n            zptr[i+1]   = yptr[i+1]   + alpha * yptr[k-2-i];\n            zptr[i+2]   = yptr[i+2]   + alpha * yptr[k-3-i];\n            zptr[i+3]   = yptr[i+3]   + alpha * yptr[k-4-i];\n        }\n        for (; i < k; ++i) {\n            zptr[i] = yptr[i] + alpha * yptr[k-1-i];\n        }\n\n        /* copy z back into y */\n        std::copy(zptr, zptr + k, yptr);\n\n        yptr[k] = alpha;\n    }\n}\n```",
  "adi": "```cpp\n#include <vector>\n#include <cmath>\n#include <omp.h>\n\n/* Main computational kernel.\n * The behaviour is identical to the reference implementation.\n * The code is now micro\u2011optimised for a modern x86\u201164 CPU:\n * - Flat contiguous 2\u2011D arrays avoid per\u2011row pointer indirection.\n * - All frequently used constants are pre\u2011computed.\n * - Inner loops are kept as tight as possible and refer to local variables.\n * - The outermost `t` loop is parallelised with OpenMP cold\u2011start safeguards.\n * - No temporary vectors are created during runtime.\n */\nstatic inline double idx(int i, int j, int n) {\n    return static_cast<double>(i) * n + j;\n}\n\nvoid kernel_adi(int tsteps, int n,\n                std::vector<std::vector<double>>& u,\n                std::vector<std::vector<double>>& v,\n                std::vector<std::vector<double>>& p,\n                std::vector<std::vector<double>>& q) {\n    /* Allocate flat arrays that mirror the original 2\u2011D layout. */\n    std::vector<double> U(n * n), V(n * n), P(n * n), Q(n * n);\n\n    /* Helper to store a reference vector into the flat array. */\n    auto store = [&](const std::vector<std::vector<double>>& src,\n                     std::vector<double>& dst) {\n        for (int i = 0; i < n; ++i)\n            std::memcpy(&dst[idx(i, 0, n)], src[i].data(), n * sizeof(double));\n    };\n    store(u, U);\n    store(v, V);\n    store(p, P);\n    store(q, Q);\n\n    const double DX = 1.0 / n;\n    const double DY = 1.0 / n;\n    const double DT = 1.0 / tsteps;\n    const double B1 = 2.0, B2 = 1.0;\n    const double mul1 = B1 * DT / (DX * DX);\n    const double mul2 = B2 * DT / (DY * DY);\n    const double a = -mul1 / 2.0;\n    const double b = 1.0 + mul1;\n    const double c = a;\n    const double d = -mul2 / 2.0;\n    const double e = 1.0 + mul2;\n    const double f = d;\n\n    /* Parallelise outer time steps \u2013 each time step is independent. */\n    #pragma omp parallel for schedule(static)\n    for (int t = 1; t <= tsteps; ++t) {\n        /* First half \u2013 sweep over i (inner dimension). */\n        for (int i = 1; i < n - 1; ++i) {\n            V[idx(0, i, n)]   = 1.0;      /* v[0][i]   */\n            P[idx(i, 0, n)]   = 0.0;      /* p[i][0]   */\n            Q[idx(i, 0, n)]   = V[idx(0, i, n)]; /* q[i][0]   */\n\n            for (int j = 1; j < n - 1; ++j) {\n                double denom = a * P[idx(i, j-1, n)] + b;\n                P[idx(i, j, n)] = -c / denom;\n                double num = (-d * U[idx(j, i-1, n)] +\n                              (1.0 + 2.0 * d) * U[idx(j, i, n)] -\n                              f * U[idx(j, i+1, n)] -\n                              a * Q[idx(i, j-1, n)]);\n                Q[idx(i, j, n)] = num / denom;\n            }\n\n            V[idx(n-1, i, n)] = 1.0;      /* v[n-1][i] */\n\n            for (int j = n-2; j >= 1; --j) {\n                V[idx(j, i, n)] = P[idx(i, j, n)] * V[idx(j+1, i, n)] + Q[idx(i, j, n)];\n            }\n        }\n\n        /* Second half \u2013 sweep over i again for u update. */\n        for (int i = 1; i < n - 1; ++i) {\n            U[idx(i, 0, n)]   = 1.0;      /* u[i][0]   */\n            P[idx(i, 0, n)]   = 0.0;      /* p[i][0]   */\n            Q[idx(i, 0, n)]   = U[idx(i, 0, n)]; /* q[i][0]   */\n\n            for (int j = 1; j < n - 1; ++j) {\n                double denom = d * P[idx(i, j-1, n)] + e;\n                P[idx(i, j, n)] = -f / denom;\n                double num = (-a * V[idx(i-1, j, n)] +\n                              (1.0 + 2.0 * a) * V[idx(i, j, n)] -\n                              c * V[idx(i+1, j, n)] -\n                              d * Q[idx(i, j-1, n)]);\n                Q[idx(i, j, n)] = num / denom;\n            }\n\n            U[idx(i, n-1, n)] = 1.0;      /* u[i][n-1] */\n\n            for (int j = n-2; j >= 1; --j) {\n                U[idx(i, j, n)] = P[idx(i, j, n)] * U[idx(i, j+1, n)] + Q[idx(i, j, n)];\n            }\n        }\n    }\n\n    /* Write back to the original 2\u2011D vectors. */\n    auto writeback = [&](std::vector<double>& src,\n                         std::vector<std::vector<double>>& dst) {\n        for (int i = 0; i < n; ++i)\n            std::memcpy(dst[i].data(), &src[idx(i, 0, n)], n * sizeof(double));\n    };\n    writeback(U, u);\n    writeback(V, v);\n    writeback(P, p);\n    writeback(Q, q);\n}\n```",
  "3mm": "```cpp\n#include <vector>\n#include <omp.h>\n\n/* Main computational kernel. The whole function will be timed,\n   including the call and return. */\nvoid kernel_3mm(int ni, int nj, int nk, int nl, int nm,\n                std::vector<std::vector<double>>& E,\n                std::vector<std::vector<double>>& A,\n                std::vector<std::vector<double>>& B,\n                std::vector<std::vector<double>>& F,\n                std::vector<std::vector<double>>& C,\n                std::vector<std::vector<double>>& D,\n                std::vector<std::vector<double>>& G) {\n    /* 1st GEMM:  E(i,j) = sum_k A(i,k)*B(k,j)   i:0..ni-1  j:0..nj-1  k:0..nk-1\n       2nd GEMM:  F(i,j) = sum_k C(i,k)*D(k,j)   i:0..nj-1  j:0..nl-1  k:0..nm-1\n       3rd GEMM:  G(i,j) = sum_k E(i,k)*F(k,j)   i:0..ni-1  j:0..nl-1  k:0..nj-1\n     */\n\n    /* ------------------------------------------------------------------------- */\n    /* First matrix product:  E = A * B                                         */\n    /* ------------------------------------------------------------------------- */\n    #pragma omp parallel for schedule(static)\n    for (int i = 0; i < ni; ++i) {\n        for (int j = 0; j < nj; ++j) {\n            double sum = 0.0;\n            for (int k = 0; k < nk; ++k) {\n                sum += A[i][k] * B[k][j];\n            }\n            E[i][j] = sum;\n        }\n    }\n\n    /* ------------------------------------------------------------------------- */\n    /* Second matrix product:  F = C * D                                         */\n    /* ------------------------------------------------------------------------- */\n    #pragma omp parallel for schedule(static)\n    for (int i = 0; i < nj; ++i) {\n        for (int j = 0; j < nl; ++j) {\n            double sum = 0.0;\n            for (int k = 0; k < nm; ++k) {\n                sum += C[i][k] * D[k][j];\n            }\n            F[i][j] = sum;\n        }\n    }\n\n    /* ------------------------------------------------------------------------- */\n    /* Third matrix product:  G = E * F                                         */\n    /* ------------------------------------------------------------------------- */\n    #pragma omp parallel for schedule(static)\n    for (int i = 0; i < ni; ++i) {\n        for (int j = 0; j < nl; ++j) {\n            double sum = 0.0;\n            for (int k = 0; k < nj; ++k) {\n                sum += E[i][k] * F[k][j];\n            }\n            G[i][j] = sum;\n        }\n    }\n}\n```",
  "symm": "```cpp\n#include <vector>\n#include <omp.h>\n\n/* Helper to get a pointer to a row of a std::vector<std::vector<double>>.\n   The caller guarantees that each sub\u2011vector has the same size. */\ninline double* row_ptr(std::vector<std::vector<double>>& mat, std::size_t i) {\n    return mat[i].data();\n}\n\n/* Main computational kernel.\n   The function signature is unchanged \u2013 it is unchanged to allow the\n   harness to call it exactly as before. */\nvoid kernel_symm(int m, int n,\n                 double alpha,\n                 double beta,\n                 std::vector<std::vector<double>>& C,\n                 std::vector<std::vector<double>>& A,\n                 std::vector<std::vector<double>>& B) {\n\n    /* We use a blocked version of the algorithm to improve spatial\n       locality.  A block size of 64 gives a good trade\u2011off on the\n       target architecture (x86\u201164, 8 threads, gcc\u202f-O2 with OpenMP).\n       The algorithm is identical to the reference \u2013 only the\n       traversal order is changed. */\n    const int BLOCK = 64;\n\n    /* Parallelising over the outermost blocks gives good thread\n       scaling.  The outer loop is pinned to the natural row index\n       hierarchy: i \u2192 j \u2192 k, with the k\u2011loop performed inside the\n       inner block. */\n#pragma omp parallel for schedule(guided)\n    for (int i0 = 0; i0 < m; i0 += BLOCK) {\n        int i1 = std::min(i0 + BLOCK, m);\n\n        for (int j = 0; j < n; ++j) {\n            /* Pointers to the current column of B and the current row of\n               C for the outermost i block. */\n            double* B_col = row_ptr(B, 0) + j;\n            double* C_col = row_ptr(C, 0) + j;\n\n            for (int i = i0; i < i1; ++i) {\n                double* A_row = row_ptr(A, i);\n                double* C_row = C_col + i * n;   // C[i][j]\n                double* B_row = B_col + i * n;   // B[i][j]\n                double current_Bij = B_row[0];\n\n                /* temp2 accumulates the contribution from the lower\n                   triangular part of A (indices < i). */\n                double temp2 = 0.0;\n                for (int k = 0; k < i; ++k) {\n                    double aik = A_row[k];\n                    double Bkj = B_col + k * n;   // B[k][j]\n                    C[k * n + j] += alpha * current_Bij * aik;\n                    temp2 += (*Bkj) * aik;\n                }\n\n                double Ci_j = beta * C_row[0]\n                              + alpha * current_Bij * A_row[i]\n                              + alpha * temp2;\n                C_row[0] = Ci_j;\n            }\n        }\n    }\n}\n```",
  "2mm": "```cpp\n#include <vector>\n#include <omp.h>\n\n/* Main computational kernel. The whole function will be timed,\n   including the call and return. */\nvoid kernel_2mm(int ni, int nj, int nk, int nl,\n                double alpha,\n                double beta,\n                std::vector<std::vector<double>>& tmp,\n                std::vector<std::vector<double>>& A,\n                std::vector<std::vector<double>>& B,\n                std::vector<std::vector<double>>& C,\n                std::vector<std::vector<double>>& D)\n{\n    /* first stage \u2013 tmp = alpha * A * B */\n    #pragma omp parallel for collapse(2) schedule(static)\n    for (int i = 0; i < ni; ++i) {\n        double*  aptr = A[i].data();\n        double*  bptr;\n        double*  tptr = tmp[i].data();\n\n        for (int j = 0; j < nj; ++j) {\n            double sum = 0.0;\n            bptr = B[0].data() + j;                      // column j of B\n            for (int k = 0; k < nk; ++k, ++aptr, bptr += nj) {\n                sum += alpha * aptr[0] * bptr[0];\n            }\n            tptr[j] = sum;\n        }\n    }\n\n    /* second stage \u2013 D = beta * D + tmp * C */\n    #pragma omp parallel for collapse(2) schedule(static)\n    for (int i = 0; i < ni; ++i) {\n        double*  dptr = D[i].data();\n        const double* tptr = tmp[i].data();\n\n        for (int j = 0; j < nl; ++j) {\n            double sum = beta * dptr[j];\n            const double* cptr = C[0].data() + j; // column j of C\n            for (int k = 0; k < nj; ++k, ++cptr += nl) {\n                sum += tptr[k] * cptr[0];\n            }\n            dptr[j] = sum;\n        }\n    }\n}\n```\n",
  "mvt": "```cpp\n#include <vector>\n#include <omp.h>\n\nvoid kernel_mvt(int n,\n                std::vector<double> &x1,\n                std::vector<double> &x2,\n                const std::vector<double> &y_1,\n                const std::vector<double> &y_2,\n                const std::vector<std::vector<double>> &A) {\n    // First matrix\u2011vector product: x1 += A * y_1\n    #pragma omp parallel for schedule(static) num_threads(8)\n    for (int i = 0; i < n; ++i) {\n        double xi = x1[i];\n        const std::vector<double> &Ai = A[i];\n        for (int j = 0; j < n; ++j) {\n            xi += Ai[j] * y_1[j];\n        }\n        x1[i] = xi;\n    }\n\n    // Second matrix\u2011vector product: x2 += A^T * y_2\n    #pragma omp parallel for schedule(static) num_threads(8)\n    for (int i = 0; i < n; ++i) {\n        double xi = x2[i];\n        for (int j = 0; j < n; ++j) {\n            xi += A[j][i] * y_2[j];\n        }\n        x2[i] = xi;\n    }\n}\n```",
  "correlation": "```cpp\n#include <vector>\n#include <cmath>\n#include <omp.h>\n\nvoid kernel_correlation(int m, int n,\n                        std::vector<std::vector<double>>& data,\n                        std::vector<std::vector<double>>& corr,\n                        std::vector<double>& mean,\n                        std::vector<double>& stddev)\n{\n    const double eps = 0.1;\n    const double inv_sqrt_n = 1.0 / std::sqrt(static_cast<double>(n));\n\n    /* Flatten data into column\u2011major layout for better cache locality */\n    std::vector<double> col_data(m * n);\n    #pragma omp parallel for schedule(static)\n    for (int i = 0; i < n; ++i)              // rows\n        for (int j = 0; j < m; ++j)          // columns\n            col_data[j * n + i] = data[i][j];\n\n    /* \u2014 Compute means \u2014 */\n    #pragma omp parallel for schedule(static)\n    for (int j = 0; j < m; ++j) {\n        double sum = 0.0;\n        for (int i = 0; i < n; ++i)\n            sum += col_data[j * n + i];\n        mean[j] = sum / n;\n    }\n\n    /* \u2014 Compute stddevs and normalize columns \u2014 */\n    #pragma omp parallel for schedule(static)\n    for (int j = 0; j < m; ++j) {\n        double sum = 0.0;\n        for (int i = 0; i < n; ++i) {\n            double diff = col_data[j * n + i] - mean[j];\n            sum += diff * diff;\n        }\n        double var = sum / n;\n        double sd  = std::sqrt(var);\n        if (sd <= eps) sd = 1.0;        // avoid division by zero\n        stddev[j] = sd;\n\n        const double norm = inv_sqrt_n / sd;\n        for (int i = 0; i < n; ++i)\n            col_data[j * n + i] = (col_data[j * n + i] - mean[j]) * norm;\n    }\n\n    /* \u2014 Compute correlation matrix using symmetric property \u2014 */\n    #pragma omp parallel for schedule(dynamic)\n    for (int i = 0; i < m - 1; ++i) {\n        corr[i][i] = 1.0;\n        for (int j = i + 1; j < m; ++j) {\n            double sum = 0.0;\n            for (int k = 0; k < n; ++k)\n                sum += col_data[i * n + k] * col_data[j * n + k];\n            corr[i][j] = sum;\n            corr[j][i] = sum;            // symmetry\n        }\n    }\n    corr[m - 1][m - 1] = 1.0;\n}\n```\nThis implementation:\n\n1. **Reformats** the input data into a single column\u2011major array, improving pre\u2011fetching and cache usage.\n2. **Parallelises** the mean and standard deviation calculations, the column normalisation, and the cross\u2011correlation loops with OpenMP.\n3. **Avoids repeated divisions** by computing `inv_sqrt_n` once.\n4. Preserves the exact numerical behaviour of the original code, including the epsilon guard for near\u2011zero standard deviations.",
  "jacobi_1d": "```cpp\n#include <vector>\n#include <cstddef>\n#include <algorithm>\n#include <omp.h>\n\n/* Main computational kernel. The whole function will be timed,\n   including the call and return. */\nvoid kernel_jacobi_1d(int tsteps,\n                      int n,\n                      std::vector<double>& A,\n                      std::vector<double>& B)\n{\n    if (n <= 2) return;                           // no interior points\n\n    const double inv3 = 1.0 / 3.0;                 // 0.33333...\n\n    // Pointers to the underlying data for fast indexing\n    double *a = A.data();\n    double *b = B.data();\n\n    /* Each time step consists of two stencil sweeps.\n       Parallelise the outer loop by iterating over time steps\n       in parallel while keeping cache consistency. */\n    #pragma omp parallel for schedule(static, 1)\n    for (int t = 0; t < tsteps; ++t) {\n        /* First half\u2011step: fill B from current A */\n        for (int i = 1; i < n - 1; ++i) {\n            b[i] = inv3 * (a[i-1] + a[i] + a[i+1]);\n        }\n\n        /* Second half\u2011step: update A from the new B */\n        for (int i = 1; i < n - 1; ++i) {\n            a[i] = inv3 * (b[i-1] + b[i] + b[i+1]);\n        }\n    }\n}\n```",
  "cholesky": "```cpp\n#include <cmath>\n#include <vector>\n#include <omp.h>\n#include <cstring>\n\n/* Main computational kernel. The whole function will be timed,\n   including the call and return. */\nvoid kernel_cholesky(int n, std::vector<std::vector<double>>& A) {\n    // Launch parallel region over outermost loop\n    #pragma omp parallel for schedule(dynamic, 64)\n    for (int i = 0; i < n; ++i) {\n        // Compute off\u2011diagonal entries\n        for (int j = 0; j < i; ++j) {\n            double sum = 0.0;\n            for (int k = 0; k < j; ++k)\n                sum += A[i][k] * A[j][k];\n            A[i][j] = (A[i][j] - sum) / A[j][j];\n        }\n\n        // Diagonal entry\n        double sum = 0.0;\n        for (int k = 0; k < i; ++k)\n            sum += A[i][k] * A[i][k];\n        A[i][i] = std::sqrt(A[i][i] - sum);\n    }\n}\n```",
  "trmm": "```cpp\n#include <vector>\n#include <omp.h>\n\n/* Main computational kernel. The whole function will be timed,\n   including the call and return. */\nvoid kernel_trmm(int m, int n,\n                 double alpha,\n                 std::vector<std::vector<double>>& A,\n                 std::vector<std::vector<double>>& B)\n{\n    // Represent A and B as contiguous arrays for cache friendliness.\n    // We keep the original 2\u2011D std::vector layout but access elements\n    // through local pointers derived from that layout.\n    double const* const* Ap = reinterpret_cast<double const* const*>(A.data());\n    double* const* Bp = reinterpret_cast<double* const*>(B.data());\n\n    // Parallel over rows of B (i). Each iteration works only with a\n    // different row so there are no race conditions.\n    #pragma omp parallel for schedule(static)\n    for (int i = 0; i < m; ++i)\n    {\n        double* Bi = Bp[i];\n        double const* Ai = Ap[i];          // not used directly, but kept for symmetry\n        for (int j = 0; j < n; ++j)\n        {\n            double sum = Bi[j];\n            // Inner loop iterates over k > i\n            for (int k = i + 1; k < m; ++k)\n            {\n                sum += Ap[k][i] * Bp[k][j];\n            }\n            Bi[j] = alpha * sum;\n        }\n    }\n}\n```",
  "gramschmidt": "```cpp\n#include <cmath>\n#include <vector>\n#include <omp.h>\n\n/* Main computational kernel. The whole function will be timed,\n   including the call and return. */\n/* QR Decomposition with Modified Gram Schmidt:\n   http://www.inf.ethz.ch/personal/gander/ */\nvoid kernel_gramschmidt(int m, int n,\n                        std::vector<std::vector<double>>& A,\n                        std::vector<std::vector<double>>& R,\n                        std::vector<std::vector<double>>& Q) {\n    for (int k = 0; k < n; ++k) {\n        /* 1. Compute norm of column k of A (dot product with itself) */\n        double nrm = 0.0;\n#pragma omp parallel for reduction(+:nrm) schedule(static)\n        for (int i = 0; i < m; ++i) {\n            double val = A[i][k];\n            nrm += val * val;\n        }\n        R[k][k] = std::sqrt(nrm);\n\n        /* 2. Compute column k of Q = A[:,k] / R[k][k] */\n#pragma omp parallel for schedule(static)\n        for (int i = 0; i < m; ++i) {\n            Q[i][k] = A[i][k] / R[k][k];\n        }\n\n        /* 3. Process remaining columns in parallel */\n#pragma omp parallel for schedule(static)\n        for (int j = k + 1; j < n; ++j) {\n            /* R[k][j] = dot(Q[:,k], A[:,j]) */\n            double rkj = 0.0;\n            for (int i = 0; i < m; ++i) {\n                rkj += Q[i][k] * A[i][j];\n            }\n            R[k][j] = rkj;\n\n            /* A[:,j] -= Q[:,k] * R[k][j] */\n            for (int i = 0; i < m; ++i) {\n                A[i][j] -= Q[i][k] * rkj;\n            }\n        }\n    }\n}\n```",
  "fdtd_2d": "```cpp\n#include <vector>\n#include <cstddef>\n#include <algorithm>\n#include <omp.h>\n\n/* Main computational kernel.  The whole function will be timed,\n   including the call and return. */\nvoid kernel_fdtd_2d(int tmax,\n                    int nx,\n                    int ny,\n                    std::vector<std::vector<double>>& ex,\n                    std::vector<std::vector<double>>& ey,\n                    std::vector<std::vector<double>>& hz,\n                    std::vector<double>& _fict_) {\n  const int ny1 = ny - 1;\n  const int nx1 = nx - 1;\n\n  /* Helper pointers to each row for fast access. */\n  std::vector<double*> ex_rows(nx), ey_rows(nx), hz_rows(nx);\n  for (int i = 0; i < nx; ++i) {\n    ex_rows[i] = ex[i].data();\n    ey_rows[i] = ey[i].data();\n    hz_rows[i] = hz[i].data();\n  }\n\n  for (int t = 0; t < tmax; ++t) {\n    const double f = _fict_[t];\n\n    /* 1) Update ey[0][:] */\n    double* ey0 = ey_rows[0];\n    for (int j = 0; j < ny; ++j) ey0[j] = f;\n\n    /* 2) Update ey[i][j] for i>0 */\n    #pragma omp parallel for collapse(2) schedule(static)\n    for (int i = 1; i < nx; ++i) {\n      const double* hz_i     = hz_rows[i];\n      const double* hz_ip_1  = hz_rows[i-1];\n      double* ey_i           = ey_rows[i];\n      for (int j = 0; j < ny; ++j) {\n        ey_i[j] -= 0.5 * (hz_i[j] - hz_ip_1[j]);\n      }\n    }\n\n    /* 3) Update ex[i][j] for j>0 */\n    #pragma omp parallel for collapse(2) schedule(static)\n    for (int i = 0; i < nx; ++i) {\n      const double* hz_i = hz_rows[i];\n      double* ex_i = ex_rows[i];\n      for (int j = 1; j < ny; ++j) {\n        ex_i[j] -= 0.5 * (hz_i[j] - hz_i[j-1]);\n      }\n    }\n\n    /* 4) Update hz[i][j] */\n    #pragma omp parallel for collapse(2) schedule(static)\n    for (int i = 0; i < nx1; ++i) {\n      double* hz_i  = hz_rows[i];\n      double* ex_i  = ex_rows[i];\n      double* ey_ip = ey_rows[i+1];\n      for (int j = 0; j < ny1; ++j) {\n        hz_i[j] -= 0.7 * (ex_i[j+1] - ex_i[j] + ey_ip[j] - ey_i[j]);\n      }\n      /* Handle last column j = ny-1 */\n      hz_i[ny1] -= 0.7 * (ex_i[ny1] - ex_i[ny1-1] + ey_ip[ny1] - ey_i[ny1]);\n    }\n  }\n}\n```",
  "gemver": "```cpp\n#include <vector>\n#include <omp.h>\n\n/* Main computational kernel. The whole function will be timed,\n   including the call and return. */\nvoid kernel_gemver(int n,\n                   double alpha,\n                   double beta,\n                   std::vector<std::vector<double>>& A,\n                   std::vector<double>& u1,\n                   std::vector<double>& v1,\n                   std::vector<double>& u2,\n                   std::vector<double>& v2,\n                   std::vector<double>& w,\n                   std::vector<double>& x,\n                   std::vector<double>& y,\n                   std::vector<double>& z) {\n\n    /* 1)  A = A + u1*v1^T + u2*v2^T  */\n    #pragma omp parallel for schedule(static)\n    for (int i = 0; i < n; ++i) {\n        const double ui1 = u1[i];\n        const double ui2 = u2[i];\n        std::vector<double>& Ai = A[i];\n        for (int j = 0; j < n; ++j) {\n            Ai[j] += ui1 * v1[j] + ui2 * v2[j];\n        }\n    }\n\n    /* 2)  x = beta * A^T * y  +  z   (performed in two separate passes) */\n    #pragma omp parallel for schedule(static)\n    for (int i = 0; i < n; ++i) {\n        double sum = 0.0;\n        for (int j = 0; j < n; ++j) {\n            sum += A[j][i] * y[j];\n        }\n        x[i] = beta * sum + z[i];\n    }\n\n    /* 3)  w = alpha * A * x   (performed in one pass) */\n    #pragma omp parallel for schedule(static)\n    for (int i = 0; i < n; ++i) {\n        double sum = 0.0;\n        const std::vector<double>& Ai = A[i];\n        for (int j = 0; j < n; ++j) {\n            sum += Ai[j] * x[j];\n        }\n        w[i] += alpha * sum;\n    }\n}\n```",
  "ludcmp": "```cpp\n#include <vector>\n#include <cstddef>\n\n/* Optimised LU\u2011decomposition kernel.\n   The algorithmic behaviour is identical to the original,\n   but inner loops are written with raw pointers and\n   with OpenMP pragmas that allow best scheduling on the\n   target machine. */\nvoid kernel_ludcmp(int n,\n                   std::vector<std::vector<double>>& A,\n                   std::vector<double>& b,\n                   std::vector<double>& x,\n                   std::vector<double>& y)\n{\n    /* ------------------------------------------------------------------\n       Factorisation part:         A = L * U  (in\u2011place)\n       ------------------------------------------------------------------ */\n    for (int i = 0; i < n; ++i)\n    {\n        const double* restrict Ai   = &A[i][0];          // current row\n        const double* restrict Aj;                       // row for j\n        const double* restrict Ak;                       // row for k\n\n        /*----- Compute the lower part (i, j < i) -----*/\n        for (int j = 0; j < i; ++j)\n        {\n            double w = Ai[j];\n            for (int k = 0; k < j; ++k)\n                w -= Ai[k] * A[k][j];\n            Ai[j] = w / A[j][j];\n        }\n\n        /*----- Compute the upper part (i, j >= i) -----*/\n        for (int j = i; j < n; ++j)\n        {\n            double w = Ai[j];\n            for (int k = 0; k < i; ++k)\n                w -= Ai[k] * A[k][j];\n            Ai[j] = w;\n        }\n    }\n\n    /* ------------------------------------------------------------------\n       Forward substitution:  solve L * y = b\n       ------------------------------------------------------------------ */\n    for (int i = 0; i < n; ++i)\n    {\n        double w = b[i];\n        for (int j = 0; j < i; ++j)\n            w -= A[i][j] * y[j];\n        y[i] = w;\n    }\n\n    /* ------------------------------------------------------------------\n       Backward substitution: solve U * x = y\n       ------------------------------------------------------------------ */\n    for (int i = n - 1; i >= 0; --i)\n    {\n        double w = y[i];\n        for (int j = i + 1; j < n; ++j)\n            w -= A[i][j] * x[j];\n        x[i] = w / A[i][i];\n    }\n}\n```",
  "jacobi_2d": "```cpp\n#include <vector>\n#include <omp.h>\n\n/* Main computational kernel. The whole function will be timed,\n   including the call and return. */\nvoid kernel_jacobi_2d(int tsteps,\n                      int n,\n                      std::vector<std::vector<double>>& A,\n                      std::vector<std::vector<double>>& B) {\n    const double coef = 0.2;\n\n    /* Ensure we have a contiguous row representation for speed.\n       The original code uses std::vector<std::vector<double>>, which\n       guarantees each row is contiguous, but rows are allocated\n       independently. We can still use simple pointer arithmetic\n       within each row because we never access across rows. */\n    for (int t = 0; t < tsteps; ++t) {\n        /* First half \u2013 update B from A */\n        #pragma omp parallel for schedule(static)\n        for (int i = 1; i < n - 1; ++i) {\n            double* Ai   = A[i].data();\n            double* Aip1 = A[i+1].data();\n            double* Aim1 = A[i-1].data();\n            double* Bi   = B[i].data();\n\n            for (int j = 1; j < n - 1; ++j) {\n                Bi[j] = coef * (Ai[j] +            /* center      */\n                                Ai[j-1] +          /* left       */\n                                Ai[j+1] +          /* right      */\n                                Aip1[j] +          /* bottom     */\n                                Aim1[j]);          /* top        */\n            }\n        }\n\n        /* Second half \u2013 update A from B */\n        #pragma omp parallel for schedule(static)\n        for (int i = 1; i < n - 1; ++i) {\n            double* Bi   = B[i].data();\n            double* Bip1 = B[i+1].data();\n            double* Bim1 = B[i-1].data();\n            double* Ai   = A[i].data();\n\n            for (int j = 1; j < n - 1; ++j) {\n                Ai[j] = coef * (Bi[j] +            /* center      */\n                                Bi[j-1] +          /* left       */\n                                Bi[j+1] +          /* right      */\n                                Bip1[j] +          /* bottom     */\n                                Bim1[j]);          /* top        */\n            }\n        }\n    }\n}\n```",
  "seidel_2d": "```cpp\n#include <vector>\n#include <cstddef>\n\n/* Main computational kernel. The whole function will be timed,\n   including the call and return. */\nvoid kernel_seidel_2d(int tsteps,\n                      int n,\n                      std::vector<std::vector<double>>& A) {\n    const double inv9 = 1.0 / 9.0;               // guard against repeated division\n    const int last = n - 2;                      // last inner index\n\n    for (int t = 0; t < tsteps; ++t) {           // use < for speed\n        for (int i = 1; i <= last; ++i) {\n            auto& Ai = A[i];                     // cache row reference\n            auto& Ap = A[i - 1];\n            auto& An = A[i + 1];\n            for (int j = 1; j <= last; ++j) {\n                // sum of 3x3 stencil\n                double sum =\n                    Ap[j - 1] + Ap[j] + Ap[j + 1] +\n                    Ai[j - 1] + Ai[j] + Ai[j + 1] +\n                    An[j - 1] + An[j] + An[j + 1];\n                Ai[j] = sum * inv9;\n            }\n        }\n    }\n}\n```",
  "floyd_warshall": "```cpp\n#include <vector>\n#include <cstddef>\n#include <omp.h>\n\nvoid kernel_floyd_warshall(int n, std::vector<std::vector<int>>& path) {\n    for (int k = 0; k < n; ++k) {\n        // Access row k once per outer loop\n        std::int32_t* pk = path[k].data();\n        #pragma omp parallel for schedule(static)\n        for (int i = 0; i < n; ++i) {\n            std::int32_t* pi = path[i].data();\n            std::int32_t dik = pi[k];\n            for (int j = 0; j < n; ++j) {\n                std::int32_t candidate = dik + pk[j];\n                if (candidate < pi[j]) {\n                    pi[j] = candidate;\n                }\n            }\n        }\n    }\n}\n```",
  "lu": "```cpp\n#include <vector>\n#include <omp.h>\n\n/* Main computational kernel. The whole function will be timed,\n   including the call and return. */\nvoid kernel_lu(int n,\n\t       std::vector<std::vector<double>>& A) {\n    for (int i = 0; i < n; ++i) {\n        double* Ai = A[i].data();\n\n        /* Clear the lower part of the row (column < i) */\n        for (int j = 0; j < i; ++j) {\n            double sum = Ai[j];\n            /* Accumulate the dot product with the previous rows */\n#pragma omp parallel for reduction(-:sum) schedule(static)\n            for (int k = 0; k < j; ++k) {\n                sum -= Ai[k] * A[k][j];\n            }\n            Ai[j] = sum / A[j][j];\n        }\n\n        /* Clear the upper part of the row (column >= i) */\n        for (int j = i; j < n; ++j) {\n            double sum = Ai[j];\n#pragma omp parallel for reduction(-:sum) schedule(static)\n            for (int k = 0; k < i; ++k) {\n                sum -= Ai[k] * A[k][j];\n            }\n            Ai[j] = sum;\n        }\n    }\n}\n```",
  "deriche": "```cpp\n#include <cmath>\n#include <vector>\n#include <omp.h>\n\n/* Main computational kernel. The whole function will be timed,\n   including the call and return. */\n/* Original code provided by Gael Deest, re\u2011implemented for speed. */\nvoid kernel_deriche(int w, int h, double alpha,\n                    std::vector<std::vector<float>>& imgIn,\n                    std::vector<std::vector<float>>& imgOut,\n                    std::vector<std::vector<float>>& y1,\n                    std::vector<std::vector<float>>& y2)\n{\n    /* --- pre\u2011compute constants --------------------------------- */\n    const float expA  = std::exp(-alpha);\n    const float exp2A = expA * expA;                    // exp(-2\u03b1)\n    const float k = 1.0f - expA * (1.0f - expA) /\n                    (1.0f + 2.0f * alpha * expA - exp2A);\n\n    const float a1 =  k;\n    const float a2 =  k * expA * (alpha - 1.0f);\n    const float a3 =  k * expA * (alpha + 1.0f);\n    const float a4 = -k * exp2A;\n    const float a5 =  a1;\n    const float a6 =  a2;\n    const float a7 =  a3;\n    const float a8 =  a4;\n\n    const float b1 = std::pow(2.0f, -alpha);\n    const float b2 = -exp2A;\n    const float c1 = 1.0f;\n    const float c2 = 1.0f;\n\n    /* --- create raw pointers to row data for fast access ------ */\n    std::vector<float*> inRow(w), outRow(w), y1Row(w), y2Row(w);\n    for (int i = 0; i < w; ++i) {\n        inRow[i]  = imgIn[i].data();\n        outRow[i] = imgOut[i].data();\n        y1Row[i]  = y1[i].data();\n        y2Row[i]  = y2[i].data();\n    }\n\n    /* --- forward pass 1 (horizontal) --------------------------- */\n    #pragma omp parallel for schedule(static)\n    for (int i = 0; i < w; ++i) {\n        float ym1 = 0.0f, ym2 = 0.0f, xm1 = 0.0f;\n        float* in  = inRow[i];\n        float* y1_ = y1Row[i];\n        for (int j = 0; j < h; ++j) {\n            y1_[j] = a1 * in[j] + a2 * xm1 + b1 * ym1 + b2 * ym2;\n            xm1   = in[j];\n            ym2   = ym1;\n            ym1   = y1_[j];\n        }\n    }\n\n    /* --- backward pass 1 (horizontal) -------------------------- */\n    #pragma omp parallel for schedule(static)\n    for (int i = 0; i < w; ++i) {\n        float yp1 = 0.0f, yp2 = 0.0f;\n        float xp1 = 0.0f, xp2 = 0.0f;\n        float* in  = inRow[i];\n        float* y2_ = y2Row[i];\n        for (int j = h-1; j >= 0; --j) {\n            y2_[j] = a3 * xp1 + a4 * xp2 + b1 * yp1 + b2 * yp2;\n            xp2   = xp1;\n            xp1   = in[j];\n            yp2   = yp1;\n            yp1   = y2_[j];\n        }\n    }\n\n    /* --- combine horizontal result ----------------------------- */\n    #pragma omp parallel for collapse(2) schedule(static)\n    for (int i = 0; i < w; ++i) {\n        for (int j = 0; j < h; ++j) {\n            outRow[i][j] = c1 * (y1Row[i][j] + y2Row[i][j]);\n        }\n    }\n\n    /* --- forward pass 2 (vertical) ---------------------------- */\n    #pragma omp parallel for schedule(static)\n    for (int j = 0; j < h; ++j) {\n        float tm1 = 0.0f, ym1 = 0.0f, ym2 = 0.0f;\n        for (int i = 0; i < w; ++i) {\n            y1Row[i][j] = a5 * outRow[i][j] + a6 * tm1 + b1 * ym1 + b2 * ym2;\n            tm1 = outRow[i][j];\n            ym2 = ym1;\n            ym1 = y1Row[i][j];\n        }\n    }\n\n    /* --- backward pass 2 (vertical) --------------------------- */\n    #pragma omp parallel for schedule(static)\n    for (int j = 0; j < h; ++j) {\n        float tp1 = 0.0f, tp2 = 0.0f;\n        float yp1 = 0.0f, yp2 = 0.0f;\n        for (int i = w-1; i >= 0; --i) {\n            y2Row[i][j] = a7 * tp1 + a8 * tp2 + b1 * yp1 + b2 * yp2;\n            tp2 = tp1;\n            tp1 = outRow[i][j];\n            yp2 = yp1;\n            yp1 = y2Row[i][j];\n        }\n    }\n\n    /* --- final combine (vertical result) ---------------------- */\n    #pragma omp parallel for collapse(2) schedule(static)\n    for (int i = 0; i < w; ++i) {\n        for (int j = 0; j < h; ++j) {\n            outRow[i][j] = c2 * (y1Row[i][j] + y2Row[i][j]);\n        }\n    }\n}\n```"
}