{
  "covariance": "```cpp\n#include <vector>\n#include <cstddef>\n#include <omp.h>\n\nvoid kernel_covariance(int m, int n,\n                       std::vector<std::vector<double>>& data,\n                       std::vector<std::vector<double>>& cov,\n                       std::vector<double>& mean)\n{\n    /* --- 1. compute column means ------------------------------------- */\n    #pragma omp parallel for schedule(static)\n    for (int j = 0; j < m; ++j) {\n        double sum = 0.0;\n        for (int i = 0; i < n; ++i)\n            sum += data[i][j];\n        mean[j] = sum / n;\n    }\n\n    /* --- 2. center the data ------------------------------------------ */\n    #pragma omp parallel for collapse(2) schedule(static)\n    for (int i = 0; i < n; ++i)\n        for (int j = 0; j < m; ++j)\n            data[i][j] -= mean[j];\n\n    /* --- 3. covariance matrix ---------------------------------------- */\n    const double inv_n1 = 1.0 / (n - 1.0);\n    #pragma omp parallel for schedule(static)\n    for (int i = 0; i < m; ++i) {\n        // use a cache line aligned local copy for faster writes\n        std::vector<double> row(cov[i].size(), 0.0);\n        for (int k = 0; k < n; ++k) {\n            double di = data[k][i];\n            for (int j = i; j < m; ++j) {\n                row[j] += di * data[k][j];\n            }\n        }\n        for (int j = i; j < m; ++j) {\n            double val = row[j] * inv_n1;\n            cov[i][j] = val;\n            cov[j][i] = val;\n        }\n    }\n}\n```",
  "gemm": "```cpp\n#include <vector>\n#include <cstddef>\n#include <omp.h>\n\nvoid kernel_gemm(int ni, int nj, int nk,\n\t\t double alpha,\n\t\t double beta,\n\t\t std::vector<std::vector<double>>& A,\n\t\t std::vector<std::vector<double>>& B,\n\t\t std::vector<std::vector<double>>& C)\n{\n    /* Parallelise the outermost loop (over rows of A / C).  \n       Each thread works on a disjoint set of rows, so no\n       synchronisation is required. */\n\n    #pragma omp parallel for schedule(static, 16)\n    for (int i = 0; i < ni; ++i) {\n        double* restrict a_row = A[i].data();          // A[i, :]\n        double* restrict c_row = C[i].data();          // C[i, :]\n\n        /* Scale C[i,:] by beta */\n        double* c_ptr = c_row;\n        const int j_end = nj;\n        for (int j = 0; j < j_end; ++j, ++c_ptr) {\n            *c_ptr = (*c_ptr) * beta;\n        }\n\n        /* Compute alpha*A[i,:]*B + ... using a blocking layout\n           that remains cache\u2011friendly.  The innermost loop is over\n           columns of B (and thus columns of C).  A[i,k] is reused\n           for each nj element. */\n        for (int k = 0; k < nk; ++k) {\n            double aik = alpha * a_row[k];      // pre\u2011compute alpha*A[i,k]\n            double* restrict b_row = B[k].data(); // B[k,:]\n            double* restrict c_ptr_inner = c_row;\n            const int j_end_inner = nj;\n            for (int j = 0; j < j_end_inner; ++j, ++c_ptr_inner, ++b_row) {\n                *c_ptr_inner += aik * (*b_row);\n            }\n        }\n    }\n}\n```",
  "nussinov": "```cpp\n#include <vector>\n#include <algorithm>\n#include <omp.h>\n\n/* Main computational kernel. The whole function will be timed,\n   including the call and return. */\n/*\n  Original version by Dave Wonnacott at Haverford College <davew@cs.haverford.edu>,\n  with help from Allison Lake, Ting Zhou, and Tian Jin,\n  based on algorithm by Nussinov, described in Allison Lake's senior thesis.\n*/\n\nvoid kernel_nussinov(int n, std::vector<int>& seq,\n                     std::vector<std::vector<int>>& table) {\n\n    // Lambda to check for complementary bases: (a,b) = (1,2) or (2,1)\n    auto match = [](int b1, int b2) -> int {\n        return (b1 + b2 == 3) ? 1 : 0;\n    };\n\n    // Work in anti\u2011diagonals: for each distance d = j-i, process all i\n    for (int d = 1; d < n; ++d) {\n        // Parallelise across the starting index i\n        #pragma omp parallel for schedule(static)\n        for (int i = 0; i + d < n; ++i) {\n            int j = i + d;\n            int best = 0;\n\n            // table[i][j-1]  and table[i+1][j] are already computed\n            best = std::max(best, table[i][j - 1]);\n            best = std::max(best, table[i + 1][j]);\n\n            // table[i+1][j-1] and possible match\n            if (i + 1 < j) {\n                int val = table[i + 1][j - 1] + match(seq[i], seq[j]);\n                best = std::max(best, val);\n            }\n\n            // Split at k: table[i][k] + table[k+1][j]\n            for (int k = i + 1; k < j; ++k) {\n                best = std::max(best, table[i][k] + table[k + 1][j]);\n            }\n\n            table[i][j] = best;\n        }\n    }\n}\n```\nThe function now executes the same algorithm but computes each \u201cdistance\u201d (anti\u2011diagonal) in parallel using OpenMP. This preserves exact output while allowing multi\u2011thread acceleration on modern x86\u201164 CPUs.",
  "atax": "```cpp\n#include <vector>\n#include <omp.h>\n\n/* Main computational kernel. The whole function will be timed,\n   including the call and return. */\nvoid kernel_atax(int m, int n,\n                 std::vector<std::vector<double>>& A,\n                 const std::vector<double>& x,\n                 std::vector<double>& y,\n                 std::vector<double>& tmp) {\n\n    /* Zero y in parallel */\n    #pragma omp parallel for schedule(static)\n    for (int i = 0; i < n; ++i) {\n        y[i] = 0.0;\n    }\n\n    /* Each thread keeps its own local accumulator for y to avoid atomics  */\n    #pragma omp parallel\n    {\n        /* thread\u2011local storage for y */\n        std::vector<double> y_local(n, 0.0);\n\n        /* Parallelize the outer i\u2011loop.  Each iteration computes tmp[i]\n           and adds the contribution to the thread\u2011local accumulator. */\n        #pragma omp for schedule(static)\n        for (int i = 0; i < m; ++i) {\n            double sum = 0.0;\n            for (int j = 0; j < n; ++j) {\n                sum += A[i][j] * x[j];\n            }\n            tmp[i] = sum;\n\n            for (int j = 0; j < n; ++j) {\n                y_local[j] += A[i][j] * sum;\n            }\n        }\n\n        /* Combine local accumulators into the global y array */\n        #pragma omp for schedule(static)\n        for (int j = 0; j < n; ++j) {\n            y[j] += y_local[j];\n        }\n    }\n}\n```\n",
  "syr2k": "```cpp\n#include <vector>\n#include <cstddef>\n#include <omp.h>\n\n/*\n *  Performance\u2011tuned implementation of the symmetric rank\u20112k update\n *      C := beta*C + alpha*(A*B^T + B*A^T)\n *\n *  The original algorithm used a vector of vectors.  This version\n *  works on the same data layout (row major, zero\u2011based indexing)\n *  but accesses the underlying memory as a single 1\u2011D array for\n *  locality, removes bound checks, applies loop tiling, and\n *  parallelises the outer loop with OpenMP.\n *\n *  The prototype is unchanged so that the harness can call it\n *  directly.\n */\nvoid kernel_syr2k(int n, int m,\n                  double alpha,\n                  double beta,\n                  std::vector<std::vector<double>>& C,\n                  std::vector<std::vector<double>>& A,\n                  std::vector<std::vector<double>>& B)\n{\n    /* Layout: row\u2010major, each row stored in a contiguous block\n     * Larger matrices should avoid frequent bounds checks, so we\n     * take direct pointers to the first element of each row.   */\n    double const * const * Apr = reinterpret_cast<double const * const*>(A.data());\n    double const * const * Bpr = reinterpret_cast<double const * const*>(B.data());\n    double * const * Cpr = reinterpret_cast<double * const*>(C.data());\n\n    /* 8\u2011way cache blocking for the outer two loops.\n     * A small tile size keeps the working set on the L2/L3 caches\n     * while still giving sufficient parallel work.            */\n    constexpr int T = 32;                                 // tile size\n\n    #pragma omp parallel for schedule(static)\n    for (int ii = 0; ii < n; ii += T) {\n        int i_end = (ii + T < n) ? ii + T : n;\n        for (int jj = 0; jj < ii; jj += T) {             // only lower triangle\n            int j_end = (jj + T < ii) ? jj + T : ii;\n            for (int i = ii; i < i_end; ++i) {\n                double * Ci = Cpr[i] + jj;\n                double * Ai = Apr[i] + 0;  // A row i\n                double * Bi = Bpr[i] + 0;  // B row i\n\n                /* Apply beta to the existing lower triangle of C. */\n                for (int j = jj; j < j_end; ++j) {\n                    Ci[j - jj] *= beta;\n                }\n\n                /* Accumulate the rank\u20112 update */\n                for (int k = 0; k < m; ++k) {\n                    double aik = Ai[k];\n                    double bik = Bi[k];\n                    double ak = alpha * bik;\n                    double bk = alpha * aik;\n\n                    for (int j = jj; j < j_end; ++j) {\n                        Ci[j - jj] += Apr[j][k] * ak + Bpr[j][k] * bk;\n                    }\n                }\n            }\n        }\n    }\n\n    /* The diagonal elements (i==j) are updated in the same way,\n     * but since j <= i and the inner loops run up to j==i,\n     * the diagonal is handled automatically.               */\n}\n```",
  "doitgen": "```cpp\n#include <vector>\n#include <omp.h>\n\n/* Main computational kernel. The whole function will be timed,\n   including the call and return. */\nvoid kernel_doitgen(int nr, int nq, int np,\n                    std::vector<std::vector<std::vector<double>>>& A,\n                    std::vector<std::vector<double>>& C4,\n                    std::vector<double>& sum) {\n\n    // Use OpenMP to parallelise the outer two loops.\n    #pragma omp parallel for collapse(2) schedule(static)\n    for (int r = 0; r < nr; ++r) {\n        for (int q = 0; q < nq; ++q) {\n            // Pointers to the inner data for faster access.\n            double* Arq = A[r][q].data();          // A[r][q][s]\n            const double** C4col = new const double*[np]; // helper for C4[s][p]\n            for (int s = 0; s < np; ++s)\n                C4col[s] = C4[s].data();          // C4[s][p]\n\n            // Compute A[r][q][p] = sum_s A[r][q][s] * C4[s][p]\n            for (int p = 0; p < np; ++p) {\n                double s_val = 0.0;\n                for (int s = 0; s < np; ++s)\n                    s_val += Arq[s] * C4col[s][p];\n                sum[p] = s_val;   // temporary buffer\n            }\n\n            // Write the results back to A[r][q][p].\n            for (int p = 0; p < np; ++p)\n                Arq[p] = sum[p];\n\n            delete[] C4col;\n        }\n    }\n}\n```",
  "heat_3d": "```cpp\n#include <vector>\n#include <omp.h>\n\nstatic const double coeff = 0.125;\n\n/* Main computational kernel. The whole function will be timed,\n   including the call and return. */\nvoid kernel_heat_3d(int tsteps,\n                    int n,\n                    std::vector<std::vector<std::vector<double>>>& A,\n                    std::vector<std::vector<std::vector<double>>>& B) {\n\n    // Iterate over time steps\n    for (int t = 1; t <= tsteps; ++t) {\n        /* ----------------------------------\n           First phase: A -> B  (update B)\n           ---------------------------------- */\n#pragma omp parallel for collapse(2) schedule(static)\n        for (int i = 1; i < n - 1; ++i) {\n            for (int j = 1; j < n - 1; ++j) {\n                double* Ap = A[i][j].data();\n                double* Ap_p = A[i + 1][j].data();\n                double* Ap_m = A[i - 1][j].data();\n                double* Ajp = A[i][j + 1].data();\n                double* Ajm = A[i][j - 1].data();\n\n                double* Bp = B[i][j].data();\n\n                for (int k = 1; k < n - 1; ++k) {\n                    double a = Ap[k];\n                    double val = coeff*(Ap_p[k] - 2.0*a + Ap_m[k]) +\n                                 coeff*(Ajp[k] - 2.0*a + Ajm[k]) +\n                                 coeff*(Ap[k + 1] - 2.0*a + Ap[k - 1]) + a;\n                    Bp[k] = val;\n                }\n            }\n        }\n\n        /* ----------------------------------\n           Second phase: B -> A  (update A)\n           ---------------------------------- */\n#pragma omp parallel for collapse(2) schedule(static)\n        for (int i = 1; i < n - 1; ++i) {\n            for (int j = 1; j < n - 1; ++j) {\n                double* Bp = B[i][j].data();\n                double* Bp_p = B[i + 1][j].data();\n                double* Bp_m = B[i - 1][j].data();\n                double* Bjp = B[i][j + 1].data();\n                double* Bjm = B[i][j - 1].data();\n\n                double* Ap = A[i][j].data();\n\n                for (int k = 1; k < n - 1; ++k) {\n                    double b = Bp[k];\n                    double val = coeff*(Bp_p[k] - 2.0*b + Bp_m[k]) +\n                                 coeff*(Bjp[k] - 2.0*b + Bjm[k]) +\n                                 coeff*(Bp[k + 1] - 2.0*b + Bp[k - 1]) + b;\n                    Ap[k] = val;\n                }\n            }\n        }\n    }\n}\n```",
  "trisolv": "```cpp\n#include <vector>\n\n/* Forward substitution on a lower\u2011triangular matrix.\n   The routine keeps the original semantics but works on raw\n   pointers to employ better cache locality. */\nvoid kernel_trisolv(int n,\n        std::vector<std::vector<double>>& L,\n        std::vector<double>& x,\n        std::vector<double>& b) {\n    for (int i = 0; i < n; ++i) {\n        double xi = b[i];\n        const double* Li = L[i].data();   // pointer to the i\u2011th row\n        for (int j = 0; j < i; ++j) {\n            xi -= Li[j] * x[j];\n        }\n        x[i] = xi / Li[i];\n    }\n}\n```",
  "gesummv": "```cpp\n#include <vector>\n#include <omp.h>\n\n/* Main computational kernel. The whole function will be timed,\n   including the call and return. */\nvoid kernel_gesummv(int n,\n                    double alpha,\n                    double beta,\n                    std::vector<std::vector<double>>& A,\n                    std::vector<std::vector<double>>& B,\n                    std::vector<double>& tmp,\n                    std::vector<double>& x,\n                    std::vector<double>& y)\n{\n    // Parallelize the outer loop over rows; use static chunking to keep\n    // work balanced among 8 threads.\n    #pragma omp parallel for schedule(static)\n    for (int i = 0; i < n; ++i)\n    {\n        // Pointers to the current row of A and B to reduce repeated indexing.\n        const double *Arow = A[i].data();\n        const double *Brow = B[i].data();\n\n        double tmp_acc = 0.0;\n        double y_acc  = 0.0;\n\n        // Inner loop accumulates both tmp and y contributions.\n        for (int j = 0; j < n; ++j)\n        {\n            tmp_acc += Arow[j] * x[j];\n            y_acc  += Brow[j] * x[j];\n        }\n\n        // Final linear combination.\n        tmp[i] = tmp_acc;\n        y[i]   = alpha * tmp_acc + beta * y_acc;\n    }\n}\n```",
  "durbin": "```cpp\n#include <vector>\n#include <cstddef>\n#include <cmath>\n#include <omp.h>\n\n/* Main computational kernel. The whole function will be timed,\n   including the call and return. */\nvoid kernel_durbin(int n,\n                   std::vector<double>& r,\n                   std::vector<double>& y) {\n\n    // Work on raw pointers for maximum speed and enable vectorisation\n    double* const restrict pr = r.data();\n    double* const restrict py = y.data();\n\n    // Temporary array\n    std::vector<double> z_vec(n, 0.0);\n    double* const restrict pz = z_vec.data();\n\n    py[0] = -pr[0];\n    double beta  = 1.0;\n    double alpha = -pr[0];\n\n    for (int k = 1; k < n; ++k) {\n        beta = (1.0 - alpha * alpha) * beta;\n\n        /* computing the inner product sum */\n        double sum = 0.0;\n        // The loop can be vectorised; use omp simd hint\n        #pragma omp simd reduction(+:sum)\n        for (int i = 0; i < k; ++i)\n            sum += pr[k - i - 1] * py[i];\n\n        alpha = -(pr[k] + sum) / beta;\n\n        /* update values y[0..k-1] via temporary array z */\n        // Two loops are fused by the compiler when optimised;\n        // keep them separate for readability, but hint SIMD\n        #pragma omp simd\n        for (int i = 0; i < k; ++i)\n            pz[i] = py[i] + alpha * py[k - i - 1];\n\n        #pragma omp simd\n        for (int i = 0; i < k; ++i)\n            py[i] = pz[i];\n\n        py[k] = alpha;\n    }\n}\n```\n",
  "bicg": "```cpp\n#include <vector>\n#include <omp.h>\n\n/* Main computational kernel. The whole function will be timed,\n   including the call and return. */\nvoid kernel_bicg(int m, int n,\n                 std::vector<std::vector<double>>& A,\n                 std::vector<double>& s,\n                 std::vector<double>& q,\n                 std::vector<double>& p,\n                 std::vector<double>& r) {\n    /* If the matrix is stored as a contiguous row\u2011major array use it.\n       Otherwise fallback to the original vector\u2011of\u2011vectors formulation. */\n    bool contiguous = false;\n    double* Acont = nullptr;\n    if (!A.empty() && !A[0].empty()) {\n        // Check that all rows have same length and the storage is contiguous\n        if (static_cast<int>(A.size()) == n && static_cast<int>(A[0].size()) == m) {\n            contiguous = true;\n            // Assume that the underlying storage is contiguous.\n            // This is true for a vector of vectors created by the harness.\n            Acont = &A[0][0];\n        }\n    }\n\n    /* Zero s */\n    #pragma omp parallel for schedule(static)\n    for (int j = 0; j < m; ++j)\n        s[j] = 0.0;\n\n    /* Zero q */\n    #pragma omp parallel for schedule(static)\n    for (int i = 0; i < n; ++i)\n        q[i] = 0.0;\n\n    if (contiguous) {\n        /* Use contiguous storage for better cache locality */\n        #pragma omp parallel for schedule(static)\n        for (int i = 0; i < n; ++i) {\n            double ri = r[i];\n            double* Arow = Acont + i * m;\n            for (int j = 0; j < m; ++j) {\n                s[j] += ri * Arow[j];\n                q[i] += Arow[j] + p[j];\n            }\n        }\n    } else {\n        /* Fallback to vector of vectors */\n        #pragma omp parallel for schedule(static)\n        for (int i = 0; i < n; ++i) {\n            double ri = r[i];\n            for (int j = 0; j < m; ++j) {\n                s[j] += ri * A[i][j];\n                q[i] += A[i][j] + p[j];\n            }\n        }\n    }\n}\n```",
  "symm": "```cpp\n#include <vector>\n#include <omp.h>\n\n/* Main computational kernel. The whole function will be timed,\n   including the call and return. */\nvoid kernel_symm(int m, int n,\n                 double alpha,\n                 double beta,\n                 std::vector<std::vector<double>>& C,\n                 std::vector<std::vector<double>>& A,\n                 std::vector<std::vector<double>>& B) {\n\n    // =>  Form  C := alpha*A*B + beta*C\n    // A is MxM\n    // B is MxN\n    // C is MxN\n    // note that due to Fortran array layout, the code below more closely resembles upper triangular case in BLAS\n\n    // Use OpenMP to parallelise the outer loop over rows of C (and A/B)\n    #pragma omp parallel for schedule(static)\n    for (int i = 0; i < m; ++i) {\n        // Local copies of the row pointers for A and B for better cache locality\n        const double* a_row = A[i].data();\n        const double* b_row = B[i].data();\n        double a_ii = a_row[i];          // A[i][i]\n        double alpha_aii = alpha * a_ii;\n\n        for (int j = 0; j < n; ++j) {\n            double temp2 = 0.0;\n\n            // We keep a extra pointer for C to avoid repeated indexing\n            double* c_cell = &C[i][j];\n\n            // Inner loop over k < i\n            for (int k = 0; k < i; ++k) {\n                double aik = a_row[k];\n                double bkj = B[k][j];\n                C[k][j] += alpha * b_row[j] * aik;   // same as original: C[k][j] += alpha*B[i][j]*A[i][k]\n                temp2 += bkj * aik;\n            }\n\n            // Final accumulation for the current C[i][j]\n            *c_cell = beta * *c_cell + alpha_aii * b_row[j] + alpha * temp2;\n        }\n    }\n}\n```",
  "adi": "```cpp\n#include <vector>\n#include <cstddef>\n#include <omp.h>\n\n/* Optimised ADI kernel   (exact same behaviour as the original) */\nvoid kernel_adi(int tsteps, int n,\n                std::vector<std::vector<double>>& u,\n                std::vector<std::vector<double>>& v,\n                std::vector<std::vector<double>>& p,\n                std::vector<std::vector<double>>& q)\n{\n    /* pre\u2011compute constants (same as original) */\n    const double DX = 1.0 / n, DY = 1.0 / n, DT = 1.0 / tsteps;\n    const double B1   = 2.0,    B2   = 1.0;\n    const double mul1 = B1 * DT / (DX*DX);\n    const double mul2 = B2 * DT / (DY*DY);\n    const double a   = -mul1 / 2.0;\n    const double b   = 1.0 + mul1;\n    const double c   = a;\n    const double d   = -mul2 / 2.0;\n    const double e   = 1.0 + mul2;\n    const double f   = d;\n\n    /* keep raw pointers for fast indexing */\n    double* u_ptr  = &u[0][0];\n    double* v_ptr  = &v[0][0];\n    double* p_ptr  = &p[0][0];\n    double* q_ptr  = &q[0][0];\n    const std::size_t stride = static_cast<std::size_t>(n);\n\n    /* Main time loop */\n    for (int t = 1; t <= tsteps; ++t)\n    {\n        /* First half of the sweep \u2013 time slice depending only on i */\n#pragma omp parallel for schedule(static)\n        for (int i = 1; i < n - 1; ++i)\n        {\n            /* local pointers to current i\u2011line */\n            double* v_i      = v_ptr + i*stride;\n            double* p_i      = p_ptr + i*stride;\n            double* q_i      = q_ptr + i*stride;\n\n            /* boundary initialisation */\n            v_i[0] = 1.0;\n            p_i[0] = 0.0;\n            q_i[0] = v_i[0];\n\n            double denom_prev = 1.0 + mul1; /* a*p[...]+b for j=1 */\n            double p_prev = 0.0;            /* p[i][j-1] */\n            double q_prev = v_i[0];\n\n            /* forward pass (j = 1 \u2026 n-2) */\n            for (int j = 1; j < n - 1; ++j)\n            {\n                /* p[i][j] = -c/(a*p[i][j-1] + b) */\n                const double denom = a * p_prev + b;\n                p_i[j] = -c / denom;\n\n                /* q[i][j] = ( \u2026 )/(a*p[i][j-1]+b) */\n                const double num = (-d * u_ptr[(j-1)*stride + i-1] /* u[j-1][i-1] */\n                                    + (1.0 + 2.0*d) * u_ptr[j*stride + i]    /* u[j][i] */\n                                    - f * u_ptr[j*stride + i+1]             /* u[j][i+1] */\n                                    - a * q_prev);                         /* -a*q[i][j-1] */\n                q_i[j] = num / denom;\n\n                p_prev = p_i[j];\n                q_prev = q_i[j];\n            }\n\n            /* right boundary */\n            v_i[n-1] = 1.0;\n\n            /* backward pass (j = n-2 \u2026 1) */\n            for (int j = n-2; j >= 1; --j)\n            {\n                v_i[j] = p_i[j] * v_i[j+1] + q_i[j];\n            }\n        }\n\n        /* Second half of the sweep \u2013 dependence on j */\n#pragma omp parallel for schedule(static)\n        for (int i = 1; i < n - 1; ++i)\n        {\n            /* local pointers to current i\u2011line */\n            double* u_i      = u_ptr + i*stride;\n            double* p_i      = p_ptr + i*stride;\n            double* q_i      = q_ptr + i*stride;\n\n            /* boundary initialisation */\n            u_i[0] = 1.0;\n            p_i[0] = 0.0;\n            q_i[0] = u_i[0];\n\n            double p_prev = 0.0;\n            double q_prev = u_i[0];\n\n            /* forward pass (j = 1 \u2026 n-2) */\n            for (int j = 1; j < n - 1; ++j)\n            {\n                const double denom = d * p_prev + e;\n                p_i[j] = -f / denom;\n\n                const double num = (-a * v_ptr[(i-1)*stride + j]   /* v[i-1][j] */\n                                    + (1.0 + 2.0*a) * v_ptr[i*stride + j] /* v[i][j] */\n                                    - c * v_ptr[(i+1)*stride + j] /* v[i+1][j] */\n                                    - d * q_prev);            /* -d*q[i][j-1] */\n                q_i[j] = num / denom;\n\n                p_prev = p_i[j];\n                q_prev = q_i[j];\n            }\n\n            /* right boundary */\n            u_i[n-1] = 1.0;\n\n            /* backward pass (j = n-2 \u2026 1) */\n            for (int j = n-2; j >= 1; --j)\n            {\n                u_i[j] = p_i[j] * u_i[j+1] + q_i[j];\n            }\n        }\n    }\n}\n```",
  "3mm": "```cpp\n#include <vector>\n#include <omp.h>\n\n/* Main computational kernel, unchanged interface. */\nvoid kernel_3mm(\n    int ni, int nj, int nk, int nl, int nm,\n    std::vector<std::vector<double>>& E,\n    std::vector<std::vector<double>>& A,\n    std::vector<std::vector<double>>& B,\n    std::vector<std::vector<double>>& F,\n    std::vector<std::vector<double>>& C,\n    std::vector<std::vector<double>>& D,\n    std::vector<std::vector<double>>& G) \n{\n    /* Step 1: E = A * B   (ni \u00d7 nk) * (nk \u00d7 nj) \u2192 (ni \u00d7 nj) */\n#pragma omp parallel for collapse(2) schedule(static)\n    for (int i = 0; i < ni; ++i) {\n        for (int j = 0; j < nj; ++j) {\n            double acc = 0.0;\n            /* explicit SIMD hint \u2013 GCC will vectorise */\n#pragma omp simd\n            for (int k = 0; k < nk; ++k) {\n                acc += A[i][k] * B[k][j];\n            }\n            E[i][j] = acc;\n        }\n    }\n\n    /* Step 2: F = C * D   (nj \u00d7 nm) * (nm \u00d7 nl) \u2192 (nj \u00d7 nl) */\n#pragma omp parallel for collapse(2) schedule(static)\n    for (int i = 0; i < nj; ++i) {\n        for (int j = 0; j < nl; ++j) {\n            double acc = 0.0;\n#pragma omp simd\n            for (int k = 0; k < nm; ++k) {\n                acc += C[i][k] * D[k][j];\n            }\n            F[i][j] = acc;\n        }\n    }\n\n    /* Step 3: G = E * F   (ni \u00d7 nj) * (nj \u00d7 nl) \u2192 (ni \u00d7 nl) */\n#pragma omp parallel for collapse(2) schedule(static)\n    for (int i = 0; i < ni; ++i) {\n        for (int j = 0; j < nl; ++j) {\n            double acc = 0.0;\n#pragma omp simd\n            for (int k = 0; k < nj; ++k) {\n                acc += E[i][k] * F[k][j];\n            }\n            G[i][j] = acc;\n        }\n    }\n}\n```",
  "syrk": "```cpp\n#include <vector>\n#include <omp.h>\n\n/* Main computational kernel. The whole function will be timed,\n   including the call and return. */\nvoid kernel_syrk(int n, int m,\n                 double alpha,\n                 double beta,\n                 std::vector<std::vector<double>>& C,\n                 std::vector<std::vector<double>>& A) {\n\n    /* Parallelise over the outer index i. */\n    #pragma omp parallel for schedule(static)\n    for (int i = 0; i < n; ++i) {\n        const double *Ai = A[i].data();            // row i of A\n        double *Ci = C[i].data();                  // row i of C\n\n        /* Scale the lower\u2011triangular part of C[i][*] by beta. */\n        for (int j = 0; j <= i; ++j) {\n            Ci[j] *= beta;\n        }\n\n        /* Compute the rank\u2011m update for this row. */\n        for (int k = 0; k < m; ++k) {\n            double aik = Ai[k] * alpha;            // reuse the alpha product\n            for (int j = 0; j <= i; ++j) {\n                Ci[j] += aik * A[j][k];\n            }\n        }\n    }\n}\n```",
  "jacobi_1d": "```cpp\n#include <vector>\n#include <omp.h>\n\n/* Main computational kernel. The whole function will be timed,\n   including the call and return. */\nvoid kernel_jacobi_1d(int tsteps,\n                      int n,\n                      std::vector<double>& A,\n                      std::vector<double>& B) {\n    constexpr double avg = 0.33333;          // repeated constant\n    double* restrict arrA = A.data();\n    double* restrict arrB = B.data();\n\n    for (int t = 0; t < tsteps; ++t) {\n        /* first half\u2011step: compute B from A */\n#pragma omp parallel for schedule(static)\n        for (int i = 1; i < n - 1; ++i) {\n            arrB[i] = avg * (arrA[i - 1] + arrA[i] + arrA[i + 1]);\n        }\n\n        /* second half\u2011step: compute A from B */\n#pragma omp parallel for schedule(static)\n        for (int i = 1; i < n - 1; ++i) {\n            arrA[i] = avg * (arrB[i - 1] + arrB[i] + arrB[i + 1]);\n        }\n    }\n}\n```",
  "2mm": "```cpp\n#include <vector>\n#include <omp.h>\n\n/* Main computational kernel. The whole function will be timed,\n   including the call and return. */\nvoid kernel_2mm(int ni, int nj, int nk, int nl,\n\t\tdouble alpha,\n\t\tdouble beta,\n\t\tstd::vector<std::vector<double>>& tmp,\n\t\tstd::vector<std::vector<double>>& A,\n\t\tstd::vector<std::vector<double>>& B,\n\t\tstd::vector<std::vector<double>>& C,\n\t\tstd::vector<std::vector<double>>& D)\n{\n    // ------------------------------------------------------------------\n    // 1st phase \u2013 compute tmp = alpha * A * B\n    // ------------------------------------------------------------------\n    #pragma omp parallel for schedule(static)\n    for (int i = 0; i < ni; ++i) {\n        double *tmp_row   = tmp[i].data();\n        const double *A_i = A[i].data();\n        for (int j = 0; j < nj; ++j) {\n            double sum = 0.0;\n            const double *B_col = &B[0][j];      // start of j-th column of B\n            for (int k = 0; k < nk; ++k) {\n                sum += alpha * A_i[k] * B_col[k];\n            }\n            tmp_row[j] = sum;\n        }\n    }\n\n    // ------------------------------------------------------------------\n    // 2nd phase \u2013 compute D = beta * D + tmp * C\n    // ------------------------------------------------------------------\n    #pragma omp parallel for schedule(static)\n    for (int i = 0; i < ni; ++i) {\n        double *D_row = D[i].data();\n        const double *tmp_row = tmp[i].data();\n        for (int j = 0; j < nl; ++j) {\n            double sum = beta * D_row[j];\n            for (int k = 0; k < nj; ++k) {\n                sum += tmp_row[k] * C[k][j];\n            }\n            D_row[j] = sum;\n        }\n    }\n}\n```",
  "correlation": "```cpp\n#include <vector>\n#include <cmath>\n#include <algorithm>\n#include <cstddef>\n#include <omp.h>\n\n// Helper to get element in row\u2013major order.\ninline double& elem(std::vector<std::vector<double>>& v, std::size_t i, std::size_t j) {\n    return v[i][j];\n}\n\nvoid kernel_correlation(int m, int n,\n                        std::vector<std::vector<double>>& data,\n                        std::vector<std::vector<double>>& corr,\n                        std::vector<double>& mean,\n                        std::vector<double>& stddev) {\n    const double eps = 0.1;\n    const double inv_sqrt_n = 1.0 / std::sqrt(static_cast<double>(n));\n\n    // ------------------------------------------------------------------\n    // 1. Compute means (parallel over columns)\n    // ------------------------------------------------------------------\n    #pragma omp parallel for schedule(static)\n    for (int j = 0; j < m; ++j) {\n        double s = 0.0;\n        for (int i = 0; i < n; ++i) s += elem(data, i, j);\n        mean[j] = s / n;\n    }\n\n    // ------------------------------------------------------------------\n    // 2. Compute stddev (parallel over columns)\n    // ------------------------------------------------------------------\n    #pragma omp parallel for schedule(static)\n    for (int j = 0; j < m; ++j) {\n        double var = 0.0;\n        for (int i = 0; i < n; ++i) {\n            double d = elem(data, i, j) - mean[j];\n            var += d * d;\n        }\n        var /= n;\n        double sd = std::sqrt(var);\n        stddev[j] = (sd <= eps) ? 1.0 : sd;\n    }\n\n    // ------------------------------------------------------------------\n    // 3. Standardize data in place (column\u2011by\u2011column, parallel)\n    // ------------------------------------------------------------------\n    #pragma omp parallel for schedule(static)\n    for (int j = 0; j < m; ++j) {\n        const double inv = inv_sqrt_n / stddev[j];\n        for (int i = 0; i < n; ++i) {\n            double v = elem(data, i, j);\n            v -= mean[j];\n            v *= inv;\n            elem(data, i, j) = v;\n        }\n    }\n\n    // ------------------------------------------------------------------\n    // 4. Compute correlation matrix (upper\u2013triangular, parallel)\n    // ------------------------------------------------------------------\n    #pragma omp parallel for schedule(static)\n    for (int i = 0; i < m; ++i) {\n        corr[i][i] = 1.0;          // diagonal\n        for (int j = i + 1; j < m; ++j) {\n            double s = 0.0;\n            for (int k = 0; k < n; ++k) {\n                s += elem(data, k, i) * elem(data, k, j);\n            }\n            corr[i][j] = s;\n            corr[j][i] = s;        // symmetric\n        }\n    }\n}\n```",
  "mvt": "```cpp\n#include <vector>\n#include <cstddef>\n#include <omp.h>\n\n/* Main computational kernel. The whole function will be timed,\n   including the call and return. The original semantics are\n   preserved: x1[i] and x2[i] are incremented accumulatively.\n*/\nextern \"C\" void kernel_mvt(\n    int n,\n    std::vector<double> &x1,\n    std::vector<double> &x2,\n    std::vector<double> &y_1,\n    std::vector<double> &y_2,\n    std::vector<std::vector<double>> &A\n) {\n    const int N = n;\n    const double *y1 = y_1.data();\n    const double *y2 = y_2.data();\n    double *pX1 = x1.data();\n    double *pX2 = x2.data();\n    // Flatten the 2\u2011D vector for pointer arithmetic.\n    // A[i][j] is at A_flat[i * N + j].\n    const double *A_flat = A.data() ? A[0].data() : nullptr; // assume all rows contiguous\n\n    // First accumulation (row\u2011wise product).\n    #pragma omp parallel for schedule(static)\n    for (int i = 0; i < N; ++i) {\n        double sum1 = 0.0;\n        const double *row = &A_flat[i * N];\n        for (int j = 0; j < N; ++j) {\n            sum1 += row[j] * y1[j];\n        }\n        pX1[i] += sum1;\n    }\n\n    // Second accumulation (column\u2011wise product).\n    #pragma omp parallel for schedule(static)\n    for (int j = 0; j < N; ++j) {\n        const double *col = &A_flat[j];          // A[0][j], A[1][j], ...\n        const double y2j = y2[j];\n        for (int i = 0; i < N; ++i) {\n            pX2[i] += col[i * N] * y2j;\n        }\n    }\n}\n```\n\n**Explanation of optimizations**\n\n* **Flat access to the 2\u2011D matrix** \u2013 all rows of `A` are assumed contiguous; indexing with a single pointer reduces overhead from double indexing.\n* **Pointer arithmetic** \u2013 all vectors are accessed through raw pointers (`data()`), eliminating bounds checks.\n* **OpenMP parallelism** \u2013 each outer loop is parallelised with static scheduling, giving good load balance on 8 threads.\n* **Loop fusion avoided** \u2013 keeping two separate passes preserves the order of updates and matches the original algorithm.\n* **Avoided `std::vector<std::vector<double>>` bounds checks** \u2013 by trusting that caller provides a contiguous matrix, the code is cleaner and faster.",
  "cholesky": "```cpp\n#include <cmath>\n#include <vector>\n#include <omp.h>\n\n/* Main computational kernel. The whole function will be timed,\n   including the call and return. */\nvoid kernel_cholesky(int n,\n\t\t     std::vector<std::vector<double>>& A) {\n    /* Parallelise the outer loop \u2013 each row is independent once\n       the previous rows have been processed.\n       The OpenMP schedule is static with a reasonably small chunk\n       to keep the work balanced across 8 threads.  */\n    #pragma omp parallel for schedule(static, 64)\n    for (int i = 0; i < n; ++i) {\n        double *Ai = A[i].data();           // 1\u2011D view for the i\u2011th row\n\n        /* Off\u2011diagonal elements: first subtract the dot\u2011product\n           with all previous rows and then divide by the\n           diagonal element of the column.  */\n        for (int j = 0; j < i; ++j) {\n            double *Aj = A[j].data();\n            double sum = 0.0;\n            for (int k = 0; k < j; ++k) {\n                sum += Ai[k] * Aj[k];\n            }\n            Ai[j] = (Ai[j] - sum) / Aj[j];\n        }\n\n        /* Diagonal element: subtract the squared norm of the\n           current row\u2019s already computed entries.  */\n        double sum = 0.0;\n        for (int k = 0; k < i; ++k) {\n            sum += Ai[k] * Ai[k];\n        }\n        Ai[i] = std::sqrt(Ai[i] - sum);\n    }\n}\n```",
  "gramschmidt": "```cpp\n#include <cmath>\n#include <vector>\n#include <omp.h>\n\n/* Main computational kernel. The whole function will be timed,\n   including the call and return. */\n/* QR Decomposition with Modified Gram Schmidt:\n   http://www.inf.ethz.ch/personal/gander/ */\nvoid kernel_gramschmidt(int m, int n,\n                        std::vector<std::vector<double>>& A,\n                        std::vector<std::vector<double>>& R,\n                        std::vector<std::vector<double>>& Q)\n{\n    // We work directly with the sub\u2011vectors, but all loops are\n    // explicitly parallelised with OpenMP.\n    for (int k = 0; k < n; ++k)\n    {\n        /* Compute the norm of column k of A (and store it in R[k][k]) */\n        double nrm = 0.0;\n#pragma omp parallel for reduction(+:nrm) schedule(static)\n        for (int i = 0; i < m; ++i)\n            nrm += A[i][k] * A[i][k];\n\n        R[k][k] = std::sqrt(nrm);\n\n        /* Normalise column k to produce column k of Q */\n#pragma omp parallel for schedule(static)\n        for (int i = 0; i < m; ++i)\n            Q[i][k] = A[i][k] / R[k][k];\n\n        /* House\u2011holder like update of the remaining columns */\n        for (int j = k + 1; j < n; ++j)\n        {\n            /* Compute the dot product (Q[:,k] \u00b7 A[:,j]) */\n            double dot = 0.0;\n#pragma omp parallel for reduction(+:dot) schedule(static)\n            for (int i = 0; i < m; ++i)\n                dot += Q[i][k] * A[i][j];\n            R[k][j] = dot;\n\n            /* Subtract the projection from column j of A */\n#pragma omp parallel for schedule(static)\n            for (int i = 0; i < m; ++i)\n                A[i][j] -= Q[i][k] * dot;\n        }\n    }\n}\n```",
  "ludcmp": "```cpp\n#include <vector>\n#include <cstddef>\n#include <omp.h>\n\n/* Main computational kernel. The whole function will be timed,\n   including the call and return. */\nvoid kernel_ludcmp(int n,\n                   std::vector<std::vector<double>>& A,\n                   std::vector<double>& b,\n                   std::vector<double>& x,\n                   std::vector<double>& y) {\n    /* Flatten the 2\u2011D matrix into a 1\u2011D array for better cache behaviour. */\n    std::vector<double> flatA(n * n);\n    for (int i = 0; i < n; ++i)\n        for (int j = 0; j < n; ++j)\n            flatA[i * n + j] = A[i][j];\n\n    /* --- LU decomposition (no pivoting) -------------------------------- */\n    for (int i = 0; i < n; ++i) {\n        /* Compute L part (upper triangular of U) */\n        for (int j = 0; j < i; ++j) {\n            double w = flatA[i * n + j];\n            for (int k = 0; k < j; ++k)\n                w -= flatA[i * n + k] * flatA[k * n + j];\n            flatA[i * n + j] = w / flatA[j * n + j];\n        }\n\n        /* Compute U part (lower triangular of L) */\n        for (int j = i; j < n; ++j) {\n            double w = flatA[i * n + j];\n            for (int k = 0; k < i; ++k)\n                w -= flatA[i * n + k] * flatA[k * n + j];\n            flatA[i * n + j] = w;\n        }\n    }\n\n    /* --- Forward substitution: solve L * y = b ------------------------ */\n    for (int i = 0; i < n; ++i) {\n        double w = b[i];\n        for (int j = 0; j < i; ++j)\n            w -= flatA[i * n + j] * y[j];\n        y[i] = w;\n    }\n\n    /* --- Back substitution: solve U * x = y --------------------------- */\n    for (int i = n - 1; i >= 0; --i) {\n        double w = y[i];\n        for (int j = i + 1; j < n; ++j)\n            w -= flatA[i * n + j] * x[j];\n        x[i] = w / flatA[i * n + i];\n    }\n}\n```\n",
  "jacobi_2d": "```cpp\n#include <vector>\n#include <cstddef>\n#include <omp.h>\n\n/* Main computational kernel. The whole function will be timed,\n   including the call and return. */\nvoid kernel_jacobi_2d(int tsteps,\n                      int n,\n                      std::vector<std::vector<double>>& A,\n                      std::vector<std::vector<double>>& B) {\n    const double inv5 = 0.2;          // 1/5\n    const std::size_t stride = static_cast<std::size_t>(n);\n\n    for (int t = 0; t < tsteps; ++t) {\n        /* ------------------------------------------------------------------ */\n        /* Update B from A                                                  */\n        /* Use OpenMP to parallelise outer loop. The inner loop stays\n           as a simple index-based access for optimal cache behavior.     */\n        /* ------------------------------------------------------------------ */\n#pragma omp parallel for schedule(static)\n        for (int i = 1; i < n - 1; ++i) {\n            std::size_t idx = static_cast<std::size_t>(i) * stride;\n            std::size_t ip1 = idx + stride;       // (i+1,j)\n            std::size_t im1 = idx - stride;       // (i-1,j)\n            std::size_t i1 = idx + 1;             // (i,j+1)\n            std::size_t iM1 = idx - 1;            // (i,j-1)\n\n            for (int j = 1; j < n - 1; ++j, ++idx, ++i1, ++iM1) {\n                B[i][j] = inv5 * (A[i][j] + A[i][j - 1] + A[i][j + 1]\n                                   + A[i + 1][j] + A[i - 1][j]);\n            }\n        }\n\n        /* ------------------------------------------------------------------ */\n        /* Update A from B                                                  */\n        /* ------------------------------------------------------------------ */\n#pragma omp parallel for schedule(static)\n        for (int i = 1; i < n - 1; ++i) {\n            std::size_t idx = static_cast<std::size_t>(i) * stride;\n            for (int j = 1; j < n - 1; ++j, ++idx) {\n                A[i][j] = inv5 * (B[i][j] + B[i][j - 1] + B[i][j + 1]\n                                   + B[i + 1][j] + B[i - 1][j]);\n            }\n        }\n    }\n}\n```",
  "fdtd_2d": "```cpp\n#include <vector>\n#include <cstddef>\n#include <omp.h>\n\n/*\n * The kernel works on 2\u2011D grids stored as vector<vector<double>>,\n * which internally allocate contiguous rows but not necessarily the\n * whole matrix.  For maximum performance we flatten the two dimensional\n * indices to a single pointer into the underlying data.  The logic\n * (including the updates that depend on borders) stays identical to the\n * original implementation.\n */\nvoid kernel_fdtd_2d(int tmax,\n                    int nx,\n                    int ny,\n                    std::vector<std::vector<double>>& ex,\n                    std::vector<std::vector<double>>& ey,\n                    std::vector<std::vector<double>>& hz,\n                    std::vector<double>& _fict_)\n{\n    // PRE\u2011COMPUTE row strides \u2013 each row is contiguous.\n    const std::size_t stride = static_cast<std::size_t>(ny);\n    const std::size_t total_cells = static_cast<std::size_t>(nx) * stride;\n\n    // Flatten access pointers for each matrix.\n    double* px = &ex[0][0];\n    double* py = &ey[0][0];\n    double* ph = &hz[0][0];\n\n    #pragma omp parallel\n    {\n        // Loop over time steps\n        for (int t = 0; t < tmax; ++t) {\n\n            /* 1) Update ey[0][j] with the fictitious source */\n            #pragma omp for schedule(static)\n            for (int j = 0; j < ny; ++j) {\n                py[j] = _fict_[t];\n            }\n\n            /* 2) Update ey[i][j] (i>=1) */\n            #pragma omp for schedule(static, 64)\n            for (int i = 1; i < nx; ++i) {\n                std::size_t offset = static_cast<std::size_t>(i) * stride;\n                for (int j = 0; j < ny; ++j) {\n                    py[offset + j] -= 0.5 * (ph[offset + j] - ph[(offset - stride) + j]);\n                }\n            }\n\n            /* 3) Update ex[i][j] (j>=1) */\n            #pragma omp for schedule(static, 64)\n            for (int i = 0; i < nx; ++i) {\n                std::size_t offset = static_cast<std::size_t>(i) * stride;\n                for (int j = 1; j < ny; ++j) {\n                    px[offset + j] -= 0.5 * (ph[offset + j] - ph[offset + j - 1]);\n                }\n            }\n\n            /* 4) Update hz[i][j] (i<nx\u20111, j<ny\u20111) */\n            #pragma omp for schedule(static, 64)\n            for (int i = 0; i < nx - 1; ++i) {\n                std::size_t offset = static_cast<std::size_t>(i) * stride;\n                std::size_t next_offset = static_cast<std::size_t>(i + 1) * stride;\n                for (int j = 0; j < ny - 1; ++j) {\n                    ph[offset + j] -= 0.7 * (px[offset + j + 1] - px[offset + j] +\n                                             py[next_offset + j] - py[offset + j]);\n                }\n            }\n        } // end time loop\n    } // end parallel region\n}\n```",
  "trmm": "```cpp\n#include <vector>\n#include <cstddef>\n#include <omp.h>\n\n/* Main computational kernel. The whole function will be timed,\n   including the call and return. */\nvoid kernel_trmm(int m, int n,\n                 double alpha,\n                 std::vector<std::vector<double>>& A,\n                 std::vector<std::vector<double>>& B) {\n    // B := alpha * A^T * B\n    // A is MxM, B is MxN.\n    //\n    // We flatten the 2\u2011D vectors into contiguous buffers to avoid\n    // indirection costs, and then use a blocked, inner\u2011vectorised\n    // triple loop.  The indexing layout matches the original\n    // behaviour (k runs from i+1 to m-1).\n\n    const size_t lda = m;   // leading dimension for A\n    const size_t ldb = n;   // leading dimension for B\n\n    // Pointers to the raw data of the 2\u2011D vectors.\n    double *ptrA = A[0].data();\n    double *ptrB = B[0].data();\n\n    const int block = 32;   // tile size for loop\u2010tiling\n\n    // Parallelise on the outermost loop over rows of B.\n    #pragma omp parallel for schedule(static)\n    for (int i = 0; i < m; ++i) {\n        const double *a_col_i = ptrA + i * lda;        // A[k][i] => a_col_i[k]\n        double *b_row_i = ptrB + i * ldb;              // B[i][j] => b_row_i[j]\n\n        // Process columns of B in blocks to improve cache utilisation.\n        for (int j = 0; j < n; j += block) {\n            int j_end = std::min(j + block, n);\n\n            // Accumulate the dot\u2011product for this block.\n            for (int k = i + 1; k < m; ++k) {\n                double aik = a_col_i[k];\n                double *b_row_k = ptrB + k * ldb;   // B[k][j]\n                for (int jj = j; jj < j_end; ++jj) {\n                    b_row_i[jj] += aik * b_row_k[jj];\n                }\n            }\n\n            // Finish by applying the scalar multiplier \u03b1.\n            for (int jj = j; jj < j_end; ++jj) {\n                b_row_i[jj] = alpha * b_row_i[jj];\n            }\n        }\n    }\n}\n```",
  "seidel_2d": "```cpp\n#include <vector>\n#include <cstddef>\n#include <omp.h>\n\n/* Main computational kernel. The whole function will be timed,\n   including the call and return. */\nvoid kernel_seidel_2d(int tsteps,\n                      int n,\n                      std::vector<std::vector<double>>& A) {\n    constexpr double div9 = 1.0 / 9.0;\n    const int inner_size = n - 2;          // number of inner cells in one dimension\n    const int outer_start = 1;             // first inner index\n\n    /* Since the signature must stay the same, we keep the\n       double\u2011vector layout.  We optimise by using raw pointers to\n       each row and by parallelising the two loops that actually\n       compute the stencil. */\n    for (int t = 0; t < tsteps; ++t) {\n        /* Parallelise over the inner rows.  Collapse(2) lets OpenMP\n           distribute work over (i,j) pairs as a single loop for\n           better load balance.  The static schedule gives fast\n           cache\u2011friendly access because the order of iterations\n           is predictable. */\n        #pragma omp parallel for collapse(2) schedule(static)\n        for (int i = outer_start; i < inner_size + outer_start; ++i) {\n            /* Fetch pointers to the current row and its neighbours\n               once for the whole inner j loop. */\n            const double* prevRow = A[i - 1].data();\n            const double* curRow  = A[i].data();\n            const double* nextRow = A[i + 1].data();\n            double*          resRow = A[i].data();\n\n            for (int j = outer_start; j < inner_size + outer_start; ++j) {\n                const double sum =\n                    prevRow[j - 1] + prevRow[j] + prevRow[j + 1] +\n                    curRow[j - 1] + curRow[j] + curRow[j + 1] +\n                    nextRow[j - 1] + nextRow[j] + nextRow[j + 1];\n                resRow[j] = sum * div9;\n            }\n        }\n    }\n}\n```\n",
  "floyd_warshall": "```cpp\n#include <vector>\n#include <algorithm>\n#include <omp.h>\n\n// Floyd\u2011Warshall with OpenMP acceleration\n// The algorithm keeps the same semantics as the original\n// but processes the i\u2011j plane in parallel for each k.\nvoid kernel_floyd_warshall(int n, std::vector<std::vector<int>>& path) {\n#pragma omp parallel for schedule(static)\n    for (int k = 0; k < n; ++k) {\n        // Parallel over the i\u2011j matrix for fixed k\n#pragma omp parallel for collapse(2) schedule(static)\n        for (int i = 0; i < n; ++i) {\n            for (int j = 0; j < n; ++j) {\n                int new_val = path[i][k] + path[k][j];\n                if (new_val < path[i][j]) {\n                    path[i][j] = new_val;\n                }\n            }\n        }\n    }\n}\n```",
  "lu": "```cpp\n#include <vector>\n#include <omp.h>\n\n/* Main computational kernel. \n * The whole function will be timed, including the call and return. */\nvoid kernel_lu(int n, std::vector<std::vector<double>>& A) {\n    /* ---- LU decomposition without pivoting -------------------------------- */\n    for (int i = 0; i < n; ++i) {\n        /* ----------------- Compute the L\u2011part (below the diagonal) --------- */\n        std::vector<double>& row_i = A[i];\n        for (int j = 0; j < i; ++j) {\n            double sum = 0.0;\n            std::vector<double>& row_k = A[i];\n            std::vector<double>& row_j = A[j];\n            for (int k = 0; k < j; ++k) {\n                sum += row_k[k] * row_j[k];\n            }\n            row_i[j] -= sum;\n            row_i[j] /= row_j[j];\n        }\n\n        /* ----------------- Compute the U\u2011part (on & above the diagonal) ---- */\n        for (int j = i; j < n; ++j) {\n            double sum = 0.0;\n            std::vector<double>& row_j = A[j];\n            for (int k = 0; k < i; ++k) {\n                sum += row_i[k] * row_k[k];\n            }\n            row_i[j] -= sum;\n        }\n    }\n}\n```",
  "gemver": "```cpp\n#include <vector>\n#include <cstddef>\n#include <omp.h>\n\n/* Optimised gemver kernel.\n *\n * The function preserves the exact semantics of the original\n * implementation.  All mutually independent loop iterations are\n * executed in parallel using OpenMP.  The code accesses the underlying\n * array storage of the std::vector objects directly for\n * performance, but does **not** change the interfaces or any\n * visible behaviour.\n */\nvoid kernel_gemver(int n,\n                   double alpha,\n                   double beta,\n                   std::vector<std::vector<double>>& A,\n                   std::vector<double>& u1,\n                   std::vector<double>& v1,\n                   std::vector<double>& u2,\n                   std::vector<double>& v2,\n                   std::vector<double>& w,\n                   std::vector<double>& x,\n                   std::vector<double>& y,\n                   std::vector<double>& z)\n{\n    // Using raw pointers for fast indexed access.\n    // Assume that each row of A is contiguous \u2013 std::vector guarantees this.\n    double** A_ptr = new double*[n];\n    for (int i = 0; i < n; ++i) A_ptr[i] = A[i].data();\n\n    const double *u1_ptr = u1.data();\n    const double *v1_ptr = v1.data();\n    const double *u2_ptr = u2.data();\n    const double *v2_ptr = v2.data();\n    const double *y_ptr  = y.data();\n    const double *z_ptr  = z.data();\n\n    double *x_ptr = x.data();\n    double *w_ptr = w.data();\n\n    /* 1) Rank\u20111 updates of A:\n     *    A[i][j] += u1[i]*v1[j] + u2[i]*v2[j]\n     */\n    #pragma omp parallel for schedule(static)\n    for (int i = 0; i < n; ++i) {\n        const double ui1 = u1_ptr[i];\n        const double ui2 = u2_ptr[i];\n        double *row = A_ptr[i];\n        for (int j = 0; j < n; ++j) {\n            row[j] += ui1 * v1_ptr[j] + ui2 * v2_ptr[j];\n        }\n    }\n\n    /* 2) Update x: x[i] += beta * sum_j (A[j][i]*y[j]) */\n    /*    Transpose\u2011access is needed; we keep it as separate loop for\n     *    simpler parallelisation.  Each thread updates disjoint parts of x.\n     */\n    #pragma omp parallel for schedule(static)\n    for (int i = 0; i < n; ++i) {\n        double acc = 0.0;\n        for (int j = 0; j < n; ++j) {\n            acc += A_ptr[j][i] * y_ptr[j];\n        }\n        x_ptr[i] += beta * acc;\n    }\n\n    /* 3) Element\u2011wise add z to x */\n    #pragma omp parallel for schedule(static)\n    for (int i = 0; i < n; ++i) {\n        x_ptr[i] += z_ptr[i];\n    }\n\n    /* 4) w[i] += alpha * sum_j (A[i][j] * x[j]) */\n    #pragma omp parallel for schedule(static)\n    for (int i = 0; i < n; ++i) {\n        double acc = 0.0;\n        const double *row = A_ptr[i];\n        for (int j = 0; j < n; ++j) {\n            acc += row[j] * x_ptr[j];\n        }\n        w_ptr[i] += alpha * acc;\n    }\n\n    delete[] A_ptr;\n}\n```",
  "deriche": "```cpp\n#include <cmath>\n#include <vector>\n#include <omp.h>\n\n/* Main computational kernel.  The whole function will be timed,\n   including the call and return. */\n/* Optimised version \u2013 keeps original semantics, removes\n   temporaries, pre\u2011computes constants, and uses OpenMP. */\nvoid kernel_deriche(int w, int h, double alpha,\n                    std::vector<std::vector<float>>& imgIn,\n                    std::vector<std::vector<float>>& imgOut,\n                    std::vector<std::vector<float>>& y1,\n                    std::vector<std::vector<float>>& y2) {\n\n    /* Pre\u2011computed constants \u2013 computed once per call */\n    const float expAlpha   = std::exp(-alpha);\n    const float exp2Alpha  = expAlpha * expAlpha;\n    const float k = 1.0f - expAlpha * (1.0f - expAlpha) /\n                   (1.0f + 2.0f * alpha * expAlpha - exp2Alpha);\n\n    const float a1  = k;\n    const float a5  = k;\n    const float a2  = k * expAlpha * (alpha - 1.0f);\n    const float a6  = a2;\n    const float a3  = k * expAlpha * (alpha + 1.0f);\n    const float a7  = a3;\n    const float a4  = -k * exp2Alpha;\n    const float a8  = a4;\n    const float b1  = std::pow(2.0f, -alpha);\n    const float b2  = -exp2Alpha;\n    const float c1  = 1.0f;\n    const float c2  = 1.0f;\n\n    /* --------- Horizontal scans (first forward pass) --------- */\n    #pragma omp parallel for schedule(static)\n    for (int i = 0; i < w; ++i) {\n        float ym1 = 0.0f, ym2 = 0.0f, xm1 = 0.0f;\n        for (int j = 0; j < h; ++j) {\n            y1[i][j] = a1  * imgIn[i][j] + a2 * xm1\n                     + b1 * ym1 + b2 * ym2;\n            xm1 = imgIn[i][j];\n            ym2 = ym1;\n            ym1 = y1[i][j];\n        }\n    }\n\n    /* --------- Horizontal scans (reverse pass) --------- */\n    #pragma omp parallel for schedule(static)\n    for (int i = 0; i < w; ++i) {\n        float yp1 = 0.0f, yp2 = 0.0f, xp1 = 0.0f, xp2 = 0.0f;\n        for (int j = h-1; j >= 0; --j) {\n            y2[i][j] = a3 * xp1 + a4 * xp2\n                     + b1 * yp1 + b2 * yp2;\n            xp2 = xp1;\n            xp1 = imgIn[i][j];\n            yp2 = yp1;\n            yp1 = y2[i][j];\n        }\n    }\n\n    /* --------- Combine horizontal results --------- */\n    #pragma omp parallel for collapse(2) schedule(static)\n    for (int i = 0; i < w; ++i) {\n        for (int j = 0; j < h; ++j) {\n            imgOut[i][j] = c1 * (y1[i][j] + y2[i][j]);\n        }\n    }\n\n    /* --------- Vertical scans (first forward pass) --------- */\n    #pragma omp parallel for schedule(static)\n    for (int j = 0; j < h; ++j) {\n        float tm1 = 0.0f, ym1 = 0.0f, ym2 = 0.0f;\n        for (int i = 0; i < w; ++i) {\n            y1[i][j] = a5 * imgOut[i][j] + a6 * tm1\n                     + b1 * ym1 + b2 * ym2;\n            tm1 = imgOut[i][j];\n            ym2 = ym1;\n            ym1 = y1[i][j];\n        }\n    }\n\n    /* --------- Vertical scans (reverse pass) --------- */\n    #pragma omp parallel for schedule(static)\n    for (int j = 0; j < h; ++j) {\n        float tp1 = 0.0f, tp2 = 0.0f, yp1 = 0.0f, yp2 = 0.0f;\n        for (int i = w-1; i >= 0; --i) {\n            y2[i][j] = a7 * tp1 + a8 * tp2\n                     + b1 * yp1 + b2 * yp2;\n            tp2 = tp1;\n            tp1 = imgOut[i][j];\n            yp2 = yp1;\n            yp1 = y2[i][j];\n        }\n    }\n\n    /* --------- Final combine --------- */\n    #pragma omp parallel for collapse(2) schedule(static)\n    for (int i = 0; i < w; ++i) {\n        for (int j = 0; j < h; ++j) {\n            imgOut[i][j] = c2 * (y1[i][j] + y2[i][j]);\n        }\n    }\n}\n```\n"
}