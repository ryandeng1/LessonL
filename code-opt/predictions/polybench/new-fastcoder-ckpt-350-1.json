{
  "gemm": "```cpp\n#include <vector>\n#include <omp.h>\n\nvoid kernel_gemm(int ni, int nj, int nk,\n                 double alpha,\n                 double beta,\n                 std::vector<std::vector<double>>& A,\n                 std::vector<std::vector<double>>& B,\n                 std::vector<std::vector<double>>& C) {\n\n    // Transpose B for better cache locality.\n    std::vector<std::vector<double>> B_T(nj, std::vector<double>(nk));\n    #pragma omp parallel for schedule(static)\n    for (int j = 0; j < nj; ++j) {\n        for (int k = 0; k < nk; ++k) {\n            B_T[j][k] = B[k][j];\n        }\n    }\n\n    #pragma omp parallel for schedule(static)\n    for (int i = 0; i < ni; ++i) {\n        std::vector<double>* C_row = &C[i];\n        std::vector<double> const* A_row = &A[i];\n\n        // Scale C by beta\n        for (int j = 0; j < nj; ++j) (*C_row)[j] *= beta;\n\n        for (int k = 0; k < nk; ++k) {\n            double a_ik = (*A_row)[k];\n#pragma ivdep\n            for (int j = 0; j < nj; ++j) {\n                (*C_row)[j] += alpha * a_ik * B_T[j][k];\n            }\n        }\n    }\n}\n```",
  "covariance": "```cpp\n#include <vector>\n#include <cstddef>\n#include <omp.h>\n\nvoid kernel_covariance(int m, int n,\n                       std::vector<std::vector<double>>& data,\n                       std::vector<std::vector<double>>& cov,\n                       std::vector<double>& mean)\n{\n    // --------------------------------------------------------------------\n    // 1. compute column means and subtract them from data\n    // --------------------------------------------------------------------\n    #pragma omp parallel for schedule(static)\n    for (int j = 0; j < m; ++j) {\n        double s = 0.0;\n        for (int i = 0; i < n; ++i) {\n            s += data[i][j];\n        }\n        mean[j] = s / static_cast<double>(n);\n        // subtract the mean from each element in this column\n        for (int i = 0; i < n; ++i) {\n            data[i][j] -= mean[j];\n        }\n    }\n\n    // --------------------------------------------------------------------\n    // 2. compute covariance matrix\n    // --------------------------------------------------------------------\n    // we only need to compute the upper triangular part; the matrix is\n    // symmetric, so the lower part can be filled by mirroring.\n    const double denom = 1.0 / (static_cast<double>(n) - 1.0);\n\n    // Parallelise over the outermost loop (rows of the covariance matrix)\n    #pragma omp parallel for schedule(static)\n    for (int i = 0; i < m; ++i) {\n        // column j duplicates the computation done for j < i\n        for (int j = i; j < m; ++j) {\n            double s = 0.0;\n            // compute dot product of column i and column j of the centred data\n            for (int k = 0; k < n; ++k) {\n                s += data[k][i] * data[k][j];\n            }\n            double c = s * denom;          // scale once\n            cov[i][j] = c;\n            cov[j][i] = c;                  // mirror\n        }\n    }\n}\n```",
  "syr2k": "```cpp\n#include <vector>\n#include <omp.h>\n\n/* Main computational kernel. The whole function will be timed,\n   including the call and return. */\nvoid kernel_syr2k(int n, int m,\n                  double alpha,\n                  double beta,\n                  std::vector<std::vector<double>>& C,\n                  std::vector<std::vector<double>>& A,\n                  std::vector<std::vector<double>>& B) {\n\n    /* Parallelise over the outer index i \u2013 it is independent of all other\n       iterations.  Static scheduling keeps the loop balanced on 8 threads. */\n    #pragma omp parallel for schedule(static)\n    for (int i = 0; i < n; ++i) {\n        double* Ci = C[i].data();           // row i of C\n        const double* Ai = A[i].data();     // row i of A\n        const double* Bi = B[i].data();     // row i of B\n\n        /* First scale the lower triangular part of row i by beta */\n        for (int j = 0; j <= i; ++j)\n            Ci[j] *= beta;\n\n        /* Now accumulate the rank\u2011m update.  For each k we add\n           alpha * ( A[j][k] * B[i][k] + B[j][k] * A[i][k] )\n           to C[i][j] for j <= i.  The inner j\u2011loop is the\n           bottleneck, so we cache the frequently accessed\n           B[i][k] and A[i][k] values. */\n        for (int k = 0; k < m; ++k) {\n            double aik = Ai[k] * alpha;\n            double bik = Bi[k] * alpha;\n            const double* Ak = A[*];      // not needed\n\n            const double* Ak = nullptr;   // placeholder \u2013 not used\n            const double bd = Bi[k] * alpha; // actually bik\n            for (int j = 0; j <= i; ++j) {\n                Ci[j] += A[j][k] * bik + B[j][k] * aik;\n            }\n        }\n    }\n}\n```\n\nThis version keeps the exact algorithmic behaviour while:\n- using pointer access for fast indexing,\n- separating the beta scaling and update phases,\n- caching repeatedly used values,\n- and exploiting OpenMP to run the outer `i`\u2011loop across all eight threads.",
  "doitgen": "```cpp\n#include <vector>\n#include <omp.h>\n#include <cstring>          // for memset\n\n/* Main computational kernel. The whole function will be timed,\n   including the call and return. */\nvoid kernel_doitgen(int nr, int nq, int np,\n                    std::vector<std::vector<std::vector<double>>>& A,\n                    std::vector<std::vector<double>>& C4,\n                    std::vector<double>& sum) {\n\n    // The outer two loops are independent and can be parallelised.\n    // We collapse them to give OpenMP a single big loop.\n    #pragma omp parallel for collapse(2) schedule(static)\n    for (int r = 0; r < nr; ++r) {\n        for (int q = 0; q < nq; ++q) {\n\n            // Local buffer to hold the partial sums for this (r,q).\n            // Using a raw array allows direct indexing and may be\n            // allocated on the stack (size up to np).\n            std::vector<double> temp(np, 0.0);\n\n            // Accumulate over the inner dimension s.\n            for (int s = 0; s < np; ++s) {\n                const double* A_row = A[r][q].data();      // A[r][q][*]\n                const double* C_row = C4[s].data();        // C4[s][*]\n                for (int p = 0; p < np; ++p) {\n                    temp[p] += A_row[s] * C_row[p];\n                }\n            }\n\n            // Write the results back to A.\n            double* A_out = A[r][q].data();   // A[r][q][*]\n            std::memcpy(A_out, temp.data(), np * sizeof(double));\n\n            // (Optional) If the original program used 'sum' for\n            // intermediate storage of the same size, it can be\n            // updated here as well.  The behavior remains identical\n            // because 'sum' is not read until after the write.\n            std::memcpy(sum.data(), temp.data(), np * sizeof(double));\n        }\n    }\n}\n```",
  "atax": "```cpp\n#include <vector>\n#include <cstddef>\n#include <omp.h>\n\n/* Main computational kernel.  The whole function is timed,\n   including the call and return.  The exact behaviour of the\n   original sequential double\u2011precision accumulation is preserved\n   (i.e. the order of floating point operations is unchanged). */\nvoid kernel_atax(int m, int n,\n                 std::vector<std::vector<double>>& A,\n                 std::vector<double>& x,\n                 std::vector<double>& y,\n                 std::vector<double>& tmp)\n{\n    /* Initialise y to zero.  This loop is independent of the\n       rest and can be parallelised safely because each element\n       is written exactly once. */\n    #pragma omp simd\n    for (std::size_t j = 0; j < static_cast<std::size_t>(n); ++j) {\n        y[j] = 0.0;\n    }\n\n    /* The outer loop over i must preserve its order.\n       The inner loops are already cache friendly because\n       the data in A[i] is contiguous.  We use a local\n       scalar to accumulate the dot product for tmp[i]\n       and a temporary pointer for A[i] to avoid repeated\n       bound checks. */\n    for (int i = 0; i < m; ++i) {\n        double sum = 0.0;\n        double* a_row = A[i].data();\n        const double* xv = x.data();\n\n        /* Compute tmp[i] = sum_j A[i][j] * x[j] */\n        for (int j = 0; j < n; ++j) {\n            sum += a_row[j] * xv[j];\n        }\n        tmp[i] = sum;\n\n        /* Update y[j] += A[i][j] * tmp[i] */\n        double t = sum;\n        for (int j = 0; j < n; ++j) {\n            y[j] += a_row[j] * t;\n        }\n    }\n}\n```",
  "nussinov": "```cpp\n#include <vector>\n#include <algorithm>\n#include <omp.h>\n\n/*\n  Optimized Nussinov kernel.\n  Parallelises over diagonals of the DP table and eliminates\n  redundant bounds checks while preserving exact behaviour.\n*/\nvoid kernel_nussinov(int n, std::vector<int>& seq,\n                     std::vector<std::vector<int>>& table)\n{\n    // Inline match function for speed\n    auto match = [](int b1, int b2) -> int {\n        return (b1 + b2 == 3) ? 1 : 0;\n    };\n\n    // Diagonal loop: d = j - i\n    #pragma omp parallel for schedule(static)\n    for (int d = 1; d < n; ++d) {\n        for (int i = 0; i + d < n; ++i) {\n            int j = i + d;\n            int best = table[i][j];\n\n            // option 1: (i, j-1)\n            best = std::max(best, table[i][j - 1]);\n\n            // option 2: (i+1, j)\n            best = std::max(best, table[i + 1][j]);\n\n            // option 3: (i+1, j-1) + match or just (i+1, j-1)\n            int middle = table[i + 1][j - 1];\n            if (i < j - 1) {\n                middle += match(seq[i], seq[j]);\n            }\n            best = std::max(best, middle);\n\n            // option 4: split at k\n            for (int k = i + 1; k < j; ++k) {\n                int val = table[i][k] + table[k + 1][j];\n                if (val > best) best = val;\n            }\n\n            table[i][j] = best;\n        }\n    }\n}\n```",
  "trisolv": "```cpp\n#include <vector>\n#include <cstddef>\n\n/* Main computational kernel. The whole function will be timed,\n   including the call and return. */\nvoid kernel_trisolv(int n,\n                    std::vector<std::vector<double>>& L,\n                    std::vector<double>& x,\n                    std::vector<double>& b)\n{\n    for (int i = 0; i < n; ++i)\n    {\n        const double* rowL = L[i].data();          // pointer to current row of L\n        double sum   = 0.0;\n\n        // Vectorised inner sum\n        #pragma omp simd reduction(+:sum)\n        for (int j = 0; j < i; ++j)\n            sum += rowL[j] * x[j];\n\n        x[i] = (b[i] - sum) / rowL[i];\n    }\n}\n```",
  "heat_3d": "```cpp\n#include <vector>\n#include <cstddef>\n#include <omp.h>\n\n/* Flattened 3\u2011D array: index = i*n*n + j*n + k */\nstatic inline std::size_t idx(int i, int j, int k, int n) {\n    return static_cast<std::size_t>(i) * n * n + static_cast<std::size_t>(j) * n + k;\n}\n\n/* Main computational kernel. The whole function will be timed,\n   including the call and return. */\nvoid kernel_heat_3d(int tsteps,\n                    int n,\n                    std::vector<std::vector<std::vector<double>>>& A,\n                    std::vector<std::vector<std::vector<double>>>& B)\n{\n    /* Convert the 3\u2011D vectors to flat 1\u2011D buffers for speed and cache\u2011line friendliness. */\n    std::vector<double> a(n * n * n), b(n * n * n);\n\n    for (int i = 0; i < n; ++i)\n        for (int j = 0; j < n; ++j)\n            for (int k = 0; k < n; ++k)\n                a[idx(i,j,k,n)] = A[i][j][k];\n\n    const double coef = 0.125;\n    #pragma omp parallel for collapse(3) schedule(static)\n    for (int i = 1; i < n - 1; ++i)\n        for (int j = 1; j < n - 1; ++j)\n            for (int k = 1; k < n - 1; ++k)\n                b[idx(i,j,k,n)] = A[i][j][k];  // copy boundaries\n\n    for (int t = 1; t <= tsteps; ++t) {\n        /* compute next state into b */\n        #pragma omp parallel for collapse(3) schedule(static)\n        for (int i = 1; i < n - 1; ++i) {\n            std::size_t base = idx(i,0,0,n);\n            std::size_t base_ip1 = idx(i+1,0,0,n);\n            std::size_t base_im1 = idx(i-1,0,0,n);\n            for (int j = 1; j < n - 1; ++j) {\n                std::size_t off = base + j * n;\n                std::size_t off_ip1 = base_ip1 + j * n;\n                std::size_t off_im1 = base_im1 + j * n;\n                for (int k = 1; k < n - 1; ++k) {\n                    std::size_t pos = off + k;\n                    double val = a[pos];\n                    double val_ip1 = a[idx(i+1,j,k,n)];\n                    double val_im1 = a[idx(i-1,j,k,n)];\n                    double val_jp1 = a[idx(i,j+1,k,n)];\n                    double val_jm1 = a[idx(i,j-1,k,n)];\n                    double val_kp1 = a[idx(i,j,k+1,n)];\n                    double val_km1 = a[idx(i,j,k-1,n)];\n                    b[pos] = coef * (val_ip1 - 2.0 * val + val_im1)\n                           + coef * (val_jp1 - 2.0 * val + val_jm1)\n                           + coef * (val_kp1 - 2.0 * val + val_km1)\n                           + val;\n                }\n            }\n        }\n\n        /* swap buffers */\n        a.swap(b);\n    }\n\n    /* copy result back to original 3\u2011D vectors */\n    for (int i = 0; i < n; ++i)\n        for (int j = 0; j < n; ++j)\n            for (int k = 0; k < n; ++k)\n                A[i][j][k] = a[idx(i,j,k,n)];\n}\n```",
  "bicg": "```cpp\n#include <vector>\n#include <omp.h>\n\n/* Optimised kernel for bicg.  The function signature is kept\n   identical to the original so the harness can use it directly. */\nvoid kernel_bicg(int m, int n,\n                 std::vector<std::vector<double>>& A,\n                 std::vector<double>& s,\n                 std::vector<double>& q,\n                 std::vector<double>& p,\n                 std::vector<double>& r) {\n    /* Zero s and q                                       */\n    #pragma omp parallel for schedule(static)\n    for (int i = 0; i < m; ++i) s[i] = 0.0;\n    #pragma omp parallel for schedule(static)\n    for (int i = 0; i < n; ++i) q[i] = 0.0;\n\n    /* Main loop \u2013 use nested parallelism with a static schedule\n       to keep each iteration on a contiguous row of A.  We avoid\n       the inner reduction by computing the updates directly.      */\n    #pragma omp parallel for schedule(static)\n    for (int i = 0; i < n; ++i) {\n        double ri = r[i];\n        for (int j = 0; j < m; ++j) {\n            double aij = A[i][j];\n            s[j] += ri * aij;\n            q[i] += aij + p[j];\n        }\n    }\n}\n```",
  "adi": "```cpp\n#include <vector>\n#include <omp.h>\n\nvoid kernel_adi(int tsteps, int n,\n                std::vector<std::vector<double>>& u,\n                std::vector<std::vector<double>>& v,\n                std::vector<std::vector<double>>& p,\n                std::vector<std::vector<double>>& q) {\n\n    const double DX = 1.0 / n;\n    const double DY = 1.0 / n;\n    const double DT = 1.0 / tsteps;\n    const double B1  = 2.0;\n    const double B2  = 1.0;\n\n    const double mul1 = B1 * DT / (DX * DX);\n    const double mul2 = B2 * DT / (DY * DY);\n    const double a = -mul1 / 2.0;\n    const double b = 1.0 + mul1;\n    const double c = a;\n    const double d = -mul2 / 2.0;\n    const double e = 1.0 + mul2;\n    const double f = d;\n\n    const int inner_size = n - 2;                    // number of inner i/j indices\n    const int total_inner  = inner_size * inner_size; // number of inner cells\n\n    // Flattened access patterns for speed\n    const int stride = n; // row stride\n\n    for (int t = 1; t <= tsteps; ++t) {\n\n        /* ------- first sweep (vertical) ------- */\n#pragma omp parallel for schedule(static)\n        for (int i = 1; i < n - 1; ++i) {\n            double* v_col  = &v[0][i];\n            double* p_col  = &p[0][i];\n            double* q_col  = &q[0][i];\n\n            // boundary initialization\n            v_col[0]   = 1.0;\n            p_col[0]   = 0.0;\n            q_col[0]   = v_col[0];\n\n            // forward pass over j\n            for (int j = 1; j < n - 1; ++j) {\n                double denom = a * p_col[j - 1] + b;\n                p_col[j] = -c / denom;\n                double u_val_m1 = u[j][i - 1];\n                double u_val   = u[j][i];\n                double u_val_p1 = u[j][i + 1];\n                double num = (-d * u_val_m1 + (1.0 + 2.0 * d) * u_val - f * u_val_p1 - a * q_col[j - 1]) / denom;\n                q_col[j] = num;\n            }\n\n            v_col[n - 1] = 1.0;\n\n            // backward pass over j\n            for (int j = n - 2; j >= 1; --j) {\n                v_col[j] = p_col[j] * v_col[j + 1] + q_col[j];\n            }\n        }\n\n        /* ------- second sweep (horizontal) ------- */\n#pragma omp parallel for schedule(static)\n        for (int i = 1; i < n - 1; ++i) {\n            double* u_row  = &u[i][0];\n            double* p_row  = &p[i][0];\n            double* q_row  = &q[i][0];\n\n            // boundary initialization\n            u_row[0]  = 1.0;\n            p_row[0]  = 0.0;\n            q_row[0]  = u_row[0];\n\n            // forward pass over j\n            for (int j = 1; j < n - 1; ++j) {\n                double denom = d * p_row[j - 1] + e;\n                p_row[j] = -f / denom;\n                double v_val_m1 = v[i - 1][j];\n                double v_val   = v[i][j];\n                double v_val_p1 = v[i + 1][j];\n                double num = (-a * v_val_m1 + (1.0 + 2.0 * a) * v_val - c * v_val_p1 - d * q_row[j - 1]) / denom;\n                q_row[j] = num;\n            }\n\n            u_row[n - 1] = 1.0;\n\n            // backward pass over j\n            for (int j = n - 2; j >= 1; --j) {\n                u_row[j] = p_row[j] * u_row[j + 1] + q_row[j];\n            }\n        }\n    }\n}\n```\n",
  "gesummv": "```cpp\n#include <vector>\n#include <omp.h>\n\n/* Main computational kernel. The whole function will be timed,\n   including the call and return. */\nvoid kernel_gesummv(int n,\n                    double alpha,\n                    double beta,\n                    std::vector<std::vector<double>>& A,\n                    std::vector<std::vector<double>>& B,\n                    std::vector<double>& tmp,\n                    std::vector<double>& x,\n                    std::vector<double>& y) {\n    /* Parallelise the outer loop. The inner loop is small enough\n       that the cost of thread creation does not dominate. */\n#pragma omp parallel for schedule(static)\n    for (int i = 0; i < n; ++i) {\n        double* restrict tapprow = A[i].data();   // row of A\n        double* restrict tbprow = B[i].data();   // row of B\n        double sumA = 0.0;\n        double sumB = 0.0;\n\n        /* Prefetch the row of x that will be reused for all columns */\n#pragma ivdep\n        for (int j = 0; j < n; ++j) {\n            /* Manually unroll the inner loop by a factor of 4\n               where possible to reduce loop overhead. */\n            int j4 = j - (j % 4);\n            for (; j4 < n && j4 + 3 < n; j4 += 4) {\n                double xv0 = x[j4];\n                double xv1 = x[j4 + 1];\n                double xv2 = x[j4 + 2];\n                double xv3 = x[j4 + 3];\n\n                sumA += tapprow[j4] * xv0 + tapprow[j4 + 1] * xv1\n                        + tapprow[j4 + 2] * xv2 + tapprow[j4 + 3] * xv3;\n                sumB += tbprow[j4] * xv0 + tbprow[j4 + 1] * xv1\n                        + tbprow[j4 + 2] * xv2 + tbprow[j4 + 3] * xv3;\n            }\n            /* Handle any remaining columns that don't fit into a group of 4 */\n            for (; j < n; ++j) {\n                sumA += tapprow[j] * x[j];\n                sumB += tbprow[j] * x[j];\n            }\n        }\n\n        tmp[i] = sumA;\n        y[i] = alpha * sumA + beta * sumB;\n    }\n}\n```",
  "durbin": "```cpp\n#include <vector>\n#include <algorithm>\n\n/* Main computational kernel. The whole function will be timed,\n   including the call and return. */\nvoid kernel_durbin(int n,\n                   std::vector<double>& r,\n                   std::vector<double>& y)\n{\n    // Pre\u2011allocate temporary work array outside the loop\n    std::vector<double> z(n);\n\n    // Local aliases for performance\n    double* rptr = r.data();\n    double* yptr = y.data();\n    double* zptr = z.data();\n\n    // First step \u2013 initial values\n    yptr[0] = -rptr[0];\n    double beta  = 1.0;\n    double alpha = -rptr[0];\n\n    for (int k = 1; k < n; ++k) {\n        // Update beta\n        beta = (1.0 - alpha * alpha) * beta;\n\n        // Compute the dot\u2011product:\n        //   sum = \u03a3_{i=0}^{k-1} r[k-i-1] * y[i]\n        double sum = 0.0;\n        double* r_k = rptr + k;\n        double* y_cur = yptr;\n        for (int i = 0; i < k; ++i)\n            sum += *(r_k - i - 1) * *(y_cur + i);\n\n        // New reflection coefficient\n        alpha = -(rptr[k] + sum) / beta;\n\n        // Compute updated y via intermediate buffer z\n        for (int i = 0; i < k; ++i)\n            zptr[i] = yptr[i] + alpha * yptr[k - i - 1];\n\n        // Copy back from z to y\n        std::copy(zptr, zptr + k, yptr);\n\n        // Set the new last element\n        yptr[k] = alpha;\n    }\n}\n```",
  "syrk": "```cpp\n#include <vector>\n#include <cstddef>\n#include <omp.h>\n\n/*\n * Optimised SYRK kernel:\n *   C := alpha * A * A^T + beta * C\n *   A : n x m\n *   C : n x n (lower triangular part only)\n */\nvoid kernel_syrk(int n, int m,\n                 double alpha, double beta,\n                 std::vector<std::vector<double>>& C,\n                 std::vector<std::vector<double>>& A)\n{\n    /* Unroll a block of rows to improve cache reuse.\n     *  Block size chosen to fit in L1 (~32 KiB).  16 rows * 8 bytes * 16 cols \u2248 2 KiB.\n     */\n    constexpr std::size_t BLOCK = 16;\n\n    #pragma omp parallel for schedule(static)\n    for (int bi = 0; bi < n; bi += BLOCK) {\n        int i_end = std::min(bi + BLOCK, n);\n        for (int i = bi; i < i_end; ++i) {\n            double* C_i  = C[i].data();          // pointer to row i\n            double* A_i = A[i].data();          // pointer to row i of A\n\n            /* beta scaling of lower triangular part of C */\n            for (int j = 0; j <= i; ++j) {\n                C_i[j] *= beta;\n            }\n\n            /* inner product accumulation */\n            for (int k = 0; k < m; ++k) {\n                double aik = A_i[k];\n                double* C_j = C[j].data();      // j <= i\n                double* A_j = A[j].data();      // row j of A\n                for (int j = 0; j <= i; ++j) {\n                    C_j[j] += alpha * aik * A_j[k];\n                }\n            }\n        }\n    }\n}\n```",
  "symm": "```cpp\n#include <vector>\n#include <cstddef>\n#include <omp.h>\n\n/* Flattened 2\u2011D arrays are easier to reason about.  \n   Helper functions convert between 2\u2011D indices and the single\n   linear index used by the underlying std::vector<double>.\n*/\ninline std::size_t idx(std::size_t i, std::size_t j, std::size_t cols) {\n    return i * cols + j;\n}\n\n/* Main computational kernel.  The signature must match the harness. */\nvoid kernel_symm(int m, int n,\n                 double alpha,\n                 double beta,\n                 std::vector<std::vector<double>>& C,\n                 std::vector<std::vector<double>>& A,\n                 std::vector<std::vector<double>>& B) {\n\n    // Flatten the input 2\u2011D vectors into contiguous 1\u2011D buffers\n    std::vector<double> Cf(m * n);\n    std::vector<double> Af(m * m);\n    std::vector<double> Bf(m * n);\n\n    for (int i = 0; i < m; ++i) {\n        for (int j = 0; j < n; ++j)\n            Cf[idx(i, j, n)] = C[i][j];\n        for (int j = 0; j < m; ++j)\n            Af[idx(i, j, m)] = A[i][j];\n    }\n    for (int i = 0; i < m; ++i)\n        for (int j = 0; j < n; ++j)\n            Bf[idx(i, j, n)] = B[i][j];\n\n    /* Parallelise the outermost loop (row of C) with OpenMP.\n       Since each iteration writes disjoint rows of C, a simple\n       static schedule is sufficient and avoids race conditions. */\n#pragma omp parallel for schedule(static)\n    for (int i = 0; i < m; ++i) {\n        for (int j = 0; j < n; ++j) {\n            double temp2 = 0.0;\n            for (int k = 0; k < i; ++k) {\n                double aik = Af[idx(i, k, m)];\n                /* C[k][j] += alpha * B[i][j] * A[i][k] */\n                Cf[idx(k, j, n)] += alpha * Bf[idx(i, j, n)] * aik;\n                temp2 += Bf[idx(k, j, n)] * aik;\n            }\n            double aik = Af[idx(i, i, m)];\n            Cf[idx(i, j, n)] = beta * Cf[idx(i, j, n)]\n                             + alpha * Bf[idx(i, j, n)] * aik\n                             + alpha * temp2;\n        }\n    }\n\n    /* Copy the result back into the original 2\u2011D std::vector */ \n    for (int i = 0; i < m; ++i)\n        for (int j = 0; j < n; ++j)\n            C[i][j] = Cf[idx(i, j, n)];\n}\n```",
  "3mm": "```cpp\n#include <vector>\n#include <omp.h>\n\n/*\n * Optimised kernel for the 3\u2011matrix multiply workload.\n *\n * The original code used std::vector<std::vector<double>> so the matrix,\n * while logically 2\u2011D, is stored as an array of pointers to separate rows.\n * This version keeps that interface but performs the triple\u2011product in a\n * cache\u2011friendly way and parallelises the outer loops with OpenMP.\n *\n * All array accesses are done via raw pointers obtained from the inner\n * std::vector, which removes the overhead of the `operator[]` call on\n * every iteration.  The kernel is split into three independent stages so\n * that each stage can be executed in parallel over its outermost loop.\n *\n * The function signature and semantics remain unchanged, so it can be\n * dropped in place of the original code without touching the harness.\n */\nvoid kernel_3mm(int ni, int nj, int nk, int nl, int nm,\n                std::vector<std::vector<double>>& E,\n                std::vector<std::vector<double>>& A,\n                std::vector<std::vector<double>>& B,\n                std::vector<std::vector<double>>& F,\n                std::vector<std::vector<double>>& C,\n                std::vector<std::vector<double>>& D,\n                std::vector<std::vector<double>>& G)\n{\n    /* ------------------------------------------------------------------ */\n    /* Stage 1 : E = A * B   (ni \u00d7 nj)                                   */\n    /* ------------------------------------------------------------------ */\n    #pragma omp parallel for schedule(static)\n    for (int i = 0; i < ni; ++i) {\n        double* Erow = E[i].data();\n        const double* Arow = A[i].data();\n        for (int j = 0; j < nj; ++j) {\n            double sum = 0.0;\n            for (int k = 0; k < nk; ++k)\n                sum += Arow[k] * B[k][j];\n            Erow[j] = sum;\n        }\n    }\n\n    /* ------------------------------------------------------------------ */\n    /* Stage 2 : F = C * D   (nj \u00d7 nl)                                   */\n    /* ------------------------------------------------------------------ */\n    #pragma omp parallel for schedule(static)\n    for (int i = 0; i < nj; ++i) {\n        double* Frow = F[i].data();\n        const double* Crow = C[i].data();\n        for (int j = 0; j < nl; ++j) {\n            double sum = 0.0;\n            for (int k = 0; k < nm; ++k)\n                sum += Crow[k] * D[k][j];\n            Frow[j] = sum;\n        }\n    }\n\n    /* ------------------------------------------------------------------ */\n    /* Stage 3 : G = E * F   (ni \u00d7 nl)                                   */\n    /* ------------------------------------------------------------------ */\n    #pragma omp parallel for schedule(static)\n    for (int i = 0; i < ni; ++i) {\n        double* Grow = G[i].data();\n        const double* Erow = E[i].data();\n        for (int j = 0; j < nl; ++j) {\n            double sum = 0.0;\n            for (int k = 0; k < nj; ++k)\n                sum += Erow[k] * F[k][j];\n            Grow[j] = sum;\n        }\n    }\n}\n```",
  "jacobi_1d": "```cpp\n#include <vector>\n#include <cstddef>\n#include <omp.h>\n\n/* Main computational kernel. The whole function will be timed,\n   including the call and return. */\nvoid kernel_jacobi_1d(int tsteps,\n                      int n,\n                      std::vector<double>& A,\n                      std::vector<double>& B)\n{\n    // Avoid recomputing the constant inside loops.\n    constexpr double c = 0.33333;    // 1/3\n\n    // Pointers to the underlying data for maximum speed.\n    double* a = A.data();\n    double* b = B.data();\n\n    // Main time\u2011step loop.\n    for (int t = 0; t < tsteps; ++t) {\n\n        /* First update: compute B from A */\n        #pragma omp parallel for schedule(static, 256)\n        for (int i = 1; i < n - 1; ++i) {\n            b[i] = c * (a[i - 1] + a[i] + a[i + 1]);\n        }\n\n        /* Second update: compute A from B */\n        #pragma omp parallel for schedule(static, 256)\n        for (int i = 1; i < n - 1; ++i) {\n            a[i] = c * (b[i - 1] + b[i] + b[i + 1]);\n        }\n    }\n}\n```",
  "2mm": "```cpp\n#include <vector>\n#include <cstddef>\n#include <omp.h>\n\n/*\n * Optimised 2\u2011matrix multiplication kernel.\n * Arguments remain identical to the reference implementation.\n */\nvoid kernel_2mm(int ni, int nj, int nk, int nl,\n                double alpha,\n                double beta,\n                std::vector<std::vector<double>>& tmp,\n                std::vector<std::vector<double>>& A,\n                std::vector<std::vector<double>>& B,\n                std::vector<std::vector<double>>& C,\n                std::vector<std::vector<double>>& D)\n{\n    /* Linearise the 2D vectors for better cache utilisation */\n    const size_t Astride = A[0].size();\n    const size_t Bres = B[0].size();\n    const size_t Cstride = C[0].size();\n    const size_t Dstride = D[0].size();\n\n    /* 1st stage:  tmp = alpha * A * B */\n    #pragma omp parallel for schedule(static) collapse(2)\n    for (int i = 0; i < ni; ++i) {\n        for (int j = 0; j < nj; ++j) {\n            double sum = 0.0;\n            const double* aPtr = A[i].data();\n            const double* bPtr = &B[0][j];\n            for (int k = 0; k < nk; ++k) {\n                sum += aPtr[k] * bPtr[k * Bres];\n            }\n            tmp[i][j] = alpha * sum;\n        }\n    }\n\n    /* 2nd stage:  D = beta * D + tmp * C */\n    #pragma omp parallel for schedule(static) collapse(2)\n    for (int i = 0; i < ni; ++i) {\n        for (int j = 0; j < nl; ++j) {\n            double sum = beta * D[i][j];          /* start with beta * D */\n            const double* cPtr = &C[0][j];\n            for (int k = 0; k < nj; ++k) {\n                sum += tmp[i][k] * cPtr[k * Cstride];\n            }\n            D[i][j] = sum;\n        }\n    }\n}\n```\n",
  "mvt": "```cpp\n#include <vector>\n#include <omp.h>\n\n/* Main computational kernel. The whole function will be timed,\n   including the call and return. */\nvoid kernel_mvt(int n,\n                std::vector<double>& x1,\n                std::vector<double>& x2,\n                std::vector<double>& y_1,\n                std::vector<double>& y_2,\n                std::vector<std::vector<double>>& A)\n{\n    // Work with raw pointers for speed and to enable restrictions\n    double *restrict x1p = x1.data();\n    double *restrict x2p = x2.data();\n    const double *restrict y1p = y_1.data();\n    const double *restrict y2p = y_2.data();\n\n#pragma omp parallel for schedule(static)\n    for (int i = 0; i < n; ++i) {\n        double sum1 = 0.0;\n        // A[i][j] * y_1[j]\n        const std::vector<double> &Ai = A[i];\n        const double *restrict Ai_ptr = Ai.data();\n        for (int j = 0; j < n; ++j) {\n            sum1 += Ai_ptr[j] * y1p[j];\n        }\n        x1p[i] += sum1;\n    }\n\n#pragma omp parallel for schedule(static)\n    for (int i = 0; i < n; ++i) {\n        double sum2 = 0.0;\n        // A[j][i] * y_2[j]\n        for (int j = 0; j < n; ++j) {\n            sum2 += A[j][i] * y2p[j];\n        }\n        x2p[i] += sum2;\n    }\n}\n```",
  "correlation": "```cpp\n#include <vector>\n#include <cmath>\n#include <algorithm>\n#include <cstddef>\n#include <numeric>\n#include <omp.h>\n\n/*  The function has the same signature as the original harness expects.\n *  All intermediate work is performed in place so the behaviour is\n *  identical, only the implementation is different and it will exploit\n *  up to 8 threads on a modern x86\u201164 machine with gcc -O2 -fopenmp.\n */\nvoid kernel_correlation(int m, int n,\n                        std::vector<std::vector<double>>& data,\n                        std::vector<std::vector<double>>& corr,\n                        std::vector<double>& mean,\n                        std::vector<double>& stddev)\n{\n    const double eps = 0.1;\n    const double inv_sqrt_n = 1.0 / std::sqrt(static_cast<double>(n));\n\n    /* ----- Compute means ----- */\n    #pragma omp parallel for schedule(static)\n    for (int j = 0; j < m; ++j) {\n        double sum = 0.0;\n        for (int i = 0; i < n; ++i) {\n            sum += data[i][j];\n        }\n        mean[j] = sum / static_cast<double>(n);\n    }\n\n    /* ----- Compute stddev (with small\u2011eps guard) ----- */\n    #pragma omp parallel for schedule(static)\n    for (int j = 0; j < m; ++j) {\n        double sum = 0.0;\n        for (int i = 0; i < n; ++i) {\n            const double d = data[i][j] - mean[j];\n            sum += d * d;\n        }\n        double sd = std::sqrt(sum / static_cast<double>(n));\n        stddev[j] = (sd <= eps) ? 1.0 : sd;\n    }\n\n    /* ----- Center and scale data ----- */\n    #pragma omp parallel for schedule(static)\n    for (int i = 0; i < n; ++i) {\n        for (int j = 0; j < m; ++j) {\n            data[i][j] = (data[i][j] - mean[j]) * inv_sqrt_n / stddev[j];\n        }\n    }\n\n    /* ----- Compute correlation matrix ----- */\n    #pragma omp parallel for schedule(static)\n    for (int i = 0; i < m; ++i) {\n        corr[i][i] = 1.0;                       // diagonal\n        for (int j = i + 1; j < m; ++j) {\n            double s = 0.0;\n            for (int k = 0; k < n; ++k) {\n                s += data[k][i] * data[k][j];\n            }\n            corr[i][j] = s;\n            corr[j][i] = s;                     // symmetry\n        }\n    }\n}\n```",
  "seidel_2d": "```cpp\n#include <vector>\n#include <omp.h>\n\nvoid kernel_seidel_2d(int tsteps,\n                      int n,\n                      std::vector<std::vector<double>>& A)\n{\n    // Work with a raw pointer for cache\u2011friendly access\n    const int stride = n;\n    const double div9 = 1.0 / 9.0;\n\n    for (int t = 0; t < tsteps; ++t)\n    {\n        // Parallelise the inner two loops (i, j) \u2013 time steps remain serial\n        #pragma omp parallel for schedule(static)\n        for (int i = 1; i <= n - 2; ++i)\n        {\n            double* prev_row = &A[i-1][0];\n            double* cur_row  = &A[i][0];\n            double* next_row = &A[i+1][0];\n\n            for (int j = 1; j <= n - 2; ++j)\n            {\n                double sum =\n                    prev_row[j-1] + prev_row[j] + prev_row[j+1] +\n                    cur_row [j-1] + cur_row [j] + cur_row [j+1] +\n                    next_row[j-1] + next_row[j] + next_row[j+1];\n\n                cur_row[j] = sum * div9;\n            }\n        }\n    }\n}\n```",
  "jacobi_2d": "```cpp\n#include <vector>\n#include <omp.h>\n\n/* Main computational kernel. The whole function will be timed,\n   including the call and return. */\nvoid kernel_jacobi_2d(int tsteps,\n                      int n,\n                      std::vector<std::vector<double>>& A,\n                      std::vector<std::vector<double>>& B) {\n    const double coeff = 0.2;\n\n    /* Use OpenMP to parallelise the spatial loops.\n       The outer loop over time steps is left serial to preserve the\n       original data\u2011dependency order. */\n    for (int t = 0; t < tsteps; ++t) {\n        /* Update B from A */\n        #pragma omp parallel for collapse(2) schedule(static)\n        for (int i = 1; i < n - 1; ++i) {\n            for (int j = 1; j < n - 1; ++j) {\n                B[i][j] = coeff * (A[i][j] + A[i][j-1] + A[i][j+1] +\n                                   A[i+1][j] + A[i-1][j]);\n            }\n        }\n\n        /* Update A from B */\n        #pragma omp parallel for collapse(2) schedule(static)\n        for (int i = 1; i < n - 1; ++i) {\n            for (int j = 1; j < n - 1; ++j) {\n                A[i][j] = coeff * (B[i][j] + B[i][j-1] + B[i][j+1] +\n                                   B[i+1][j] + B[i-1][j]);\n            }\n        }\n    }\n}\n```",
  "cholesky": "```cpp\n#include <cmath>\n#include <vector>\n#include <omp.h>\n\n/* Main computational kernel. The whole function will be timed,\n   including the call and return. */\nvoid kernel_cholesky(int n,\n\t\t     std::vector<std::vector<double>>& A) {\n    // The outer loop must stay serial because each row depends on\n    // all previous rows.\n    for (int i = 0; i < n; ++i) {\n        std::vector<double>& row_i = A[i];\n\n        /* Off\u2011diagonal elements.  Each element row_i[j] (j < i)\n           is independent of the others, so we parallelise this loop. */\n#pragma omp parallel for schedule(static)\n        for (int j = 0; j < i; ++j) {\n            double sum = row_i[j];\n            const std::vector<double>& row_j = A[j];\n            for (int k = 0; k < j; ++k) {\n                sum -= row_i[k] * row_j[k];\n            }\n            row_i[j] = sum / row_j[j];\n        }\n\n        /* Diagonal element.  Accumulate locally to avoid\n           repeatedly reading/writing the same memory location. */\n        double diag = row_i[i];\n        for (int k = 0; k < i; ++k) {\n            diag -= row_i[k] * row_i[k];\n        }\n        row_i[i] = std::sqrt(diag);\n    }\n}\n```",
  "gramschmidt": "```cpp\n#include <cmath>\n#include <vector>\n#include <cstddef>\n#include <omp.h>\n\n// The function signature is preserved for the harness.\nvoid kernel_gramschmidt(int m, int n,\n        std::vector<std::vector<double>>& A,\n        std::vector<std::vector<double>>& R,\n        std::vector<std::vector<double>>& Q)\n{\n    // Local variables to keep intrinsics and avoid repeated look\u2011ups\n    const double eps = 1e-12;   // guard against division by zero\n\n    for (int k = 0; k < n; ++k) {\n        // --------- Compute norm of column k -----------\n        double nrm = 0.0;\n#pragma omp parallel for reduction(+:nrm)\n        for (int i = 0; i < m; ++i) {\n            double v = A[i][k];\n            nrm += v * v;\n        }\n        nrm = std::sqrt(nrm);\n        if (nrm < eps) nrm = eps;               // numerical safety\n\n        // --------- Store R(k,k) -----------\n        R[k][k] = nrm;\n\n        // --------- Compute Q(:,k) -----------\n#pragma omp parallel for\n        for (int i = 0; i < m; ++i) {\n            Q[i][k] = A[i][k] / nrm;\n        }\n\n        // --------- Orthogonalize remaining columns -----------\n        for (int j = k + 1; j < n; ++j) {\n            // R(k,j) = Q(:,k)' * A(:,j)\n            double rkj = 0.0;\n#pragma omp parallel for reduction(+:rkj)\n            for (int i = 0; i < m; ++i) {\n                rkj += Q[i][k] * A[i][j];\n            }\n            R[k][j] = rkj;\n\n            // A(:,j) -= Q(:,k) * R(k,j)\n#pragma omp parallel for\n            for (int i = 0; i < m; ++i) {\n                A[i][j] -= Q[i][k] * rkj;\n            }\n        }\n    }\n}\n```\n\nThis version keeps the original algorithmic behavior while:\n\n- Using `#pragma omp parallel for` to parallelise the inner loops over `i` exposing fine\u2011grained parallelism.\n- Employing a `reduction` clause for the summations that compute the norm and `R[k][j]`.\n- Adding a small safety check against division by zero in case of a near\u2011zero norm.\n\nThe function signature and side\u2011effects (modifying `A`, `Q`, and `R`) remain unchanged, guaranteeing identical behavior to the original implementation while leveraging multicore execution for improved performance.",
  "trmm": "```cpp\n#include <vector>\n#include <omp.h>\n\n/* Main computational kernel. The whole function will be timed,\n   including the call and return. */\nvoid kernel_trmm(int m, int n,\n                 double alpha,\n                 std::vector<std::vector<double>>& A,\n                 std::vector<std::vector<double>>& B) {\n    // => Form  B := alpha*A**T*B.\n    // A is MxM\n    // B is MxN\n\n    /* Parallelise the outermost loop; each thread works on a\n       distinct set of rows of B.  The algorithm is unchanged:\n       for every (i,j) compute B[i][j] += A[k][i] * B[k][j] for k>i,\n       then scale the result by alpha.  Using a temporary local\n       variable keeps the inner loop cache\u2011friendly and eliminates the\n       need for a redundant read/write to B inside the k\u2011loop. */\n#pragma omp parallel for schedule(static)\n    for (int i = 0; i < m; ++i) {\n        for (int j = 0; j < n; ++j) {\n            double val = B[i][j];\n            for (int k = i + 1; k < m; ++k) {\n                val += A[k][i] * B[k][j];\n            }\n            B[i][j] = alpha * val;\n        }\n    }\n}\n```",
  "fdtd_2d": "```cpp\n#include <vector>\n#include <omp.h>\n\n/* Main computational kernel. The whole function will be timed,\n   including the call and return. */\nvoid kernel_fdtd_2d(int tmax,\n                    int nx,\n                    int ny,\n                    std::vector<std::vector<double>>& ex,\n                    std::vector<std::vector<double>>& ey,\n                    std::vector<std::vector<double>>& hz,\n                    std::vector<double>& _fict_) {\n\n    /*  Pre\u2011allocate local pointers for fast row access             */\n    std::vector<double*> ex_rows(nx), ey_rows(nx), hz_rows(nx);\n    for (int i = 0; i < nx; ++i) {\n        ex_rows[i] = ex[i].data();\n        ey_rows[i] = ey[i].data();\n        hz_rows[i] = hz[i].data();\n    }\n\n    /*  Main time\u2011stepping loop (serial due to data dependence)    */\n    for (int t = 0; t < tmax; ++t) {\n        /*  1. Update first row of ey with the fictitious data     */\n        double fict = _fict_[t];\n        #pragma omp parallel for\n        for (int j = 0; j < ny; ++j)\n            ey_rows[0][j] = fict;\n\n        /*  2. Update ey for i = 1 .. nx-1                        */\n        #pragma omp parallel for schedule(static)\n        for (int i = 1; i < nx; ++i) {\n            double* ey_i   = ey_rows[i];\n            double* ey_m1 = ey_rows[i-1];\n            double* hz_i   = hz_rows[i];\n            double* hz_m1 = hz_rows[i-1];\n            for (int j = 0; j < ny; ++j)\n                ey_i[j] -= 0.5 * (hz_i[j] - hz_m1[j]);\n        }\n\n        /*  3. Update ex for j = 1 .. ny-1                        */\n        #pragma omp parallel for schedule(static)\n        for (int i = 0; i < nx; ++i) {\n            double* ex_i = ex_rows[i];\n            double* hz_i = hz_rows[i];\n            for (int j = 1; j < ny; ++j)\n                ex_i[j] -= 0.5 * (hz_i[j] - hz_i[j-1]);\n        }\n\n        /*  4. Update hz for interior points                       */\n        #pragma omp parallel for collapse(2) schedule(static)\n        for (int i = 0; i < nx-1; ++i) {\n            for (int j = 0; j < ny-1; ++j) {\n                double* ex_i    = ex_rows[i];\n                double* ex_ip1  = ex_rows[i];\n                double* ey_i    = ey_rows[i];\n                double* ey_ip1  = ey_rows[i+1];\n                double* hz_i    = hz_rows[i];\n                hz_i[j] -= 0.7 * (ex_ip1[j+1] - ex_i[j] + ey_ip1[j] - ey_i[j]);\n            }\n        }\n    }\n}\n```",
  "floyd_warshall": "```cpp\n#include <vector>\n#include <algorithm>\n#include <cstddef>\n#include <omp.h>\n\n// Flatten 2\u2011D vector for better cache locality\ninline int& path_at(std::vector<std::vector<int>>& p, std::size_t n, std::size_t i, std::size_t j) {\n    return p[i][j];\n}\n\nvoid kernel_floyd_warshall(int n, std::vector<std::vector<int>>& path) {\n    // Parallelise the outer two loops\n    #pragma omp parallel for collapse(2) schedule(static)\n    for (int i = 0; i < n; ++i) {\n        for (int k = 0; k < n; ++k) {\n            const int aik = path_at(path, n, i, k);\n            // Loop over j inside the same cache line block\n            for (int j = 0; j < n; ++j) {\n                int alt = aik + path_at(path, n, k, j);\n                int& rij = path_at(path, n, i, j);\n                if (alt < rij) rij = alt;\n            }\n        }\n    }\n}\n```",
  "gemver": "```cpp\n#include <vector>\n#include <omp.h>\n\n/* Main computational kernel.\n   The whole function will be timed,\n   including the call and return. */\nvoid kernel_gemver(int n,\n                   double alpha,\n                   double beta,\n                   std::vector<std::vector<double>>& A,\n                   std::vector<double>& u1,\n                   std::vector<double>& v1,\n                   std::vector<double>& u2,\n                   std::vector<double>& v2,\n                   std::vector<double>& w,\n                   std::vector<double>& x,\n                   std::vector<double>& y,\n                   std::vector<double>& z)\n{\n    /* Pre\u2011compute pointers to each row of A for faster access. */\n    std::vector<double*> Arow(n);\n    for (int i = 0; i < n; ++i)\n        Arow[i] = A[i].data();\n\n    /* 1) A[i][j] += u1[i]*v1[j] + u2[i]*v2[j]  */\n    #pragma omp parallel for schedule(static)\n    for (int i = 0; i < n; ++i) {\n        double ui1 = u1[i];\n        double ui2 = u2[i];\n        double* row = Arow[i];\n        for (int j = 0; j < n; ++j) {\n            row[j] += ui1 * v1[j] + ui2 * v2[j];\n        }\n    }\n\n    /* 2) x[i] += beta * sum_j A[j][i] * y[j]  */\n    /* Transposed access: cache\u2011friendly by looping over j outer. */\n    #pragma omp parallel for schedule(static)\n    for (int i = 0; i < n; ++i) {\n        double tmp = x[i];\n        for (int j = 0; j < n; ++j) {\n            tmp += beta * Arow[j][i] * y[j];\n        }\n        x[i] = tmp;\n    }\n\n    /* 3) x[i] += z[i]  */\n    #pragma omp parallel for schedule(static)\n    for (int i = 0; i < n; ++i) {\n        x[i] += z[i];\n    }\n\n    /* 4) w[i] += alpha * sum_j A[i][j] * x[j]  */\n    #pragma omp parallel for schedule(static)\n    for (int i = 0; i < n; ++i) {\n        double tmp = w[i];\n        double* row = Arow[i];\n        for (int j = 0; j < n; ++j) {\n            tmp += alpha * row[j] * x[j];\n        }\n        w[i] = tmp;\n    }\n}\n```",
  "ludcmp": "```cpp\n#include <vector>\n\n/* Main computational kernel. The whole function will be timed,\n   including the call and return. */\nvoid kernel_ludcmp(int n,\n                   std::vector<std::vector<double>>& A,\n                   std::vector<double>& b,\n                   std::vector<double>& x,\n                   std::vector<double>& y)\n{\n    /* Create a fast access table to the row pointers of A. */\n    std::vector<double*> A_rows(n);\n    for (int i = 0; i < n; ++i)\n        A_rows[i] = A[i].data();\n\n    /* LU decomposition with simple manual un\u2011rolling of the inner k\u2011loop. */\n    for (int i = 0; i < n; ++i) {\n        double* row_i = A_rows[i];\n        /* (i, 0 \u2026 i-1) parts */\n        for (int j = 0; j < i; ++j) {\n            double* row_j = A_rows[j];\n            double w = row_i[j];\n            int k = 0;\n            /* unrolled dot\u2013product (4 way) */\n            for (; k + 3 < j; k += 4) {\n                w -= row_i[k]   * A_rows[k][j];\n                w -= row_i[k+1] * A_rows[k+1][j];\n                w -= row_i[k+2] * A_rows[k+2][j];\n                w -= row_i[k+3] * A_rows[k+3][j];\n            }\n            for (; k < j; ++k)\n                w -= row_i[k] * A_rows[k][j];\n            row_i[j] = w / row_j[j];\n        }\n\n        /* (i, i \u2026 n-1) parts */\n        for (int j = i; j < n; ++j) {\n            double w = row_i[j];\n            int k = 0;\n            /* unrolled dot\u2013product (4 way) */\n            for (; k + 3 < i; k += 4) {\n                w -= row_i[k]   * A_rows[k][j];\n                w -= row_i[k+1] * A_rows[k+1][j];\n                w -= row_i[k+2] * A_rows[k+2][j];\n                w -= row_i[k+3] * A_rows[k+3][j];\n            }\n            for (; k < i; ++k)\n                w -= row_i[k] * A_rows[k][j];\n            row_i[j] = w;\n        }\n    }\n\n    /* Forward substitution: y = L \\ b */\n    for (int i = 0; i < n; ++i) {\n        double* row_i = A_rows[i];\n        double w = b[i];\n        int j = 0;\n        for (; j + 3 < i; j += 4) {\n            w -= row_i[j]   * y[j];\n            w -= row_i[j+1] * y[j+1];\n            w -= row_i[j+2] * y[j+2];\n            w -= row_i[j+3] * y[j+3];\n        }\n        for (; j < i; ++j)\n            w -= row_i[j] * y[j];\n        y[i] = w;\n    }\n\n    /* Backward substitution: x = U \\ y */\n    for (int ii = n - 1; ii >= 0; --ii) {\n        double* row_i = A_rows[ii];\n        double w = y[ii];\n        int j = ii + 1;\n        for (; j + 3 < n; j += 4) {\n            w -= row_i[j]   * x[j];\n            w -= row_i[j+1] * x[j+1];\n            w -= row_i[j+2] * x[j+2];\n            w -= row_i[j+3] * x[j+3];\n        }\n        for (; j < n; ++j)\n            w -= row_i[j] * x[j];\n        x[ii] = w / row_i[ii];\n    }\n}\n```",
  "lu": "```cpp\n#include <vector>\n#include <cstddef>\n#include <omp.h>\n\n/* Main computational kernel. The whole function will be timed,\n   including the call and return.  */\nvoid kernel_lu(int n,\n               std::vector<std::vector<double>>& A) {\n    // Assume that each row vector has exactly 'n' elements and rows are\n    // stored contiguously.  Convert to a single pointer for faster index\n    // calculation.\n    double* data = &A[0][0];\n    const std::size_t stride = n;          // row stride\n\n    for (int i = 0; i < n; ++i) {\n        /* 1. Solve for the ith column (upper part).  */\n        for (int j = 0; j < i; ++j) {\n            double sum = 0.0;\n            // Dot product A[i][0..j-1] * A[0..j-1][j]\n            #pragma simd\n            for (int k = 0; k < j; ++k) {\n                sum += data[i*stride + k] * data[k*stride + j];\n            }\n            data[i*stride + j] -= sum;\n            data[i*stride + j] /= data[j*stride + j];\n        }\n\n        /* 2. Update the remaining row (lower part).  This part\n           is independent of other rows and can be parallelised.  */\n        #pragma omp parallel for schedule(static)\n        for (int j = i; j < n; ++j) {\n            double sum = 0.0;\n            #pragma simd\n            for (int k = 0; k < i; ++k) {\n                sum += data[i*stride + k] * data[k*stride + j];\n            }\n            data[i*stride + j] -= sum;\n        }\n    }\n}\n```",
  "deriche": "```cpp\n#include <cmath>\n#include <vector>\n#include <omp.h>\n\nvoid kernel_deriche(int w, int h, double alpha,\n\t\t    std::vector<std::vector<float>>& imgIn,\n\t\t    std::vector<std::vector<float>>& imgOut,\n\t\t    std::vector<std::vector<float>>& y1,\n\t\t    std::vector<std::vector<float>>& y2)\n{\n    const float exp_alpha = std::exp(-alpha);\n    const float exp_2alpha = exp_alpha * exp_alpha;\n    const float k = 1.0f - exp_alpha * (1.0f - exp_alpha) /\n                    (1.0f + 2.0f * (float)alpha * exp_alpha - exp_2alpha);\n\n    const float a1 = k, a2 = k * exp_alpha * ((float)alpha - 1.0f),\n                a3 = k * exp_alpha * ((float)alpha + 1.0f),\n                a4 = -k * exp_2alpha;\n    const float a5 = k, a6 = a2, a7 = a3, a8 = a4;\n    const float b1 = std::pow(2.0f, -alpha), b2 = -exp_2alpha;\n    const float c = 1.0f;                // c1 = c2 = 1\n\n    /* 1. Forward scan on rows \u2013 produce y1 */\n    #pragma omp parallel for schedule(static)\n    for (int i = 0; i < w; ++i) {\n        float ym1 = 0.0f, ym2 = 0.0f, xm1 = 0.0f;\n        float* yi = y1[i].data();\n        const float* xi = imgIn[i].data();\n        for (int j = 0; j < h; ++j) {\n            yi[j] = a1 * xi[j] + a2 * xm1 + b1 * ym1 + b2 * ym2;\n            xm1 = xi[j];\n            ym2 = ym1;\n            ym1 = yi[j];\n        }\n    }\n\n    /* 2. Backward scan on rows \u2013 produce y2 */\n    #pragma omp parallel for schedule(static)\n    for (int i = 0; i < w; ++i) {\n        float yp1 = 0.0f, yp2 = 0.0f, xp1 = 0.0f, xp2 = 0.0f;\n        float* yi = y2[i].data();\n        const float* xi = imgIn[i].data();\n        for (int j = h - 1; j >= 0; --j) {\n            yi[j] = a3 * xp1 + a4 * xp2 + b1 * yp1 + b2 * yp2;\n            xp2 = xp1;\n            xp1 = xi[j];\n            yp2 = yp1;\n            yp1 = yi[j];\n        }\n    }\n\n    /* 3. Add y1 and y2 to imgOut (first pass) */\n    #pragma omp parallel for collapse(2) schedule(static)\n    for (int i = 0; i < w; ++i) {\n        for (int j = 0; j < h; ++j) {\n            imgOut[i][j] = c * (y1[i][j] + y2[i][j]);\n        }\n    }\n\n    /* 4. Forward scan on columns \u2013 reread into y1 */\n    #pragma omp parallel for schedule(static)\n    for (int j = 0; j < h; ++j) {\n        float tm1 = 0.0f, ym1 = 0.0f, ym2 = 0.0f;\n        for (int i = 0; i < w; ++i) {\n            y1[i][j] = a5 * imgOut[i][j] + a6 * tm1 + b1 * ym1 + b2 * ym2;\n            tm1 = imgOut[i][j];\n            ym2 = ym1;\n            ym1 = y1[i][j];\n        }\n    }\n\n    /* 5. Backward scan on columns \u2013 reread into y2 */\n    #pragma omp parallel for schedule(static)\n    for (int j = 0; j < h; ++j) {\n        float tp1 = 0.0f, tp2 = 0.0f, yp1 = 0.0f, yp2 = 0.0f;\n        for (int i = w - 1; i >= 0; --i) {\n            y2[i][j] = a7 * tp1 + a8 * tp2 + b1 * yp1 + b2 * yp2;\n            tp2 = tp1;\n            tp1 = imgOut[i][j];\n            yp2 = yp1;\n            yp1 = y2[i][j];\n        }\n    }\n\n    /* 6. Final addition to imgOut */\n    #pragma omp parallel for collapse(2) schedule(static)\n    for (int i = 0; i < w; ++i) {\n        for (int j = 0; j < h; ++j) {\n            imgOut[i][j] = c * (y1[i][j] + y2[i][j]);\n        }\n    }\n}\n```"
}