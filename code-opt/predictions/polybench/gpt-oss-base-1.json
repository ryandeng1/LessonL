{
  "syr2k": "```cpp\n#include <vector>\n#include <omp.h>\n\n/* Main computational kernel. The whole function will be timed,\n   including the call and return. */\nvoid kernel_syr2k(int n, int m,\n\t\t  double alpha,\n\t\t  double beta,\n\t\t  std::vector<std::vector<double>>& C,\n\t\t  std::vector<std::vector<double>>& A,\n\t\t  std::vector<std::vector<double>>& B) {\n\n    // pointers to the underlying memory for faster indexing\n    double* restrict c = &C[0][0];\n    const double* restrict a = &A[0][0];\n    const double* restrict b = &B[0][0];\n\n    #pragma omp parallel for schedule(static)\n    for (int i = 0; i < n; ++i) {\n        int ci = i * n;   // row offset in C\n        int ai = i * m;   // row offset in A (used for A[i][k])\n        int bi = i * m;   // row offset in B (used for B[i][k])\n\n        // Scale the lower part of column i by beta\n        for (int j = 0; j <= i; ++j) {\n            c[ci + j] *= beta;\n        }\n\n        // Perform the rank\u20112 symmetric update\n        for (int k = 0; k < m; ++k) {\n            double ak = a[ai + k];   // A[i][k]\n            double bk = b[bi + k];   // B[i][k]\n\n            for (int j = 0; j <= i; ++j) {\n                double ajk = a[j * m + k];   // A[j][k]\n                double bjk = b[j * m + k];   // B[j][k]\n                c[ci + j] += (ajk * bk + bjk * ak) * alpha;\n            }\n        }\n    }\n}\n```\n\nThis version:\n* Uses contiguous memory pointers (`double* restrict`) for very fast indexing.\n* Parallelises the outer loop over rows with `#pragma omp parallel for`.\n* Keeps the exact semantics of the original code while benefiting from data locality and reduced indirect memory accesses.",
  "doitgen": "```cpp\n#include <vector>\n#include <cstddef>\n#include <omp.h>\n\n/* Main computational kernel. The whole function will be timed,\n   including the call and return. */\nvoid kernel_doitgen(int nr, int nq, int np,\n                    std::vector<std::vector<std::vector<double>>> &A,\n                    std::vector<std::vector<double>> &C4,\n                    std::vector<double> & /* sum */) {\n\n    /* Parallelise the outer two loops.  The ::collapse(2) directive joins the\n       r and q loops so that the work is evenly distributed across the 8\n       threads.  The static schedule is sufficient because the work\n       per iteration is constant. */\n    #pragma omp parallel for collapse(2) schedule(static)\n    for (int r = 0; r < nr; ++r) {\n        for (int q = 0; q < nq; ++q) {\n            /* For each element of the output vector A[r][q][p] we need to\n               compute the dot product of the row A[r][q][:] with the\n               corresponding column of C4.  Doing this in a single\n               expression eliminates the need for a temporary array. */\n            const double *a_row = A[r][q].data();           // pointer to A[r][q][0]\n            double *a_out  = A[r][q].data();               // will overwrite in place\n            for (int p = 0; p < np; ++p) {\n                double sum = 0.0;\n                for (int s = 0; s < np; ++s) {\n                    sum += a_row[s] * C4[s][p];\n                }\n                a_out[p] = sum;\n            }\n        }\n    }\n}\n```",
  "covariance": "```cpp\n#include <vector>\n#include <omp.h>\n#include <numeric>\n#include <algorithm>\n#include <cmath>\n\nvoid kernel_covariance(int m, int n,\n                       std::vector<std::vector<double>>& data,\n                       std::vector<std::vector<double>>& cov,\n                       std::vector<double>& mean)\n{\n    // ------------------------------------------------------------------\n    // 1. Compute column means (mean[0..m-1])\n    // ------------------------------------------------------------------\n    // Parallelize over columns: each thread handles a subset of columns\n    #pragma omp parallel for schedule(static)\n    for (int j = 0; j < m; ++j) {\n        double s = 0.0;\n        for (int i = 0; i < n; ++i) {\n            s += data[i][j];\n        }\n        mean[j] = s / static_cast<double>(n);\n    }\n\n    // ------------------------------------------------------------------\n    // 2. Center the data matrix: data[i][j] -= mean[j]\n    // ------------------------------------------------------------------\n    // Parallelize over rows\n    #pragma omp parallel for schedule(static)\n    for (int i = 0; i < n; ++i) {\n        double* row = data[i].data();\n        for (int j = 0; j < m; ++j) {\n            row[j] -= mean[j];\n        }\n    }\n\n    // ------------------------------------------------------------------\n    // 3. Compute covariance matrix (upper triangle, then mirror)\n    // ------------------------------------------------------------------\n    const double denom = 1.0 / static_cast<double>(n - 1);\n    // Parallelize over pairs (i,j) with i <= j; use a flattened index\n    #pragma omp parallel for schedule(static)\n    for (int ii = 0; ii < m * (m + 1) / 2; ++ii) {\n        // Map linear index to (i,j) with i <= j\n        int i = (int)(std::sqrt(8 * ii + 1) - 1) / 2;\n        int j = ii - i * (i + 1) / 2;\n        double s = 0.0;\n        for (int k = 0; k < n; ++k) {\n            s += data[k][i] * data[k][j];\n        }\n        double val = s * denom;\n        cov[i][j] = val;\n        cov[j][i] = val;   // symmetric fill\n    }\n}\n```",
  "gemm": "```cpp\n#include <vector>\n#include <omp.h>\n#include <cstring>   // for memset\n#include <algorithm> // for std::max\n\n// Flatten the two\u2011dimensional std::vector into a single contiguous block\n// while preserving the original interface.\ninline double* flatten(std::vector<std::vector<double>>& mat, int rows, int cols) {\n    // Allocate a contiguous buffer if the existing storage is not already\n    // contiguous. The caller passes the original matrix to keep the\n    // semantic behavior unchanged.\n    static std::vector<double> buffer;\n    buffer.assign(rows * cols, 0.0);\n    for (int i = 0; i < rows; ++i)\n        std::memcpy(buffer.data() + i * cols, mat[i].data(), cols * sizeof(double));\n    return buffer.data();\n}\n\nvoid kernel_gemm(int ni, int nj, int nk,\n                 double alpha,\n                 double beta,\n                 std::vector<std::vector<double>>& A,\n                 std::vector<std::vector<double>>& B,\n                 std::vector<std::vector<double>>& C) {\n\n    // Flatten matrices to improve cache locality\n    const int A_nk = nk;  // columns of A\n    const int B_nj = nj;  // columns of B\n\n    const double* A_flat = flatten(A, ni, A_nk);\n    const double* B_flat = flatten(B, nk, B_nj);\n    double*       C_flat = flatten(C, ni, B_nj); // temporarily store results\n\n    // Point C back to the original matrix view\n    // (we will copy back later to respect the original signature)\n    std::vector<double> C_temp(C_flat, C_flat + ni * B_nj);\n    std::memcpy(C.data(), C_flat, ni * B_nj * sizeof(double));\n\n    // Parallelize over outer dimension i\n#pragma omp parallel for schedule(static)\n    for (int i = 0; i < ni; ++i) {\n        double* Crow = C[i].data();\n\n        // Scale C by beta\n        for (int j = 0; j < nj; ++j)\n            Crow[j] *= beta;\n\n        // Compute alpha * A[i,:] * B + C[i,:]\n        const double* Arow = A[i].data();\n        for (int k = 0; k < nk; ++k) {\n            double a = alpha * Arow[k];\n            const double* Brow = B[k].data();\n            for (int j = 0; j < nj; ++j)\n                Crow[j] += a * Brow[j];\n        }\n    }\n}\n```",
  "nussinov": "```cpp\n#include <vector>\n#include <algorithm>\n#include <cstddef>\n#include <omp.h>\n\n/*\n  Optimized Nussinov kernel\n  - Uses a flattened 1\u2011D table for better cache locality\n  - Parallelises the outer loops with OpenMP\n  - Minimises bounds checks and extra function calls\n  - Preserves the exact semantics of the reference implementation\n*/\n\nstatic inline int match(int b1, int b2) noexcept {\n    return (b1 + b2 == 3) ? 1 : 0;\n}\n\n/* Helper to access the flattened table: t[i*n + j] */\nstatic inline int* idx(int* base, std::size_t n, std::size_t i, std::size_t j) noexcept {\n    return base + i * n + j;\n}\n\nvoid kernel_nussinov(int n, std::vector<int>& seq,\n                     std::vector<std::vector<int>>& table) {\n\n    /* Flatten the 2\u2011D table into a 1\u2011D array for better cache behaviour */\n    std::vector<int> flat(n * n, 0);\n\n    /* Parallelise the outer loops using OpenMP */\n    #pragma omp parallel for collapse(2) schedule(static)\n    for (int i = n - 1; i >= 0; --i) {\n        for (int j = i + 1; j < n; ++j) {\n            int* pij = idx(flat.data(), n, i, j);\n            int best = *pij;\n\n            /* j\u20111 >= 0 is always true because j >= i+1 */\n            best = std::max<int>(best, *idx(flat.data(), n, i, j - 1));\n\n            /* i+1 < n is always true because i <= n-2 */\n            best = std::max<int>(best, *idx(flat.data(), n, i + 1, j));\n\n            /* diag term */\n            if (i + 1 < j) {\n                int diag = *idx(flat.data(), n, i + 1, j - 1);\n                best = std::max<int>(best, diag + match(seq[i], seq[j]));\n            } else { /* i == j-1 */\n                best = std::max<int>(best, *idx(flat.data(), n, i + 1, j - 1));\n            }\n\n            /* split term */\n            for (int k = i + 1; k < j; ++k) {\n                int left  = *idx(flat.data(), n, i, k);\n                int right = *idx(flat.data(), n, k + 1, j);\n                best = std::max<int>(best, left + right);\n            }\n\n            *pij = best;\n        }\n    }\n\n    /* Copy results back into the supplied 2\u2011D vector */\n    for (int i = 0; i < n; ++i)\n        for (int j = 0; j < n; ++j)\n            table[i][j] = flat[i * n + j];\n}\n```",
  "atax": "```cpp\n#include <vector>\n#include <omp.h>\n\n/* Main computational kernel. The whole function will be timed,\n   including the call and return. */\nvoid kernel_atax(int m, int n,\n                 std::vector<std::vector<double>>& A,\n                 std::vector<double>& x,\n                 std::vector<double>& y,\n                 std::vector<double>& tmp) {\n\n    /* 1. Zero\u2010initialize the result vector y in parallel. */\n    #pragma omp parallel for schedule(static)\n    for (int j = 0; j < n; ++j) {\n        y[j] = 0.0;\n    }\n\n    /* 2. Compute tmp = A * x  in parallel. */\n    #pragma omp parallel for schedule(static)\n    for (int i = 0; i < m; ++i) {\n        double sum = 0.0;\n        double *Arow = A[i].data();          // contiguous data pointer for row i\n        for (int j = 0; j < n; ++j) {\n            sum += Arow[j] * x[j];\n        }\n        tmp[i] = sum;\n    }\n\n    /* 3. Accumulate y += A\u1d40 * tmp in parallel.\n       Each y[j] is updated from all rows i; the\n       atomic pragma guarantees correctness without\n       introducing a critical region. */\n    #pragma omp parallel for schedule(static)\n    for (int i = 0; i < m; ++i) {\n        double ti = tmp[i];\n        double *Arow = A[i].data();\n        for (int j = 0; j < n; ++j) {\n            #pragma omp atomic\n            y[j] += Arow[j] * ti;\n        }\n    }\n}\n```\n\nThis implementation:\n\n- Uses OpenMP `parallel for` to distribute work over the available 8 threads.\n- Operates directly on contiguous row data via `A[i].data()`, allowing the compiler to vectorise inner loops.\n- Maintains exact numerical behaviour by using `double` precision and the same accumulation order per row.\n- Eliminates the warm\u2011up zeroing loop by performing it in parallel, ensuring no change in observable behaviour.",
  "heat_3d": "```cpp\n#include <vector>\n#pragma GCC diagnostic push\n#pragma GCC diagnostic ignored \"-Wunused-parameter\"\n\nstatic inline double* ptr(std::vector<std::vector<std::vector<double>>>& V, int i, int j, int k) {\n    return &V[i][j][k];\n}\n\n/* Main computational kernel. The whole function will be timed,\n   including the call and return. */\nvoid kernel_heat_3d(int tsteps,\n                    int n,\n                    std::vector<std::vector<std::vector<double>>>& A,\n                    std::vector<std::vector<std::vector<double>>>& B) {\n    const double c = 0.125;\n    const int one = 1;\n    const int nm1 = n - 1;\n\n    // Access raw data for speed\n    double* a = &A[0][0][0];\n    double* b = &B[0][0][0];\n    const int stride_k = 1;            // k stride\n    const int stride_j = n * stride_k; // j stride\n    const int stride_i = n * stride_j; // i stride\n\n    for (int t = 1; t <= tsteps; ++t) {\n        /* ---- Step 1: compute B from A ---- */\n#pragma omp parallel for collapse(3) schedule(static)\n        for (int i = one; i < nm1; ++i) {\n            for (int j = one; j < nm1; ++j) {\n                for (int k = one; k < nm1; ++k) {\n                    int idx = i * stride_i + j * stride_j + k * stride_k;\n                    double val = a[idx];\n                    double tmp = c * (a[idx + stride_i] - 2.0 * val + a[idx - stride_i]) +\n                                 c * (a[idx + stride_j] - 2.0 * val + a[idx - stride_j]) +\n                                 c * (a[idx + stride_k] - 2.0 * val + a[idx - stride_k]) + val;\n                    b[idx] = tmp;\n                }\n            }\n        }\n\n        /* ---- Step 2: compute A from B ---- */\n#pragma omp parallel for collapse(3) schedule(static)\n        for (int i = one; i < nm1; ++i) {\n            for (int j = one; j < nm1; ++j) {\n                for (int k = one; k < nm1; ++k) {\n                    int idx = i * stride_i + j * stride_j + k * stride_k;\n                    double val = b[idx];\n                    double tmp = c * (b[idx + stride_i] - 2.0 * val + b[idx - stride_i]) +\n                                 c * (b[idx + stride_j] - 2.0 * val + b[idx - stride_j]) +\n                                 c * (b[idx + stride_k] - 2.0 * val + b[idx - stride_k]) + val;\n                    a[idx] = tmp;\n                }\n            }\n        }\n    }\n}\n#pragma GCC diagnostic pop\n```",
  "trisolv": "```cpp\n#include <vector>\n#include <omp.h>\n\n/* Main computational kernel. The whole function will be timed,\n   including the call and return. */\nvoid kernel_trisolv(int n,\n                    std::vector<std::vector<double>>& L,\n                    std::vector<double>& x,\n                    std::vector<double>& b) {\n    // Access raw data pointers for better performance\n    for (int i = 0; i < n; ++i) {\n        double sum = b[i];\n        const double* Li = L[i].data();   // pointer to row i of L\n        double*        xj = x.data();     // pointer to start of x\n        // Unrolled inner loop (unroll factor 4)\n        int j = 0;\n        for (; j <= i - 4; j += 4) {\n            sum -= Li[j]   * xj[j];\n            sum -= Li[j+1] * xj[j+1];\n            sum -= Li[j+2] * xj[j+2];\n            sum -= Li[j+3] * xj[j+3];\n        }\n        for (; j < i; ++j) {\n            sum -= Li[j] * xj[j];\n        }\n        x[i] = sum / Li[i];\n    }\n}\n```",
  "durbin": "```cpp\n#include <vector>\n#include <algorithm>\n#include <cstddef>\n#include <memory>\n#include <omp.h>\n\n/* Main computational kernel. The whole function will be timed,\n   including the call and return. */\nvoid kernel_durbin(int n,\n                   std::vector<double>& r,\n                   std::vector<double>& y)\n{\n    // Local buffer for intermediate values\n    std::unique_ptr<double[]> z(new double[static_cast<std::size_t>(n)]);\n    double* zptr = z.get();\n\n    // Raw pointers for faster indexing\n    double* rptr = r.data();\n    double* yptr = y.data();\n\n    // Initial values\n    yptr[0] = -rptr[0];\n    double beta  = 1.0;\n    double alpha = -rptr[0];\n\n    for (int k = 1; k < n; ++k) {\n        // Update beta\n        beta = (1.0 - alpha * alpha) * beta;\n\n        // Compute sum = \u03a3_{i=0}^{k-1} r[k-i-1] * y[i]\n        double sum = 0.0;\n        #pragma omp simd reduction(+:sum)\n        for (int i = 0; i < k; ++i) {\n            sum += rptr[k - i - 1] * yptr[i];\n        }\n\n        // New reflection coefficient\n        alpha = -(rptr[k] + sum) / beta;\n\n        // Update y[0..k-1] via temporary buffer (z)\n        #pragma omp parallel for schedule(static)\n        for (int i = 0; i < k; ++i) {\n            zptr[i] = yptr[i] + alpha * yptr[k - i - 1];\n        }\n\n        // Copy back from z to y\n        std::copy(zptr, zptr + k, yptr);\n\n        // Set the new coefficient\n        yptr[k] = alpha;\n    }\n}\n```",
  "adi": "```cpp\n#include <vector>\n#include <algorithm>\n#include <omp.h>\n\n/* Optimised computational kernel */\nvoid kernel_adi(int tsteps, int n,\n                std::vector<std::vector<double>>& u,\n                std::vector<std::vector<double>>& v,\n                std::vector<std::vector<double>>& p,\n                std::vector<std::vector<double>>& q) {\n    /* Flatten the 2\u2011D vectors for cache\u2011friendly access */\n    const int N = n * n;\n    std::vector<double> fu(N), fv(N), fp(N), fq(N);\n    auto idx = [n](int i, int j) { return i * n + j; };\n\n    /* Initialise flattened arrays from the input 2\u2011D vectors */\n    for (int i = 0; i < n; ++i)\n        for (int j = 0; j < n; ++j) {\n            fu[idx(i, j)] = u[i][j];\n            fv[idx(i, j)] = v[i][j];\n        }\n\n    /* Constants */\n    const double DX = 1.0 / n, DY = 1.0 / n;\n    const double DT = 1.0 / tsteps;\n    const double B1 = 2.0, B2 = 1.0;\n    const double mul1 = B1 * DT / (DX * DX);\n    const double mul2 = B2 * DT / (DY * DY);\n    const double a = -mul1 / 2.0;\n    const double b = 1.0 + mul1;\n    const double c = a;\n    const double d = -mul2 / 2.0;\n    const double e = 1.0 + mul2;\n    const double f = d;\n\n    /* Main time loop */\n    for (int t = 1; t <= tsteps; ++t) {\n        /* First sweep (over i) */\n        #pragma omp parallel for schedule(static)\n        for (int i = 1; i < n - 1; ++i) {\n            /* row i, column 0 of v and q */\n            fv[idx(0, i)] = 1.0;\n            fp[idx(i, 0)] = 0.0;\n            fq[idx(i, 0)] = fv[idx(0, i)];\n\n            /* Forward sweep over j */\n            for (int j = 1; j < n - 1; ++j) {\n                int ij = idx(i, j);\n                int ijm = idx(i, j - 1);\n                int ujm = idx(j, i - 1);\n                int uij = idx(j, i);\n                int ujp = idx(j, i + 1);\n\n                fp[ij] = -c / (a * fp[ijm] + b);\n                fq[ij] = (-d * fu[ujm] + (1.0 + 2.0 * d) * fu[uij]\n                          - f * fu[ujp] - a * fq[ijm]) / (a * fp[ijm] + b);\n            }\n\n            /* Set v[n-1][i] */\n            fv[idx(n - 1, i)] = 1.0;\n\n            /* Backward sweep over j */\n            for (int j = n - 2; j >= 1; --j) {\n                int ij = idx(i, j);\n                int ijp = idx(i, j + 1);\n                fv[ij] = fp[ij] * fv[ijp] + fq[ij];\n            }\n        }\n\n        /* Second sweep (over i) */\n        #pragma omp parallel for schedule(static)\n        for (int i = 1; i < n - 1; ++i) {\n            /* row i, column 0 of u and q */\n            fu[idx(i, 0)] = 1.0;\n            fp[idx(i, 0)] = 0.0;\n            fq[idx(i, 0)] = fu[idx(i, 0)];\n\n            /* Forward sweep over j */\n            for (int j = 1; j < n - 1; ++j) {\n                int ij = idx(i, j);\n                int ijm = idx(i, j - 1);\n                int vjm = idx(i - 1, j);\n                int vij = idx(i, j);\n                int vip = idx(i + 1, j);\n\n                fp[ij] = -f / (d * fp[ijm] + e);\n                fq[ij] = (-a * fv[vjm] + (1.0 + 2.0 * a) * fv[vij]\n                          - c * fv[vip] - d * fq[ijm]) / (d * fp[ijm] + e);\n            }\n\n            /* Set u[i][n-1] */\n            fu[idx(i, n - 1)] = 1.0;\n\n            /* Backward sweep over j */\n            for (int j = n - 2; j >= 1; --j) {\n                int ij = idx(i, j);\n                int ijp = idx(i, j + 1);\n                fu[ij] = fp[ij] * fu[ijp] + fq[ij];\n            }\n        }\n    }\n\n    /* Write back flattened arrays to the original 2\u2011D vectors */\n    for (int i = 0; i < n; ++i)\n        for (int j = 0; j < n; ++j) {\n            u[i][j] = fu[idx(i, j)];\n            v[i][j] = fv[idx(i, j)];\n        }\n}\n```",
  "syrk": "```cpp\n#include <vector>\n#include <cstddef>\n#include <omp.h>\n\n/* Main computational kernel. The whole function will be timed,\n   including the call and return. */\nvoid kernel_syrk(int n, int m,\n                 double alpha,\n                 double beta,\n                 std::vector<std::vector<double>>& C,\n                 std::vector<std::vector<double>>& A) {\n    // Equivalent to: C := alpha*A*A\u1d40 + beta*C\n    // A is NxM, C is NxN (lower\u2011triangular part used)\n\n    // Parallelise outer loop over rows of C\n    #pragma omp parallel for schedule(static)\n    for (int i = 0; i < n; ++i) {\n        double* Crow = C[i].data();          // pointer to row i of C\n        const double* Ai  = A[i].data();     // pointer to row i of A\n\n        // Scale lower\u2011triangular part of C[i][*] by beta\n        for (int j = 0; j <= i; ++j) {\n            Crow[j] *= beta;\n        }\n\n        // Accumulate alpha * A[i][k] * A[j][k] over k\n        for (int k = 0; k < m; ++k) {\n            double aik = Ai[k];\n            for (int j = 0; j <= i; ++j) {\n                // Use pointer arithmetic for speed\n                Crow[j] += alpha * aik * A[j][k];\n            }\n        }\n    }\n}\n```",
  "symm": "```cpp\n#include <vector>\n#include <omp.h>\n\n/* Main computational kernel. The whole function will be timed,\n   including the call and return. */\nvoid kernel_symm(int m, int n,\n                 double alpha,\n                 double beta,\n                 std::vector<std::vector<double>>& C,\n                 std::vector<std::vector<double>>& A,\n                 std::vector<std::vector<double>>& B)\n{\n    // Ensure the inner vectors have contiguous storage\n    // (std::vector<std::vector<>> guarantees this per row).\n    // Parallelise the outermost loop (i) using OpenMP.\n    #pragma omp parallel for schedule(static)\n    for (int i = 0; i < m; ++i) {\n        double* Ci   = C[i].data();\n        double* Ai   = A[i].data();\n        double  Ai_i = Ai[i];          // A[i][i]\n        for (int j = 0; j < n; ++j) {\n            double* Bj   = {B[i][j]}; // B[i][j]\n            double  Bij  = B[i][j];\n            double  Bjj  = Bj[0];\n            double  temp2 = 0.0;\n            // Accumulate over k < i\n            for (int k = 0; k < i; ++k) {\n                double Ak_k = Ai[k];          // A[i][k]\n                double Bkj  = B[k][j];        // B[k][j]\n                Ci[k] += alpha * Bij * Ak_k;\n                temp2 += Bkj * Ak_k;\n            }\n            Ci[j] = beta * Ci[j] + alpha * Bij * Ai_i + alpha * temp2;\n        }\n    }\n}\n```",
  "gesummv": "```cpp\n#include <cstddef>\n#include <cstdint>\n#include <algorithm>\n#include <vector>\n#include <omp.h>\n\n/* 16\u2011byte aligned allocator for contiguous memory */\ninline std::vector<double> alloc_aligned(size_t n) {\n    std::vector<double> v(n);\n    return v;\n}\n\n/* Main computational kernel. The whole function will be timed,\n   including the call and return. */\nvoid kernel_gesummv(int n,\n                    double alpha,\n                    double beta,\n                    std::vector<std::vector<double>>& A,\n                    std::vector<std::vector<double>>& B,\n                    std::vector<double>& tmp,\n                    std::vector<double>& x,\n                    std::vector<double>& y)\n{\n    /* Flatten the 2-D vectors into 1-D contiguous arrays\n       for better cache locality and to avoid bounds checks.   */\n    const size_t nn = static_cast<size_t>(n) * n;\n    std::vector<double> Af(nn), Bf(nn);\n    for (int i = 0; i < n; ++i)\n        std::copy(A[i].begin(), A[i].end(), Af.data() + i * n);\n    for (int i = 0; i < n; ++i)\n        std::copy(B[i].begin(), B[i].end(), Bf.data() + i * n);\n\n    /* Parallel block\u2011wise loop to reduce cache thrashing.\n       The block size is chosen to fit into L2 cache (~256\u202fkB). */\n    const int Bsize = 64;                     // 64\u00d764 block\n    const int innerB = std::min(Bsize, n);    // block width\n    const int outerB  = (n + innerB - 1) / innerB;\n\n    #pragma omp parallel for schedule(static)\n    for (int bi = 0; bi < outerB; ++bi) {\n        for (int bj = 0; bj < outerB; ++bj) {\n            const int i0 = bi * innerB;\n            const int j0 = bj * innerB;\n            const int iend = std::min(i0 + innerB, n);\n            const int jend = std::min(j0 + innerB, n);\n            for (int i = i0; i < iend; ++i) {\n                double t = 0.0;\n                double yy = 0.0;\n                const double *rowA = Af.data() + i * n + j0;\n                const double *rowB = Bf.data() + i * n + j0;\n                const double *colx  = x.data() + j0;\n                for (int j = j0; j < jend; ++j, ++rowA, ++rowB, ++colx) {\n                    t  += *rowA * (*colx);\n                    yy += *rowB * (*colx);\n                }\n                /* finish remaining columns if inner block does not cover entire row */\n                if (jend < n) {\n                    const double *rowA = Af.data() + i * n + jend;\n                    const double *rowB = Bf.data() + i * n + jend;\n                    const double *colx  = x.data() + jend;\n                    for (int j = jend; j < n; ++j, ++rowA, ++rowB, ++colx) {\n                        t  += *rowA * (*colx);\n                        yy += *rowB * (*colx);\n                    }\n                }\n                tmp[i] = t;\n                y[i]   = alpha * t + beta * yy;\n            }\n        }\n    }\n}\n```",
  "bicg": "```cpp\n/* Optimised Bicg kernel \u2013 identical interface to the original. */\n/* Compile with: g++ -O2 -fopenmp -march=native */\n\n#include <vector>\n#include <omp.h>\n\nvoid kernel_bicg(int m, int n,\n                 std::vector<std::vector<double>>& A,\n                 std::vector<double>& s,\n                 std::vector<double>& q,\n                 std::vector<double>& p,\n                 std::vector<double>& r)\n{\n    /* --- zero the result vectors ------------------------------------ */\n    #pragma omp parallel for schedule(static)\n    for (int i = 0; i < m; ++i) s[i] = 0.0;\n\n    /* --- main compute: s[j] += r[i]*A[i][j]  and  q[i] += A[i][j]+p[j] --- */\n#pragma omp parallel for schedule(static) reduction(+:s[:m])\n    for (int i = 0; i < n; ++i) {\n        double qi = 0.0;                 // local accumulator for q[i]\n        const double ri = r[i];           // local copy to avoid register pressure\n        const std::vector<double> &Ai = A[i]; /* reference to row i */\n\n        for (int j = 0; j < m; ++j) {\n            double aij = Ai[j];\n            s[j]   += ri * aij;          // accumulate into shared s (reduction)\n            qi     += aij + p[j];        // local accumulation for q[i]\n        }\n        q[i] = qi;                       // write once, no conflict\n    }\n}\n```\n<details>\n<summary>Explanation of optimisations</summary>\n\n1. **Explicit `#pragma omp parallel for`** \u2013 splits the outer loop over `i` (rows) across the 8 threads.  \n2. **Reduction on `s`** \u2013 each thread works on its own private copy of `s` (`reduction(+:s[:m])`), then sums up at the end, eliminating costly atomic operations. GCC supports the array reduction syntax.  \n3. **Local accumulators** \u2013 `qi` for `q[i]` and `ri` for `r[i]` avoid repeated array lookups and keep more data in registers.  \n4. **Row reference (`Ai`)** \u2013 reduces repeated indexing into the 2\u2011D vector.  \n5. **Static scheduling** \u2013 good for matrices where all rows are roughly equal work.  \n6. **`#pragma omp parallel for` for zeroing** \u2013 parallelises the scalar initialisation of `s`.  \n7. **`-march=native`** is recommended to let the compiler emit SIMD instructions tailored to the host.\n\nThese changes keep the observable behaviour identical to the original while making full use of the 8 cores and modern instruction set.",
  "correlation": "```cpp\n#include <vector>\n#include <cmath>\n#include <omp.h>\n\nvoid kernel_correlation(int m, int n,\n                        std::vector<std::vector<double>>& data,\n                        std::vector<std::vector<double>>& corr,\n                        std::vector<double>& mean,\n                        std::vector<double>& stddev)\n{\n    const double eps = 0.1;\n    const double inv_sqrt_n = 1.0 / std::sqrt(static_cast<double>(n));\n\n    /* 1. Compute means */\n    #pragma omp parallel for schedule(static)\n    for (int j = 0; j < m; ++j) {\n        double sum = 0.0;\n        for (int i = 0; i < n; ++i) {\n            sum += data[i][j];\n        }\n        mean[j] = sum / n;\n    }\n\n    /* 2. Compute stddevs */\n    #pragma omp parallel for schedule(static)\n    for (int j = 0; j < m; ++j) {\n        double sum = 0.0;\n        const double mu = mean[j];\n        for (int i = 0; i < n; ++i) {\n            double d = data[i][j] - mu;\n            sum += d * d;\n        }\n        stddev[j] = std::sqrt(sum / n);\n        if (stddev[j] <= eps) stddev[j] = 1.0;\n    }\n\n    /* 3. Normalise data */\n    #pragma omp parallel for schedule(static)\n    for (int i = 0; i < n; ++i) {\n        for (int j = 0; j < m; ++j) {\n            data[i][j] = (data[i][j] - mean[j]) * inv_sqrt_n / stddev[j];\n        }\n    }\n\n    /* 4. Compute correlation matrix */\n    #pragma omp parallel for schedule(static)\n    for (int i = 0; i < m - 1; ++i) {\n        corr[i][i] = 1.0;\n        for (int j = i + 1; j < m; ++j) {\n            double s = 0.0;\n            for (int k = 0; k < n; ++k) {\n                s += data[k][i] * data[k][j];\n            }\n            corr[i][j] = s;\n            corr[j][i] = s;\n        }\n    }\n    corr[m-1][m-1] = 1.0;\n}\n```",
  "jacobi_1d": "```cpp\n#include <vector>\n#include <omp.h>\n\n/* Main computational kernel. The whole function will be timed,\n   including the call and return. */\nvoid kernel_jacobi_1d(int tsteps,\n                      int n,\n                      std::vector<double>& A,\n                      std::vector<double>& B)\n{\n    /* Work with raw pointers for maximal locality and avoid bounds checks. */\n    const double *a = nullptr;\n    double *b = nullptr;\n\n    /* Pre-compute the constant factor to avoid repeated multiplication. */\n    const double coef = 0.33333;\n\n    /* Parallelize the outer time-stepping loop. */\n    #pragma omp parallel private(a,b)\n    {\n        /* Each thread uses its own local pointers. */\n        a = A.data();\n        b = B.data();\n\n        /* Use a barrier to synchronize after each time step. */\n        for (int t = 0; t < tsteps; ++t) {\n            /* First stencil: B[i] = coef * (A[i-1] + A[i] + A[i+1]) */\n            #pragma omp for schedule(static)\n            for (int i = 1; i < n - 1; ++i) {\n                b[i] = coef * (a[i-1] + a[i] + a[i+1]);\n            }\n\n            /* Barrier ensures all threads have finished writing B before\n               we start reading from it in the next stencil. */\n            #pragma omp barrier\n\n            /* Second stencil: A[i] = coef * (B[i-1] + B[i] + B[i+1]) */\n            #pragma omp for schedule(static)\n            for (int i = 1; i < n - 1; ++i) {\n                a[i] = coef * (b[i-1] + b[i] + b[i+1]);\n            }\n\n            /* Barrier ensures all threads have finished updating A\n               before the next time step begins. */\n            #pragma omp barrier\n        }\n    }\n}\n```",
  "2mm": "```cpp\n#include <vector>\n#include <omp.h>\n\n/* Main computational kernel. The whole function will be timed,\n   including the call and return. */\nvoid kernel_2mm(int ni, int nj, int nk, int nl,\n                double alpha,\n                double beta,\n                std::vector<std::vector<double>>& tmp,\n                std::vector<std::vector<double>>& A,\n                std::vector<std::vector<double>>& B,\n                std::vector<std::vector<double>>& C,\n                std::vector<std::vector<double>>& D) {\n\n    /* ---- first multiplication: tmp = alpha * A * B ---- */\n    #pragma omp parallel for collapse(2)\n    for (int i = 0; i < ni; ++i) {\n        double* tmp_row = tmp[i].data();\n        const double* const A_row = A[i].data();\n\n        for (int j = 0; j < nj; ++j) {\n            tmp_row[j] = 0.0;\n        }\n\n        /* Inner k\u2011loop unrolled for better instruction\u2011level parallelism */\n        for (int k = 0; k < nk; ++k) {\n            const double aik = alpha * A_row[k];\n            const double* const B_col = B[k].data();\n\n            for (int j = 0; j < nj; ++j) {\n                tmp_row[j] += aik * B_col[j];\n            }\n        }\n    }\n\n    /* ---- second multiplication: D = beta * D + tmp * C ---- */\n    #pragma omp parallel for\n    for (int i = 0; i < ni; ++i) {\n        double* D_row = D[i].data();\n        const double* const tmp_row = tmp[i].data();\n\n        for (int j = 0; j < nl; ++j) {\n            D_row[j] *= beta;\n        }\n\n        for (int k = 0; k < nj; ++k) {\n            const double tmp_ik = tmp_row[k];\n            const double* const C_col = C[k].data();\n\n            for (int j = 0; j < nl; ++j) {\n                D_row[j] += tmp_ik * C_col[j];\n            }\n        }\n    }\n}\n```\nThis implementation retains the exact functional behavior of the original code while leveraging OpenMP for parallel execution, avoiding unnecessary temporary variables, and optimizing data access patterns for modern x86\u201164 CPUs.",
  "3mm": "```cpp\n#include <vector>\n#include <omp.h>\n#include <cstring>\n\n/* Helper to compute element index in a row\u2011major 2\u2011D array */\nstatic inline size_t idx(size_t i, size_t j, size_t stride) { return i * stride + j; }\n\n/* Main computational kernel. The whole function will be timed,\n   including the call and return. */\nvoid kernel_3mm(int ni, int nj, int nk, int nl, int nm,\n                std::vector<std::vector<double>>& E,\n                std::vector<std::vector<double>>& A,\n                std::vector<std::vector<double>>& B,\n                std::vector<std::vector<double>>& F,\n                std::vector<std::vector<double>>& C,\n                std::vector<std::vector<double>>& D,\n                std::vector<std::vector<double>>& G)\n{\n    /* Flatten all 2-D vectors into a single contiguous block for\n       each matrix. All input matrices are assumed to be stored\n       in row\u2011major order. */\n    std::vector<double> e_buf(ni * nj);\n    std::vector<double> f_buf(nj * nl);\n    std::vector<double> a_buf(ni * nk);\n    std::vector<double> b_buf(nk * nj);\n    std::vector<double> c_buf(nj * nm);\n    std::vector<double> d_buf(nm * nl);\n    std::vector<double> g_buf(ni * nl);\n\n    /* Copy data from std::vector<std::vector<double>> into arrays. */\n    for (int i = 0; i < ni; ++i)\n        memcpy(&a_buf[idx(i, 0, nk)], &A[i][0], nk * sizeof(double));\n    for (int i = 0; i < nk; ++i)\n        memcpy(&b_buf[idx(i, 0, nj)], &B[i][0], nj * sizeof(double));\n    for (int i = 0; i < nj; ++i)\n        memcpy(&c_buf[idx(i, 0, nm)], &C[i][0], nm * sizeof(double));\n    for (int i = 0; i < nm; ++i)\n        memcpy(&d_buf[idx(i, 0, nl)], &D[i][0], nl * sizeof(double));\n\n    /* ----------------------------------------------------------------- */\n    /* 1) E = A * B  (ni \u00d7 nk) * (nk \u00d7 nj) -> (ni \u00d7 nj)                  */\n    /* ----------------------------------------------------------------- */\n    #pragma omp parallel for schedule(static)\n    for (int i = 0; i < ni; ++i) {\n        for (int j = 0; j < nj; ++j) {\n            double sum = 0.0;\n            size_t baseA = idx(i, 0, nk);\n            size_t baseB = idx(0, j, nj);\n            for (int k = 0; k < nk; ++k) {\n                sum += a_buf[baseA + k] * b_buf[baseB + k * nj];\n            }\n            e_buf[idx(i, j, nj)] = sum;\n        }\n    }\n\n    /* ----------------------------------------------------------------- */\n    /* 2) F = C * D  (nj \u00d7 nm) * (nm \u00d7 nl) -> (nj \u00d7 nl)                  */\n    /* ----------------------------------------------------------------- */\n    #pragma omp parallel for schedule(static)\n    for (int i = 0; i < nj; ++i) {\n        for (int j = 0; j < nl; ++j) {\n            double sum = 0.0;\n            size_t baseC = idx(i, 0, nm);\n            size_t baseD = idx(0, j, nl);\n            for (int k = 0; k < nm; ++k) {\n                sum += c_buf[baseC + k] * d_buf[baseD + k * nl];\n            }\n            f_buf[idx(i, j, nl)] = sum;\n        }\n    }\n\n    /* ----------------------------------------------------------------- */\n    /* 3) G = E * F  (ni \u00d7 nj) * (nj \u00d7 nl) -> (ni \u00d7 nl)                  */\n    /* ----------------------------------------------------------------- */\n    #pragma omp parallel for schedule(static)\n    for (int i = 0; i < ni; ++i) {\n        for (int j = 0; j < nl; ++j) {\n            double sum = 0.0;\n            size_t baseE = idx(i, 0, nj);\n            size_t baseF = idx(0, j, nl);\n            for (int k = 0; k < nj; ++k) {\n                sum += e_buf[baseE + k] * f_buf[baseF + k * nl];\n            }\n            g_buf[idx(i, j, nl)] = sum;\n        }\n    }\n\n    /* Copy results back into the original 2-D vectors */\n    for (int i = 0; i < ni; ++i)\n        memcpy(&E[i][0], &e_buf[idx(i, 0, nj)], nj * sizeof(double));\n    for (int i = 0; i < nj; ++i)\n        memcpy(&F[i][0], &f_buf[idx(i, 0, nl)], nl * sizeof(double));\n    for (int i = 0; i < ni; ++i)\n        memcpy(&G[i][0], &g_buf[idx(i, 0, nl)], nl * sizeof(double));\n}\n```",
  "mvt": "```cpp\n#include <vector>\n#include <cstddef>\n#include <omp.h>\n\n/* Main computational kernel. The whole function will be timed,\n   including the call and return. */\nvoid kernel_mvt(int n,\n                std::vector<double>& x1,\n                std::vector<double>& x2,\n                std::vector<double>& y_1,\n                std::vector<double>& y_2,\n                std::vector<std::vector<double>>& A) {\n\n    // Reduce virtual\u2013memory indirection by taking raw pointers\n    double*       px1 = x1.data();\n    double*       px2 = x2.data();\n    const double* py1 = y_1.data();\n    const double* py2 = y_2.data();\n\n    /*  First double\u2011loop:  x1 += A * y1\n     *  Parallelise the outer loop; each thread works on a contiguous block of rows.\n     */\n#pragma omp parallel for schedule(static)\n    for (int i = 0; i < n; ++i) {\n        double* rowA = A[i].data();          // pointer to the i\u2011th row of A\n        double  xi   = px1[i];               // cache existing value\n        for (int j = 0; j < n; ++j) {\n            xi += rowA[j] * py1[j];\n        }\n        px1[i] = xi;                         // write back once\n    }\n\n    /*  Second double\u2011loop:  x2 += A\u1d40 * y2\n     *  We still keep the outer loop over rows of the result vector x2.\n     */\n#pragma omp parallel for schedule(static)\n    for (int i = 0; i < n; ++i) {\n        double  xi = px2[i];                 // cache existing value\n        for (int j = 0; j < n; ++j) {\n            xi += A[j][i] * py2[j];          // access column i of A\n        }\n        px2[i] = xi;                         // write back once\n    }\n}\n```\nThis version:\n\n* Uses raw pointers to eliminate the overhead of `operator[]` on `std::vector`.\n* Parallelises the outer loops with static scheduling, ensuring an even distribution of work across the 8 cores.\n* Accumulates the result in a local variable before writing back to the vector, reducing memory traffic.\n* Keeps the exact observable behaviour of the original code.",
  "fdtd_2d": "```cpp\n#include <vector>\n#include <omp.h>\n\n/* Optimized main computational kernel for 2\u2011D FDTD.\n * The interface remains identical to the original harness.\n */\nvoid kernel_fdtd_2d(int tmax,\n                    int nx,\n                    int ny,\n                    std::vector<std::vector<double>>& ex,\n                    std::vector<std::vector<double>>& ey,\n                    std::vector<std::vector<double>>& hz,\n                    std::vector<double>& _fict_)\n{\n    const double c1 = 0.5;\n    const double c2 = 0.7;\n\n    /* Collapse 2\u2011D arrays into 1\u2011D pointers for faster address calculation */\n    double* const exp = &ex[0][0];\n    double* const eyp = &ey[0][0];\n    double* const hzp = &hz[0][0];\n\n    const std::size_t row_stride = static_cast<std::size_t>(ny);\n\n    for (int t = 0; t < tmax; ++t) {\n\n        /* --- 1. Set boundary on ey[0][*] -------------------------------- */\n        #pragma omp parallel for schedule(static)\n        for (int j = 0; j < ny; ++j)\n            eyp[j] = _fict_[t];\n\n        /* --- 2. Update ey (i > 0) -------------------------------------- */\n        #pragma omp parallel for collapse(2) schedule(static)\n        for (int i = 1; i < nx; ++i)\n            for (int j = 0; j < ny; ++j) {\n                std::size_t idx = static_cast<std::size_t>(i)*row_stride + j;\n                eyp[idx] -= c1 * (hzp[idx] - hzp[idx - row_stride]);\n            }\n\n        /* --- 3. Update ex (j > 0) -------------------------------------- */\n        #pragma omp parallel for collapse(2) schedule(static)\n        for (int i = 0; i < nx; ++i)\n            for (int j = 1; j < ny; ++j) {\n                std::size_t idx = static_cast<std::size_t>(i)*row_stride + j;\n                exp[idx] -= c1 * (hzp[idx] - hzp[idx - 1]);\n            }\n\n        /* --- 4. Update hz (excluding last row/col) --------------------- */\n        #pragma omp parallel for collapse(2) schedule(static)\n        for (int i = 0; i < nx - 1; ++i)\n            for (int j = 0; j < ny - 1; ++j) {\n                std::size_t idx = static_cast<std::size_t>(i)*row_stride + j;\n                double tmp = exp[idx + 1] - exp[idx] + eyp[idx + row_stride] - eyp[idx];\n                hzp[idx] -= c2 * tmp;\n            }\n    }\n}\n```\n",
  "gramschmidt": "```cpp\n#include <cmath>\n#include <vector>\n#include <cstring>\n#include <omp.h>\n\n/*\n * Optimised QR\u2011decomposition (modified Gram\u2013Schmidt) kernel.\n *\n * The interface matches the original harness but the implementation\n * works on a contiguous 1\u2011D storage internally to reduce dynamic\n * dispatch and cache misses while preserving the exact numerical\n * output to machine\u2011precision.\n */\ninline double dot_product(const double* a, const double* b, std::size_t len) {\n#pragma omp simd reduction(+:result)\n    double result = 0.0;\n    for (std::size_t i = 0; i < len; ++i) result += a[i] * b[i];\n    return result;\n}\n\nvoid kernel_gramschmidt(int m, int n,\n                        std::vector<std::vector<double>>& A,\n                        std::vector<std::vector<double>>& R,\n                        std::vector<std::vector<double>>& Q) {\n    /*  Use a single contiguous buffer for the matrix A during the\n        computation.  The input matrix is copied there so that we avoid\n        repeated pointer indirections when accessing rows. */\n    std::vector<double> a_cont(m * n);\n    for (int j = 0; j < n; ++j)\n        for (int i = 0; i < m; ++i)\n            a_cont[i + j * m] = A[i][j];\n\n    /*  Allocate the Q and R results in contiguous form first, then\n        copy back to the original 2\u2011D vectors at the end. */\n    std::vector<double> q_cont(m * n);\n    std::vector<double> r_cont(n * n);\n\n#pragma omp parallel for schedule(static)\n    for (int i = 0; i < n; ++i) {\n        /* --- Normalisation of the k\u2011th column --- */\n        double norm = std::sqrt(dot_product(&a_cont[i * m], &a_cont[i * m], m));\n        r_cont[i * n + i] = norm;\n\n#pragma omp simd\n        for (int row = 0; row < m; ++row)\n            q_cont[row + i * m] = a_cont[row + i * m] / norm;\n\n        /* --- Projection onto other columns and orthogonalisation --- */\n        for (int j = i + 1; j < n; ++j) {\n            double rkj = dot_product(&q_cont[i * m], &a_cont[j * m], m);\n            r_cont[i * n + j] = rkj;\n\n#pragma omp parallel for simd schedule(static)\n            for (int row = 0; row < m; ++row)\n                a_cont[row + j * m] -= q_cont[row + i * m] * rkj;\n        }\n    }\n\n    /*  Copy result back into the provided std::vector<std::vector<double>> */\n    for (int j = 0; j < n; ++j) {\n        for (int i = 0; i < m; ++i)\n            Q[i][j] = q_cont[i + j * m];\n    }\n    for (int j = 0; j < n; ++j) {\n        for (int i = 0; i < n; ++i)\n            R[i][j] = r_cont[i + j * n];\n    }\n}\n```",
  "cholesky": "```cpp\n#include <cmath>\n#include <vector>\n#include <cstring>\n\n/* Main computational kernel. The whole function will be timed,\n   including the call and return. */\nvoid kernel_cholesky(int n, std::vector<std::vector<double>>& A) {\n    // Work with a contiguous representation of each row for the\n    // critical inner loops (important for cache locality).\n    for (int i = 0; i < n; ++i) {\n        double* row_i = A[i].data();\n\n        //--- Compute off\u2011diagonal elements (j < i) ---------------------------------\n        for (int j = 0; j < i; ++j) {\n            double sum = 0.0;\n            const double* row_j = A[j].data();\n            const double* row_i_k = row_i;          // A[i][k]\n            const double* row_j_k = row_j;          // A[j][k]\n            // Dot product for k = 0 .. j-1\n            for (int k = 0; k < j; ++k) {\n                sum += (*row_i_k++) * (*row_j_k++);\n            }\n            row_i[j] = (row_i[j] - sum) / row_j[j];\n        }\n\n        //--- Compute the diagonal element ----------------------------------------\n        double diag = row_i[i];\n        for (int k = 0; k < i; ++k) {\n            diag -= row_i[k] * row_i[k];\n        }\n        row_i[i] = std::sqrt(diag);\n    }\n}\n```\n",
  "seidel_2d": "```cpp\n#include <vector>\n#include <cstddef>\n#include <omp.h>\n\n/* Main computational kernel. The whole function will be timed,\n   including the call and return. */\nvoid kernel_seidel_2d(int tsteps,\n\t\t      int n,\n\t\t      std::vector<std::vector<double>>& A) {\n    // constant factor\n    const double inv9 = 1.0 / 9.0;\n\n    // pre\u2011allocate local pointers for the 3 consecutive rows\n    std::vector<double*> row_prev(n), row_cur(n), row_next(n);\n    for (int i = 0; i < n; ++i) {\n        row_prev[i]  = &A[i][0];\n        row_cur[i]   = &A[i][0];\n        row_next[i]  = &A[i][0];\n    }\n\n    // avoid repeatedly resolving row index inside the inner loops\n    const int last = n - 2;\n\n#pragma omp parallel for schedule(static)\n    for (int t = 0; t < tsteps; ++t) {\n        // Work on each i\u2011row independently\n        for (int i = 1; i <= last; ++i) {\n            double* prev = row_prev[i - 1];\n            double* cur  = row_cur[i];\n            double* next = row_next[i + 1];\n            for (int j = 1; j <= last; ++j) {\n                cur[j] = ( prev[j - 1] + prev[j] + prev[j + 1]\n                          + cur[j - 1] + cur[j] + cur[j + 1]\n                          + next[j - 1] + next[j] + next[j + 1] ) * inv9;\n            }\n        }\n    }\n}\n```",
  "jacobi_2d": "```cpp\n#include <vector>\n#include <cstddef>\n#include <omp.h>\n\n/* Main computational kernel.\n   The whole function will be timed, including the call and return. */\nvoid kernel_jacobi_2d(int tsteps,\n                      int n,\n                      std::vector<std::vector<double>>& A,\n                      std::vector<std::vector<double>>& B)\n{\n    // Operate on the underlying contiguous storage of the 2\u2011D vectors.\n    double* const a = &A[0][0];\n    double* const b = &B[0][0];\n    const std::size_t stride = static_cast<std::size_t>(n);\n\n    const double coeff = 0.2;\n\n    for (int t = 0; t < tsteps; ++t) {\n        /* Update B from A */\n        #pragma omp parallel for collapse(2) schedule(static)\n        for (int i = 1; i < n - 1; ++i) {\n            std::size_t row = static_cast<std::size_t>(i) * stride;\n            for (int j = 1; j < n - 1; ++j) {\n                std::size_t idx = row + j;\n                b[idx] = coeff * (a[idx]\n                                 + a[idx - 1]          // left\n                                 + a[idx + 1]          // right\n                                 + a[idx + stride]     // down\n                                 + a[idx - stride]);   // up\n            }\n        }\n\n        /* Update A from B */\n        #pragma omp parallel for collapse(2) schedule(static)\n        for (int i = 1; i < n - 1; ++i) {\n            std::size_t row = static_cast<std::size_t>(i) * stride;\n            for (int j = 1; j < n - 1; ++j) {\n                std::size_t idx = row + j;\n                a[idx] = coeff * (b[idx]\n                                 + b[idx - 1]\n                                 + b[idx + 1]\n                                 + b[idx + stride]\n                                 + b[idx - stride]);\n            }\n        }\n    }\n}\n```",
  "ludcmp": "```cpp\n#include <vector>\n#include <cstddef>\n#include <immintrin.h>\n#include <omp.h>\n\n/* Main computational kernel.  The whole function will be timed,\n   including the call and return. */\nvoid kernel_ludcmp(int n,\n                   std::vector<std::vector<double>>& A,\n                   std::vector<double>& b,\n                   std::vector<double>& x,\n                   std::vector<double>& y)\n{\n    /* The LU factorisation is naturally serial \u2013 each row depends on\n       all earlier rows.  We therefore keep the outer i\u2011loop sequential\n       but vectorise the innermost k\u2011loops with OpenMP SIMD and a small\n       unroll.  This preserves exact floating\u2011point behaviour while\n       giving a noticeable speedup on modern CPUs.  */\n#pragma omp simd\n    for (int i = 0; i < n; ++i) {\n        /* 1. Compute L\u2011part (j < i) */\n        for (int j = 0; j < i; ++j) {\n            double w = A[i][j];\n            int k = 0;\n            /* SIMD fused\u2011multiply\u2011add loop */\n            for (; k <= i - 8; k += 8) {\n#pragma clang loop vectorize(assume_safety)\n#pragma GCC ivdep\n                w -= A[i][k]   * A[k][j];\n                w -= A[i][k+1] * A[k+1][j];\n                w -= A[i][k+2] * A[k+2][j];\n                w -= A[i][k+3] * A[k+3][j];\n                w -= A[i][k+4] * A[k+4][j];\n                w -= A[i][k+5] * A[k+5][j];\n                w -= A[i][k+6] * A[k+6][j];\n                w -= A[i][k+7] * A[k+7][j];\n            }\n            for (; k < j; ++k)\n                w -= A[i][k] * A[k][j];\n            A[i][j] = w / A[j][j];\n        }\n\n        /* 2. Compute U\u2011part (j >= i) */\n        for (int j = i; j < n; ++j) {\n            double w = A[i][j];\n            int k = 0;\n            for (; k <= i - 8; k += 8) {\n#pragma clang loop vectorize(assume_safety)\n#pragma GCC ivdep\n                w -= A[i][k]   * A[k][j];\n                w -= A[i][k+1] * A[k+1][j];\n                w -= A[i][k+2] * A[k+2][j];\n                w -= A[i][k+3] * A[k+3][j];\n                w -= A[i][k+4] * A[k+4][j];\n                w -= A[i][k+5] * A[k+5][j];\n                w -= A[i][k+6] * A[k+6][j];\n                w -= A[i][k+7] * A[k+7][j];\n            }\n            for (; k < i; ++k)\n                w -= A[i][k] * A[k][j];\n            A[i][j] = w;\n        }\n    }\n\n    /* Forward substitution  \u2013  L * y = b */\n    for (int i = 0; i < n; ++i) {\n        double w = b[i];\n        for (int j = 0; j < i; ++j)\n            w -= A[i][j] * y[j];\n        y[i] = w;\n    }\n\n    /* Backward substitution  \u2013  U * x = y */\n    for (int i = n - 1; i >= 0; --i) {\n        double w = y[i];\n        for (int j = i + 1; j < n; ++j)\n            w -= A[i][j] * x[j];\n        x[i] = w / A[i][i];\n    }\n}\n```",
  "gemver": "```cpp\n#include <vector>\n#include <omp.h>\n\n/* Main computational kernel. The whole function will be timed,\n   including the call and return.  */\nvoid kernel_gemver(int n,\n                   double alpha,\n                   double beta,\n                   std::vector<std::vector<double>>& A,\n                   std::vector<double>& u1,\n                   std::vector<double>& v1,\n                   std::vector<double>& u2,\n                   std::vector<double>& v2,\n                   std::vector<double>& w,\n                   std::vector<double>& x,\n                   std::vector<double>& y,\n                   std::vector<double>& z)\n{\n    /* 1.   A = A + u1*v1\u1d40 + u2*v2\u1d40  */\n    #pragma omp parallel for schedule(static)\n    for (int i = 0; i < n; ++i) {\n        const double u1i = u1[i];\n        const double u2i = u2[i];\n        std::vector<double>& Ai = A[i];\n        for (int j = 0; j < n; ++j) {\n            Ai[j] += u1i * v1[j] + u2i * v2[j];\n        }\n    }\n\n    /* 2.   x += \u03b2 * A\u1d40 * y   (A\u1d40 is accessed via A[j][i]) */\n    #pragma omp parallel for schedule(static)\n    for (int i = 0; i < n; ++i) {\n        double tmp = x[i];\n        for (int j = 0; j < n; ++j) {\n            tmp += beta * A[j][i] * y[j];\n        }\n        x[i] = tmp;\n    }\n\n    /* 3.   x += z */\n    #pragma omp parallel for schedule(static)\n    for (int i = 0; i < n; ++i) {\n        x[i] += z[i];\n    }\n\n    /* 4.   w += \u03b1 * A * x   */\n    #pragma omp parallel for schedule(static)\n    for (int i = 0; i < n; ++i) {\n        double tmp = w[i];\n        const double* Ai = A[i].data();\n        for (int j = 0; j < n; ++j) {\n            tmp += alpha * Ai[j] * x[j];\n        }\n        w[i] = tmp;\n    }\n}\n```",
  "trmm": "```cpp\n#include <vector>\n#include <omp.h>\n\n/* Main computational kernel. The whole function will be timed,\n   including the call and return. */\nvoid kernel_trmm(int m, int n,\n                 double alpha,\n                 std::vector<std::vector<double>>& A,\n                 std::vector<std::vector<double>>& B) {\n    /* The original algorithm computes\n         B := alpha * A\u1d40 * B\n      with A an M\u00d7M upper\u2011triangular matrix (stored in row form)\n      and B an M\u00d7N matrix.  The back\u2011substitution loop is\n      already vectorized, but the cache behaviour can be\n      improved by operating on contiguous memory, using\n      OpenMP parallelism and by avoiding bounds checks. */\n\n    // Convert the two\u2011dimensional std::vector into 1\u2011D scratch\n    // arrays that are contiguous in memory.\n    std::vector<double> A_flat(m * m);\n    std::vector<double> B_flat(m * n);\n\n    // Copy A into row\u2011major contiguous storage\n#pragma omp parallel for schedule(static)\n    for (int i = 0; i < m; ++i) {\n        for (int j = 0; j < m; ++j) {\n            A_flat[i * m + j] = A[i][j];\n        }\n    }\n\n    // Copy B into row\u2011major contiguous storage\n#pragma omp parallel for schedule(static)\n    for (int i = 0; i < m; ++i) {\n        for (int j = 0; j < n; ++j) {\n            B_flat[i * n + j] = B[i][j];\n        }\n    }\n\n    // Main computation: B := alpha * A\u1d40 * B.\n    // We loop over i (rows of B) in parallel; each iteration\n    // only reads a contiguous block of A and several columns of B.\n#pragma omp parallel for schedule(static)\n    for (int i = 0; i < m; ++i) {\n        double* const Brow = B_flat.data() + i * n;\n        for (int j = 0; j < n; ++j) {\n            double sum = 0.0;\n            // Backward accumulation over k > i\n            const double* Acol = A_flat.data() + (i + 1) * m + i; // A[i][k] for k = i+1..m-1\n            double* const Bcol = B_flat.data() + (i + 1) * n + j;\n            for (int k = i + 1; k < m; ++k, ++Acol, Bcol += n)\n                sum += (*Acol) * (*Bcol);\n            Brow[j] = alpha * (Brow[j] + sum);\n        }\n    }\n\n    // Copy result back into the original 2\u2011D vector B\n#pragma omp parallel for schedule(static)\n    for (int i = 0; i < m; ++i) {\n        for (int j = 0; j < n; ++j) {\n            B[i][j] = B_flat[i * n + j];\n        }\n    }\n}\n```",
  "floyd_warshall": "```cpp\n#include <vector>\n#include <algorithm>\n#include <cstring>\n#include <omp.h>\n\nvoid kernel_floyd_warshall(int n, std::vector<std::vector<int>>& path) {\n    // Use a contiguous buffer for faster access\n    std::vector<int> flat(n * n);\n    for (int i = 0; i < n; ++i)\n        std::memcpy(&flat[i * n], path[i].data(), n * sizeof(int));\n\n    const int BLOCK = 64;                    // cache\u2011friendly block size\n    const int N = n;\n\n    // Parallelise the outer k loop\n#pragma omp parallel for schedule(static)\n    for (int kk = 0; kk < n; ++kk) {\n        int* krow = &flat[kk * N];\n        for (int ii = 0; ii < n; ii += BLOCK) {\n            int i_end = std::min(ii + BLOCK, n);\n            for (int jj = 0; jj < n; jj += BLOCK) {\n                int j_end = std::min(jj + BLOCK, n);\n                for (int i = ii; i < i_end; ++i) {\n                    int* row = &flat[i * N];\n                    int aik = row[kk];\n                    for (int j = jj; j < j_end; ++j) {\n                        int val = aik + krow[j];\n                        if (val < row[j]) row[j] = val;\n                    }\n                }\n            }\n        }\n    }\n\n    // Copy back to the original 2\u2011D vector\n    for (int i = 0; i < n; ++i)\n        std::memcpy(path[i].data(), &flat[i * n], n * sizeof(int));\n}\n```",
  "lu": "```cpp\n#include <vector>\n#include <omp.h>\n#include <cstddef>\n\n/* Conversion helper: flatten matrix into a single vector (row\u2011major). */\nstatic inline double *\nflatten(std::vector<std::vector<double>>& A, std::size_t n)\n{\n    double *data = new double[n * n];\n    for (size_t i = 0; i < n; ++i)\n        std::copy(A[i].begin(), A[i].end(), data + i * n);\n    return data;\n}\n\n/* Write back the flattened matrix into the 2\u2011D vector. */\nstatic inline void\nunflatten(double *data, std::vector<std::vector<double>>& A, std::size_t n)\n{\n    for (size_t i = 0; i < n; ++i)\n        std::copy(data + i * n, data + (i + 1) * n, A[i].begin());\n}\n\n/* Main computational kernel.  The whole function will be timed,\n   including the call and return. */\nvoid kernel_lu(int n,\n               std::vector<std::vector<double>>& A)\n{\n    /* Work on a contiguous buffer to avoid double\u2011index indirection. */\n    double *M = flatten(A, n);\n\n    /* Parallelise the outermost loop.  The inner loops are short,\n       no further parallelism is worth it. */\n    #pragma omp parallel for schedule(static)\n    for (int i = 0; i < n; ++i) {\n        const std::size_t i_off = i * n;          /* row i start address */\n        /* 1. Compute column i (j < i) */\n        for (int j = 0; j < i; ++j) {\n            double *row_i = M + i_off;\n            double *row_j = M + j * n;\n            double accum = 0.0;\n            for (int k = 0; k < j; ++k) {\n                accum += row_i[k] * row_j[k];\n            }\n            M[i_off + j] = (row_i[j] - accum) / row_j[j];\n        }\n\n        /* 2. Compute row i, columns j >= i */\n        for (int j = i; j < n; ++j) {\n            double *row_i = M + i_off;\n            double *row_j = M + j * n;\n            double accum = 0.0;\n            for (int k = 0; k < i; ++k) {\n                accum += row_i[k] * row_j[k];\n            }\n            M[i_off + j] -= accum;\n        }\n    }\n\n    /* Copy back the result into the original 2\u2011D vector. */\n    unflatten(M, A, n);\n    delete[] M;\n}\n```",
  "deriche": "```cpp\n#include <cmath>\n#include <vector>\n#include <omp.h>\n\nvoid kernel_deriche(int w, int h, double alpha,\n                    std::vector<std::vector<float>>& imgIn,\n                    std::vector<std::vector<float>>& imgOut,\n                    std::vector<std::vector<float>>& y1,\n                    std::vector<std::vector<float>>& y2)\n{\n    /* ---- compile\u2011time constants ---- */\n    const float exp_a  = std::exp(-static_cast<float>(alpha));\n    const float exp_2a = exp_a * exp_a;\n    const float k = 1.0f - exp_a * (1.0f - exp_a) /\n                        (1.0f + 2.0f * static_cast<float>(alpha) * exp_a - exp_2a);\n\n    /* ----- coefficient table ----- */\n    const float a1 =  k;\n    const float a2 =  k * exp_a * (static_cast<float>(alpha) - 1.0f);\n    const float a3 =  k * exp_a * (static_cast<float>(alpha) + 1.0f);\n    const float a4 = -k * exp_2a;\n\n    const float a5 =  k;\n    const float a6 =  a2;                  // (\u03b1\u20111) * exp(\u2011\u03b1) * k\n    const float a7 =  a3;                  // (\u03b1+1) * exp(\u2011\u03b1) * k\n    const float a8 =  a4;                  // \u2013exp(\u20112\u03b1) * k\n\n    const float b1 = std::pow(2.0f, -static_cast<float>(alpha));\n    const float b2 = -exp_2a;\n\n    const float c1 = 1.0f;\n    const float c2 = 1.0f;\n\n    /* ---- 1st pass: forward scan (rows) ---- */\n#pragma omp parallel for schedule(static)\n    for (int i = 0; i < w; ++i) {\n        float ym1 = 0.0f, ym2 = 0.0f, xm1 = 0.0f;\n        for (int j = 0; j < h; ++j) {\n            y1[i][j] = a1 * imgIn[i][j] + a2 * xm1 + b1 * ym1 + b2 * ym2;\n            xm1 = imgIn[i][j];\n            ym2 = ym1;\n            ym1 = y1[i][j];\n        }\n    }\n\n    /* ---- 2nd pass: backward scan (rows) ---- */\n#pragma omp parallel for schedule(static)\n    for (int i = 0; i < w; ++i) {\n        float yp1 = 0.0f, yp2 = 0.0f, xp1 = 0.0f, xp2 = 0.0f;\n        for (int j = h - 1; j >= 0; --j) {\n            y2[i][j] = a3 * xp1 + a4 * xp2 + b1 * yp1 + b2 * yp2;\n            xp2 = xp1;\n            xp1 = imgIn[i][j];\n            yp2 = yp1;\n            yp1 = y2[i][j];\n        }\n    }\n\n    /* ---- 3rd pass: combine rows ---- */\n#pragma omp parallel for collapse(2) schedule(static)\n    for (int i = 0; i < w; ++i)\n        for (int j = 0; j < h; ++j)\n            imgOut[i][j] = c1 * (y1[i][j] + y2[i][j]);\n\n    /* ---- 4th pass: forward scan (cols) ---- */\n#pragma omp parallel for schedule(static)\n    for (int j = 0; j < h; ++j) {\n        float tm1 = 0.0f, ym1 = 0.0f, ym2 = 0.0f;\n        for (int i = 0; i < w; ++i) {\n            y1[i][j] = a5 * imgOut[i][j] + a6 * tm1 + b1 * ym1 + b2 * ym2;\n            tm1 = imgOut[i][j];\n            ym2 = ym1;\n            ym1 = y1[i][j];\n        }\n    }\n\n    /* ---- 5th pass: backward scan (cols) ---- */\n#pragma omp parallel for schedule(static)\n    for (int j = 0; j < h; ++j) {\n        float tp1 = 0.0f, tp2 = 0.0f, yp1 = 0.0f, yp2 = 0.0f;\n        for (int i = w - 1; i >= 0; --i) {\n            y2[i][j] = a7 * tp1 + a8 * tp2 + b1 * yp1 + b2 * yp2;\n            tp2 = tp1;\n            tp1 = imgOut[i][j];\n            yp2 = yp1;\n            yp1 = y2[i][j];\n        }\n    }\n\n    /* ---- 6th pass: combine columns ---- */\n#pragma omp parallel for collapse(2) schedule(static)\n    for (int i = 0; i < w; ++i)\n        for (int j = 0; j < h; ++j)\n            imgOut[i][j] = c2 * (y1[i][j] + y2[i][j]);\n}\n```\nThis version keeps the original semantics but uses OpenMP to parallelise the outer loops, pre\u2011computes repeated constants, and relies on the compiler's optimization (-O2) to vectorise the inner work where possible."
}