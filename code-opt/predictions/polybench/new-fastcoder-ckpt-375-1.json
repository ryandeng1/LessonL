{
  "doitgen": "```cpp\n#include <vector>\n#include <omp.h>\n\n/* Main computational kernel. The whole function will be timed,\n   including the call and return. */\nvoid kernel_doitgen(int nr, int nq, int np,\n                    std::vector<std::vector<std::vector<double>>>& A,\n                    std::vector<std::vector<double>>& C4,\n                    std::vector<double>& sum) {\n\n    /* Local buffer for the inner product to reduce repeated allocation */\n    std::vector<double> local_sum(np);\n\n    /* Parallelise over the outer two loops; the innermost loops are\n       small enough that a single thread works best for the inner product\n       and copy steps. */\n    #pragma omp parallel for collapse(2) schedule(static)\n    for (int r = 0; r < nr; ++r) {\n        for (int q = 0; q < nq; ++q) {\n            // Compute A[r][q][p] = sum_s A[r][q][s] * C4[s][p]\n            for (int p = 0; p < np; ++p) {\n                double acc = 0.0;\n                const auto& a_vec = A[r][q];\n                const auto& c_col = C4[p];\n                for (int s = 0; s < np; ++s) {\n                    acc += a_vec[s] * c_col[s];\n                }\n                local_sum[p] = acc;\n            }\n            // Write back the result\n            auto& a_vec = A[r][q];\n            for (int p = 0; p < np; ++p) {\n                a_vec[p] = local_sum[p];\n            }\n        }\n    }\n}\n```",
  "gemm": "```cpp\n#include <vector>\n#include <cstddef>\n#include <omp.h>\n\n// NOTE:\n// This implementation keeps the same public interface as the original\n// kernel while re\u2011implementing the inner loops in a cache\u2011friendly and\n// easily parallelisable form.\n//\n// * The 2\u2011D std::vector containers are accessed through direct indexing;\n//   this keeps memory allocation semantics identical to the original\n//   code (the caller supplies the containers).\n// * All arithmetic is performed in double precision in the order that\n//   the original triple\u2011loop would produce, so the output is bit\u2011for\u2011bit\n//   identical (given identical compiler flags and FP precision).\n// * Parallelism is added at the outermost 'i' loop using OpenMP.\n//\n// The compiler optimisation flags and the target architecture (x86\u201164\n// with eight hardware threads) are assumed to be supplied by the build\n// system, so we only need the pragmas here.\nvoid kernel_gemm(int ni, int nj, int nk,\n                 double alpha, double beta,\n                 std::vector<std::vector<double>>& A,\n                 std::vector<std::vector<double>>& B,\n                 std::vector<std::vector<double>>& C) {\n\n    // Parallelise over the leading dimension 'i'.\n    #pragma omp parallel for schedule(static)\n    for (int i = 0; i < ni; ++i) {\n        // Scale the i\u2011th row of C by beta.\n        for (int j = 0; j < nj; ++j) {\n            C[i][j] *= beta;\n        }\n\n        // Main computation: C[i][j] += alpha * A[i][k] * B[k][j];\n        // The loop order follows the original implementation to keep\n        // the same numerical rounding order.\n        for (int k = 0; k < nk; ++k) {\n            double aik = alpha * A[i][k];\n            for (int j = 0; j < nj; ++j) {\n                C[i][j] += aik * B[k][j];\n            }\n        }\n    }\n}\n```",
  "syr2k": "```cpp\n#include <vector>\n#include <cstddef>\n#include <omp.h>\n\n/* Main computational kernel. The whole function will be timed,\n   including the call and return. */\nvoid kernel_syr2k(int n, int m,\n                  double alpha,\n                  double beta,\n                  std::vector<std::vector<double>>& C,\n                  std::vector<std::vector<double>>& A,\n                  std::vector<std::vector<double>>& B) {\n\n    // Pre\u2011compute products that are independent of loops\n    const double alphad = alpha;\n    const double betad  = beta;\n\n    // Parallelise the outer loop over `i`\n    #pragma omp parallel for schedule(static)\n    for (int i = 0; i < n; ++i) {\n        // Row pointers for cache efficiency\n        std::vector<double>& Ci = C[i];\n        const std::vector<double>& Bi = B[i];\n\n        // 1. Scale the lower\u2011triangular part by beta\n        for (int j = 0; j <= i; ++j) {\n            Ci[j] *= betad;\n        }\n\n        // 2. Accumulate the rank\u20112 updates\n        for (int k = 0; k < m; ++k) {\n            double aik = A[i][k];\n            double bkj = Bi[k];            // B[i][k]\n            for (int j = 0; j <= i; ++j) {\n                Ci[j] += (A[j][k] * bkj + B[j][k] * aik) * alphad;\n            }\n        }\n    }\n}\n```",
  "atax": "```cpp\n#include <vector>\n#include <cstddef>\n#include <omp.h>\n\n/* Main computational kernel. The whole function will be timed,\n   including the call and return. */\nvoid kernel_atax(int m, int n,\n                 std::vector<std::vector<double>>& A,\n                 std::vector<double>& x,\n                 std::vector<double>& y,\n                 std::vector<double>& tmp) {\n\n    /* Zero out y and tmp in parallel */\n#pragma omp parallel for schedule(static)\n    for (int i = 0; i < n; ++i)\n        y[i] = 0.0;\n\n#pragma omp parallel for schedule(static)\n    for (int i = 0; i < m; ++i)\n        tmp[i] = 0.0;\n\n    /* Main computation: for each row compute tmp[i] and update y */\n#pragma omp parallel for reduction(+:y[:n]) schedule(static)\n    for (int i = 0; i < m; ++i) {\n        double sum = 0.0;\n        const std::vector<double>& Ai = A[i];\n        for (int j = 0; j < n; ++j)\n            sum += Ai[j] * x[j];\n        tmp[i] = sum;\n        for (int j = 0; j < n; ++j)\n            y[j] += Ai[j] * sum;\n    }\n}\n```",
  "covariance": "```cpp\n#include <vector>\n#include <cstddef>\n#include <omp.h>\n\n//------------------------------------------------------------------------------\n// Compute means, center data, and covariance matrix.\n// Parallelized with OpenMP for a typical 8\u2011thread x86\u201164 environment.\n//\nvoid kernel_covariance(int m, int n,\n                       std::vector<std::vector<double>>& data,\n                       std::vector<std::vector<double>>& cov,\n                       std::vector<double>& mean)\n{\n    // ----------------------------------------------------------------------\n    // 1. Compute column means.\n    // ----------------------------------------------------------------------\n    #pragma omp parallel for schedule(static)\n    for (int j = 0; j < m; ++j) {\n        double sum = 0.0;\n        for (int i = 0; i < n; ++i) {\n            sum += data[i][j];\n        }\n        mean[j] = sum / static_cast<double>(n);\n    }\n\n    // ----------------------------------------------------------------------\n    // 2. Center data by subtracting means.\n    //    Operate row\u2011wise to keep working set small.\n    // ----------------------------------------------------------------------\n    #pragma omp parallel for schedule(static)\n    for (int i = 0; i < n; ++i) {\n        double* row = data[i].data();\n        for (int j = 0; j < m; ++j) {\n            row[j] -= mean[j];\n        }\n    }\n\n    // ----------------------------------------------------------------------\n    // 3. Compute covariance matrix.\n    //    Only upper triangle is computed; lower triangle filled by symmetry.\n    //    Parallelize over outer dimension to keep each thread working on a\n    //    disjoint set of rows of the covariance matrix.\n    // ----------------------------------------------------------------------\n    #pragma omp parallel for schedule(static)\n    for (int i = 0; i < m; ++i) {\n        // local pointer for row i of cov to avoid repeated bounds checks\n        std::vector<double>& covRow = cov[i];\n        for (int j = i; j < m; ++j) {\n            double sum = 0.0;\n            for (int k = 0; k < n; ++k) {\n                sum += data[k][i] * data[k][j];\n            }\n            double value = sum / static_cast<double>(n - 1);\n            covRow[j] = value;\n            cov[j][i] = value;   // symmetry\n        }\n    }\n}\n```",
  "heat_3d": "```cpp\n#include <vector>\n#include <omp.h>\n\n/* Main computational kernel. The whole function will be timed,\n   including the call and return. */\nvoid kernel_heat_3d(int tsteps,\n                    int n,\n                    std::vector<std::vector<std::vector<double>>>& A,\n                    std::vector<std::vector<std::vector<double>>>& B)\n{\n    const double coef = 0.125;\n    const int N = n - 2;                 // interior size\n\n    /* Parallelise all innermost loops.  The collapse(3) directive makes\n       the triple nested loops a single large iteration space, giving\n       good load\u2011balance across the 8 threads.  The nested loops are\n       independent across the t\u2011step, so the outer time loop remains\n       serial. */\n#pragma omp parallel for collapse(3) schedule(static)\n    for (int t = 1; t <= tsteps; ++t) {\n        // First phase: A -> B\n        for (int i = 1; i < n - 1; ++i) {\n            for (int j = 1; j < n - 1; ++j) {\n                for (int k = 1; k < n - 1; ++k) {\n                    double valA = A[i][j][k];\n                    double val =\n                        coef * (A[i+1][j][k] - 2.0 * valA + A[i-1][j][k]) +\n                        coef * (A[i][j+1][k] - 2.0 * valA + A[i][j-1][k]) +\n                        coef * (A[i][j][k+1] - 2.0 * valA + A[i][j][k-1]) +\n                        valA;\n                    B[i][j][k] = val;\n                }\n            }\n        }\n\n        // Second phase: B -> A\n        for (int i = 1; i < n - 1; ++i) {\n            for (int j = 1; j < n - 1; ++j) {\n                for (int k = 1; k < n - 1; ++k) {\n                    double valB = B[i][j][k];\n                    double val =\n                        coef * (B[i+1][j][k] - 2.0 * valB + B[i-1][j][k]) +\n                        coef * (B[i][j+1][k] - 2.0 * valB + B[i][j-1][k]) +\n                        coef * (B[i][j][k+1] - 2.0 * valB + B[i][j][k-1]) +\n                        valB;\n                    A[i][j][k] = val;\n                }\n            }\n        }\n    }\n}\n```",
  "trisolv": "```cpp\n#include <vector>\n#include <cstddef>\n#include <omp.h>\n\n/* Main computational kernel. The whole function will be timed,\n   including the call and return. */\nvoid kernel_trisolv(int n,\n                    std::vector<std::vector<double>>& L,\n                    std::vector<double>& x,\n                    std::vector<double>& b) {\n    // The triangular solve is inherently serial, but the inner loop\n    // contains a simple dot product that benefits from vectorisation.\n    for (int i = 0; i < n; ++i) {\n        double sum = b[i];\n        const double* Li = L[i].data();\n        // Vectorise the dot product with the previous x values.\n        #pragma omp simd reduction(-:sum)\n        for (int j = 0; j < i; ++j) {\n            sum -= Li[j] * x[j];\n        }\n        x[i] = sum / Li[i];\n    }\n}\n```",
  "nussinov": "```cpp\n#include <vector>\n#include <algorithm>\n#include <omp.h>\n\n/* Main computational kernel.  The whole function will be timed,\n   including the call and return. */\nvoid kernel_nussinov(int n, std::vector<int>& seq, std::vector<std::vector<int>>& table)\n{\n    /* Match function: returns 1 if bases are complementary, else 0. */\n    auto match = [](int b1, int b2) -> int {\n        return (b1 + b2) == 3 ? 1 : 0;\n    };\n\n    /* Flatten the 2\u2011D table into a 1\u2011D array for faster memory access. */\n    auto idx = [n](int i, int j) noexcept -> int { return i * n + j; };\n\n    /* Convert std::vector<std::vector<int>> to a raw int pointer. */\n    int* tbl = new int[n * n];\n    for (int i = 0; i < n; ++i)\n        for (int j = 0; j < n; ++j)\n            tbl[i * n + j] = table[i][j];\n\n    /* Main DP loops.  The outer i loop runs backwards because each\n       cell depends on values from smaller i indices.  The inner j loop\n       also runs forward.  All accesses are done through the flattened\n       array for better cache locality. */\n    for (int i = n - 1; i >= 0; --i) {\n        int base_i = i * n;                      // row start\n        int base_ip1 = (i + 1) * n;              // next row start\n        for (int j = i + 1; j < n; ++j) {\n            int cur = tbl[base_i + j];\n\n            /* table[i][j-1] --> same row, previous column */\n            if (j - 1 >= 0)\n                cur = std::max(cur, tbl[base_i + j - 1]);\n\n            /* table[i+1][j] --> next row, same column */\n            if (i + 1 < n)\n                cur = std::max(cur, tbl[base_ip1 + j]);\n\n            /* table[i+1][j-1] + match(seq[i],seq[j])   (or without match) */\n            if (j - 1 >= 0 && i + 1 < n) {\n                int t = tbl[base_ip1 + j - 1];\n                if (i < j - 1) /* base pairing allowed */\n                    t += match(seq[i], seq[j]);\n                cur = std::max(cur, t);\n            }\n\n            /* f(i,k) + f(k+1,j) for k in (i, j) */\n            for (int k = i + 1; k < j; ++k) {\n                int left = tbl[base_i + k];\n                int right = tbl[(k + 1) * n + j];\n                cur = std::max(cur, left + right);\n            }\n\n            tbl[base_i + j] = cur;\n        }\n    }\n\n    /* Copy results back into the original table structure. */\n    for (int i = 0; i < n; ++i)\n        for (int j = 0; j < n; ++j)\n            table[i][j] = tbl[i * n + j];\n\n    delete[] tbl;\n}\n```\n",
  "gesummv": "```cpp\n#include <vector>\n#include <cstddef>\n#include <omp.h>\n\n/* Main computational kernel. The whole function will be timed,\n   including the call and return. */\nvoid kernel_gesummv(int n,\n                    double alpha,\n                    double beta,\n                    std::vector<std::vector<double>>& A,\n                    std::vector<std::vector<double>>& B,\n                    std::vector<double>& tmp,\n                    std::vector<double>& x,\n                    std::vector<double>& y)\n{\n    /* Parallelise over rows.  Each worker works on a contiguous slice\n       of rows so that there is no false\u2011sharing on the temporaries. */\n#pragma omp parallel for schedule(static)\n    for (int i = 0; i < n; ++i)\n    {\n        double tmp_acc = 0.0;\n        double y_acc   = 0.0;\n        const double* Arow = A[i].data();\n        const double* Brow = B[i].data();\n        const double* X    = x.data();\n\n        /* Unroll the inner loop by a factor of 4 for better ILP. */\n        int j = 0;\n        const int UNROLL = 4;\n        for (; j <= n - UNROLL; j += UNROLL)\n        {\n            tmp_acc += Arow[j] * X[j];\n            y_acc   += Brow[j] * X[j];\n            tmp_acc += Arow[j+1] * X[j+1];\n            y_acc   += Brow[j+1] * X[j+1];\n            tmp_acc += Arow[j+2] * X[j+2];\n            y_acc   += Brow[j+2] * X[j+2];\n            tmp_acc += Arow[j+3] * X[j+3];\n            y_acc   += Brow[j+3] * X[j+3];\n        }\n        /* Remaining iterations */\n        for (; j < n; ++j)\n        {\n            tmp_acc += Arow[j] * X[j];\n            y_acc   += Brow[j] * X[j];\n        }\n\n        tmp[i] = tmp_acc;\n        y[i]   = alpha * tmp_acc + beta * y_acc;\n    }\n}\n```",
  "bicg": "```cpp\n#include <vector>\n#include <omp.h>\n\n/* Main computational kernel. The whole function will be timed,\n   including the call and return. */\nvoid kernel_bicg(int m, int n,\n                 std::vector<std::vector<double>>& A,\n                 std::vector<double>& s,\n                 std::vector<double>& q,\n                 std::vector<double>& p,\n                 std::vector<double>& r) {\n    /* Initialise s to 0 (size m) */\n    #pragma omp parallel for schedule(static)\n    for (int i = 0; i < m; ++i) {\n        s[i] = 0.0;\n    }\n\n    /* Initialise q to 0 (size n) and perform the two summations.\n       The outer loop over i is parallelised; each thread works on a\n       distinct set of rows.  The updates to s[j] are performed with a\n       reduction over the rows belonging to that thread. */\n    #pragma omp parallel for schedule(static)\n    for (int i = 0; i < n; ++i) {\n        double sum_q = 0.0;      // per-thread local accumulator for q[i]\n        for (int j = 0; j < m; ++j) {\n            /* Update s[j] \u2013 this is safely done in parallel because\n               each thread only writes to the rows for which\n               i is in its schedule. */\n            #pragma omp atomic\n            s[j] += r[i] * A[i][j];\n\n            /* Accumulate q[i] locally and add the column value. */\n            sum_q += A[i][j] + p[j];\n        }\n        q[i] = sum_q;\n    }\n}\n```",
  "adi": "```cpp\n#include <vector>\n#include <omp.h>\n\n/* Main computational kernel. The whole function will be timed,\n   including the call and return.  The implementation below\n   preserves the exact behaviour of the original code but\n   achieves much higher performance on a modern x86\u201164 processor\n   by:\n   1. Using contiguous access patterns (work\u2011only on inner loops).\n   2. Parallelising the outer loops with OpenMP.\n   3. Avoiding unnecessary multiplications/divisions inside tight loops.\n   4. Using strict\u2011aliasing casts to allow the compiler to optimise\n      better without changing the observed semantics. */\n\nvoid kernel_adi(int tsteps, int n,\n                std::vector<std::vector<double>>& u,\n                std::vector<std::vector<double>>& v,\n                std::vector<std::vector<double>>& p,\n                std::vector<std::vector<double>>& q)\n{\n    const double DX = 1.0 / static_cast<double>(n);\n    const double DY = 1.0 / static_cast<double>(n);\n    const double DT = 1.0 / static_cast<double>(tsteps);\n\n    const double B1 = 2.0, B2 = 1.0;\n    const double mul1 = B1 * DT / (DX * DX);\n    const double mul2 = B2 * DT / (DY * DY);\n\n    const double a = -mul1 / 2.0;\n    const double b = 1.0 + mul1;\n    const double c = a;\n    const double d = -mul2 / 2.0;\n    const double e = 1.0 + mul2;\n    const double f = d;\n\n    /* Pre\u2011extract the underlying contiguous data to reduce the\n       overhead of double indexing inside the critical loops.  The\n       references are kept valid because the vectors are not resized\n       afterwards.  The casts to raw pointers enable the compiler\n       to use vectorised loads/stores when possible. */\n    double* restrict const u_data = &u[0][0];\n    double* restrict const v_data = &v[0][0];\n    double* restrict const p_data = &p[0][0];\n    double* restrict const q_data = &q[0][0];\n\n    const size_t stride = n;            // Row stride in the contiguous layout\n\n    for (int t = 1; t <= tsteps; ++t) {\n        /* First sweep \u2013 update v and compute p,q for columns */\n        #pragma omp parallel for schedule(static)\n        for (int i = 1; i < n - 1; ++i) {\n            double* v_col = v_data + i * stride;          // v[*,i]\n            double* p_col = p_data + i * stride;          // p[*,i]\n            double* q_col = q_data + i * stride;          // q[*,i]\n\n            v_col[0] = 1.0;\n            p_col[0] = 0.0;\n            q_col[0] = v_col[0];\n\n            /* Forward sweep over j (row index) */\n            for (int j = 1; j < n - 1; ++j) {\n                const double denom = a * p_col[j-1] + b;\n                p_col[j] = -c / denom;\n                q_col[j] = (-d * u_data[(j)*stride + i-1]\n                             + (1.0 + 2.0 * d) * u_data[(j)*stride + i]\n                             - f * u_data[(j)*stride + i+1]\n                             - a * q_col[j-1]) / denom;\n            }\n\n            v_col[n-1] = 1.0;\n\n            /* Backward sweep */\n            for (int j = n-2; j >= 1; --j) {\n                v_col[j] = p_col[j] * v_col[j+1] + q_col[j];\n            }\n        }\n\n        /* Second sweep \u2013 update u and compute p,q for rows */\n        #pragma omp parallel for schedule(static)\n        for (int i = 1; i < n - 1; ++i) {\n            double* u_row = u_data + i * stride;          // u[i,*]\n            double* p_row = p_data + i * stride;          // p[i,*]\n            double* q_row = q_data + i * stride;          // q[i,*]\n\n            u_row[0] = 1.0;\n            p_row[0] = 0.0;\n            q_row[0] = u_row[0];\n\n            for (int j = 1; j < n - 1; ++j) {\n                const double denom = d * p_row[j-1] + e;\n                p_row[j] = -f / denom;\n                q_row[j] = (-a * v_data[(i-1)*stride + j]\n                             + (1.0 + 2.0 * a) * v_data[i*stride + j]\n                             - c * v_data[(i+1)*stride + j]\n                             - d * q_row[j-1]) / denom;\n            }\n\n            u_row[n-1] = 1.0;\n\n            for (int j = n-2; j >= 1; --j) {\n                u_row[j] = p_row[j] * u_row[j+1] + q_row[j];\n            }\n        }\n    }\n}\n```",
  "durbin": "```cpp\n#include <vector>\n#include <algorithm>\n#include <cstring>\n\n/* Main computational kernel. The whole function will be timed,\n   including the call and return. */\nvoid kernel_durbin(int n,\n                   std::vector<double>& r,\n                   std::vector<double>& y) {\n    // Allocate a temporary buffer once; will hold intermediate y values.\n    std::vector<double> z(n);\n    y[0]  = -r[0];\n    double beta  = 1.0;\n    double alpha = -r[0];\n\n    // Use raw pointers for slightly faster access in the inner loops.\n    double * const rptr = r.data();\n    double * const yptr = y.data();\n    double * const zptr = z.data();\n\n    for (int k = 1; k < n; ++k) {\n        beta = (1.0 - alpha * alpha) * beta;\n\n        // Compute the dot product sum = \u03a3 r[k-1-i]*y[i]\n        double sum = 0.0;\n        int i = 0;\n#pragma clang loop vectorize(enable)\n        for (; i + 3 < k; i += 4) {            // 4-way loop unroll\n            sum += rptr[k - 1 - i]   * yptr[i];\n            sum += rptr[k - 2 - i]   * yptr[i + 1];\n            sum += rptr[k - 3 - i]   * yptr[i + 2];\n            sum += rptr[k - 4 - i]   * yptr[i + 3];\n        }\n        for (; i < k; ++i) {\n            sum += rptr[k - 1 - i] * yptr[i];\n        }\n\n        alpha = -(rptr[k] + sum) / beta;\n\n        // Update y[0..k-1] via z buffer.\n        i = 0;\n#pragma clang loop vectorize(enable)\n        for (; i + 3 < k; i += 4) {\n            zptr[i]     = yptr[i]     + alpha * yptr[k - 1 - i];\n            zptr[i + 1] = yptr[i + 1] + alpha * yptr[k - 2 - i];\n            zptr[i + 2] = yptr[i + 2] + alpha * yptr[k - 3 - i];\n            zptr[i + 3] = yptr[i + 3] + alpha * yptr[k - 4 - i];\n        }\n        for (; i < k; ++i) {\n            zptr[i] = yptr[i] + alpha * yptr[k - 1 - i];\n        }\n\n        // Copy back to y.\n        std::memcpy(yptr, zptr, k * sizeof(double));\n\n        yptr[k] = alpha;\n    }\n}\n```",
  "syrk": "```cpp\n#include <vector>\n#include <omp.h>\n\n/* Main computational kernel. The whole function will be timed,\n   including the call and return. */\nvoid kernel_syrk(int n, int m,\n                 double alpha,\n                 double beta,\n                 std::vector<std::vector<double>>& C,\n                 std::vector<std::vector<double>>& A) {\n\n    // Parallelise the outermost loop over i. Each iteration updates a\n    // distinct row of C, so it is embarrassingly parallel.\n    #pragma omp parallel for schedule(static)\n    for (int i = 0; i < n; ++i) {\n        // Access the row of C for fast pointer arithmetic.\n        double* Ci = C[i].data();           // C[i][0..n-1]\n        const double* Ai = A[i].data();     // A[i][0..m-1]\n        for (int j = 0; j <= i; ++j) {\n            double val = Ci[j] * beta;      // C[i][j] *= beta\n            const double* Aj = A[j].data(); // A[j][0..m-1]\n\n            // Accumulate alpha * A[i][k] * A[j][k] over k.\n            double* p = &val;\n            for (int k = 0; k < m; ++k) {\n                *p += alpha * Ai[k] * Aj[k];\n            }\n            Ci[j] = val;\n        }\n    }\n}\n```",
  "3mm": "```cpp\n#include <vector>\n#include <omp.h>\n\n/* Main computational kernel. The whole function will be timed,\n   including the call and return. */\nvoid kernel_3mm(int ni, int nj, int nk, int nl, int nm,\n                std::vector<std::vector<double>>& E,\n                std::vector<std::vector<double>>& A,\n                std::vector<std::vector<double>>& B,\n                std::vector<std::vector<double>>& F,\n                std::vector<std::vector<double>>& C,\n                std::vector<std::vector<double>>& D,\n                std::vector<std::vector<double>>& G)\n{\n    /* 1) Compute E = A * B  (ni x nj) */\n    #pragma omp parallel for collapse(2) schedule(static)\n    for (int i = 0; i < ni; ++i) {\n        for (int j = 0; j < nj; ++j) {\n            double sum = 0.0;\n            double *Ap = A[i].data();\n            double *Bp = B[0].data();  // B is column\u2011major relative to indexing\n            for (int k = 0; k < nk; ++k) {\n                sum += Ap[k] * Bp[k * nj + j];\n            }\n            E[i][j] = sum;\n        }\n    }\n\n    /* 2) Compute F = C * D  (nj x nl) */\n    #pragma omp parallel for collapse(2) schedule(static)\n    for (int i = 0; i < nj; ++i) {\n        for (int j = 0; j < nl; ++j) {\n            double sum = 0.0;\n            double *Cp = C[i].data();\n            double *Dp = D[0].data();  // D is column\u2011major relative to indexing\n            for (int k = 0; k < nm; ++k) {\n                sum += Cp[k] * Dp[k * nl + j];\n            }\n            F[i][j] = sum;\n        }\n    }\n\n    /* 3) Compute G = E * F  (ni x nl) */\n    #pragma omp parallel for collapse(2) schedule(static)\n    for (int i = 0; i < ni; ++i) {\n        for (int j = 0; j < nl; ++j) {\n            double sum = 0.0;\n            double *Ep = E[i].data();\n            double *Fp = F[0].data();  // F is column\u2011major relative to indexing\n            for (int k = 0; k < nj; ++k) {\n                sum += Ep[k] * Fp[k * nl + j];\n            }\n            G[i][j] = sum;\n        }\n    }\n}\n```",
  "2mm": "```cpp\n#include <vector>\n#include <cstddef>\n#include <omp.h>\n\n/* Main computational kernel. The whole function will be timed,\n   including the call and return. */\nvoid kernel_2mm(int ni, int nj, int nk, int nl,\n\t\tdouble alpha,\n\t\tdouble beta,\n\t\tstd::vector<std::vector<double>>& tmp,\n\t\tstd::vector<std::vector<double>>& A,\n\t\tstd::vector<std::vector<double>>& B,\n\t\tstd::vector<std::vector<double>>& C,\n\t\tstd::vector<std::vector<double>>& D)\n{\n    /* ----------------- 1st phase : tmp = alpha*A*B ----------------- */\n    /* We parallelise outermost loops. The inner loops are short enough\n       that the overhead of OpenMP threading is negligible. */\n#pragma omp parallel for collapse(2) schedule(static)\n    for (int i = 0; i < ni; ++i) {\n        const double* Ai = A[i].data();\n        double* tmp_i = tmp[i].data();\n        for (int j = 0; j < nj; ++j) {\n            double sum = 0.0;\n            for (int k = 0; k < nk; ++k)\n                sum += alpha * Ai[k] * B[k][j];\n            tmp_i[j] = sum;\n        }\n    }\n\n    /* ----------------- 2nd phase : D = beta*D + tmp*C -------------- */\n#pragma omp parallel for collapse(2) schedule(static)\n    for (int i = 0; i < ni; ++i) {\n        double* Di = D[i].data();\n        const double* tmp_i = tmp[i].data();\n        for (int j = 0; j < nl; ++j) {\n            double prod = Di[j] * beta;\n            for (int k = 0; k < nj; ++k)\n                prod += tmp_i[k] * C[k][j];\n            Di[j] = prod;\n        }\n    }\n}\n```",
  "mvt": "```cpp\n#include <vector>\n#include <omp.h>\n\n/* Main computational kernel. The whole function will be timed,\n   including the call and return. */\nvoid kernel_mvt(int n,\n                std::vector<double>& x1,\n                std::vector<double>& x2,\n                std::vector<double>& y_1,\n                std::vector<double>& y_2,\n                std::vector<std::vector<double>>& A) {\n    /* First part \u2013 row\u2011wise multiplication:   x1[i] += A[i][j] * y_1[j] */\n    #pragma omp parallel for schedule(static)\n    for (int i = 0; i < n; ++i) {\n        double sum = 0.0;\n        const std::vector<double>& Ai = A[i];\n        for (int j = 0; j < n; ++j) {\n            sum += Ai[j] * y_1[j];\n        }\n        x1[i] += sum;\n    }\n\n    /* Second part \u2013 column\u2011wise multiplication: x2[i] += A[j][i] * y_2[j] */\n    #pragma omp parallel for schedule(static)\n    for (int i = 0; i < n; ++i) {\n        double sum = 0.0;\n        for (int j = 0; j < n; ++j) {\n            sum += A[j][i] * y_2[j];\n        }\n        x2[i] += sum;\n    }\n}\n```\nThe implementation keeps the original interface, uses OpenMP to parallelise the outer loops, and reduces indirection by caching row pointers. The inner loops are kept simple to maximize vectorisation by the compiler.",
  "symm": "```cpp\n#include <vector>\n#include <omp.h>\n\n/* Main computational kernel. The whole function will be timed,\n   including the call and return. */\nvoid kernel_symm(int m, int n,\n                 double alpha,\n                 double beta,\n                 std::vector<std::vector<double>>& C,\n                 std::vector<std::vector<double>>& A,\n                 std::vector<std::vector<double>>& B) {\n\n    // Parallelise the outermost loop; each iteration works on a distinct row of A.\n    #pragma omp parallel for schedule(static)\n    for (int i = 0; i < m; ++i) {\n        // Pointers to the current row of C, A, and B for fast access.\n        double* C_i = C[i].data();\n        const double* A_i = A[i].data();\n        const double ai_i = A_i[i];            // A[i][i] - reused for all j\n        const double* B_i = B[i].data();\n\n        // Iterate over columns of B/C\n        for (int j = 0; j < n; ++j) {\n            const double bij = B_i[j];\n            double temp2 = 0.0;\n\n            // Inner loop over k < i\n            for (int k = 0; k < i; ++k) {\n                const double aik = A_i[k];\n                double* C_kj = &C[k][j];\n                const double bkij = B[k][j];\n\n                *C_kj += alpha * bij * aik;   // Update C[k][j]\n                temp2 += bkij * aik;          // Accumulate for final C[i][j]\n            }\n\n            C_i[j] = beta * C_i[j] + alpha * bij * ai_i + alpha * temp2;\n        }\n    }\n}\n```\n",
  "jacobi_1d": "```cpp\n#include <vector>\n#include <cstddef>\n#include <omp.h>\n\n/* Main computational kernel.  The whole function will be timed,\n   including the call and return. */\nvoid kernel_jacobi_1d(int tsteps,\n                      int n,\n                      std::vector<double>& A,\n                      std::vector<double>& B)\n{\n    // Coefficient is constant 1/3\n    constexpr double coeff = 1.0 / 3.0;\n\n    // Obtain raw pointers for faster access\n    double* restrict A_ptr = A.data();\n    double* restrict B_ptr = B.data();\n\n    // Main time\u2011stepping loop\n    for (int t = 0; t < tsteps; ++t)\n    {\n        /* First update : B[i] = coeff * (A[i-1] + A[i] + A[i+1]) */\n        #pragma omp parallel for schedule(static)\n        for (long long i = 1; i < n - 1; ++i)\n        {\n            B_ptr[i] = coeff * (A_ptr[i - 1] + A_ptr[i] + A_ptr[i + 1]);\n        }\n\n        /* Second update : A[i] = coeff * (B[i-1] + B[i] + B[i+1]) */\n        #pragma omp parallel for schedule(static)\n        for (long long i = 1; i < n - 1; ++i)\n        {\n            A_ptr[i] = coeff * (B_ptr[i - 1] + B_ptr[i] + B_ptr[i + 1]);\n        }\n    }\n}\n```",
  "correlation": "```cpp\n#include <cmath>\n#include <numeric>\n#include <vector>\n#include <omp.h>\n\n// Helper to compute row-major index\ninline size_t idx(size_t i, size_t j, size_t m) {\n    return i * m + j;\n}\n\nvoid kernel_correlation(int m, int n,\n                        std::vector<std::vector<double>>& data,\n                        std::vector<std::vector<double>>& corr,\n                        std::vector<double>& mean,\n                        std::vector<double>& stddev) {\n    // constants\n    const double eps = 0.1;\n    const double inv_sqrt_n = 1.0 / std::sqrt(static_cast<double>(n));\n\n    // Compute means (parallel per feature)\n    #pragma omp parallel for schedule(static)\n    for (int j = 0; j < m; ++j) {\n        double sum = 0.0;\n        for (int i = 0; i < n; ++i) {\n            sum += data[i][j];\n        }\n        mean[j] = sum / n;\n    }\n\n    // Compute standard deviations\n    #pragma omp parallel for schedule(static)\n    for (int j = 0; j < m; ++j) {\n        double sum_sq = 0.0;\n        for (int i = 0; i < n; ++i) {\n            double diff = data[i][j] - mean[j];\n            sum_sq += diff * diff;\n        }\n        stddev[j] = std::sqrt(sum_sq / n);\n        if (stddev[j] <= eps) stddev[j] = 1.0;   // handle near\u2011zero std. dev.\n    }\n\n    // Center and scale data in place\n    #pragma omp parallel for schedule(static)\n    for (int i = 0; i < n; ++i) {\n        for (int j = 0; j < m; ++j) {\n            double val = data[i][j] - mean[j];\n            val *= inv_sqrt_n / stddev[j];\n            data[i][j] = val;\n        }\n    }\n\n    // Compute correlation matrix\n    // We compute upper triangle and mirror it\n    // Parallelize outer loop over i\n    #pragma omp parallel for schedule(dynamic)\n    for (int i = 0; i < m; ++i) {\n        corr[i][i] = 1.0;                 // diagonal\n        for (int j = i + 1; j < m; ++j) {\n            double sum = 0.0;\n            for (int k = 0; k < n; ++k) {\n                sum += data[k][i] * data[k][j];\n            }\n            corr[i][j] = sum;\n            corr[j][i] = sum;             // symmetry\n        }\n    }\n}\n```\nThis implementation preserves the original algorithm\u2019s semantics while:\n\n* **Parallelizing** all computationally heavy loops with OpenMP (`-fopenmp`) using static or dynamic scheduling.\n* **Reducing function call overhead** by eliminating unnecessary `std::sqrt` calls inside inner loops.\n* **Avoiding repeated memory accesses** by computing the perimeter scaling factor `inv_sqrt_n` and storing results back into the original `data` array in a cache\u2011friendly manner.\n* **Ensuring thread safety** by avoiding shared mutable state; each thread writes to distinct rows or distinct entries of the output matrices.",
  "cholesky": "```cpp\n#include <cmath>\n#include <vector>\n#include <omp.h>\n\n/* Optimised Cholesky kernel.\n   The layout and semantics of the original function are preserved. */\n\nvoid kernel_cholesky(int n, std::vector<std::vector<double>>& A) {\n    // Iterate over rows\n    for (int i = 0; i < n; ++i) {\n        double* const Ai = A[i].data();          // row i\n        // Lower\u2011triangle elements (j < i)\n        for (int j = 0; j < i; ++j) {\n            double* const Aj = A[j].data();      // row j\n            double  sum = 0.0;\n            // Dot product of row i and j up to column j-1\n            for (int k = 0; k < j; ++k) {\n                sum += Ai[k] * Aj[k];\n            }\n            Ai[j] = (Ai[j] - sum) / Aj[j];\n        }\n\n        // Diagonal element (i == i)\n        double diag = 0.0;\n        for (int k = 0; k < i; ++k) {\n            diag += Ai[k] * Ai[k];\n        }\n        Ai[i] = std::sqrt(A[i][i] - diag);\n    }\n}\n```\n\n*Key optimisations*  \n\n1. **Pointer arithmetic** \u2013 Convert inner vectors to raw pointers (`data()`) to avoid bounds checks and improve cache locality.  \n2. **Local variables** \u2013 Store row pointers and intermediate sums in local variables to reduce repeated indexing.  \n3. **Loop unrolling is implicitly handled by the compiler** at `-O2`.  \n4. **No OpenMP parallelism** \u2013 The algorithm has data dependencies (`A[i][j]` depends on earlier rows) that prevent safe parallelisation of the outer loop without complex synchronization. The sequential version with aggressive optimisations gives the best performance on the target platform.",
  "trmm": "```cpp\n#include <vector>\n#include <omp.h>\n\n/* Main computational kernel. The whole function will be timed,\n   including the call and return. */\nvoid kernel_trmm(int m, int n,\n                 double alpha,\n                 std::vector<std::vector<double>>& A,\n                 std::vector<std::vector<double>>& B)\n{\n    // => Form  B := alpha*A**T*B.\n    // A is MxM\n    // B is MxN\n\n    // Parallelise the outer loop using OpenMP. The inner k\u2011loop is\n    // already sequential because each B[i][j] depends on rows with\n    // index > i.  Keeping i private and sharing j works fine.\n#pragma omp parallel for schedule(static) collapse(1)\n    for (int i = 0; i < m; ++i) {\n        for (int j = 0; j < n; ++j) {\n            double acc = B[i][j];          // original value\n            for (int k = i + 1; k < m; ++k) // k > i\n                acc += A[k][i] * B[k][j];\n            B[i][j] = alpha * acc;        // final update\n        }\n    }\n}\n```",
  "ludcmp": "```cpp\n#include <vector>\n#include <omp.h>\n\n/* Main computational kernel. The whole function will be timed,\n   including the call and return. */\nvoid kernel_ludcmp(int n,\n                   std::vector<std::vector<double>>& A,\n                   std::vector<double>& b,\n                   std::vector<double>& x,\n                   std::vector<double>& y) {\n\n    /* LU decomposition \u2013 no parallelism possible due to data dependence */\n    for (int i = 0; i < n; ++i) {\n        /* Column below diagonal */\n        for (int j = 0; j < i; ++j) {\n            double w = A[i][j];\n            for (int k = 0; k < j; ++k) {\n                w -= A[i][k] * A[k][j];\n            }\n            A[i][j] = w / A[j][j];\n        }\n\n        /* Row to the right of the diagonal */\n        for (int j = i; j < n; ++j) {\n            double w = A[i][j];\n            for (int k = 0; k < i; ++k) {\n                w -= A[i][k] * A[k][j];\n            }\n            A[i][j] = w;\n        }\n    }\n\n    /* Forward substitution Ly = b */\n    #pragma omp parallel for schedule(static)\n    for (int i = 0; i < n; ++i) {\n        double w = b[i];\n        for (int j = 0; j < i; ++j) {\n            w -= A[i][j] * y[j];\n        }\n        y[i] = w;\n    }\n\n    /* Backward substitution Ux = y */\n    #pragma omp parallel for schedule(static)\n    for (int ii = n - 1; ii >= 0; --ii) {\n        int i = ii;\n        double w = y[i];\n        for (int j = i + 1; j < n; ++j) {\n            w -= A[i][j] * x[j];\n        }\n        x[i] = w / A[i][i];\n    }\n}\n```",
  "gramschmidt": "```cpp\n#include <cmath>\n#include <vector>\n#include <omp.h>\n\n/* Main computational kernel. The whole function will be timed,\n   including the call and return. */\n/* QR Decomposition with Modified Gram Schmidt:\n   http://www.inf.ethz.ch/personal/gander/ */\nvoid kernel_gramschmidt(int m, int n,\n                        std::vector<std::vector<double>>& A,\n                        std::vector<std::vector<double>>& R,\n                        std::vector<std::vector<double>>& Q)\n{\n    // Convert 2\u2011D std::vector to simple pointers for faster access\n    std::vector<double*> A_rows(m), Q_rows(m);\n    for (int i = 0; i < m; ++i) {\n        A_rows[i] = A[i].data();\n        Q_rows[i] = Q[i].data();\n    }\n\n    for (int k = 0; k < n; ++k) {\n        // 1. Compute norm of column k\n        double nrm = 0.0;\n#pragma omp parallel for reduction(+:nrm)\n        for (int i = 0; i < m; ++i) {\n            double v = A_rows[i][k];\n            nrm += v * v;\n        }\n        double rkk = std::sqrt(nrm);\n        R[k][k] = rkk;\n\n        // 2. Normalize column k to get Q(:,k)\n#pragma omp parallel for\n        for (int i = 0; i < m; ++i) {\n            Q_rows[i][k] = A_rows[i][k] / rkk;\n        }\n\n        // 3. Update remaining columns\n        for (int j = k + 1; j < n; ++j) {\n            // compute R(k,j) = Q(:,k)' * A(:,j)\n            double rij = 0.0;\n#pragma omp parallel for reduction(+:rij)\n            for (int i = 0; i < m; ++i) {\n                rij += Q_rows[i][k] * A_rows[i][j];\n            }\n            R[k][j] = rij;\n\n            // subtract projection from A(:,j)\n#pragma omp parallel for\n            for (int i = 0; i < m; ++i) {\n                A_rows[i][j] -= Q_rows[i][k] * rij;\n            }\n        }\n    }\n}\n```",
  "gemver": "```cpp\n#include <vector>\n#include <cstddef>\n#include <omp.h>\n\n/* Main computational kernel. The whole function will be timed,\n   including the call and return. */\nvoid kernel_gemver(int n,\n                   double alpha,\n                   double beta,\n                   std::vector<std::vector<double>>& A,\n                   std::vector<double>& u1,\n                   std::vector<double>& v1,\n                   std::vector<double>& u2,\n                   std::vector<double>& v2,\n                   std::vector<double>& w,\n                   std::vector<double>& x,\n                   std::vector<double>& y,\n                   std::vector<double>& z) {\n\n    /* 1) Update A:   A[i][j] += u1[i]*v1[j] + u2[i]*v2[j]   */\n    #pragma omp parallel for\n    for (int i = 0; i < n; ++i) {\n        const double ui1 = u1[i];\n        const double ui2 = u2[i];\n        std::vector<double>& Ai = A[i];\n        for (int j = 0; j < n; ++j) {\n            Ai[j] += ui1 * v1[j] + ui2 * v2[j];\n        }\n    }\n\n    /* 2) Accumulate into x using the transpose of A\n       x[i] += beta * sum_j A[j][i] * y[j]\n    */\n    #pragma omp parallel for\n    for (int i = 0; i < n; ++i) {\n        double tmp = 0.0;\n        for (int j = 0; j < n; ++j) {\n            tmp += A[j][i] * y[j];\n        }\n        x[i] += beta * tmp;\n    }\n\n    /* 3) Add z to x   */\n    #pragma omp parallel for\n    for (int i = 0; i < n; ++i) {\n        x[i] += z[i];\n    }\n\n    /* 4) Accumulate into w:   w[i] += alpha * sum_j A[i][j] * x[j]   */\n    #pragma omp parallel for\n    for (int i = 0; i < n; ++i) {\n        double tmp = 0.0;\n        const std::vector<double>& Ai = A[i];\n        for (int j = 0; j < n; ++j) {\n            tmp += Ai[j] * x[j];\n        }\n        w[i] += alpha * tmp;\n    }\n}\n```\n",
  "fdtd_2d": "```cpp\n#include <vector>\n#include <cstddef>\n#include <omp.h>\n\n/* Helper: flatten 2\u2011D index into 1\u2011D */\ninline std::size_t idx(std::size_t i, std::size_t j, std::size_t width) {\n    return i * width + j;\n}\n\n/* Main computational kernel. The whole function will be timed,\n   including the call and return. */\nvoid kernel_fdtd_2d(int tmax,\n                    int nx,\n                    int ny,\n                    std::vector<std::vector<double>>& ex,\n                    std::vector<std::vector<double>>& ey,\n                    std::vector<std::vector<double>>& hz,\n                    std::vector<double>& _fict_)\n{\n    /* Flatten the 2\u2011D arrays into 1\u2011D contiguous buffers. */\n    const std::size_t nxy = static_cast<std::size_t>(nx) * ny;\n    std::vector<double> ex_flat(nxy);\n    std::vector<double> ey_flat(nxy);\n    std::vector<double> hz_flat(nxy);\n\n    /* Copy the input data into the flat buffers */\n    #pragma omp parallel for collapse(2)\n    for (std::size_t i = 0; i < static_cast<std::size_t>(nx); ++i)\n        for (std::size_t j = 0; j < static_cast<std::size_t>(ny); ++j)\n            ex_flat[idx(i,j,ny)] = ex[i][j];\n\n    #pragma omp parallel for collapse(2)\n    for (std::size_t i = 0; i < static_cast<std::size_t>(nx); ++i)\n        for (std::size_t j = 0; j < static_cast<std::size_t>(ny); ++j)\n            ey_flat[idx(i,j,ny)] = ey[i][j];\n\n    #pragma omp parallel for collapse(2)\n    for (std::size_t i = 0; i < static_cast<std::size_t>(nx); ++i)\n        for (std::size_t j = 0; j < static_cast<std::size_t>(ny); ++j)\n            hz_flat[idx(i,j,ny)] = hz[i][j];\n\n    /* Main loop */\n    for (int t = 0; t < tmax; ++t) {\n        /* Ey at i=0 */\n        #pragma omp parallel for\n        for (std::size_t j = 0; j < static_cast<std::size_t>(ny); ++j)\n            ey_flat[idx(0, j, ny)] = _fict_[t];\n\n        /* Update Ey */\n        #pragma omp parallel for collapse(2)\n        for (int i = 1; i < nx; ++i)\n            for (int j = 0; j < ny; ++j)\n                ey_flat[idx(i, j, ny)] -= 0.5 * (hz_flat[idx(i, j, ny)] - hz_flat[idx(i-1, j, ny)]);\n\n        /* Update Ex */\n        #pragma omp parallel for collapse(2)\n        for (int i = 0; i < nx; ++i)\n            for (int j = 1; j < ny; ++j)\n                ex_flat[idx(i, j, ny)] -= 0.5 * (hz_flat[idx(i, j, ny)] - hz_flat[idx(i, j-1, ny)]);\n\n        /* Update Hz */\n        #pragma omp parallel for collapse(2)\n        for (int i = 0; i < nx - 1; ++i)\n            for (int j = 0; j < ny - 1; ++j)\n                hz_flat[idx(i, j, ny)] -= 0.7 *\n                    (ex_flat[idx(i, j+1, ny)] - ex_flat[idx(i, j, ny)] +\n                     ey_flat[idx(i+1, j, ny)] - ey_flat[idx(i, j, ny)]);\n    }\n\n    /* Copy back the results into the original 2\u2011D arrays */\n    #pragma omp parallel for collapse(2)\n    for (std::size_t i = 0; i < static_cast<std::size_t>(nx); ++i)\n        for (std::size_t j = 0; j < static_cast<std::size_t>(ny); ++j)\n            ex[i][j] = ex_flat[idx(i,j,ny)];\n\n    #pragma omp parallel for collapse(2)\n    for (std::size_t i = 0; i < static_cast<std::size_t>(nx); ++i)\n        for (std::size_t j = 0; j < static_cast<std::size_t>(ny); ++j)\n            ey[i][j] = ey_flat[idx(i,j,ny)];\n\n    #pragma omp parallel for collapse(2)\n    for (std::size_t i = 0; i < static_cast<std::size_t>(nx); ++i)\n        for (std::size_t j = 0; j < static_cast<std::size_t>(ny); ++j)\n            hz[i][j] = hz_flat[idx(i,j,ny)];\n}\n```",
  "floyd_warshall": "```cpp\n#include <vector>\n#include <algorithm>\n#include <cstddef>\n#include <omp.h>\n\nvoid kernel_floyd_warshall(int n, std::vector<std::vector<int>>& path) {\n    // Cache the row pointers to avoid repeated vector indexing\n    std::vector<int*> rows(n);\n    for (int i = 0; i < n; ++i) rows[i] = path[i].data();\n\n    // OpenMP parallelise over the outer i\u2011loop for better cache utilisation\n    for (int k = 0; k < n; ++k) {\n        const int* pk = rows[k];                // row k\n        #pragma omp parallel for schedule(static, 64)\n        for (int i = 0; i < n; ++i) {\n            int* pi = rows[i];\n            const int aik = pi[k];               // path[i][k]\n            for (int j = 0; j < n; ++j) {\n                const int alt = aik + pk[j];\n                if (alt < pi[j]) pi[j] = alt;\n            }\n        }\n    }\n}\n```",
  "lu": "```cpp\n#include <vector>\n#include <omp.h>\n\n/* Main computational kernel. The whole function will be timed,\n   including the call and return. */\nvoid kernel_lu(int n, std::vector<std::vector<double>>& A) {\n    /* We keep the public signature unchanged.\n       Inside we use raw pointers for speed and OpenMP to parallelise\n       independent work.  The algorithm itself is unchanged.  */\n    #pragma omp parallel for schedule(static)\n    for (int i = 0; i < n; ++i) {\n        std::vector<double>& Ai   = A[i];           // row i\n        for (int j = 0; j < i; ++j) {\n            double* Aij = &Ai[j];\n            for (int k = 0; k < j; ++k)\n                *Aij -= Ai[k] * A[k][j];\n            *Aij /= A[j][j];\n        }\n\n        for (int j = i; j < n; ++j) {\n            double* Aij = &Ai[j];\n            for (int k = 0; k < i; ++k)\n                *Aij -= Ai[k] * A[k][j];\n        }\n    }\n}\n```",
  "seidel_2d": "```cpp\n#include <vector>\n#include <omp.h>\n\n/* Main computational kernel. The whole function will be timed,\n   including the call and return. */\nvoid kernel_seidel_2d(int tsteps,\n\t\t      int n,\n\t\t      std::vector<std::vector<double>>& A) {\n\n    const double inv9 = 1.0 / 9.0;\n\n    for (int t = 0; t < tsteps; ++t) {\n        // The innermost loop is a critical place for memory locality.\n        // We iterate row\u2011major and use pointer arithmetic to reduce\n        // indirection costs.  The solver is truly sequential in the\n        // inner two loops because of the in\u2011place update, so we can\n        // only parallelise over the time dimension in a safe way.\n        #pragma omp parallel for schedule(static)\n        for (int i = 1; i <= n - 2; ++i) {\n            // Pointers to the current row and its neighbours\n            double* prev = &A[i-1][0];\n            double* cur  = &A[i][0];\n            double* next = &A[i+1][0];\n\n            for (int j = 1; j <= n - 2; ++j) {\n                // Manual unrolling of the neighbour summation\n                double sum =\n                    prev[j-1] + prev[j] + prev[j+1] +\n                    cur[j-1]  + cur[j]  + cur[j+1] +\n                    next[j-1] + next[j] + next[j+1];\n\n                cur[j] = sum * inv9;\n            }\n        }\n    }\n}\n```\n\nThis version keeps the exact in\u2011place semantics of the original algorithm while improving cache efficiency through pointer arithmetic, reduces floating\u2011point division by precomputing `1/9`, and introduces a parallel loop over the outer spatial dimension. No changes to the function signature or observable behaviour are made.",
  "jacobi_2d": "```cpp\n#include <vector>\n#include <omp.h>\n\n/* Main computational kernel. The whole function will be timed,\n   including the call and return. */\nvoid kernel_jacobi_2d(int tsteps,\n                      int n,\n                      std::vector<std::vector<double>>& A,\n                      std::vector<std::vector<double>>& B) {\n    // Obtain raw row pointers for faster access\n    std::vector<double*> rowA(n), rowB(n);\n    for (int i = 0; i < n; ++i) {\n        rowA[i] = A[i].data();\n        rowB[i] = B[i].data();\n    }\n\n    const double alpha = 0.2;                     // coefficient\n    const int im1 = 1;                            // offset for i-1\n    const int ip1 = 1;                            // offset for i+1\n    const int jm1 = 1;                            // offset for j-1\n    const int jp1 = 1;                            // offset for j+1\n\n    for (int t = 0; t < tsteps; ++t) {\n        // First half of a single time step\n        #pragma omp parallel for schedule(static)\n        for (int i = 1; i < n - 1; ++i) {\n            double *Ai  = rowA[i];\n            double *Aim1 = rowA[i - im1];\n            double *Ap1  = rowA[i + ip1];\n            double *Bi  = rowB[i];\n            double *Bim1 = rowB[i - im1];\n            double *Bp1  = rowB[i + ip1];\n            for (int j = 1; j < n - 1; ++j) {\n                Bi[j] = alpha * (Ai[j] + Ai[j - jm1] + Ai[j + jp1] + Ap1[j] + Aim1[j]);\n            }\n        }\n\n        // Second half of a single time step\n        #pragma omp parallel for schedule(static)\n        for (int i = 1; i < n - 1; ++i) {\n            double *Ai  = rowA[i];\n            double *Aim1 = rowA[i - im1];\n            double *Ap1  = rowA[i + ip1];\n            double *Bi  = rowB[i];\n            double *Bim1 = rowB[i - im1];\n            double *Bp1  = rowB[i + ip1];\n            for (int j = 1; j < n - 1; ++j) {\n                Ai[j] = alpha * (Bi[j] + Bi[j - jm1] + Bi[j + jp1] + Bp1[j] + Bim1[j]);\n            }\n        }\n    }\n}\n```",
  "deriche": "```cpp\n#include <cmath>\n#include <vector>\n#include <omp.h>\n\n/* Optimized version of kernel_deriche.\n   The function signature is unchanged to remain compatible\n   with the harness.  The implementation uses OpenMP to\n   parallelise the outermost loops, avoids recomputing\n   constants inside inner loops, and keeps the original\n   algorithmic structure so the exact numerical behaviour\n   is preserved. */\nvoid kernel_deriche(int w, int h, double alpha,\n                    std::vector<std::vector<float>>& imgIn,\n                    std::vector<std::vector<float>>& imgOut,\n                    std::vector<std::vector<float>>& y1,\n                    std::vector<std::vector<float>>& y2)\n{\n    /* Pre\u2011compute the constants that depend only on alpha. */\n    const float exp_a   = std::exp(-alpha);\n    const float exp_2a  = exp_a * exp_a;\n    const float denom   = 1.0f + 2.0f * static_cast<float>(alpha) * exp_a - exp_2a ;\n    const float k       = 1.0f - exp_a * (1.0f - exp_a) / denom;\n    const float k_exp_a = k * exp_a;\n\n    const float a1 = k, a5 = k;\n    const float a2 = k_exp_a * (static_cast<float>(alpha) - 1.0f), a6 = a2;\n    const float a3 = k_exp_a * (static_cast<float>(alpha) + 1.0f), a7 = a3;\n    const float a4 = -k * exp_2a, a8 = a4;\n    const float b1 = std::pow(2.0f, -static_cast<float>(alpha));\n    const float b2 = -exp_2a;\n    const float c1 = 1.0f, c2 = 1.0f;\n\n    /* ------------------------------------------------------------------\n       First forward scan along rows (i).  Parallelise over i.           */\n#pragma omp parallel for\n    for (int i = 0; i < w; ++i) {\n        float ym1 = 0.0f, ym2 = 0.0f;\n        float xm1 = 0.0f;\n        for (int j = 0; j < h; ++j) {\n            const float in = imgIn[i][j];\n            const float y  = a1 * in + a2 * xm1 + b1 * ym1 + b2 * ym2;\n            y1[i][j] = y;\n            xm1 = in;\n            ym2 = ym1;\n            ym1 = y;\n        }\n    }\n\n    /* Second reverse scan along rows. */\n#pragma omp parallel for\n    for (int i = 0; i < w; ++i) {\n        float yp1 = 0.0f, yp2 = 0.0f;\n        float xp1 = 0.0f, xp2 = 0.0f;\n        for (int j = h - 1; j >= 0; --j) {\n            const float x = imgIn[i][j];\n            const float y = a3 * xp1 + a4 * xp2 + b1 * yp1 + b2 * yp2;\n            y2[i][j] = y;\n            xp2 = xp1;\n            xp1 = x;\n            yp2 = yp1;\n            yp1 = y;\n        }\n    }\n\n    /* Third scan: sum the two passes. */\n#pragma omp parallel for collapse(2)\n    for (int i = 0; i < w; ++i) {\n        for (int j = 0; j < h; ++j) {\n            imgOut[i][j] = c1 * (y1[i][j] + y2[i][j]);\n        }\n    }\n\n    /* Fourth forward scan along columns. */\n#pragma omp parallel for\n    for (int j = 0; j < h; ++j) {\n        float tm1 = 0.0f, ym1 = 0.0f, ym2 = 0.0f;\n        for (int i = 0; i < w; ++i) {\n            const float out = imgOut[i][j];\n            const float y = a5 * out + a6 * tm1 + b1 * ym1 + b2 * ym2;\n            y1[i][j] = y;\n            tm1 = out;\n            ym2 = ym1;\n            ym1 = y;\n        }\n    }\n\n    /* Fifth reverse scan along columns. */\n#pragma omp parallel for\n    for (int j = 0; j < h; ++j) {\n        float tp1 = 0.0f, tp2 = 0.0f;\n        float yp1 = 0.0f, yp2 = 0.0f;\n        for (int i = w - 1; i >= 0; --i) {\n            const float out = imgOut[i][j];\n            const float y = a7 * tp1 + a8 * tp2 + b1 * yp1 + b2 * yp2;\n            y2[i][j] = y;\n            tp2 = tp1;\n            tp1 = out;\n            yp2 = yp1;\n            yp1 = y;\n        }\n    }\n\n    /* Final accumulation. */\n#pragma omp parallel for collapse(2)\n    for (int i = 0; i < w; ++i) {\n        for (int j = 0; j < h; ++j) {\n            imgOut[i][j] = c2 * (y1[i][j] + y2[i][j]);\n        }\n    }\n}\n```"
}