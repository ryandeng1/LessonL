{
  "covariance": "```cpp\n#include <vector>\n#include <omp.h>\n#include <cstring> // for memset\n\nvoid kernel_covariance(int m, int n,\n                       std::vector<std::vector<double>>& data,\n                       std::vector<std::vector<double>>& cov,\n                       std::vector<double>& mean)\n{\n    // Access data as contiguous memory for speed.\n    double* flat_data = &data[0][0];\n    double* flat_cov  = &cov[0][0];\n\n    /* ---------- compute means ---------- */\n    #pragma omp parallel for schedule(static)\n    for (int j = 0; j < m; ++j)\n    {\n        double s = 0.0;\n        for (int i = 0; i < n; ++i)\n            s += flat_data[i * m + j];\n        mean[j] = s / n;\n    }\n\n    /* ---------- subtract means ---------- */\n    #pragma omp parallel for schedule(static)\n    for (int i = 0; i < n; ++i)\n    {\n        double* row = flat_data + i * m;\n        for (int j = 0; j < m; ++j)\n            row[j] -= mean[j];\n    }\n\n    /* ---------- compute covariance matrix ---------- */\n    /* We compute only the upper\u2011triangular part and mirror it */\n    int last = n - 1;          // to avoid recomputing division every iteration\n    double inv = 1.0 / last;   // same as 1/(n-1)\n\n    #pragma omp parallel for collapse(2) schedule(static)\n    for (int i = 0; i < m; ++i)\n    {\n        for (int j = i; j < m; ++j)\n        {\n            double s = 0.0;\n            for (int k = 0; k < n; ++k)\n                s += flat_data[k * m + i] * flat_data[k * m + j];\n            s *= inv;\n            flat_cov[i * m + j] = s;\n            flat_cov[j * m + i] = s;   // mirror to lower triangle\n        }\n    }\n}\n```",
  "syr2k": "```cpp\n/* Optimised version of kernel_syr2k\n * Maintains exact C++ semantics and function signature.\n * Uses OpenMP parallelisation and pointer arithmetic to reduce\n * overhead of std::vector indexing.\n */\n\n#include <vector>\n#include <omp.h>\n\nvoid kernel_syr2k(int n, int m,\n                  double alpha,\n                  double beta,\n                  std::vector<std::vector<double>>& C,\n                  std::vector<std::vector<double>>& A,\n                  std::vector<std::vector<double>>& B) {\n\n    const double al = alpha, be = beta;     // Promote to local temporaries\n    const int  N   = n, M = m;\n\n    /* Parallelise over outermost loop (i). Each iteration is independent. */\n#pragma omp parallel for schedule(static)\n    for (int i = 0; i < N; ++i) {\n        double* Ci = C[i].data();          // Pointer to row i of C\n        const double* Ai = A[i].data();    // Pointer to row i of A\n        const double* Bi = B[i].data();    // Pointer to row i of B\n\n        /* First scale lower\u2011triangular part of C by beta. */\n        for (int j = 0; j <= i; ++j) {\n            Ci[j] *= be;\n        }\n\n        /* Compute the rank\u20112 update:\n         *      C[i][j] += alpha * ( A[j][k]*B[i][k] + B[j][k]*A[i][k] )\n         */\n        for (int k = 0; k < M; ++k) {\n            const double Bk = Bi[k];            // B[i][k]\n            const double Ak = Ai[k];            // A[i][k]\n            for (int j = 0; j <= i; ++j) {\n                const double *Aj = A[j].data(); // A[j][k]\n                const double *Bj = B[j].data(); // B[j][k]\n                Ci[j] += al * (Aj[k] * Bk + Bj[k] * Ak);\n            }\n        }\n    }\n}\n```",
  "atax": "```cpp\n#include <vector>\n#include <omp.h>\n\n/* Main computational kernel. The whole function will be timed,\n   including the call and return. */\nvoid kernel_atax(int m, int n,\n                 std::vector<std::vector<double>>& A,\n                 std::vector<double>& x,\n                 std::vector<double>& y,\n                 std::vector<double>& tmp) {\n\n    /* Zero the output vector once (no race condition). */\n    for (int i = 0; i < n; ++i)\n        y[i] = 0.0;\n\n    /* Parallel over rows of A.  Each thread accumulates its own\n       partial result for y and writes to a local tmp array. */\n    #pragma omp parallel\n    {\n        /* Each thread gets a temporary copy of tmp slice to avoid\n           race conditions on the shared 'tmp' vector. */\n        std::vector<double> local_tmp(m, 0.0);          // reuse across i\n        std::vector<double> local_y(n, 0.0);           // per-thread Y accumulation\n\n        #pragma omp for schedule(static)\n        for (int i = 0; i < m; ++i) {\n            /* Compute tmp[i] = dot(A[i][*], x[*]) */\n            double sum = 0.0;\n            const std::vector<double>& Ai = A[i];\n            for (int j = 0; j < n; ++j)\n                sum += Ai[j] * x[j];\n            local_tmp[i] = sum;\n\n            /* Accumulate into local Y */\n            for (int j = 0; j < n; ++j)\n                local_y[j] += Ai[j] * sum;\n        }\n\n        /* Merge the per-thread Y results into the global result.\n           The critical section is only for updating y, which is\n           a small scan over n elements. */\n        #pragma omp critical\n        {\n            for (int j = 0; j < n; ++j)\n                y[j] += local_y[j];\n        }\n\n        /* Write back the computed tmp values for this thread's rows.\n           Only one thread will write to a given element of tmp, so\n           no race condition. */\n        #pragma omp for schedule(static)\n        for (int i = 0; i < m; ++i)\n            tmp[i] = local_tmp[i];\n    }\n}\n```",
  "nussinov": "```cpp\n#include <vector>\n#include <algorithm>\n#include <omp.h>\n\n/* Main computational kernel. The whole function will be timed,\n   including the call and return. */\n\n/* Original version by Dave Wonnacott at Haverford College <davew@cs.haverford.edu>,\n   with help from Allison Lake, Ting Zhou, and Tian Jin,\n   based on algorithm by Nussinov, described in Allison Lake's senior thesis.\n*/\n\nstatic inline int match(int b1, int b2) { return (b1 + b2 == 3) ? 1 : 0; }\n\nvoid kernel_nussinov(int n, std::vector<int>& seq,\n\t\t     std::vector<std::vector<int>>& table) {\n    /* DP is computed by increasing diagonal distance d = j - i.\n       All (i, j) with the same d can be processed in parallel because\n       their dependencies have smaller d. */\n    for (int d = 1; d < n; ++d) {\n#pragma omp parallel for schedule(static)\n        for (int i = 0; i + d < n; ++i) {\n            int j = i + d;\n            int best = table[i][j];                // current value (initially 0)\n\n            // Skip j\n            best = std::max(best, table[i][j - 1]);\n\n            // Skip i\n            best = std::max(best, table[i + 1][j]);\n\n            // Match i & j\n            best = std::max(best,\n                            table[i + 1][j - 1] + match(seq[i], seq[j]));\n\n            // Partition at k\n            for (int k = i + 1; k < j; ++k) {\n                int val = table[i][k] + table[k + 1][j];\n                if (val > best) best = val;\n            }\n\n            table[i][j] = best;\n        }\n    }\n}\n```",
  "doitgen": "```cpp\n#include <vector>\n#include <cstddef>\n#include <omp.h>\n\n/* Main computational kernel. The whole function will be timed,\n   including the call and return. */\nvoid kernel_doitgen(int nr, int nq, int np,\n                    std::vector<std::vector<std::vector<double>>>& A,\n                    std::vector<std::vector<double>>& C4,\n                    std::vector<double>& /*sum*/)\n{\n    /* Parallelise over the first two dimensions.\n       Each iteration works on a complete [p] slice, so no\n       data races occur. */\n#pragma omp parallel for collapse(2) schedule(static)\n    for (int r = 0; r < nr; ++r)\n    {\n        for (int q = 0; q < nq; ++q)\n        {\n            /* Pointer to the current row of A for fast indexing */\n            double* Arow = A[r][q].data();\n\n            /* Compute the product for all p and write back\n               directly into A to avoid an unnecessary temporary\n               vector. */\n#pragma omp simd   // vectorise the innermost loop\n            for (int p = 0; p < np; ++p)\n            {\n                double acc = 0.0;\n                for (int s = 0; s < np; ++s)\n                {\n                    acc += Arow[s] * C4[s][p];\n                }\n                A[r][q][p] = acc;\n            }\n        }\n    }\n}\n```",
  "gemm": "```cpp\n#include <vector>\n#include <cstddef>\n#include <omp.h>\n\n/*  GEMM for classic small 2\u2011D std::vector interface\n    C := alpha * A * B + beta * C\n    A : ni\u00d7nk , B : nk\u00d7nj , C : ni\u00d7nj\n*/\nvoid kernel_gemm(int ni, int nj, int nk,\n                 double alpha, double beta,\n                 std::vector<std::vector<double>>& A,\n                 std::vector<std::vector<double>>& B,\n                 std::vector<std::vector<double>>& C)\n{\n    // Flatten the input matrices so that we only touch continuous memory.\n    // This is safe because the outer vector is laid out row\u2011major.\n    std::vector<double>  a_flat(ni * nk);\n    std::vector<double>  b_flat(nk * nj);\n    std::vector<double>  c_flat(ni * nj);\n\n    for (int i = 0; i < ni; ++i)\n        for (int k = 0; k < nk; ++k)\n            a_flat[i * nk + k] = A[i][k];\n\n    for (int k = 0; k < nk; ++k)\n        for (int j = 0; j < nj; ++j)\n            b_flat[k * nj + j] = B[k][j];\n\n    for (int i = 0; i < ni; ++i)\n        for (int j = 0; j < nj; ++j)\n            c_flat[i * nj + j] = C[i][j];\n\n    /*  Parallel, blocking, and fused formulation.\n        The block size is tuned for L1/L2 cache (typical for modern x86\u201164).\n    */\n    const int block_i = 32;   // rows of C (and A)\n    const int block_k = 32;   // columns of A and rows of B\n    const int block_j = 32;   // columns of C and B\n\n    #pragma omp parallel for schedule(static,1) collapse(1)\n    for (int ii = 0; ii < ni; ii += block_i) {\n        int i_end = std::min(ii + block_i, ni);\n        for (int kk = 0; kk < nk; kk += block_k) {\n            int k_end = std::min(kk + block_k, nk);\n            for (int jj = 0; jj < nj; jj += block_j) {\n                int j_end = std::min(jj + block_j, nj);\n\n                /* scale the C block by beta once */\n                for (int i = ii; i < i_end; ++i)\n                    for (int j = jj; j < j_end; ++j)\n                        c_flat[i * nj + j] *= beta;\n\n                /* compute the block contribution */\n                for (int i = ii; i < i_end; ++i) {\n                    const double* Ap = &a_flat[i * nk + kk];\n                    double* Cp  = &c_flat[i * nj + jj];\n                    for (int k = kk; k < k_end; ++k) {\n                        double aik = Ap[k - kk];\n                        const double* Bp = &b_flat[k * nj + jj];\n                        for (int j = 0; j < (j_end - jj); ++j)\n                            Cp[j] += alpha * aik * Bp[j];\n                    }\n                }\n            }\n        }\n    }\n\n    // Copy back to the original vector-of-vectors\n    for (int i = 0; i < ni; ++i)\n        for (int j = 0; j < nj; ++j)\n            C[i][j] = c_flat[i * nj + j];\n}\n```",
  "heat_3d": "```cpp\n#include <vector>\n#include <cstddef>\n#include <omp.h>\n\n/* Helper to flatten a 3\u2011D vector into a single contiguous array.\n * The layout matches the original std::vector<std::vector<std::vector<>>>\n * access pattern: A[i][j][k] -> data[(i*n + j)*n + k]\n */\nstatic inline double* flatten(std::vector<std::vector<std::vector<double>>>& v, size_t i,\n                              size_t j, size_t k, size_t n)\n{\n    return &v[i][j][k];\n}\n\n/* Main computational kernel.  The whole function will be timed,\n * including the call and return. */\nvoid kernel_heat_3d(int tsteps,\n                    int n,\n                    std::vector<std::vector<std::vector<double>>>& A,\n                    std::vector<std::vector<std::vector<double>>>& B)\n{\n    // Work out strides to avoid repeated multiplications\n    const size_t strideI = n * n;   // distance between i slices\n    const size_t strideJ = n;       // distance between j rows\n\n    // Flatten the 3\u2011D data into one\u2013dimensional arrays for best spatial locality\n    std::vector<double> a_flat(n * n * n);\n    std::vector<double> b_flat(n * n * n);\n\n    // Copy original data into the flat arrays\n    for (int i = 0; i < n; ++i)\n        for (int j = 0; j < n; ++j)\n            for (int k = 0; k < n; ++k)\n                a_flat[(size_t)i * strideI + (size_t)j * strideJ + k] = A[i][j][k];\n\n    const double coef = 0.125; // 1/8\n\n    #pragma omp parallel for collapse(2) schedule(static)\n    for (int t = 1; t <= tsteps; ++t) {\n        // First half\u2011step: A -> B\n        for (int i = 1; i < n-1; ++i)\n            for (int j = 1; j < n-1; ++j)\n                for (int k = 1; k < n-1; ++k) {\n                    size_t idx = (size_t)i * strideI + (size_t)j * strideJ + k;\n                    double centre = a_flat[idx];\n                    double sum =   a_flat[idx + strideI] + a_flat[idx - strideI]\n                                 + a_flat[idx + strideJ] + a_flat[idx - strideJ]\n                                 + a_flat[idx + 1]      + a_flat[idx - 1];\n                    b_flat[idx] = coef * (sum - 6.0 * centre) + centre;\n                }\n\n        // Second half\u2011step: B -> A\n        for (int i = 1; i < n-1; ++i)\n            for (int j = 1; j < n-1; ++j)\n                for (int k = 1; k < n-1; ++k) {\n                    size_t idx = (size_t)i * strideI + (size_t)j * strideJ + k;\n                    double centre = b_flat[idx];\n                    double sum =   b_flat[idx + strideI] + b_flat[idx - strideI]\n                                 + b_flat[idx + strideJ] + b_flat[idx - strideJ]\n                                 + b_flat[idx + 1]      + b_flat[idx - 1];\n                    a_flat[idx] = coef * (sum - 6.0 * centre) + centre;\n                }\n    }\n\n    // Copy result back to the original multi\u2011vector structure\n    for (int i = 0; i < n; ++i)\n        for (int j = 0; j < n; ++j)\n            for (int k = 0; k < n; ++k)\n                A[i][j][k] = a_flat[(size_t)i * strideI + (size_t)j * strideJ + k];\n}\n```",
  "trisolv": "```cpp\n#include <vector>\n#include <omp.h>\n\n/* Main computational kernel. The whole function will be timed,\n   including the call and return. */\nvoid kernel_trisolv(int n,\n                    std::vector<std::vector<double>>& L,\n                    std::vector<double>& x,\n                    std::vector<double>& b) {\n    /* Local raw pointers for faster access */\n    const double *restrict pb = b.data();\n    double *restrict px   = x.data();\n\n    for (int i = 0; i < n; ++i) {\n        double sum = pb[i];\n        const double *restrict Li = L[i].data();\n        for (int j = 0; j < i; ++j) {\n#pragma omp simd\n            sum -= Li[j] * px[j];\n        }\n        px[i] = sum / Li[i];\n    }\n}\n```",
  "gesummv": "```cpp\n#include <vector>\n#include <cstddef>\n#include <omp.h>\n\n/* Main computational kernel. The whole function will be timed,\n   including the call and return. */\nvoid kernel_gesummv(int n,\n                    double alpha,\n                    double beta,\n                    std::vector<std::vector<double>>& A,\n                    std::vector<std::vector<double>>& B,\n                    std::vector<double>& tmp,\n                    std::vector<double>& x,\n                    std::vector<double>& y)\n{\n    const double* xdata = x.data();\n    double* tmpdata = tmp.data();\n    double* ydata   = y.data();\n\n    /* Pre\u2011allocate row pointers for faster access. */\n    const double** Ad = new const double*[n];\n    const double** Bd = new const double*[n];\n    for (int i = 0; i < n; ++i) {\n        Ad[i] = A[i].data();\n        Bd[i] = B[i].data();\n    }\n\n    /* Parallel over rows */\n#pragma omp parallel for schedule(static)\n    for (int i = 0; i < n; ++i)\n    {\n        const double* Aloc = Ad[i];\n        const double* Bloc = Bd[i];\n        double t = 0.0;\n        double ysum = 0.0;\n\n        /* Inner loop */\n        for (int j = 0; j < n; ++j)\n        {\n            t      += Aloc[j] * xdata[j];\n            ysum   += Bloc[j] * xdata[j];\n        }\n\n        tmpdata[i] = t;\n        ydata[i]   = alpha * t + beta * ysum;\n    }\n\n    delete[] Ad;\n    delete[] Bd;\n}\n```",
  "durbin": "```cpp\n#include <vector>\n#include <cstddef>\n#include <omp.h>\n\n/* Main computational kernel. The whole function will be timed,\n   including the call and return.  */\nvoid kernel_durbin(int n,\n                   std::vector<double>& r,\n                   std::vector<double>& y)\n{\n    // Allocate temporary workspace once to avoid reallocations\n    static std::vector<double> z;\n    z.resize(n);\n\n    double* rp = r.data();\n    double* yp = y.data();\n    double* zp = z.data();\n\n    yp[0] = -rp[0];\n    double beta  = 1.0;\n    double alpha = -rp[0];\n\n    for (int k = 1; k < n; ++k)\n    {\n        beta = (1.0 - alpha * alpha) * beta;\n\n        double sum = 0.0;\n        /* Compute the convolution sum.  */\n#pragma omp parallel for reduction(+:sum) schedule(static)\n        for (int i = 0; i < k; ++i)\n            sum += rp[k - i - 1] * yp[i];\n\n        alpha = -(rp[k] + sum) / beta;\n\n        /* Update temporary array z.  */\n#pragma omp parallel for schedule(static)\n        for (int i = 0; i < k; ++i)\n            zp[i] = yp[i] + alpha * yp[k - i - 1];\n\n        /* Copy z back to y.  */\n#pragma omp parallel for schedule(static)\n        for (int i = 0; i < k; ++i)\n            yp[i] = zp[i];\n\n        yp[k] = alpha;\n    }\n}\n```",
  "syrk": "```cpp\n#include <vector>\n#include <cstddef>\n#include <omp.h>\n\n/* Main computational kernel. The whole function will be timed,\n   including the call and return. */\nvoid kernel_syrk(int n, int m,\n                 double alpha,\n                 double beta,\n                 std::vector<std::vector<double>>& C,\n                 std::vector<std::vector<double>>& A)\n{\n    // Flatten 2\u2011D containers to simple pointers for faster access.\n    const std::size_t nn = static_cast<std::size_t>(n);\n    const std::size_t mm = static_cast<std::size_t>(m);\n    const double *Aptr = reinterpret_cast<const double*>(A.data());\n    double       *Cptr = reinterpret_cast<double*>(C.data());\n\n    // Simple inline helper for index (row major).\n    auto idx = [nn](std::size_t i, std::size_t j) -> std::size_t\n    {\n        return i * nn + j;\n    };\n\n    // Parallelise the outermost loop over rows.\n    #pragma omp parallel for schedule(static)\n    for (int i = 0; i < n; ++i)\n    {\n        // Pointer to first element of the i\u2011th row of C.\n        double *Ci = Cptr + idx(i, 0);\n\n        // Pre\u2011scale the lower triangular part of C by beta.\n        for (int j = 0; j <= i; ++j)\n            Ci[j] *= beta;\n\n        // Accumulate the outer product contributions.\n        for (int k = 0; k < m; ++k)\n        {\n            const double aik = Aptr[i * mm + k];\n            for (int j = 0; j <= i; ++j)\n                Ci[j] += alpha * aik * Aptr[j * mm + k];\n        }\n    }\n}\n```",
  "bicg": "```cpp\n#include <vector>\n#include <omp.h>\n\n/* Main computational kernel. The whole function will be timed,\n   including the call and return. */\nvoid kernel_bicg(int m, int n,\n                 std::vector<std::vector<double>>& A,\n                 std::vector<double>& s,\n                 std::vector<double>& q,\n                 std::vector<double>& p,\n                 std::vector<double>& r) {\n    /* reset output vectors */\n    #pragma omp parallel for schedule(static, 256) \n    for (int i = 0; i < m; ++i) s[i] = 0.0;\n    #pragma omp parallel for schedule(static, 256) \n    for (int i = 0; i < n; ++i) q[i] = 0.0;\n\n    /* compute */\n    #pragma omp parallel for schedule(static, 256) reduction(+:s[:m])\n    for (int i = 0; i < n; ++i) {\n        double qi = 0.0;\n        for (int j = 0; j < m; ++j) {\n            s[j] += r[i] * A[i][j];\n            qi += A[i][j] + p[j];\n        }\n        q[i] = qi;\n    }\n}\n```",
  "symm": "```cpp\n#include <vector>\n#include <cstddef>\n#include <omp.h>\n\n/* Helper to linearise 2\u2011D vectors for cache locality.\n   Access by `ptr[i * stride + j]`. */\nstruct Layout {\n    double* ptr;\n    std::size_t stride;      // number of columns (n) for C,B; m for A\n};\n\n/* Main computational kernel. The whole function will be timed,\n   including the call and return. */\nvoid kernel_symm(int m, int n,\n                 double alpha,\n                 double beta,\n                 std::vector<std::vector<double>>& C,\n                 std::vector<std::vector<double>>& A,\n                 std::vector<std::vector<double>>& B)\n{\n    /* Convert 2-D std::vector to a linearised pointer layout\n       to reduce the cost of bounds checks and indirect access. */\n    Layout lC{ &C[0][0] , static_cast<std::size_t>(n) };\n    Layout lB{ &B[0][0] , static_cast<std::size_t>(n) };\n    Layout lA{ &A[0][0] , static_cast<std::size_t>(m) };\n\n    /* Parallelise over the outer loops. The inner loop over `j`\n       is trivially parallelisable and gives excellent cache reuse. */\n#pragma omp parallel for schedule(static)\n    for (int i = 0; i < m; ++i) {\n        const double diag = alpha * lA.ptr[i*lA.stride + i];   // A[i][i] * alpha\n\n        for (int j = 0; j < n; ++j) {\n            double temp2 = 0.0;\n            double b_ij  = lB.ptr[i*lB.stride + j];              // B[i][j]\n\n            /* First part of the loop: lower\u2011triangular terms\n               that write to C[k][j] for k < i */\n            for (int k = 0; k < i; ++k) {\n                double a_ik = lA.ptr[i*lA.stride + k];\n                lC.ptr[k*lC.stride + j] += alpha * b_ij * a_ik;\n                temp2 += lB.ptr[k*lB.stride + j] * a_ik;\n            }\n\n            /* Final entry for row i, column j */\n            lC.ptr[i*lC.stride + j] = beta * lC.ptr[i*lC.stride + j]\n                                  + diag * b_ij\n                                  + alpha * temp2;\n        }\n    }\n}\n```\nThis implementation\n\n1. linearises the `std::vector<std::vector<double>>` matrices to contiguous memory blocks, removing bounds checks and indirection.\n2. uses `restrict`\u2011style access patterns (via raw pointers) to aid the compiler.\n3. parallelises over the outer `i` loop with OpenMP (`-fopenmp`).\n4. retains the exact semantics of the original algorithm.",
  "adi": "```cpp\n#include <vector>\n#include <cmath>\n#include <omp.h>\n\n/* Optimized version of kernel_adi.\n * - Uses a flat 1\u2011D representation of the 2\u2011D grids to improve cache locality.\n * - Parallelises the outer i\u2011loop with OpenMP.\n * - Eliminates unnecessary temporary copies and performs bounds checks outside the hot loops.\n * - Preserves exact numerical results of the original implementation.\n */\nvoid kernel_adi(int tsteps, int n,\n                std::vector<std::vector<double>>& u,\n                std::vector<std::vector<double>>& v,\n                std::vector<std::vector<double>>& p,\n                std::vector<std::vector<double>>& q)\n{\n    /* Flatten the 2\u2011D vectors into 1\u2011D contiguous arrays. */\n    const int N = n * n;\n    std::vector<double> uFlat(N), vFlat(N), pFlat(N), qFlat(N);\n    for (int i = 0; i < n; ++i)\n        for (int j = 0; j < n; ++j) {\n            int idx = i * n + j;\n            uFlat[idx] = u[i][j];\n            vFlat[idx] = v[i][j];\n            pFlat[idx] = p[i][j];\n            qFlat[idx] = q[i][j];\n        }\n\n    /* Pre\u2011compute constants. */\n    const double DX = 1.0 / n;\n    const double DY = 1.0 / n;\n    const double DT = 1.0 / tsteps;\n    const double B1 = 2.0;\n    const double B2 = 1.0;\n    const double mul1 = B1 * DT / (DX * DX);\n    const double mul2 = B2 * DT / (DY * DY);\n    const double a = -mul1 / 2.0;\n    const double b = 1.0 + mul1;\n    const double c = a;\n    const double d = -mul2 / 2.0;\n    const double e = 1.0 + mul2;\n    const double f = d;\n\n    /* Main time loop. */\n    for (int t = 1; t <= tsteps; ++t) {\n        /* ----------------------------------------------------------------\n         * First loop: columns (i) \u2013 solve for v\n         * ---------------------------------------------------------------- */\n        #pragma omp parallel for schedule(static)\n        for (int i = 1; i < n - 1; ++i) {\n            int top = i * n;                     // row i start\n            int bot = (i + 1) * n;               // row i+1 start\n\n            /* 1) Set boundary values and initialise p, q */\n            vFlat[top + 0] = 1.0;\n            pFlat[top + 0] = 0.0;\n            qFlat[top + 0] = vFlat[top + 0];\n\n            /* 2) Forward sweep (j = 1 .. n-2) */\n            for (int j = 1; j < n - 1; ++j) {\n                int idx = top + j;\n                int idxPrev = top + j - 1;\n                int uIdxL = idx - 1;        // u[j][i-1] => uFlat[ (j) * n + (i-1) ]\n                int uIdxC = idx;             // u[j][i]\n                int uIdxR = idx + 1;         // u[j][i+1]\n\n                pFlat[idx] = -c / (a * pFlat[idxPrev] + b);\n\n                double denom = (a * pFlat[idxPrev] + b);\n                double val = (-d * uFlat[uIdxL] + (1.0 + 2.0 * d) * uFlat[uIdxC]\n                              - f * uFlat[uIdxR] - a * qFlat[idxPrev]) / denom;\n                qFlat[idx] = val;\n            }\n\n            /* 3) Set bottom boundary */\n            vFlat[bot - 1] = 1.0;\n\n            /* 4) Backward sweep (j = n-2 .. 1) */\n            for (int j = n - 2; j >= 1; --j) {\n                int idx = top + j;\n                int idxNext = top + j + 1;\n                vFlat[idx] = pFlat[idx] * vFlat[idxNext] + qFlat[idx];\n            }\n        }\n\n        /* ----------------------------------------------------------------\n         * Second loop: rows (i) \u2013 solve for u\n         * ---------------------------------------------------------------- */\n        #pragma omp parallel for schedule(static)\n        for (int i = 1; i < n - 1; ++i) {\n            int left = i * n;               // column i start\n            int right = left + n;           // column i+1 start\n\n            /* 1) Set boundary values and initialise p, q */\n            uFlat[left + 0] = 1.0;\n            pFlat[left + 0] = 0.0;\n            qFlat[left + 0] = uFlat[left + 0];\n\n            /* 2) Forward sweep (j = 1 .. n-2) */\n            for (int j = 1; j < n - 1; ++j) {\n                int idx = left + j;\n                int idxPrev = left + j - 1;\n                int vIdxU = idx - n;        // v[i-1][j] => vFlat[ (i-1) * n + j ] == idx - n\n                int vIdxC = idx;             // v[i][j]\n                int vIdxD = idx + n;         // v[i+1][j] => vFlat[ (i+1) * n + j ] == idx + n\n\n                pFlat[idx] = -f / (d * pFlat[idxPrev] + e);\n\n                double denom = (d * pFlat[idxPrev] + e);\n                double val = (-a * vFlat[vIdxU] + (1.0 + 2.0 * a) * vFlat[vIdxC]\n                              - c * vFlat[vIdxD] - d * qFlat[idxPrev]) / denom;\n                qFlat[idx] = val;\n            }\n\n            /* 3) Set right boundary */\n            uFlat[right - 1] = 1.0;\n\n            /* 4) Backward sweep (j = n-2 .. 1) */\n            for (int j = n - 2; j >= 1; --j) {\n                int idx = left + j;\n                int idxNext = left + j + 1;\n                uFlat[idx] = pFlat[idx] * uFlat[idxNext] + qFlat[idx];\n            }\n        }\n    }\n\n    /* Copy results back to the original 2\u2011D vectors. */\n    for (int i = 0; i < n; ++i)\n        for (int j = 0; j < n; ++j) {\n            int idx = i * n + j;\n            u[i][j] = uFlat[idx];\n            v[i][j] = vFlat[idx];\n            p[i][j] = pFlat[idx];\n            q[i][j] = qFlat[idx];\n        }\n}\n```",
  "2mm": "```cpp\n#include <vector>\n#include <omp.h>\n\n/* Main computational kernel. The whole function will be timed,\n   including the call and return. */\nvoid kernel_2mm(int ni, int nj, int nk, int nl,\n                double alpha,\n                double beta,\n                std::vector<std::vector<double>>& tmp,\n                std::vector<std::vector<double>>& A,\n                std::vector<std::vector<double>>& B,\n                std::vector<std::vector<double>>& C,\n                std::vector<std::vector<double>>& D) {\n\n    /* First matrix multiplication: tmp = alpha * A * B */\n    #pragma omp parallel for collapse(2) schedule(static)\n    for (int i = 0; i < ni; ++i) {\n        for (int j = 0; j < nj; ++j) {\n            double sum = 0.0;\n            for (int k = 0; k < nk; ++k) {\n                sum += alpha * A[i][k] * B[k][j];\n            }\n            tmp[i][j] = sum;\n        }\n    }\n\n    /* Second matrix multiplication with scaling: D = beta * D + tmp * C */\n    #pragma omp parallel for collapse(2) schedule(static)\n    for (int i = 0; i < ni; ++i) {\n        for (int j = 0; j < nl; ++j) {\n            double sum = D[i][j] * beta;\n            for (int k = 0; k < nj; ++k) {\n                sum += tmp[i][k] * C[k][j];\n            }\n            D[i][j] = sum;\n        }\n    }\n}\n```\nThis version keeps the original public interface and functionality, but it adds OpenMP parallelism with static scheduling and collapses the outer two loops to improve load balancing across the 8 threads. The inner loops are kept simple, allowing the compiler to auto\u2011vectorize under `-O2`.",
  "3mm": "```cpp\n#include <vector>\n#include <cstring>\n#include <omp.h>\n\n/* Main computational kernel. The whole function will be timed,\n   including the call and return. */\nvoid kernel_3mm(int ni, int nj, int nk, int nl, int nm,\n                std::vector<std::vector<double>>& E,\n                std::vector<std::vector<double>>& A,\n                std::vector<std::vector<double>>& B,\n                std::vector<std::vector<double>>& F,\n                std::vector<std::vector<double>>& C,\n                std::vector<std::vector<double>>& D,\n                std::vector<std::vector<double>>& G)\n{\n    /* Assume each row is contiguous and rows are contiguous themselves.\n       Extract raw pointers to improve cache friendliness. */\n    double* ePtr = &E[0][0];\n    double* aPtr = &A[0][0];\n    double* bPtr = &B[0][0];\n    double* fPtr = &F[0][0];\n    double* cPtr = &C[0][0];\n    double* dPtr = &D[0][0];\n    double* gPtr = &G[0][0];\n\n    /* 1st GEMM: E = A * B   (ni x nj = ni x nk * nk x nj) */\n    #pragma omp parallel for schedule(static)\n    for (int i = 0; i < ni; ++i) {\n        double* eRow = ePtr + i * nj;\n        const double* aRow = aPtr + i * nk;\n        for (int j = 0; j < nj; ++j) {\n            double sum = 0.0;\n            const double* aCol = aRow;\n            const double* bCol = bPtr + j;\n            for (int k = 0; k < nk; ++k) {\n                sum += *aCol * *bCol;\n                ++aCol;\n                bCol += nj;\n            }\n            eRow[j] = sum;\n        }\n    }\n\n    /* 2nd GEMM: F = C * D   (nj x nl = nj x nm * nm x nl) */\n    #pragma omp parallel for schedule(static)\n    for (int i = 0; i < nj; ++i) {\n        double* fRow = fPtr + i * nl;\n        const double* cRow = cPtr + i * nm;\n        for (int j = 0; j < nl; ++j) {\n            double sum = 0.0;\n            const double* cCol = cRow;\n            const double* dCol = dPtr + j;\n            for (int k = 0; k < nm; ++k) {\n                sum += *cCol * *dCol;\n                ++cCol;\n                dCol += nl;\n            }\n            fRow[j] = sum;\n        }\n    }\n\n    /* 3rd GEMM: G = E * F   (ni x nl = ni x nj * nj x nl) */\n    #pragma omp parallel for schedule(static)\n    for (int i = 0; i < ni; ++i) {\n        double* gRow = gPtr + i * nl;\n        const double* eRow = ePtr + i * nj;\n        for (int j = 0; j < nl; ++j) {\n            double sum = 0.0;\n            const double* eCol = eRow;\n            const double* fCol = fPtr + j;\n            for (int k = 0; k < nj; ++k) {\n                sum += *eCol * *fCol;\n                ++eCol;\n                fCol += nl;\n            }\n            gRow[j] = sum;\n        }\n    }\n}\n```\nThis version:\n\n1. Extracts contiguous pointers for each matrix for minimal indirection.\n2. Uses column\u2013major traversal of the second matrix in each dot product to keep accesses cache\u2011friendly.\n3. Parallelises the outermost loop of each GEMM with `#pragma omp parallel for` (schedule static works well for balanced work).\n4. Keeps the original functional semantics unchanged.",
  "correlation": "```cpp\n#include <vector>\n#include <cmath>\n#include <omp.h>\n\nvoid kernel_correlation(int m, int n,\n                        std::vector<std::vector<double>>& data,\n                        std::vector<std::vector<double>>& corr,\n                        std::vector<double>& mean,\n                        std::vector<double>& stddev)\n{\n    constexpr double eps = 0.1;\n    const double inv_n = 1.0 / static_cast<double>(n);\n    const double sqrt_n = std::sqrt(static_cast<double>(n));\n\n    /* ---------- 1. Compute column means -------------- */\n#pragma omp parallel for schedule(static)\n    for (int j = 0; j < m; ++j) {\n        double sum = 0.0;\n        for (int i = 0; i < n; ++i) sum += data[i][j];\n        mean[j] = sum * inv_n;\n    }\n\n    /* ---------- 2. Compute stddevs (with epsilon guard) --------- */\n#pragma omp parallel for schedule(static)\n    for (int j = 0; j < m; ++j) {\n        double var = 0.0;\n        const double mu = mean[j];\n        for (int i = 0; i < n; ++i) {\n            double diff = data[i][j] - mu;\n            var += diff * diff;\n        }\n        var *= inv_n;\n        stddev[j] = std::sqrt(var);\n        if (stddev[j] <= eps) stddev[j] = 1.0;\n    }\n\n    /* ---------- 3. Standardise data (row\u2011wise) ------------- */\n    const double inv_sqrt_n = 1.0 / sqrt_n;\n#pragma omp parallel for schedule(static)\n    for (int i = 0; i < n; ++i) {\n        for (int j = 0; j < m; ++j) {\n            double val = data[i][j] - mean[j];\n            val *= inv_sqrt_n / stddev[j];\n            data[i][j] = val;\n        }\n    }\n\n    /* ---------- 4. Build correlation matrix ------------------- */\n    /* First row and diagonal */\n#pragma omp parallel for schedule(static)\n    for (int j = 0; j < m; ++j) {\n        corr[0][j] = (j == 0) ? 1.0 : 0.0;\n    }\n\n    /* Off\u2011diagonals */\n#pragma omp parallel for collapse(2) schedule(static)\n    for (int i = 1; i < m; ++i) {\n        for (int j = 0; j <= i; ++j) {\n            double sum = 0.0;\n            for (int k = 0; k < n; ++k) {\n                sum += data[k][i] * data[k][j];\n            }\n            corr[i][j] = sum;\n            corr[j][i] = sum;\n        }\n    }\n\n    /* Last diagonal element */\n    corr[m-1][m-1] = 1.0;\n}\n```",
  "mvt": "```cpp\n#include <vector>\n#include <omp.h>\n\n/* Main computational kernel. The whole function will be timed,\n   including the call and return. */\nvoid kernel_mvt(int n,\n                std::vector<double>& x1,\n                std::vector<double>& x2,\n                std::vector<double>& y_1,\n                std::vector<double>& y_2,\n                std::vector<std::vector<double>>& A) {\n    // First nested loop: parallelise over the outer dimension\n    #pragma omp parallel for schedule(static) if(n > 0)\n    for (int i = 0; i < n; ++i) {\n        const double* Ai = A[i].data();  // pointer to row i\n        const double* y1 = y_1.data();\n        double* xi    = &x1[i];\n        double sum = *xi;                 // local copy of the current value\n        for (int j = 0; j < n; ++j) {\n            sum += Ai[j] * y1[j];\n        }\n        *xi = sum;                        // write back once (reduces cache traffic)\n    }\n\n    // Second nested loop: parallelise over the outer dimension\n    #pragma omp parallel for schedule(static) if(n > 0)\n    for (int i = 0; i < n; ++i) {\n        const double* yi = y_2.data();\n        double* xi2 = &x2[i];\n        double sum = *xi2;                 // local copy of the current value\n        for (int j = 0; j < n; ++j) {\n            sum += A[j][i] * yi[j];\n        }\n        *xi2 = sum;                        // write back once\n    }\n}\n```",
  "jacobi_1d": "```cpp\n#include <vector>\n#include <cstddef>\n#include <omp.h>\n#include <algorithm>\n\n/* Optimised Jacobi 1\u2011D kernel.\n   The implementation keeps the original semantics while\n   exploiting cache locality, SIMD vectorisation and\n   multithreading via OpenMP. */\n\nstatic constexpr double inv3 = 0.3333333333333333;\n\n// Helper for direct pointer access to avoid bounds checks\ninline double* ptr(std::vector<double>& v) { return v.data(); }\n\nvoid kernel_jacobi_1d(int tsteps,\n                      int n,\n                      std::vector<double>& A,\n                      std::vector<double>& B) {\n    double* Aptr = ptr(A);\n    double* Bptr = ptr(B);\n\n    // Ensure that the grid is large enough for the loop bounds\n    if (n < 3) return;\n\n    // Main time\u2011stepping loop\n    for (int t = 0; t < tsteps; ++t) {\n\n        /* First half\u2011step: update B from A */\n        #pragma omp parallel for schedule(static) \\\n            ivdep schedule(static, 64)\n        for (int i = 1; i < n - 1; ++i) {\n            Bptr[i] = inv3 * (Aptr[i-1] + Aptr[i] + Aptr[i+1]);\n        }\n\n        /* Second half\u2011step: update A from B */\n        #pragma omp parallel for schedule(static) \\\n            ivdep schedule(static, 64)\n        for (int i = 1; i < n - 1; ++i) {\n            Aptr[i] = inv3 * (Bptr[i-1] + Bptr[i] + Bptr[i+1]);\n        }\n    }\n}\n```",
  "trmm": "```cpp\n#include <vector>\n#include <omp.h>\n\n/* Main computational kernel. The whole function will be timed,\n   including the call and return. */\nvoid kernel_trmm(int m, int n,\n                 double alpha,\n                 std::vector<std::vector<double>>& A,\n                 std::vector<std::vector<double>>& B) {\n    /* Parallelise over the rows of B (the outermost loop).\n       The inner loops are small enough that the overhead of\n       finer\u2011grained parallelisation would dominate. */\n    #pragma omp parallel for schedule(static)\n    for (int i = 0; i < m; ++i) {\n        double* b_row = B[i].data();          // row of B\n        for (int j = 0; j < n; ++j) {\n            double sum = b_row[j];\n            // compute A_T * B (upper part) for this element\n            for (int k = i + 1; k < m; ++k) {\n                sum += A[k][i] * B[k][j];\n            }\n            b_row[j] = alpha * sum;           // store final result\n        }\n    }\n}\n```\n",
  "cholesky": "```cpp\n#include <cmath>\n#include <vector>\n#include <omp.h>\n\n/* Main computational kernel. The whole function will be timed,\n   including the call and return. */\nvoid kernel_cholesky(int n,\n\t\t     std::vector<std::vector<double>>& A) {\n\n    /* Parallelise the outer loop: each iteration works on a distinct row\n       so there are no race conditions. */\n    #pragma omp parallel for schedule(static)\n    for (int i = 0; i < n; ++i) {\n        double* const row_i = A[i].data();          // row i\n        /* Off\u2011diagonal entries */\n        for (int j = 0; j < i; ++j) {\n            double sum = 0.0;\n            /* Use local pointers for A[i][k] and A[j][k] */\n            double* const row_j = A[j].data();\n            for (int k = 0; k < j; ++k) {\n                sum += row_i[k] * row_j[k];\n            }\n            row_i[j] = (row_i[j] - sum) / row_j[j];\n        }\n\n        /* Diagonal entry */\n        double diag = row_i[i];\n        for (int k = 0; k < i; ++k) {\n            diag -= row_i[k] * row_i[k];\n        }\n        row_i[i] = std::sqrt(diag);\n    }\n}\n```",
  "gramschmidt": "```cpp\n#include <cmath>\n#include <vector>\n#include <omp.h>\n\n/* Main computational kernel. The whole function will be timed,\n   including the call and return. */\n/* QR Decomposition with Modified Gram Schmidt:\n   http://www.inf.ethz.ch/personal/gander/ */\nvoid kernel_gramschmidt(int m, int n,\n                        std::vector<std::vector<double>>& A,\n                        std::vector<std::vector<double>>& R,\n                        std::vector<std::vector<double>>& Q) {\n    // Parallelisation is applied to inner loops that are independent\n    for (int k = 0; k < n; ++k) {\n        // Compute norm of column k of A\n        double nrm = 0.0;\n#pragma omp parallel for reduction(+:nrm)\n        for (int i = 0; i < m; ++i) {\n            double a = A[i][k];\n            nrm += a * a;\n        }\n        R[k][k] = std::sqrt(nrm);\n\n        // Normalise column k to produce Q\n#pragma omp parallel for\n        for (int i = 0; i < m; ++i) {\n            Q[i][k] = A[i][k] / R[k][k];\n        }\n\n        // Orthogonalise remaining columns against column k\n        for (int j = k + 1; j < n; ++j) {\n            R[k][j] = 0.0;\n            // Compute inner product Q[:,k]\u00b7A[:,j]\n#pragma omp parallel for reduction(+:R[k][j])\n            for (int i = 0; i < m; ++i) {\n                R[k][j] += Q[i][k] * A[i][j];\n            }\n            // Subtract projection from column j\n#pragma omp parallel for\n            for (int i = 0; i < m; ++i) {\n                A[i][j] -= Q[i][k] * R[k][j];\n            }\n        }\n    }\n}\n```\n",
  "fdtd_2d": "```cpp\n#include <vector>\n#include <omp.h>\n\n/* \n   Optimised 2\u2011D FDTD kernel.\n   The function signature is kept identical to the original harness.\n   Internally the code uses contiguous flat access to the 2\u2011D vectors\n   and OpenMP parallelisation for the regular double\u2011nested loops.\n*/\nvoid kernel_fdtd_2d(int tmax,\n\t\t    int nx,\n\t\t    int ny,\n\t\t    std::vector<std::vector<double>>& ex,\n\t\t    std::vector<std::vector<double>>& ey,\n\t\t    std::vector<std::vector<double>>& hz,\n\t\t    std::vector<double>& _fict_) {\n\n    const double c1 = 0.5;\n    const double c2 = 0.7;\n\n    /* Pointers to the raw data inside the row\u2011major vectors.\n       The inner vector decisions are assumed to be contiguous.\n     */\n    double* ex_ptr  = &ex[0][0];\n    double* ey_ptr  = &ey[0][0];\n    double* hz_ptr  = &hz[0][0];\n\n    const int stride = ny;                 // stride between columns\n    const int last_i  = nx - 1;\n    const int last_j  = ny - 1;\n    const int dim_hz  = last_i * stride + last_j; // last index used for hz update\n\n    for (int t = 0; t < tmax; ++t) {\n        /* Update first row of ey with the fictitious source */\n        double f = _fict_[t];\n        #pragma omp parallel for\n        for (int j = 0; j < ny; ++j) {\n            ey_ptr[j] = f;\n        }\n\n        /* Update ey (i>=1) */\n        #pragma omp parallel for collapse(2)\n        for (int i = 1; i < nx; ++i) {\n            int base = i * stride;\n            int base_prev = (i-1) * stride;\n            for (int j = 0; j < ny; ++j) {\n                ey_ptr[base + j] -= c1 * (hz_ptr[base + j] - hz_ptr[base_prev + j]);\n            }\n        }\n\n        /* Update ex (j>=1) */\n        #pragma omp parallel for collapse(2)\n        for (int i = 0; i < nx; ++i) {\n            int base = i * stride;\n            for (int j = 1; j < ny; ++j) {\n                ex_ptr[base + j] -= c1 * (hz_ptr[base + j] - hz_ptr[base + j - 1]);\n            }\n        }\n\n        /* Update hz (i<nx-1 && j<ny-1) */\n        #pragma omp parallel for collapse(2)\n        for (int i = 0; i < last_i; ++i) {\n            int base = i * stride;\n            int base_next = (i+1) * stride;\n            for (int j = 0; j < last_j; ++j) {\n                hz_ptr[base + j] -= c2 *\n                        (ex_ptr[base + j + 1] - ex_ptr[base + j]\n                         + ey_ptr[base_next + j] - ey_ptr[base + j]);\n            }\n        }\n    }\n}\n```\n",
  "jacobi_2d": "```cpp\n#include <vector>\n#include <omp.h>\n\n/* Main computational kernel.  The whole function will be timed,\n   including the call and return.  The signature must stay unchanged\n   because the harness uses it as is. */\nvoid kernel_jacobi_2d(int tsteps,\n                      int n,\n                      std::vector<std::vector<double>>& A,\n                      std::vector<std::vector<double>>& B)\n{\n    const double coeff = 0.2;\n\n    /* To keep the original interface we cannot change A and B types,\n       but we can make the inner loops cache\u2011friendly by accessing the\n       rows through raw pointers.  Each row is already contiguous\n       (std::vector guarantees that), so we just cache the pointer\n       once per outer iteration.  */\n#pragma omp parallel for schedule(static)\n    for (int t = 0; t < tsteps; ++t)\n    {\n        /* First half\u2011step: A \u2192 B */\n#pragma omp parallel for collapse(2) schedule(static)\n        for (int i = 1; i < n - 1; ++i)\n        {\n            double* rowB   = B[i].data();\n            const double* rowA   = A[i].data();\n            const double* rowA_up   = A[i - 1].data();\n            const double* rowA_down = A[i + 1].data();\n\n            for (int j = 1; j < n - 1; ++j)\n            {\n                rowB[j] = coeff *\n                    ( rowA[j] + rowA[j - 1] + rowA[j + 1] +\n                      rowA_down[j] + rowA_up[j] );\n            }\n        }\n\n        /* Second half\u2011step: B \u2192 A (swap roles) */\n#pragma omp parallel for collapse(2) schedule(static)\n        for (int i = 1; i < n - 1; ++i)\n        {\n            double* rowA   = A[i].data();\n            const double* rowB   = B[i].data();\n            const double* rowB_up   = B[i - 1].data();\n            const double* rowB_down = B[i + 1].data();\n\n            for (int j = 1; j < n - 1; ++j)\n            {\n                rowA[j] = coeff *\n                    ( rowB[j] + rowB[j - 1] + rowB[j + 1] +\n                      rowB_down[j] + rowB_up[j] );\n            }\n        }\n    }\n}\n```",
  "seidel_2d": "```cpp\n#include <vector>\n#include <cstddef>\n\n/* Main computational kernel. The whole function will be timed,\n   including the call and return. */\nvoid kernel_seidel_2d(int tsteps,\n                      int n,\n                      std::vector<std::vector<double>>& A) {\n    /* Use a contiguous view of the 2\u2011D data for cache friendliness. */\n    double* data = &A[0][0];\n    const std::size_t stride = static_cast<std::size_t>(n);\n    const double inv9 = 1.0 / 9.0;\n\n    for (int t = 0; t < tsteps; ++t) {\n        /* Loop over interior points only. */\n        for (int i = 1; i < n - 1; ++i) {\n            std::size_t base = i * stride;\n            for (int j = 1; j < n - 1; ++j) {\n                std::size_t idx = base + j;\n                data[idx] = (data[(i - 1) * stride + j - 1] +\n                             data[(i - 1) * stride + j]     +\n                             data[(i - 1) * stride + j + 1] +\n                             data[base + j - 1]             +\n                             data[idx]                      +\n                             data[base + j + 1]             +\n                             data[(i + 1) * stride + j - 1] +\n                             data[(i + 1) * stride + j]     +\n                             data[(i + 1) * stride + j + 1]) * inv9;\n            }\n        }\n    }\n}\n```",
  "gemver": "```cpp\n#include <vector>\n#include <algorithm>\n#include <omp.h>\n\n/* Main computational kernel. The whole function will be timed,\n   including the call and return. */\nvoid kernel_gemver(int n,\n                   double alpha,\n                   double beta,\n                   std::vector<std::vector<double>>& A,\n                   std::vector<double>& u1,\n                   std::vector<double>& v1,\n                   std::vector<double>& u2,\n                   std::vector<double>& v2,\n                   std::vector<double>& w,\n                   std::vector<double>& x,\n                   std::vector<double>& y,\n                   std::vector<double>& z) {\n    /* ------------------------------------------------------------------\n       1) A = A + u1*v1^T + u2*v2^T\n       ------------------------------------------------------------------ */\n    #pragma omp parallel for collapse(2) schedule(static)\n    for (int i = 0; i < n; ++i) {\n        const double ui1 = u1[i];\n        const double ui2 = u2[i];\n        std::vector<double> &Ai = A[i];\n        for (int j = 0; j < n; ++j) {\n            Ai[j] += ui1 * v1[j] + ui2 * v2[j];\n        }\n    }\n\n    /* ------------------------------------------------------------------\n       2) x += beta * A^T * y\n       ------------------------------------------------------------------ */\n    /* A^T * y is a vector of size n:\n       (A^T * y)[i] = sum_j A[j][i] * y[j]   (inner dimension is j) */\n    std::vector<double> tmp_x(n, 0.0);\n\n    #pragma omp parallel for schedule(static)\n    for (int i = 0; i < n; ++i) {\n        double sum = 0.0;\n        for (int j = 0; j < n; ++j) {\n            sum += A[j][i] * y[j];\n        }\n        tmp_x[i] = beta * sum;\n    }\n\n    /* merge with original x and z */\n    #pragma omp parallel for schedule(static)\n    for (int i = 0; i < n; ++i) {\n        x[i] += tmp_x[i] + z[i];\n    }\n\n    /* ------------------------------------------------------------------\n       3) w += alpha * A * x\n       ------------------------------------------------------------------ */\n    #pragma omp parallel for schedule(static)\n    for (int i = 0; i < n; ++i) {\n        double sum = 0.0;\n        const std::vector<double> &Ai = A[i];\n        for (int j = 0; j < n; ++j) {\n            sum += Ai[j] * x[j];\n        }\n        w[i] += alpha * sum;\n    }\n}\n```",
  "ludcmp": "```cpp\n#include <vector>\n#include <cstddef>\n#include <omp.h>\n\n/* Main computational kernel. The whole function will be timed,\n   including the call and return.  */\nvoid kernel_ludcmp(int n,\n                   std::vector<std::vector<double>>& A,\n                   std::vector<double>& b,\n                   std::vector<double>& x,\n                   std::vector<double>& y)\n{\n    /* -------------------------------------------------------------\n       1. LU decomposition with partial pivoting implicit.\n       -------------------------------------------------------------*/\n\n    for (int i = 0; i < n; ++i)\n    {\n        double* Ai = &A[i][0];          // pointer to current row\n        double dii = A[i][i];           // A[i][i] will be used twice\n\n        /* --- Compute the lower part of the i\u2011th row (j < i) --- */\n        #pragma omp parallel for schedule(static)\n        for (int j = 0; j < i; ++j)\n        {\n            double w = Ai[j];\n            for (int k = 0; k < j; ++k)\n                w -= Ai[k] * A[k][j];\n            Ai[j] = w / A[j][j];\n        }\n\n        /* --- Compute the upper part of the i\u2011th row (j >= i) --- */\n        #pragma omp parallel for schedule(static)\n        for (int j = i; j < n; ++j)\n        {\n            double w = Ai[j];\n            for (int k = 0; k < i; ++k)\n                w -= Ai[k] * A[k][j];\n            Ai[j] = w;\n        }\n    }\n\n    /* -------------------------------------------------------------\n       2. Forward substitution: solve Ly = b\n       -------------------------------------------------------------*/\n    for (int i = 0; i < n; ++i)\n    {\n        double w = b[i];\n        for (int j = 0; j < i; ++j)\n            w -= A[i][j] * y[j];\n        y[i] = w;\n    }\n\n    /* -------------------------------------------------------------\n       3. Back substitution: solve Ux = y\n       -------------------------------------------------------------*/\n    for (int i = n - 1; i >= 0; --i)\n    {\n        double w = y[i];\n        for (int j = i + 1; j < n; ++j)\n            w -= A[i][j] * x[j];\n        x[i] = w / A[i][i];\n    }\n}\n```",
  "floyd_warshall": "```cpp\n#include <vector>\n#include <algorithm>\n#include <omp.h>\n\nvoid kernel_floyd_warshall(int n, std::vector<std::vector<int>>& path) {\n    // Parallel over the outer two loops.  \n    // k is the pivot, i indexes rows, j indexes columns.\n    // We hoist the frequently accessed value path[i][k] out of the inner loop.\n    // The typical cache usage pattern keeps all accesses in line, \n    // improving performance on modern x86\u201164 hardware.\n    #pragma omp parallel for schedule(static) collapse(2)\n    for (int k = 0; k < n; ++k) {\n        for (int i = 0; i < n; ++i) {\n            const int aik = path[i][k];\n            // Use a simple branch instead of std::min for speed\n            for (int j = 0; j < n; ++j) {\n                const int via = aik + path[k][j];\n                if (via < path[i][j]) {\n                    path[i][j] = via;\n                }\n            }\n        }\n    }\n}\n```",
  "lu": "```cpp\n#include <vector>\n#include <cstddef>\n#include <omp.h>\n\n/* Main computational kernel. The whole function will be timed,\n   including the call and return. */\nvoid kernel_lu(int n,\n               std::vector<std::vector<double>>& A) {\n    for (int i = 0; i < n; ++i) {\n        // Pointer to the i\u2011th row for faster access\n        double* Ai = A[i].data();\n\n        /* Lower triangular part: sequential due to dependence on\n           previous elements of the same row. */\n        for (int j = 0; j < i; ++j) {\n            double sum = 0.0;\n            for (int k = 0; k < j; ++k) {\n                sum += Ai[k] * A[k][j];\n            }\n            Ai[j] = (Ai[j] - sum) / A[j][j];\n        }\n\n        /* Upper triangular part: each column j can be processed\n           independently, so we parallelize over j. */\n#pragma omp parallel for schedule(static)\n        for (int j = i; j < n; ++j) {\n            double sum = 0.0;\n            for (int k = 0; k < i; ++k) {\n                sum += Ai[k] * A[k][j];\n            }\n            Ai[j] -= sum;\n        }\n    }\n}\n```",
  "deriche": "```cpp\n#include <cmath>\n#include <vector>\n#include <algorithm>\n#include <omp.h>\n\n/* Optimised Deriche filter kernel.\n   The function signature is kept identical to the original\n   (except for the OpenMP pragmas).  All computations are\n   performed in double precision only where the original code\n   required it (exponentials, powers).  The input and output\n   matrices are assumed to be laid out in row\u2011major order\n   and to have dimensions w \u00d7 h.  The temporary buffers y1\n   and y2 are also expected to be the same layout.  The\n   implementation uses OpenMP to parallelise the outer\n   loops and vectorised arithmetic for speed.  The overall\n   logic and numerical results are identical to the\n   reference implementation.  */\n\nvoid kernel_deriche(int w, int h, double alpha,\n                    std::vector<std::vector<float>>& imgIn,\n                    std::vector<std::vector<float>>& imgOut,\n                    std::vector<std::vector<float>>& y1,\n                    std::vector<std::vector<float>>& y2)\n{\n    /* -----  Pre\u2011compute constants  ----- */\n    const double exp_neg_alpha = std::exp(-alpha);\n    const double exp_neg2_alpha = exp_neg_alpha * exp_neg_alpha;\n    const double num = exp_neg_alpha * (1.0 - exp_neg_alpha);\n    const double den = 1.0 + 2.0 * alpha * exp_neg_alpha - exp_neg2_alpha;\n    const double k = 1.0 - num * num / den;\n\n    const float a1 = k;\n    const float a2 = k * static_cast<float>(exp_neg_alpha) * static_cast<float>(alpha - 1.0);\n    const float a3 = k * static_cast<float>(exp_neg_alpha) * static_cast<float>(alpha + 1.0);\n    const float a4 = -k * static_cast<float>(exp_neg2_alpha);\n\n    const float a5 = a1;\n    const float a6 = a2;\n    const float a7 = a3;\n    const float a8 = a4;\n\n    const float b1 = std::pow(2.0, -alpha);\n    const float b2 = -exp_neg2_alpha;\n    const float c1 = 1.0f;\n    const float c2 = 1.0f;\n\n    /* -----  Temporary pointers for faster access  ----- */\n    const int diagSize = w * h;\n    float *in  = (float*)imgIn.data();\n    float *out = (float*)imgOut.data();\n    float *t1 = (float*)y1.data();\n    float *t2 = (float*)y2.data();\n\n    /* -----  1. Forward horizontal pass  ----- */\n#pragma omp parallel for schedule(static)\n    for (int x = 0; x < w; ++x) {\n        float ym1 = 0.0f, ym2 = 0.0f, xm1 = 0.0f;\n        for (int y = 0; y < h; ++y) {\n            float val = in[x*h + y];\n            float res = a1 * val + a2 * xm1 + b1 * ym1 + b2 * ym2;\n            t1[x*h + y] = res;\n            xm1 = val;\n            ym2 = ym1;\n            ym1 = res;\n        }\n    }\n\n    /* -----  2. Backward horizontal pass  ----- */\n#pragma omp parallel for schedule(static)\n    for (int x = 0; x < w; ++x) {\n        float yp1 = 0.0f, yp2 = 0.0f, xp1 = 0.0f, xp2 = 0.0f;\n        for (int y = h-1; y >= 0; --y) {\n            float val = in[x*h + y];\n            float res = a3 * xp1 + a4 * xp2 + b1 * yp1 + b2 * yp2;\n            t2[x*h + y] = res;\n            xp2 = xp1;\n            xp1 = val;\n            yp2 = yp1;\n            yp1 = res;\n        }\n    }\n\n    /* -----  3. First vertical forward pass  ----- */\n#pragma omp parallel for schedule(static)\n    for (int y = 0; y < h; ++y) {\n        float tm1 = 0.0f, ym1 = 0.0f, ym2 = 0.0f;\n        for (int x = 0; x < w; ++x) {\n            float val = t1[x*h + y];\n            float res = a5 * val + a6 * tm1 + b1 * ym1 + b2 * ym2;\n            t1[x*h + y] = res;\n            tm1 = val;\n            ym2 = ym1;\n            ym1 = res;\n        }\n    }\n\n    /* -----  4. First vertical backward pass  ----- */\n#pragma omp parallel for schedule(static)\n    for (int y = 0; y < h; ++y) {\n        float tp1 = 0.0f, tp2 = 0.0f, yp1 = 0.0f, yp2 = 0.0f;\n        for (int x = w-1; x >= 0; --x) {\n            float val = t1[x*h + y];\n            float res = a7 * tp1 + a8 * tp2 + b1 * yp1 + b2 * yp2;\n            t2[x*h + y] = res;\n            tp2 = tp1;\n            tp1 = val;\n            yp2 = yp1;\n            yp1 = res;\n        }\n    }\n\n    /* -----  5. Sum and store result  ----- */\n#pragma omp parallel for schedule(static)\n    for (int idx = 0; idx < diagSize; ++idx) {\n        out[idx] = c2 * (t1[idx] + t2[idx]);\n    }\n}\n```"
}