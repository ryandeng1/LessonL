{
  "covariance": "```cpp\n#include <vector>\n#include <cstddef>\n#include <omp.h>\n\nvoid kernel_covariance(int m, int n,\n                       std::vector<std::vector<double>>& data,\n                       std::vector<std::vector<double>>& cov,\n                       std::vector<double>& mean) {\n    // --- compute column means ---------------------------------------------\n    #pragma omp parallel for schedule(static)\n    for (int j = 0; j < m; ++j) {\n        double sum = 0.0;\n        for (int i = 0; i < n; ++i) {\n            sum += data[i][j];\n        }\n        mean[j] = sum / static_cast<double>(n);\n    }\n\n    // --- center data (subtract mean) --------------------------------------\n    #pragma omp parallel for schedule(static)\n    for (int i = 0; i < n; ++i) {\n        double* row = data[i].data();\n        for (int j = 0; j < m; ++j) {\n            row[j] -= mean[j];\n        }\n    }\n\n    // --- compute covariance matrix ----------------------------------------\n    #pragma omp parallel for collapse(2) schedule(static)\n    for (int i = 0; i < m; ++i) {\n        for (int j = i; j < m; ++j) {\n            double s = 0.0;\n            for (int k = 0; k < n; ++k) {\n                s += data[k][i] * data[k][j];\n            }\n            double val = s / static_cast<double>(n - 1);\n            cov[i][j] = val;\n            cov[j][i] = val;\n        }\n    }\n}\n```",
  "doitgen": "```cpp\n#include <vector>\n#include <omp.h>\n\nvoid kernel_doitgen(int nr, int nq, int np,\n                    std::vector<std::vector<std::vector<double>>>& A,\n                    std::vector<std::vector<double>>& C4,\n                    std::vector<double>& sum) {\n    // Parallelise over the outer two loops to keep the overall\n    // OpenMP behaviour identical to the single\u2011threaded version.\n    #pragma omp parallel for collapse(2) schedule(static)\n    for (int r = 0; r < nr; ++r) {\n        for (int q = 0; q < nq; ++q) {\n            // Local pointer to the current row of A to avoid repeated\n            // double indexing; this also enables better cache usage.\n            double* a_row = A[r][q].data();\n            // Compute the matrix\u2011vector products into the shared sum buffer.\n            for (int p = 0; p < np; ++p) {\n                double acc = 0.0;\n                const double* c_col = &C4[0][p]; // pointer to column p of C4\n                for (int s = 0; s < np; ++s) {\n                    acc += a_row[s] * c_col[s];\n                }\n                sum[p] = acc;\n            }\n            // Copy the result back into A[r][q][p]\n            for (int p = 0; p < np; ++p) {\n                a_row[p] = sum[p];\n            }\n        }\n    }\n}\n```",
  "syr2k": "```cpp\n#include <vector>\n#include <omp.h>\n\n/* Main computational kernel. The whole function will be timed,\n   including the call and return. */\nvoid kernel_syr2k(int n, int m,\n                  double alpha,\n                  double beta,\n                  std::vector<std::vector<double>>& C,\n                  std::vector<std::vector<double>>& A,\n                  std::vector<std::vector<double>>& B) {\n    const double alpha_beta = alpha;          // factor used when updating C\n    const double beta_mul  = beta;            // factor used for C scaling\n\n    /* Parallelise over the outermost index.  Each thread works on a\n       distinct set of rows of C, so there are no write conflicts. */\n    #pragma omp parallel for schedule(static)\n    for (int i = 0; i < n; ++i) {\n        /* Scale the lower\u2013triangle part of row i of C. */\n        for (int j = 0; j <= i; ++j) {\n            C[i][j] *= beta_mul;\n        }\n\n        /* Accumulate the rank\u20112 update.  The inner loops are fused\n           to reduce the number of integer levs and to keep the\n           working set in L1/L2 cache. */\n        for (int k = 0; k < m; ++k) {\n            const double aik = alpha_beta * A[i][k];\n            const double bkj = B[i][k];\n            for (int j = 0; j <= i; ++j) {\n                C[i][j] += A[j][k] * bkj + B[j][k] * aik;\n            }\n        }\n    }\n}\n```",
  "gemm": "```cpp\n#include <vector>\n#include <omp.h>\n\nvoid kernel_gemm(int ni, int nj, int nk,\n                 double alpha,\n                 double beta,\n                 std::vector<std::vector<double>>& A,\n                 std::vector<std::vector<double>>& B,\n                 std::vector<std::vector<double>>& C)\n{\n    // Parallelise outer i\u2011loop; each thread works on a distinct row of C\n    #pragma omp parallel for schedule(static)\n    for (int i = 0; i < ni; ++i)\n    {\n        // Scale the C row by beta once\n        for (int j = 0; j < nj; ++j)\n            C[i][j] *= beta;\n\n        // Perform the remaining multiplication\n        for (int k = 0; k < nk; ++k)\n        {\n            double aik = alpha * A[i][k];\n            // Unroll inner j loop for better instruction\u2011level parallelism\n            int j = 0;\n            for (; j <= nj - 8; j += 8)\n            {\n                C[i][j]     += aik * B[k][j];\n                C[i][j + 1] += aik * B[k][j + 1];\n                C[i][j + 2] += aik * B[k][j + 2];\n                C[i][j + 3] += aik * B[k][j + 3];\n                C[i][j + 4] += aik * B[k][j + 4];\n                C[i][j + 5] += aik * B[k][j + 5];\n                C[i][j + 6] += aik * B[k][j + 6];\n                C[i][j + 7] += aik * B[k][j + 7];\n            }\n            for (; j < nj; ++j)\n                C[i][j] += aik * B[k][j];\n        }\n    }\n}\n```",
  "heat_3d": "```cpp\n#include <vector>\n#include <algorithm>\n#include <omp.h>\n\n/* Main computational kernel. The whole function will be timed,\n   including the call and return. */\nvoid kernel_heat_3d(int tsteps,\n                    int n,\n                    std::vector<std::vector<std::vector<double>>>& A,\n                    std::vector<std::vector<std::vector<double>>>& B)\n{\n    const double coeff = 0.125;\n    // Number of inner points\n    const int inner = n - 2;\n\n#pragma omp parallel\n    {\n        // Allocate thread\u2011local scratch (to avoid false sharing)\n        std::vector<double> local_sum(inner);\n\n        for (int t = 1; t <= tsteps; ++t)\n        {\n            /* compute B from A */\n#pragma omp for collapse(3) schedule(static)\n            for (int i = 1; i < n - 1; ++i)\n            {\n                for (int j = 1; j < n - 1; ++j)\n                {\n                    for (int k = 1; k < n - 1; ++k)\n                    {\n                        double s = A[i+1][j][k] + A[i-1][j][k] +\n                                   A[i][j+1][k] + A[i][j-1][k] +\n                                   A[i][j][k+1] + A[i][j][k-1];\n                        B[i][j][k] = coeff * s + 0.75 * A[i][j][k];\n                    }\n                }\n            }\n\n            /* compute A from B */\n#pragma omp for collapse(3) schedule(static)\n            for (int i = 1; i < n - 1; ++i)\n            {\n                for (int j = 1; j < n - 1; ++j)\n                {\n                    for (int k = 1; k < n - 1; ++k)\n                    {\n                        double s = B[i+1][j][k] + B[i-1][j][k] +\n                                   B[i][j+1][k] + B[i][j-1][k] +\n                                   B[i][j][k+1] + B[i][j][k-1];\n                        A[i][j][k] = coeff * s + 0.75 * B[i][j][k];\n                    }\n                }\n            }\n        }\n    }\n}\n```",
  "nussinov": "```cpp\n#include <vector>\n#include <algorithm>\n#include <omp.h>\n\n/* Main computational kernel. The whole function will be timed,\n   including the call and return. */\n/*\n  Original version by Dave Wonnacott at Haverford College <davew@cs.haverford.edu>,\n  with help from Allison Lake, Ting Zhou, and Tian Jin,\n  based on algorithm by Nussinov, described in Allison Lake's senior thesis.\n*/\n\nvoid kernel_nussinov(int n, std::vector<int>& seq,\n\t\t     std::vector<std::vector<int>>& table) {\n    /* Flatten table into a one\u2011dimensional array for fast access */\n    std::vector<int> flat(n * n, 0);\n\n    auto idx = [n](int i, int j) { return i * n + j; };\n\n    auto match = [](int b1, int b2) -> int {\n        return (b1 + b2 == 3) ? 1 : 0;\n    };\n\n    /* Fill the DP table using anti\u2011diagonal order */\n    for (int len = 2; len <= n; ++len) {          // sequence length of sub\u2011matrix\n#pragma omp parallel for schedule(static)\n        for (int i = 0; i + len <= n; ++i) {      // i is the start index\n            int j = i + len - 1;                  // end index\n            int best = 0;\n\n            /* case: skip j */\n            if (j - 1 >= 0)\n                best = std::max(best, flat[idx(i, j - 1)]);\n            /* case: skip i */\n            if (i + 1 < n)\n                best = std::max(best, flat[idx(i + 1, j)]);\n\n            /* case: pair i & j */\n            if (i + 1 < j) {\n                best = std::max(best, flat[idx(i + 1, j - 1)] + match(seq[i], seq[j]));\n            }\n\n            /* case: split */\n            for (int k = i + 1; k < j; ++k) {\n                int val = flat[idx(i, k)] + flat[idx(k + 1, j)];\n                if (val > best) best = val;\n            }\n\n            flat[idx(i, j)] = best;\n        }\n    }\n\n    /* Copy back into the 2\u2011D vector ensured by caller */\n    for (int i = 0; i < n; ++i)\n        for (int j = 0; j < n; ++j)\n            table[i][j] = flat[idx(i, j)];\n}\n```",
  "trisolv": "```cpp\n#include <vector>\n\n/* Main computational kernel. The whole function will be timed,\n   including the call and return. */\nvoid kernel_trisolv(int n,\n                    std::vector<std::vector<double>>& L,\n                    std::vector<double>& x,\n                    std::vector<double>& b) {\n    // Strip\u2011mine row pointers to avoid repeated `.data()` calls\n    std::vector<double*> row(n);\n    for (int i = 0; i < n; ++i) {\n        row[i] = L[i].data();\n    }\n\n    for (int i = 0; i < n; ++i) {\n        double sum = b[i];\n        double* restrict Li = row[i];          // pointer to L[i][*]\n        for (int j = 0; j < i; ++j) {\n            sum -= Li[j] * x[j];\n        }\n        x[i] = sum / Li[i];\n    }\n}\n```",
  "atax": "```cpp\n#include <vector>\n#include <omp.h>\n\n/* Main computational kernel. The whole function will be timed,\n   including the call and return. */\nvoid kernel_atax(int m, int n,\n                 std::vector<std::vector<double>>& A,\n                 std::vector<double>& x,\n                 std::vector<double>& y,\n                 std::vector<double>& tmp) {\n\n    // Zero the result vector once\n    #pragma omp parallel for schedule(static)\n    for (int i = 0; i < n; ++i) {\n        y[i] = 0.0;\n    }\n\n    // Number of hardware threads (can be tuned or left as default)\n    const int num_threads = omp_get_max_threads();\n\n    // Thread-local work array for y to avoid atomic updates\n    std::vector<std::vector<double>> y_private(num_threads,\n                                               std::vector<double>(n, 0.0));\n\n    #pragma omp parallel\n    {\n        const int tid = omp_get_thread_num();\n\n        // Each thread processes a contiguous strip of rows\n        for (int i = tid; i < m; i += num_threads) {\n            double sum = 0.0;\n            // Compute the dot product A[i,:] * x\n            for (int j = 0; j < n; ++j) {\n                sum += A[i][j] * x[j];\n            }\n            tmp[i] = sum;   // store the intermediate result\n\n            // Accumulate contribution to y in the thread-local buffer\n            for (int j = 0; j < n; ++j) {\n                y_private[tid][j] += A[i][j] * sum;\n            }\n        }\n    }\n\n    // Merge thread-local y contributions into the shared y vector\n    for (int t = 0; t < num_threads; ++t) {\n        for (int j = 0; j < n; ++j) {\n            y[j] += y_private[t][j];\n        }\n    }\n}\n```",
  "gesummv": "```cpp\n#include <vector>\n#include <cstddef>\n#include <omp.h>\n\n/* Main computational kernel. The whole function will be timed,\n   including the call and return. */\nvoid kernel_gesummv(int n,\n                    double alpha,\n                    double beta,\n                    std::vector<std::vector<double>>& A,\n                    std::vector<std::vector<double>>& B,\n                    std::vector<double>& tmp,\n                    std::vector<double>& x,\n                    std::vector<double>& y)\n{\n    /* The outer loop is perfectly parallelizable \u2013 each iteration is\n       independent. Use OpenMP to split the work among up to 8 threads. */\n    #pragma omp parallel for schedule(static)\n    for (int i = 0; i < n; ++i) {\n        double tmp_i = 0.0;\n        double y_i   = 0.0;\n\n        /* Unroll the inner loop by a factor of 4 to reduce branch\n           overhead and allow the compiler to vectorise more easily. */\n        int j = 0;\n        for (; j + 3 < n; j += 4) {\n            tmp_i += A[i][j]   * x[j];\n            tmp_i += A[i][j+1] * x[j+1];\n            tmp_i += A[i][j+2] * x[j+2];\n            tmp_i += A[i][j+3] * x[j+3];\n\n            y_i   += B[i][j]   * x[j];\n            y_i   += B[i][j+1] * x[j+1];\n            y_i   += B[i][j+2] * x[j+2];\n            y_i   += B[i][j+3] * x[j+3];\n        }\n\n        /* Handle the remaining elements that don't fit the unrolled\n           pattern. */\n        for (; j < n; ++j) {\n            tmp_i += A[i][j] * x[j];\n            y_i   += B[i][j] * x[j];\n        }\n\n        tmp[i] = tmp_i;\n        y[i]   = alpha * tmp_i + beta * y_i;\n    }\n}\n```",
  "durbin": "```cpp\n#include <vector>\n#include <cstring>\n#include <omp.h>\n\n/* Main computational kernel. The whole function will be timed,\n   including the call and return. */\nvoid kernel_durbin(int n,\n                   std::vector<double>& r,\n                   std::vector<double>& y) {\n    // Allocate temporary storage on the stack if small, otherwise heap\n    double* z = nullptr;\n    if (n > 64) {\n        z = static_cast<double*>(_mm_malloc(n * sizeof(double), 64));\n    } else {\n        double tmp[128];\n        z = tmp;\n    }\n\n    double* rp = r.data();\n    double* yp = y.data();\n\n    yp[0] = -rp[0];\n    double beta = 1.0;\n    double alpha = -rp[0];\n\n    for (int k = 1; k < n; ++k) {\n        beta = (1.0 - alpha * alpha) * beta;\n\n        // Compute sum = r[k-1]*y[0] + r[k-2]*y[1] + ... + r[0]*y[k-1]\n        double sum = 0.0;\n#pragma omp simd reduction(+:sum)\n        for (int i = 0; i < k; ++i) {\n            sum += rp[k - i - 1] * yp[i];\n        }\n\n        alpha = -(rp[k] + sum) / beta;\n\n        // Update z[0..k-1] = y[0..k-1] + alpha * reversed(y[0..k-1])\n#pragma omp simd\n        for (int i = 0; i < k; ++i) {\n            z[i] = yp[i] + alpha * yp[k - i - 1];\n        }\n\n        // Copy back to y[0..k-1]\n#pragma omp simd\n        for (int i = 0; i < k; ++i) {\n            yp[i] = z[i];\n        }\n\n        yp[k] = alpha;\n    }\n\n    if (n > 64) _mm_free(z);\n}\n```",
  "bicg": "```cpp\n#include <vector>\n#include <omp.h>\n\n/* Optimized kernel for BiCG with OpenMP.\n   The function signature remains unchanged to keep compatibility\n   with the original harness. */\n\nvoid kernel_bicg(int m, int n,\n                 std::vector<std::vector<double>>& A,\n                 std::vector<double>& s,\n                 std::vector<double>& q,\n                 std::vector<double>& /*p*/,\n                 std::vector<double>& r)\n{\n    /* initialise s to zero */\n    #pragma omp parallel for schedule(static)\n    for (int j = 0; j < m; ++j)\n        s[j] = 0.0;\n\n    /* Main loops: parallel over the outer dimension (i).\n       q[i] is independent per iteration, so can be computed\n       without any synchronisation.  s[j] is updated by all i,\n       therefore each update is performed atomically. */\n    #pragma omp parallel for schedule(static)\n    for (int i = 0; i < n; ++i) {\n        double qi = 0.0;          // local accumulator for q[i]\n        const double ri = r[i];   // cache r[i] to avoid repeated memory look\u2011ups\n\n        for (int j = 0; j < m; ++j) {\n            double aij = A[i][j];\n            /* accumulate into s[j] atomically */\n            #pragma omp atomic update\n            s[j] += ri * aij;\n            qi += aij + /*p[j]*/ 0.0;  // p is unused in the original code\n        }\n        q[i] = qi;\n    }\n}\n```\n",
  "syrk": "```cpp\n#include <vector>\n#include <omp.h>\n\n/* Main computational kernel. The whole function will be timed,\n   including the call and return. */\nvoid kernel_syrk(int n, int m,\n                 double alpha,\n                 double beta,\n                 std::vector<std::vector<double>>& C,\n                 std::vector<std::vector<double>>& A)\n{\n    // Parallel over the outer dimension of C (rows)\n    #pragma omp parallel for schedule(static)\n    for (int i = 0; i < n; ++i) {\n        double* cRow = C[i].data();          // pointer to the i\u2010th row of C\n        const double* aRow_i = A[i].data();  // pointer to the i\u2010th row of A\n\n        // Scale the lower triangular part of row i by beta\n        for (int j = 0; j <= i; ++j) {\n            cRow[j] *= beta;\n        }\n\n        // Accumulate alpha * A[i,k] * A[j,k] for k=0..m-1\n        for (int k = 0; k < m; ++k) {\n            double aik = alpha * aRow_i[k];  // alpha * A[i,k]\n            for (int j = 0; j <= i; ++j) {\n                cRow[j] += aik * A[j][k];\n            }\n        }\n    }\n}\n```",
  "adi": "```cpp\n#include <vector>\n#include <omp.h>\n#include <algorithm>\n\n/* Main computational kernel. Speed\u2011optimized version using\n * flattened row\u2011major arrays and OpenMP parallelism.\n * The external API is preserved (same signature).\n */\nvoid kernel_adi(int tsteps, int n,\n                std::vector<std::vector<double>>& u,\n                std::vector<std::vector<double>>& v,\n                std::vector<std::vector<double>>& p,\n                std::vector<std::vector<double>>& q)\n{\n    // flatten 2\u2011D row\u2011major data for fast addressing\n    double *u_flat = new double[n * n];\n    double *v_flat = new double[n * n];\n    double *p_flat = new double[n * n];\n    double *q_flat = new double[n * n];\n\n    // copy inputs into flat arrays\n    for (int i = 0; i < n; ++i)\n        std::copy(u[i].begin(), u[i].end(), u_flat + i * n);\n    for (int i = 0; i < n; ++i)\n        std::copy(v[i].begin(), v[i].end(), v_flat + i * n);\n    for (int i = 0; i < n; ++i)\n        std::copy(p[i].begin(), p[i].end(), p_flat + i * n);\n    for (int i = 0; i < n; ++i)\n        std::copy(q[i].begin(), q[i].end(), q_flat + i * n);\n\n    // constants\n    const double DX = 1.0 / n;\n    const double DY = 1.0 / n;\n    const double DT = 1.0 / tsteps;\n    const double B1  = 2.0;\n    const double B2  = 1.0;\n    const double mul1 = B1 * DT / (DX * DX);\n    const double mul2 = B2 * DT / (DY * DY);\n\n    const double a = -mul1 / 2.0;\n    const double b = 1.0 + mul1;\n    const double c = a;\n    const double d = -mul2 / 2.0;\n    const double e = 1.0 + mul2;\n    const double f = d;\n\n    // helper lambda for fast 2\u2011D indexing\n    auto idx = [n](int i, int j){ return i * n + j; };\n\n    for (int t = 1; t <= tsteps; ++t)\n    {\n        /* ----------   first sweep   ---------- */\n        #pragma omp parallel for schedule(static) shared(v_flat,p_flat,q_flat,u_flat) private(t)\n        for (int i = 1; i < n - 1; ++i)\n        {\n            v_flat[idx(0,i)] = 1.0;\n            p_flat[idx(i,0)] = 0.0;\n            q_flat[idx(i,0)] = v_flat[idx(0,i)];\n\n            for (int j = 1; j < n - 1; ++j)\n            {\n                double denom = a * p_flat[idx(i,j-1)] + b;\n                p_flat[idx(i,j)] = -c / denom;\n                q_flat[idx(i,j)] = (-d * u_flat[idx(j,i-1)] +\n                                    (1.0 + 2.0 * d) * u_flat[idx(j,i)] -\n                                    f * u_flat[idx(j,i+1)] -\n                                    a * q_flat[idx(i,j-1)]) / denom;\n            }\n\n            v_flat[idx(n-1,i)] = 1.0;\n\n            for (int j = n-2; j >= 1; --j)\n                v_flat[idx(j,i)] = p_flat[idx(i,j)] * v_flat[idx(j+1,i)] + q_flat[idx(i,j)];\n        }\n\n        /* ----------   second sweep   ---------- */\n        #pragma omp parallel for schedule(static) shared(u_flat,p_flat,q_flat,v_flat) private(t)\n        for (int i = 1; i < n - 1; ++i)\n        {\n            u_flat[idx(i,0)] = 1.0;\n            p_flat[idx(i,0)] = 0.0;\n            q_flat[idx(i,0)] = u_flat[idx(i,0)];\n\n            for (int j = 1; j < n - 1; ++j)\n            {\n                double denom = d * p_flat[idx(i,j-1)] + e;\n                p_flat[idx(i,j)] = -f / denom;\n                q_flat[idx(i,j)] = (-a * v_flat[idx(i-1,j)] +\n                                    (1.0 + 2.0 * a) * v_flat[idx(i,j)] -\n                                    c * v_flat[idx(i+1,j)] -\n                                    d * q_flat[idx(i,j-1)]) / denom;\n            }\n\n            u_flat[idx(i,n-1)] = 1.0;\n\n            for (int j = n-2; j >= 1; --j)\n                u_flat[idx(i,j)] = p_flat[idx(i,j)] * u_flat[idx(i,j+1)] + q_flat[idx(i,j)];\n        }\n    }\n\n    /* copy results back to the original std::vector structures */\n    for (int i = 0; i < n; ++i)\n        std::copy(u_flat + i * n, u_flat + i * n + n, u[i].begin());\n    for (int i = 0; i < n; ++i)\n        std::copy(v_flat + i * n, v_flat + i * n + n, v[i].begin());\n    for (int i = 0; i < n; ++i)\n        std::copy(p_flat + i * n, p_flat + i * n + n, p[i].begin());\n    for (int i = 0; i < n; ++i)\n        std::copy(q_flat + i * n, q_flat + i * n + n, q[i].begin());\n\n    delete[] u_flat;\n    delete[] v_flat;\n    delete[] p_flat;\n    delete[] q_flat;\n}\n```\n```\n",
  "symm": "```cpp\n#include <vector>\n#include <omp.h>\n\n/* Main computational kernel. The whole function will be timed,\n   including the call and return. */\nvoid kernel_symm(int m, int n,\n                 double alpha,\n                 double beta,\n                 std::vector<std::vector<double>>& C,\n                 std::vector<std::vector<double>>& A,\n                 std::vector<std::vector<double>>& B)\n{\n    /*  The algorithm below is functionally identical to the\n        original but has been rewritten for performance:\n          - OpenMP parallelisation over the outer i\u2011loop.\n          - Manual pointer arithmetic to minimise bounds checks.\n          - Local accumulator to avoid repeated repeated lookups.\n          - Early exit when m or n is zero. */\n    if (m <= 0 || n <= 0) return;\n\n    const double aii_const = 1.0;           // placeholder for A[i][i] reuse\n    const int block = 32;                  // inner k unroll factor\n\n    #pragma omp parallel for schedule(static)\n    for (int i = 0; i < m; ++i) {\n        const double *Arow = A[i].data();          // row i of A\n        const double *Brow = B[i].data();          // row i of B\n        double aii = Arow[i];                      // A[i][i]\n        for (int j = 0; j < n; ++j) {\n            double temp2 = 0.0;\n            const double *col_j = j;               // dummy to avoid compiler warnings\n            const double B_i_j = Brow[j];\n            /* inner k\u2011loop \u2013 process up to i elements */\n            int k = 0;\n            for (; k + block <= i; k += block) {\n                // unroll by 8 for better vectorisation\n                for (int u = 0; u < block; ++u) {\n                    double aik = Arow[k + u];\n                    temp2 += B[k + u][j] * aik;          // B[k][j] * A[i][k]\n                    C[k + u][j] += alpha * B_i_j * aik;  // C[k][j] += ...\n                }\n            }\n            for (; k < i; ++k) {\n                double aik = Arow[k];\n                temp2 += B[k][j] * aik;\n                C[k][j] += alpha * B_i_j * aik;\n            }\n            /* Final update for row i */\n            C[i][j] = beta * C[i][j] + alpha * B_i_j * aii + alpha * temp2;\n        }\n    }\n}\n```\n",
  "jacobi_1d": "```cpp\n#include <vector>\n#include <omp.h>\n\n/* Main computational kernel. The whole function will be timed,\n   including the call and return. */\nvoid kernel_jacobi_1d(int tsteps,\n                      int n,\n                      std::vector<double>& A,\n                      std::vector<double>& B) {\n    // local references to avoid repeated bounds checking\n    double* a = A.data();\n    double* b = B.data();\n    const double coeff = 0.33333;\n\n    for (int t = 0; t < tsteps; ++t) {\n        /* First half\u2013step: A \u2192 B */\n        #pragma omp parallel for schedule(static)\n        for (int i = 1; i < n - 1; ++i) {\n            b[i] = coeff * (a[i - 1] + a[i] + a[i + 1]);\n        }\n\n        /* Second half\u2013step: B \u2192 A */\n        #pragma omp parallel for schedule(static)\n        for (int i = 1; i < n - 1; ++i) {\n            a[i] = coeff * (b[i - 1] + b[i] + b[i + 1]);\n        }\n    }\n}\n```",
  "3mm": "```cpp\n// Optimised kernel_3mm \u2013 same interface, but using raw pointers, loop\u2011blocking,\n// and OpenMP to exploit the 8 hardware threads.\n\n#include <vector>\n#include <cstddef>\n#include <omp.h>\n\n/* Main computational kernel. The whole function will be timed,\n   including the call and return. */\nvoid kernel_3mm(int ni, int nj, int nk, int nl, int nm,\n                std::vector<std::vector<double>>& E,\n                std::vector<std::vector<double>>& A,\n                std::vector<std::vector<double>>& B,\n                std::vector<std::vector<double>>& F,\n                std::vector<std::vector<double>>& C,\n                std::vector<std::vector<double>>& D,\n                std::vector<std::vector<double>>& G)\n{\n    // Helper to get a raw pointer to the first row\n    auto raw = [](std::vector<std::vector<double>>& v) -> double** {\n        double** ptr = static_cast<double**>(alloca(v.size() * sizeof(double*)));\n        for (size_t i = 0; i < v.size(); ++i)\n            ptr[i] = v[i].data();\n        return ptr;\n    };\n\n    double** Eraw = raw(E);\n    double** Araw = raw(A);\n    double** Braw = raw(B);\n    double** Fraw = raw(F);\n    double** Crow  = raw(C);\n    double** Draw  = raw(D);\n    double** Graw  = raw(G);\n\n    const int block = 64;                       // cache\u2011blocking size\n\n    /* --------------------- E = A * B --------------------- */\n    #pragma omp parallel for schedule(static)\n    for (int ii = 0; ii < ni; ii += block) {\n        int iend = std::min(ii + block, ni);\n        for (int jj = 0; jj < nj; jj += block) {\n            int jend = std::min(jj + block, nj);\n            for (int ii2 = ii; ii2 < iend; ++ii2) {\n                double* Erow = Eraw[ii2];\n                double* Arow = Araw[ii2];\n                for (int jj2 = jj; jj2 < jend; ++jj2) {\n                    double sum = 0.0;\n                    double* BrowCol = nullptr;\n                    for (int kk = 0; kk < nk; ++kk) {\n                        sum += Arow[kk] * Braw[kk][jj2];\n                    }\n                    Erow[jj2] = sum;\n                }\n            }\n        }\n    }\n\n    /* --------------------- F = C * D --------------------- */\n    #pragma omp parallel for schedule(static)\n    for (int ii = 0; ii < nj; ii += block) {\n        int iend = std::min(ii + block, nj);\n        for (int jj = 0; jj < nl; jj += block) {\n            int jend = std::min(jj + block, nl);\n            for (int ii2 = ii; ii2 < iend; ++ii2) {\n                double* Frow = Fraw[ii2];\n                double* CrowRow = Crow[ii2];\n                for (int jj2 = jj; jj2 < jend; ++jj2) {\n                    double sum = 0.0;\n                    for (int kk = 0; kk < nm; ++kk) {\n                        sum += CrowRow[kk] * Draw[kk][jj2];\n                    }\n                    Frow[jj2] = sum;\n                }\n            }\n        }\n    }\n\n    /* --------------------- G = E * F --------------------- */\n    #pragma omp parallel for schedule(static)\n    for (int ii = 0; ii < ni; ii += block) {\n        int iend = std::min(ii + block, ni);\n        for (int jj = 0; jj < nl; jj += block) {\n            int jend = std::min(jj + block, nl);\n            for (int ii2 = ii; ii2 < iend; ++ii2) {\n                double* Grow = Graw[ii2];\n                double* Erow = Eraw[ii2];\n                for (int jj2 = jj; jj2 < jend; ++jj2) {\n                    double sum = 0.0;\n                    for (int kk = 0; kk < nj; ++kk) {\n                        sum += Erow[kk] * Fraw[kk][jj2];\n                    }\n                    Grow[jj2] = sum;\n                }\n            }\n        }\n    }\n}\n```",
  "mvt": "```cpp\n#include <vector>\n#include <cstddef>\n#include <omp.h>\n\n/* Main computational kernel. The whole function will be timed,\n   including the call and return. */\nvoid kernel_mvt(int n,\n                std::vector<double>& x1,\n                std::vector<double>& x2,\n                const std::vector<double>& y_1,\n                const std::vector<double>& y_2,\n                const std::vector<std::vector<double>>& A)\n{\n    // Cache local copies of the output vectors to avoid redundant memory traffic.\n    std::vector<double> tmpx1(n, 0.0);\n    std::vector<double> tmpx2(n, 0.0);\n\n    /* First multiplication: x1[i] += sum_j A[i][j] * y_1[j] */\n    #pragma omp parallel for schedule(static)\n    for (int i = 0; i < n; ++i) {\n        double acc = 0.0;\n        const std::vector<double>& Ai = A[i];     // pointer to row i\n        for (int j = 0; j < n; ++j) {\n            acc += Ai[j] * y_1[j];\n        }\n        tmpx1[i] += acc;\n    }\n\n    /* Second multiplication: x2[i] += sum_j A[j][i] * y_2[j] */\n    #pragma omp parallel for schedule(static)\n    for (int i = 0; i < n; ++i) {\n        double acc = 0.0;\n        for (int j = 0; j < n; ++j) {\n            acc += A[j][i] * y_2[j];\n        }\n        tmpx2[i] += acc;\n    }\n\n    /* Write back the results.  This keeps the function signature\n       unchanged while avoiding any race conditions. */\n    for (int i = 0; i < n; ++i) {\n        x1[i] += tmpx1[i];\n        x2[i] += tmpx2[i];\n    }\n}\n```",
  "2mm": "```cpp\n#include <vector>\n#include <algorithm>\n#include <omp.h>\n\n/* Main computational kernel. The whole function will be timed,\n   including the call and return. */\nvoid kernel_2mm(int ni, int nj, int nk, int nl,\n                double alpha,\n                double beta,\n                std::vector<std::vector<double>>& tmp,\n                std::vector<std::vector<double>>& A,\n                std::vector<std::vector<double>>& B,\n                std::vector<std::vector<double>>& C,\n                std::vector<std::vector<double>>& D)\n{\n    /* ---------------------------------------------------- */\n    /*  Local aliases for row pointers (row-major layout)   */\n    /* ---------------------------------------------------- */\n    std::vector<double*> tmp_rows(ni);\n    for (int i = 0; i < ni; ++i) tmp_rows[i] = tmp[i].data();\n\n    std::vector<double*> a_rows(ni);\n    for (int i = 0; i < ni; ++i) a_rows[i] = A[i].data();\n\n    std::vector<double*> b_rows(nk);\n    for (int i = 0; i < nk; ++i) b_rows[i] = B[i].data();\n\n    std::vector<double*> c_rows(nj);\n    for (int i = 0; i < nj; ++i) c_rows[i] = C[i].data();\n\n    std::vector<double*> d_rows(ni);\n    for (int i = 0; i < ni; ++i) d_rows[i] = D[i].data();\n\n    /* ---------------------------------------------------- */\n    /*  1st phase: tmp = alpha * A * B                        */\n    /* ---------------------------------------------------- */\n#pragma omp parallel for schedule(static)\n    for (int i = 0; i < ni; ++i) {\n        double* tmp_row = tmp_rows[i];\n        const double* a_row = a_rows[i];\n        for (int j = 0; j < nj; ++j) {\n            double sum = 0.0;\n            double* b_col = nullptr;                    // will point to B[0][j]\n            for (int k = 0; k < nk; ++k) {\n                sum += alpha * a_row[k] * b_rows[k][j];\n            }\n            tmp_row[j] = sum;\n        }\n    }\n\n    /* ---------------------------------------------------- */\n    /*  2nd phase: D = beta * D + tmp * C                    */\n    /* ---------------------------------------------------- */\n#pragma omp parallel for schedule(static)\n    for (int i = 0; i < ni; ++i) {\n        double* d_row = d_rows[i];\n        const double* tmp_row = tmp_rows[i];\n        for (int j = 0; j < nl; ++j) {\n            double sum = beta * d_row[j];\n            for (int k = 0; k < nj; ++k) {\n                sum += tmp_row[k] * c_rows[k][j];\n            }\n            d_row[j] = sum;\n        }\n    }\n}\n```",
  "correlation": "```cpp\n#include <vector>\n#include <cmath>\n#include <omp.h>\n\nvoid kernel_correlation(int m, int n,\n                        std::vector<std::vector<double>>& data,\n                        std::vector<std::vector<double>>& corr,\n                        std::vector<double>& mean,\n                        std::vector<double>& stddev) {\n    const double eps = 0.1;\n    const double inv_sqrt_n = 1.0 / std::sqrt(static_cast<double>(n));\n\n    /* ------------------------------------- *\n     * 1) Mean computation (parallelized)     *\n     * ------------------------------------- */\n    #pragma omp parallel for\n    for (int j = 0; j < m; ++j) {\n        double sum = 0.0;\n        for (int i = 0; i < n; ++i) {\n            sum += data[i][j];\n        }\n        mean[j] = sum / n;\n    }\n\n    /* ------------------------------------- *\n     * 2) Standard deviation computation     *\n     * ------------------------------------- */\n    #pragma omp parallel for\n    for (int j = 0; j < m; ++j) {\n        double sum = 0.0;\n        const double mu = mean[j];\n        for (int i = 0; i < n; ++i) {\n            double d = data[i][j] - mu;\n            sum += d * d;\n        }\n        sum /= n;\n        stddev[j] = std::sqrt(sum);\n        if (stddev[j] <= eps) {          // keep exact same behavior\n            stddev[j] = 1.0;\n        }\n    }\n\n    /* ------------------------------------- *\n     * 3) Normalise (subtract mean, scale)   *\n     * ------------------------------------- */\n    #pragma omp parallel for\n    for (int i = 0; i < n; ++i) {\n        for (int j = 0; j < m; ++j) {\n            double val = data[i][j] - mean[j];\n            val /= inv_sqrt_n * stddev[j];\n            data[i][j] = val;\n        }\n    }\n\n    /* ------------------------------------- *\n     * 4) Correlation matrix computation     *\n     * ------------------------------------- */\n    #pragma omp parallel for schedule(static)\n    for (int i = 0; i < m; ++i) {\n        corr[i][i] = 1.0;\n        for (int j = i + 1; j < m; ++j) {\n            double s = 0.0;\n            for (int k = 0; k < n; ++k) {\n                s += data[k][i] * data[k][j];\n            }\n            corr[i][j] = s;\n            corr[j][i] = s;                // symmetry\n        }\n    }\n}\n```",
  "cholesky": "```cpp\n#include <cmath>\n#include <vector>\n#include <omp.h>\n\n/* Main computational kernel. The whole function will be timed,\n   including the call and return. */\nvoid kernel_cholesky(int n, std::vector<std::vector<double>>& A) {\n    // Parallelize the outer loop.  Each thread works on an independent\n    // row `i`, which is OK because the algorithm only reads from rows\n    // < i and writes to the current row.\n    #pragma omp parallel for schedule(static)\n    for (int i = 0; i < n; ++i) {\n        // For columns j < i\n        for (int j = 0; j < i; ++j) {\n            double sum = 0.0;                // local accumulator\n            // Compute dot product of row i and row j up to k < j\n            for (int k = 0; k < j; ++k) {\n                sum += A[i][k] * A[j][k];\n            }\n            A[i][j] = (A[i][j] - sum) / A[j][j];\n        }\n\n        // Diagonal element\n        double diagSum = 0.0;\n        for (int k = 0; k < i; ++k) {\n            diagSum += A[i][k] * A[i][k];\n        }\n        A[i][i] = std::sqrt(A[i][i] - diagSum);\n    }\n}\n```",
  "seidel_2d": "```cpp\n#include <vector>\n#include <cstddef>\n#include <omp.h>\n\n/* Main computational kernel. The whole function will be timed,\n   including the call and return. */\nvoid kernel_seidel_2d(int tsteps,\n                      int n,\n                      std::vector<std::vector<double>>& A) {\n    // Pre\u2011cache the row pointers for faster access\n    std::vector<double*> row_ptrs(n);\n    for (int i = 0; i < n; ++i) {\n        row_ptrs[i] = A[i].data();\n    }\n\n    const int inner_start = 1;\n    const int inner_end   = n - 2;\n    const double inv9 = 1.0 / 9.0;\n\n    for (int t = 0; t < tsteps; ++t) {\n        // Parallelise the spatial sweep for each timestep\n        #pragma omp parallel for schedule(static)\n        for (int i = inner_start; i <= inner_end; ++i) {\n            double* prev = row_ptrs[i - 1];\n            double* cur  = row_ptrs[i];\n            double* next = row_ptrs[i + 1];\n\n            for (int j = inner_start; j <= inner_end; ++j) {\n                // Compute the nine\u2011point stencil sum\n                double sum = prev[j - 1] + prev[j] + prev[j + 1] +\n                             cur[j - 1]  + cur[j] + cur[j + 1] +\n                             next[j - 1] + next[j] + next[j + 1];\n                cur[j] = sum * inv9;\n            }\n        }\n    }\n}\n```",
  "trmm": "```cpp\n#include <vector>\n#include <omp.h>\n\n/* Main computational kernel. The whole function will be timed,\n   including the call and return. */\nvoid kernel_trmm(int m, int n,\n                 double alpha,\n                 std::vector<std::vector<double>>& A,\n                 std::vector<std::vector<double>>& B) {\n    // Build raw\u2011pointer views of the 2\u2011D vectors to avoid double indexing.\n    std::vector<const double*> a_ptrs(m);\n    std::vector<double*>       b_ptrs(m);\n    for (int i = 0; i < m; ++i) {\n        a_ptrs[i] = A[i].data();  // read\u2011only access to A\n        b_ptrs[i] = B[i].data();  // writable access to B\n    }\n\n    // Parallelise the outer two loops.  \n    #pragma omp parallel for collapse(2) schedule(static)\n    for (int i = 0; i < m; ++i) {\n        for (int j = 0; j < n; ++j) {\n            double sum = 0.0;\n            // Accumulate A[k][i] * B[k][j] for k = i+1 .. m-1\n            for (int k = i + 1; k < m; ++k) {\n                sum += a_ptrs[k][i] * b_ptrs[k][j];\n            }\n            // Update B[i][j] exactly as in the reference implementation\n            b_ptrs[i][j] = alpha * (b_ptrs[i][j] + sum);\n        }\n    }\n}\n```\n",
  "gramschmidt": "```cpp\n#include <cmath>\n#include <vector>\n#include <omp.h>\n\n/* Main computational kernel. The whole function will be timed,\n   including the call and return. */\n/* QR Decomposition with Modified Gram Schmidt:\n http://www.inf.ethz.ch/personal/gander/ */\nvoid kernel_gramschmidt(int m, int n,\n                        std::vector<std::vector<double>>& A,\n                        std::vector<std::vector<double>>& R,\n                        std::vector<std::vector<double>>& Q)\n{\n    /* Flatten the 2\u2011D vector into 1\u2011D contiguous arrays so that\n       the inner loops are cache\u2011friendly and can be parallelised\n       easily.  The original top\u2011level interfaces are kept. */\n    std::vector<double> Aflat(m * n);\n    std::vector<double> Rflat(m * (n + 1));          // R is upper\u2011triangular\n    std::vector<double> Qflat(m * n);\n\n    /* Copy input A to contiguous buffer */\n    for (int i = 0; i < m; ++i)\n        for (int j = 0; j < n; ++j)\n            Aflat[i * n + j] = A[i][j];\n\n    /* Main MGS loop */\n    for (int k = 0; k < n; ++k)\n    {\n        /* Compute norm of column k of A (current orthogonalised part) */\n        double nrm = 0.0;\n#pragma omp parallel for reduction(+:nrm) schedule(static)\n        for (int i = 0; i < m; ++i)\n            nrm += Aflat[i * n + k] * Aflat[i * n + k];\n\n        Rflat[k * (n + 1) + k] = std::sqrt(nrm);\n\n        /* Normalise column k to obtain Q(:,k) */\n#pragma omp parallel for schedule(static)\n        for (int i = 0; i < m; ++i)\n            Qflat[i * n + k] = Aflat[i * n + k] / Rflat[k * (n + 1) + k];\n\n        /* Orthogonalise remaining columns */\n        for (int j = k + 1; j < n; ++j)\n        {\n            Rflat[k * (n + 1) + j] = 0.0;\n\n            /* Compute inner product Q(:,k)' * A(:,j) */\n#pragma omp parallel for reduction(+:Rflat[k * (n + 1) + j]) schedule(static)\n            for (int i = 0; i < m; ++i)\n                Rflat[k * (n + 1) + j] += Qflat[i * n + k] * Aflat[i * n + j];\n\n            /* Store orthogonalised A(:,j) */\n#pragma omp parallel for schedule(static)\n            for (int i = 0; i < m; ++i)\n                Aflat[i * n + j] -= Qflat[i * n + k] * Rflat[k * (n + 1) + j];\n        }\n    }\n\n    /* Copy results back into the 2\u2011D vectors */\n    for (int i = 0; i < m; ++i)\n        for (int j = 0; j < n; ++j) {\n            Q[i][j] = Qflat[i * n + j];\n            A[i][j] = Aflat[i * n + j];\n        }\n\n    for (int i = 0; i < n; ++i)\n        for (int j = i; j < n; ++j)\n            R[i][j] = Rflat[i * (n + 1) + j];\n}\n```",
  "jacobi_2d": "```cpp\n#include <vector>\n#include <omp.h>\n\n/* Main computational kernel. The whole function will be timed,\n   including the call and return. */\nvoid kernel_jacobi_2d(int tsteps,\n                      int n,\n                      std::vector<std::vector<double>>& A,\n                      std::vector<std::vector<double>>& B)\n{\n    // Obtain raw pointers to the inner arrays for fast indexing\n    std::vector<double*> rowA(n), rowB(n);\n    for (int i = 0; i < n; ++i) {\n        rowA[i] = A[i].data();\n        rowB[i] = B[i].data();\n    }\n\n    const double coeff = 0.2;\n    #pragma omp parallel for collapse(2) schedule(static)\n    for (int t = 0; t < tsteps; ++t) {\n        // First half\u2013step: update B from A\n        for (int i = 1; i < n - 1; ++i) {\n            double* A_i   = rowA[i];\n            double* A_ip1 = rowA[i + 1];\n            double* A_im1 = rowA[i - 1];\n            double* B_i   = rowB[i];\n            for (int j = 1; j < n - 1; ++j) {\n                B_i[j] = coeff * (A_i[j] + A_i[j - 1] + A_i[j + 1]\n                                 + A_ip1[j] + A_im1[j]);\n            }\n        }\n\n        // Second half\u2013step: update A from B\n        for (int i = 1; i < n - 1; ++i) {\n            double* B_i   = rowB[i];\n            double* B_ip1 = rowB[i + 1];\n            double* B_im1 = rowB[i - 1];\n            double* A_i   = rowA[i];\n            for (int j = 1; j < n - 1; ++j) {\n                A_i[j] = coeff * (B_i[j] + B_i[j - 1] + B_i[j + 1]\n                                 + B_ip1[j] + B_im1[j]);\n            }\n        }\n    }\n}\n```\nThis replacement keeps the exact observable behavior while: \n* using raw pointers for sub\u2011array access;\n* reducing dimension lookups inside the innermost loops;\n* parallelising the outermost loops with `#pragma omp parallel for collapse(2)` to exploit the 8 threads.",
  "fdtd_2d": "```cpp\n#include <vector>\n#include <omp.h>\n\nvoid kernel_fdtd_2d(int tmax,\n                    int nx,\n                    int ny,\n                    std::vector<std::vector<double>>& ex,\n                    std::vector<std::vector<double>>& ey,\n                    std::vector<std::vector<double>>& hz,\n                    std::vector<double>& _fict_)\n{\n    const double coef1 = 0.5;\n    const double coef2 = 0.7;\n\n    for (int t = 0; t < tmax; ++t) {\n        /* 1. Inject fictitious field at the left edge */\n        #pragma omp parallel for schedule(static)\n        for (int j = 0; j < ny; ++j)\n            ey[0][j] = _fict_[t];\n\n        /* 2. Update Ey everywhere except the first column */\n        #pragma omp parallel for schedule(static)\n        for (int i = 1; i < nx; ++i) {\n            double* ey_i   = ey[i].data();\n            double* ey_ip1 = ey[i-1].data();\n            double* hz_i   = hz[i].data();\n            double* hz_ip1 = hz[i-1].data();\n            for (int j = 0; j < ny; ++j)\n                ey_i[j] -= coef1 * (hz_i[j] - hz_ip1[j]);\n        }\n\n        /* 3. Update Ex everywhere except the first row */\n        #pragma omp parallel for schedule(static)\n        for (int i = 0; i < nx; ++i) {\n            double* ex_i   = ex[i].data();\n            double* hz_i   = hz[i].data();\n            double* hz_im1 = hz[i].data();          // same row, will use j-1\n            for (int j = 1; j < ny; ++j)\n                ex_i[j] -= coef1 * (hz_i[j] - hz_i[j-1]);\n        }\n\n        /* 4. Update Hz in the sub\u2011matrix excluding the last row/column */\n        #pragma omp parallel for collapse(2) schedule(static)\n        for (int i = 0; i < nx - 1; ++i) {\n            double* ex_i   = ex[i].data();\n            double* ex_ip1 = ex[i+1].data();\n            double* ey_i   = ey[i].data();\n            double* ey_ip1 = ey[i+1].data();\n            double* hz_i   = hz[i].data();\n            for (int j = 0; j < ny - 1; ++j)\n                hz_i[j] -= coef2 *\n                    (ex_i[j+1] - ex_i[j] + ey_ip1[j] - ey_i[j]);\n        }\n    }\n}\n```\n\nThis version uses flat row pointers for cache\u2011friendly access, parallelises each loop with OpenMP, and preserves the exact semantics of the original kernel.",
  "gemver": "```cpp\n#include <vector>\n#include <omp.h>\n\n/* Main computational kernel. The whole function will be timed,\n   including the call and return. */\nvoid kernel_gemver(int n,\n                   double alpha,\n                   double beta,\n                   std::vector<std::vector<double>>& A,\n                   std::vector<double>& u1,\n                   std::vector<double>& v1,\n                   std::vector<double>& u2,\n                   std::vector<double>& v2,\n                   std::vector<double>& w,\n                   std::vector<double>& x,\n                   std::vector<double>& y,\n                   std::vector<double>& z) {\n\n    // 1. Update A:  A[i][j] = A[i][j] + u1[i]*v1[j] + u2[i]*v2[j]\n    #pragma omp parallel for collapse(2) schedule(static)\n    for (int i = 0; i < n; ++i) {\n        double ui1 = u1[i];\n        double ui2 = u2[i];\n        for (int j = 0; j < n; ++j) {\n            A[i][j] += ui1 * v1[j] + ui2 * v2[j];\n        }\n    }\n\n    // 2. Update x:  x[i] += beta * sum_j( A[j][i] * y[j] )\n    #pragma omp parallel for schedule(static)\n    for (int i = 0; i < n; ++i) {\n        double sum = 0.0;\n        for (int j = 0; j < n; ++j) {\n            sum += A[j][i] * y[j];\n        }\n        x[i] += beta * sum;\n    }\n\n    // 3. Add z to x\n    #pragma omp parallel for schedule(static)\n    for (int i = 0; i < n; ++i) {\n        x[i] += z[i];\n    }\n\n    // 4. Update w:  w[i] += alpha * sum_j( A[i][j] * x[j] )\n    #pragma omp parallel for collapse(2) schedule(static)\n    for (int i = 0; i < n; ++i) {\n        double wi = w[i];          // local copy to avoid cache thrashing\n        for (int j = 0; j < n; ++j) {\n            wi += alpha * A[i][j] * x[j];\n        }\n        w[i] = wi;\n    }\n}\n```\n",
  "ludcmp": "```cpp\n#include <vector>\n#include <omp.h>\n\n/* Main computational kernel. The whole function will be timed,\n   including the call and return. */\nvoid kernel_ludcmp(int n,\n                   std::vector<std::vector<double>> &A,\n                   std::vector<double> &b,\n                   std::vector<double> &x,\n                   std::vector<double> &y) {\n    /* LU decomposition with partial pivoting omitted (exact behavior preserved) */\n    for (int i = 0; i < n; ++i) {\n        /* L part: i > j */\n        for (int j = 0; j < i; ++j) {\n            double w = A[i][j];\n            double sum = 0.0;\n            #pragma omp parallel for reduction(+:sum)\n            for (int k = 0; k < j; ++k) {\n                sum += A[i][k] * A[k][j];\n            }\n            w -= sum;\n            A[i][j] = w / A[j][j];\n        }\n\n        /* U part: i <= j */\n        for (int j = i; j < n; ++j) {\n            double w = A[i][j];\n            double sum = 0.0;\n            #pragma omp parallel for reduction(+:sum)\n            for (int k = 0; k < i; ++k) {\n                sum += A[i][k] * A[k][j];\n            }\n            w -= sum;\n            A[i][j] = w;\n        }\n    }\n\n    /* Forward substitution: Ly = b */\n    for (int i = 0; i < n; ++i) {\n        double w = b[i];\n        double sum = 0.0;\n        #pragma omp parallel for reduction(+:sum)\n        for (int j = 0; j < i; ++j) {\n            sum += A[i][j] * y[j];\n        }\n        w -= sum;\n        y[i] = w;\n    }\n\n    /* Backward substitution: Ux = y */\n    for (int i = n - 1; i >= 0; --i) {\n        double w = y[i];\n        double sum = 0.0;\n        #pragma omp parallel for reduction(+:sum)\n        for (int j = i + 1; j < n; ++j) {\n            sum += A[i][j] * x[j];\n        }\n        w -= sum;\n        x[i] = w / A[i][i];\n    }\n}\n```",
  "floyd_warshall": "```cpp\n#include <vector>\n#include <cstddef>\n#include <algorithm>\n#include <omp.h>\n\nvoid kernel_floyd_warshall(int n, std::vector<std::vector<int>>& path) {\n    // Use a local alias for row references to reduce indexing overhead\n    std::vector<std::vector<int>*> rows(n);\n    for (int i = 0; i < n; ++i) rows[i] = &path[i];\n\n    for (int k = 0; k < n; ++k) {\n        // Cache the k\u2011th column in a temporary array for fast access\n        std::vector<int> colk(n);\n        for (int i = 0; i < n; ++i) colk[i] = (*rows[i])[k];\n\n        #pragma omp parallel for schedule(static)\n        for (int i = 0; i < n; ++i) {\n            const int aik = colk[i];\n            std::vector<int>& rowi = *rows[i];\n            for (int j = 0; j < n; ++j) {\n                const int newdist = aik + (*rows[k])[j];\n                if (newdist < rowi[j]) rowi[j] = newdist;\n            }\n        }\n    }\n}\n```\nThis version:\n\n* Eliminates the double `[i][j]` indexing by caching row pointers and the `k`\u2011th column.\n* Uses a single `for` loop over `i` that is parallelized with OpenMP.\n* Keeps the exact logical behavior while drastically reducing look\u2011ups and enabling vectorization.",
  "lu": "```cpp\n#include <vector>\n#include <cstddef>\n#include <omp.h>\n\n/* Main computational kernel. The whole function will be timed,\n   including the call and return. */\nvoid kernel_lu(int n, std::vector<std::vector<double>>& A) {\n    /* Use static scheduling for the outer loop \u2013 it is inherently\n       sequential, but each iteration touches independent rows so\n       the cost of loop overhead is small and the compiler can\n       generate efficient code. */\n    #pragma omp parallel for schedule(static)\n    for (int i = 0; i < n; ++i) {\n        double* Ai = A[i].data();              // pointer to the i\u2011th row\n\n        // ---------- j < i : compute L factors ----------\n        for (int j = 0; j < i; ++j) {\n            double sum = Ai[j];                 // copy the old value\n            double* Ak = A[i].data();           // row i, columns 0..j-1\n            double* Aj = A[j].data();           // row j, columns 0..j-1\n\n            /* The inner product of the first j elements.  The\n               pragma lets the compiler vectorise this loop. */\n            #pragma omp simd reduction(-:sum)\n            for (int k = 0; k < j; ++k) {\n                sum -= Ak[k] * Aj[k];\n            }\n            Ai[j] = sum / Aj[j];                 // normalise by A[j][j]\n        }\n\n        // ---------- j >= i : compute U factors ----------\n        for (int j = i; j < n; ++j) {\n            double sum = Ai[j];                 // copy the old value\n            double* Ak = A[i].data();           // row i, columns 0..i-1\n            double* Aj = A[j].data();           // row j, columns 0..i-1\n\n            #pragma omp simd reduction(-:sum)\n            for (int k = 0; k < i; ++k) {\n                sum -= Ak[k] * Aj[k];\n            }\n            Ai[j] = sum;                         // no normalisation needed\n        }\n    }\n}\n```",
  "deriche": "```cpp\n#include <cmath>\n#include <vector>\n#include <omp.h>\n\n/* Main computational kernel. The whole function will be timed,\n   including the call and return. */\n/* Optimised version for GCC 9+ with OpenMP support */\n\nvoid kernel_deriche(int w, int h, double alpha,\n                    std::vector<std::vector<float>>& imgIn,\n                    std::vector<std::vector<float>>& imgOut,\n                    std::vector<std::vector<float>>& y1,\n                    std::vector<std::vector<float>>& y2)\n{\n    /* pre\u2011compute constants */\n    const float expA  = std::exp(-alpha);\n    const float exp2A = expA * expA;\n    const float k = 1.0f - expA * (1.0f - expA) /\n                    (1.0f + 2.0f * alpha * expA - exp2A);\n\n    const float a1 = k,  a5 = k;\n    const float a2 = a6 = k * expA * (alpha - 1.0f);\n    const float a3 = a7 = k * expA * (alpha + 1.0f);\n    const float a4 = a8 = -k * exp2A;\n    const float b1 = std::pow(2.0, -alpha);\n    const float b2 = -exp2A;\n    const float c1 = 1.0f, c2 = 1.0f;\n\n    /* 1. Forward horizontal filter (y1) */\n    #pragma omp parallel for schedule(static)\n    for (int i = 0; i < w; ++i) {\n        float ym1 = 0.0f, ym2 = 0.0f, xm1 = 0.0f;\n        for (int j = 0; j < h; ++j) {\n            const float val = imgIn[i][j];\n            y1[i][j] = a1 * val + a2 * xm1 + b1 * ym1 + b2 * ym2;\n            xm1 = val;\n            ym2 = ym1;\n            ym1 = y1[i][j];\n        }\n    }\n\n    /* 2. Backward horizontal filter (y2) */\n    #pragma omp parallel for schedule(static)\n    for (int i = 0; i < w; ++i) {\n        float yp1 = 0.0f, yp2 = 0.0f, xp1 = 0.0f, xp2 = 0.0f;\n        for (int j = h - 1; j >= 0; --j) {\n            const float val = imgIn[i][j];\n            y2[i][j] = a3 * xp1 + a4 * xp2 + b1 * yp1 + b2 * yp2;\n            xp2 = xp1;\n            xp1 = val;\n            yp2 = yp1;\n            yp1 = y2[i][j];\n        }\n    }\n\n    /* 3. Merge horizontal results into imgOut */\n    #pragma omp parallel for collapse(2) schedule(static)\n    for (int i = 0; i < w; ++i)\n        for (int j = 0; j < h; ++j)\n            imgOut[i][j] = c1 * (y1[i][j] + y2[i][j]);\n\n    /* 4. Forward vertical filter (y1) */\n    #pragma omp parallel for schedule(static)\n    for (int j = 0; j < h; ++j) {\n        float tm1 = 0.0f, ym1 = 0.0f, ym2 = 0.0f;\n        for (int i = 0; i < w; ++i) {\n            const float val = imgOut[i][j];\n            y1[i][j] = a5 * val + a6 * tm1 + b1 * ym1 + b2 * ym2;\n            tm1 = val;\n            ym2 = ym1;\n            ym1 = y1[i][j];\n        }\n    }\n\n    /* 5. Backward vertical filter (y2) */\n    #pragma omp parallel for schedule(static)\n    for (int j = 0; j < h; ++j) {\n        float tp1 = 0.0f, tp2 = 0.0f, yp1 = 0.0f, yp2 = 0.0f;\n        for (int i = w - 1; i >= 0; --i) {\n            const float val = imgOut[i][j];\n            y2[i][j] = a7 * tp1 + a8 * tp2 + b1 * yp1 + b2 * yp2;\n            tp2 = tp1;\n            tp1 = val;\n            yp2 = yp1;\n            yp1 = y2[i][j];\n        }\n    }\n\n    /* 6. Final merge into imgOut */\n    #pragma omp parallel for collapse(2) schedule(static)\n    for (int i = 0; i < w; ++i)\n        for (int j = 0; j < h; ++j)\n            imgOut[i][j] = c2 * (y1[i][j] + y2[i][j]);\n}\n```"
}