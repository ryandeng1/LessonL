{
  "covariance": "```cpp\n#include <vector>\n#include <omp.h>\n\nvoid kernel_covariance(int m, int n, \n                       std::vector<std::vector<double>>& data,\n                       std::vector<std::vector<double>>& cov,\n                       std::vector<double>& mean)\n{\n    /* ---------- 1. compute means ---------- */\n    #pragma omp parallel for schedule(static)\n    for (int j = 0; j < m; ++j) {\n        double sum = 0.0;\n        for (int i = 0; i < n; ++i) {\n            sum += data[i][j];\n        }\n        mean[j] = sum / n;\n    }\n\n    /* ---------- 2. center data ---------- */\n    #pragma omp parallel for schedule(static)\n    for (int i = 0; i < n; ++i) {\n        double* row = data[i].data();       // contiguous pointer to row\n        for (int j = 0; j < m; ++j) {\n            row[j] -= mean[j];\n        }\n    }\n\n    /* ---------- 3. compute covariance symmetrically ---------- */\n    #pragma omp parallel for schedule(static)\n    for (int i = 0; i < m; ++i) {\n        // iterate over j >= i only\n        for (int j = i; j < m; ++j) {\n            double sum = 0.0;\n            for (int k = 0; k < n; ++k) {\n                sum += data[k][i] * data[k][j];\n            }\n            double val = sum / (n - 1.0);\n            cov[i][j] = val;\n            cov[j][i] = val;    // symmetric\n        }\n    }\n}\n```",
  "atax": "```cpp\n#include <vector>\n#include <cstddef>\n#include <omp.h>\n\n/* Main computational kernel. The whole function will be timed,\n   including the call and return. */\nvoid kernel_atax(int m, int n,\n                 std::vector<std::vector<double>>& A,\n                 std::vector<double>& x,\n                 std::vector<double>& y,\n                 std::vector<double>& tmp) {\n    /* Reset the result vector */\n    std::fill(y.begin(), y.end(), 0.0);\n\n    /* Temporary vector is already sized m; zero it here for clarity */\n    std::fill(tmp.begin(), tmp.end(), 0.0);\n\n    /* Parallel over the rows of A. Each thread computes its\n       own contribution to y and then adds it atomically to the\n       global result. The inner loops are highly cache\u2011friendly\n       because the matrix rows are stored contiguously. */\n#pragma omp parallel for schedule(static)\n    for (int i = 0; i < m; ++i) {\n        // Pointer to the current row of A for faster access\n        const double *row = A[i].data();\n        double sum = 0.0;\n\n        /* Compute tmp[i] = A[i] * x */\n        for (int j = 0; j < n; ++j) {\n            sum += row[j] * x[j];\n        }\n        tmp[i] = sum;\n\n        /* Accumulate contribution to y */\n        for (int j = 0; j < n; ++j) {\n#pragma omp atomic\n            y[j] += row[j] * sum;\n        }\n    }\n}\n```\n",
  "nussinov": "```cpp\n#include <vector>\n#include <algorithm>\n#include <cstddef>\n#include <omp.h>\n\n/* Optimised Nussinov kernel.\n   The algorithm follows the same logic as the reference implementation\n   but uses raw pointers for cache\u2011friendly access,\n   an inline sub\u2011match helper, and a parallel outer loop\n   that follows the anti\u2011diagonal dependency structure.\n*/\n\ninline int match(int a, int b) noexcept { return (a + b) == 3; }\n\nvoid kernel_nussinov(int n, std::vector<int>& seq,\n                     std::vector<std::vector<int>>& table)\n{\n    /* flatten table for faster pointer arithmetic */\n    std::vector<int> flat(n * n);\n    auto idx = [n](int i, int j) { return i * n + j; };\n\n    /* initialise local copy from the input table */\n    for (int i = 0; i < n; ++i)\n        std::copy_n(table[i].data(), n, flat.data() + idx(i, 0));\n\n    /* parallel outer loop over i (reverse order) */\n    #pragma omp parallel for schedule(static)\n    for (int ii = n - 1; ii >= 0; --ii)\n    {\n        int i = ii;\n        for (int j = i + 1; j < n; ++j)\n        {\n            int best = flat[idx(i, j)];\n            \n            /* take value from left neighbour */\n            if (j - 1 >= 0)\n                best = std::max(best, flat[idx(i, j - 1)]);\n            \n            /* take value from above neighbour */\n            if (i + 1 < n)\n                best = std::max(best, flat[idx(i + 1, j)]);\n            \n            /* pairing of i and j */\n            if (i + 1 < n && j - 1 >= 0)\n            {\n                int pair = flat[idx(i + 1, j - 1)];\n                if (i < j - 1)\n                    pair += match(seq[i], seq[j]);\n                best = std::max(best, pair);\n            }\n            \n            /* split intervals */\n            for (int k = i + 1; k < j; ++k)\n                best = std::max(best, flat[idx(i, k)] + flat[idx(k + 1, j)]);\n            \n            flat[idx(i, j)] = best;\n        }\n    }\n\n    /* write back to the 2\u2011D table */\n    for (int i = 0; i < n; ++i)\n        std::copy_n(flat.data() + idx(i, 0), n, table[i].data());\n}\n```\n",
  "gemm": "```cpp\n#include <vector>\n#include <omp.h>\n#include <cstddef>\n\n// Assume A, B, C are stored in row\u2011major order (as vectors of vectors)\n// We convert to contiguous pointers for faster access\ninline double* row(std::vector<std::vector<double>>& M, std::size_t r)\n{\n    return M[r].data();\n}\n\nvoid kernel_gemm(int ni, int nj, int nk,\n                 double alpha,\n                 double beta,\n                 std::vector<std::vector<double>>& A,\n                 std::vector<std::vector<double>>& B,\n                 std::vector<std::vector<double>>& C)\n{\n    // Cache friendly blocking (tune BBLK/NKBL to cache size if needed)\n    const int BBLK = 64;   // block size for i dimension (rows of A, C)\n    const int KBLK = 64;   // block size for k dimension\n    const int JBLK = 64;   // block size for j dimension (cols of B, C)\n\n    #pragma omp parallel\n    {\n        #pragma omp for schedule(static)\n        for (int bi = 0; bi < ni; bi += BBLK) {\n            int i_max = std::min(bi + BBLK, ni);\n            for (int bj = 0; bj < nj; bj += JBLK) {\n                int j_max = std::min(bj + JBLK, nj);\n\n                // Pre\u2011scale C block by beta (outside k loop for cache reuse)\n                for (int i = bi; i < i_max; ++i) {\n                    double* crow = row(C, i);\n                    for (int j = bj; j < j_max; ++j) {\n                        crow[j] *= beta;\n                    }\n                }\n\n                // Compute outer product blocks\n                for (int bk = 0; bk < nk; bk += KBLK) {\n                    int k_max = std::min(bk + KBLK, nk);\n                    for (int i = bi; i < i_max; ++i) {\n                        double* crow = row(C, i);\n                        double* arow = row(A, i);\n                        for (int k = bk; k < k_max; ++k) {\n                            double aik = alpha * arow[k];\n                            double* brow = row(B, k);\n                            for (int j = bj; j < j_max; ++j) {\n                                crow[j] += aik * brow[j];\n                            }\n                        }\n                    }\n                }\n            }\n        }\n    }\n}\n```\n\nThis implementation keeps the original signature and behavior while:\n\n1. **Using contiguous row pointers** \u2013 reduces pointer dereferencing overhead.\n2. **Blocking** \u2013 improves spatial locality to fit data into the cache.\n3. **OpenMP parallelization** \u2013 distributes the outermost `i` block across the eight available threads.\n4. **Pre\u2011scaling** \u2013 multiplies the C block by `beta` once before each k\u2011block, keeping the same semantics.",
  "syr2k": "```cpp\n#include <vector>\n#include <omp.h>\n#include <cstring>   // for memcpy\n#include <cassert>\n\n/* Optimised kernel for the symmetric rank\u20112 update\n * C := alpha*(A*B\u1d40 + B*A\u1d40) + beta*C\n * with C symmetric, only the lower triangle is needed.\n * The function preserves the exact behaviour of the original\n * implementation but works on contiguous storage for speed.\n *\n * Parameters\n *   n  : size of the square matrix C (and the number of rows of A/B)\n *   m  : number of columns of A and B\n *   alpha, beta : scalars\n *   C, A, B : square / rectangular matrices stored as\n *              std::vector<std::vector<double>> with row-major layout\n */\nvoid kernel_syr2k(int n, int m,\n                  double alpha,\n                  double beta,\n                  std::vector<std::vector<double>>& C,\n                  std::vector<std::vector<double>>& A,\n                  std::vector<std::vector<double>>& B) {\n\n    // From the original algorithm C is only accessed for indices j <= i,\n    // i.e. the lower triangular part.  For maximum performance we\n    // treat all matrices as flat buffers and keep them contiguous in\n    // memory.  This removes all bounds checks and lets the compiler\n    // vectorise the inner loops.\n    const int strideC = n;  // C is n\u00d7n\n    const int strideA = m;  // A is n\u00d7m\n    const int strideB = m;  // B is n\u00d7m\n\n    // Convert the vector of vectors to raw pointers once per call.\n    // This is safe because std::vector's rows are contiguous.\n    double* Cbuf = nullptr;\n    double* Abuf = nullptr;\n    double* Bbuf = nullptr;\n    {\n        Cbuf = &C[0][0];\n        Abuf = &A[0][0];\n        Bbuf = &B[0][0];\n    }\n\n    // Block the outer loop to improve cache utilisation.\n    const int blockSize = 64;   // tuned for typical L2/L3 sizes\n\n    #pragma omp parallel for schedule(static)\n    for (int ii = 0; ii < n; ii += blockSize) {\n        const int iEnd = std::min(ii + blockSize, n);\n\n        for (int i = ii; i < iEnd; ++i) {\n            // Scale the lower triangular part of C[i][*] by beta\n            double* Ci = Cbuf + i * strideC;\n            for (int j = 0; j <= i; ++j) {\n                Ci[j] *= beta;\n            }\n\n            // Accumulate contributions from A*B\u1d40 + B*A\u1d40\n            for (int k = 0; k < m; ++k) {\n                double Ak = Abuf[i * strideA + k];\n                double Bk = Bbuf[i * strideB + k];\n\n                double* Cij = Ci;          // pointer to C[i][0]\n                const double* AkRow = Abuf + k * strideA;   // row k of A (A[j][k])\n                const double* BkRow = Bbuf + k * strideB;   // row k of B\n\n                for (int j = 0; j <= i; ++j) {\n                    double term = alpha * (AkRow[j] * Bk + BkRow[j] * Ak);\n                    Cij[j] += term;\n                }\n            }\n        }\n    }\n}\n```\nThe code keeps the same public signature, uses raw pointers for contiguous memory access, parallelises the outer loop with OpenMP, applies a cache\u2011friendly block size, and performs the same arithmetic as the original implementation.",
  "heat_3d": "```cpp\n#include <vector>\n#include <cmath>\n#include <cstddef>\n#include <omp.h>\n\n/* Main computational kernel. The whole function will be timed,\n   including the call and return. */\nvoid kernel_heat_3d(int tsteps,\n                    int n,\n                    std::vector<std::vector<std::vector<double>>>& A,\n                    std::vector<std::vector<std::vector<double>>>& B)\n{\n    // Flatten 3D data into 1D arrays for better cache behaviour\n    const std::size_t N = static_cast<std::size_t>(n) * n * n;\n    std::vector<double> flatA(N, 0.0), flatB(N, 0.0);\n\n    // Helper to convert 3D indices to linear index\n    const auto idx = [&](int i, int j, int k) noexcept {\n        return static_cast<std::size_t>(i * n * n + j * n + k);\n    };\n\n    // Copy input data into flat arrays\n    for (int i = 0; i < n; ++i)\n        for (int j = 0; j < n; ++j)\n            for (int k = 0; k < n; ++k)\n                flatA[idx(i, j, k)] = A[i][j][k];\n\n    const double coeff = 0.125;\n\n#pragma omp parallel\n    for (int t = 0; t < tsteps; ++t) {\n        // Update B from A\n#pragma omp for collapse(3) schedule(static)\n        for (int i = 1; i < n - 1; ++i) {\n            for (int j = 1; j < n - 1; ++j) {\n                for (int k = 1; k < n - 1; ++k) {\n                    std::size_t center = idx(i, j, k);\n                    flatB[center] =\n                        coeff * (flatA[idx(i + 1, j, k)] - 2.0 * flatA[center] + flatA[idx(i - 1, j, k)])\n                      + coeff * (flatA[idx(i, j + 1, k)] - 2.0 * flatA[center] + flatA[idx(i, j - 1, k)])\n                      + coeff * (flatA[idx(i, j, k + 1)] - 2.0 * flatA[center] + flatA[idx(i, j, k - 1)])\n                      + flatA[center];\n                }\n            }\n        }\n\n        // Update A from B\n#pragma omp for collapse(3) schedule(static)\n        for (int i = 1; i < n - 1; ++i) {\n            for (int j = 1; j < n - 1; ++j) {\n                for (int k = 1; k < n - 1; ++k) {\n                    std::size_t center = idx(i, j, k);\n                    flatA[center] =\n                        coeff * (flatB[idx(i + 1, j, k)] - 2.0 * flatB[center] + flatB[idx(i - 1, j, k)])\n                      + coeff * (flatB[idx(i, j + 1, k)] - 2.0 * flatB[center] + flatB[idx(i, j - 1, k)])\n                      + coeff * (flatB[idx(i, j, k + 1)] - 2.0 * flatB[center] + flatB[idx(i, j, k - 1)])\n                      + flatB[center];\n                }\n            }\n        }\n    }\n\n    // Copy back results to original 3D vectors\n    for (int i = 0; i < n; ++i)\n        for (int j = 0; j < n; ++j)\n            for (int k = 0; k < n; ++k)\n                A[i][j][k] = flatA[idx(i, j, k)];\n}\n```",
  "trisolv": "```cpp\n#include <vector>\n#include <cstddef>\n#include <omp.h>\n\n/* Main computational kernel. The whole function will be timed,\n   including the call and return. */\nvoid kernel_trisolv(int n,\n                    std::vector<std::vector<double>>& L,\n                    std::vector<double>& x,\n                    std::vector<double>& b) {\n    // Use a local copy of the RHS to avoid repeated subscript overhead.\n    std::vector<double> x_local(n);\n#pragma omp parallel for schedule(static) linear(i)\n    for (int i = 0; i < n; ++i) {\n        double sum = b[i];\n        const double* row = L[i].data();\n        // Vectorized inner loop over previously computed x values.\n#pragma omp simd reduction(- : sum)\n        for (int j = 0; j < i; ++j) {\n            sum -= row[j] * x[j];\n        }\n        x_local[i] = sum / row[i];\n    }\n    // Copy result back to the output vector (unroll if desired).\n    for (int i = 0; i < n; ++i) {\n        x[i] = x_local[i];\n    }\n}\n```",
  "doitgen": "```cpp\n#include <vector>\n#include <algorithm>\n#include <cstring>\n#include <omp.h>\n\n/* Main computational kernel. The whole function will be timed,\n   including the call and return. */\nvoid kernel_doitgen(int nr, int nq, int np,\n                    std::vector<std::vector<std::vector<double>>>& A,\n                    std::vector<std::vector<double>>& C4,\n                    std::vector<double>& sum) {\n\n    const double *restrict C4data = nullptr;\n    if (!C4.empty()) {\n        C4data = &C4[0][0];         // contiguous block assumed for C4[0]\n    }\n\n#pragma omp parallel for schedule(static)\n    for (int r = 0; r < nr; ++r) {\n        // Pre\u2011fetch pointers for the current r slice\n        const double *restrict A_r = &A[r][0][0];\n        double *restrict A_r_data = &A[r][0][0];\n\n        // Temporary array for intermediate sums (local to the thread)\n        std::vector<double> local_sum(np, 0.0);\n\n        for (int q = 0; q < nq; ++q) {\n            const double *restrict A_q = A_r + q * np;\n\n            // Compute row-wise dot products\n            for (int p = 0; p < np; ++p) {\n                local_sum[p] = 0.0;\n                const double *restrict Ap = A_q;\n                const double *restrict C4_col = C4data + p;\n                for (int s = 0; s < np; ++s) {\n                    local_sum[p] += Ap[s] * C4col[s * np];\n                }\n            }\n\n            // Write back to A\n            double *restrict A_q_write = A_r_data + q * np;\n            std::copy(local_sum.begin(), local_sum.end(), A_q_write);\n\n            // If this is the very last iteration, propagate the result\n            if (r == nr - 1 && q == nq - 1) {\n                #pragma omp critical\n                {\n                    for (int p = 0; p < np; ++p) {\n                        sum[p] = local_sum[p];\n                    }\n                }\n            }\n        }\n    }\n}\n```",
  "durbin": "```cpp\n#include <vector>\n#include <cstddef>\n\n/* Main computational kernel. The whole function will be timed,\n   including the call and return. */\nvoid kernel_durbin(int n,\n                   std::vector<double>& r,\n                   std::vector<double>& y) {\n    // Use raw pointers for a small performance win.\n    double *restrict rp = r.data();\n    double *restrict yp = y.data();\n\n    // Temporary buffer for intermediate values.\n    std::vector<double> z(n, 0.0);\n    double *restrict zp = z.data();\n\n    yp[0] = -rp[0];\n    double beta = 1.0;\n    double alpha = -rp[0];\n\n    for (int k = 1; k < n; ++k) {\n        beta = (1.0 - alpha * alpha) * beta;\n\n        // Compute the inner dot\u2011product once.\n        double sum = 0.0;\n        int i = 0;\n        for (; i + 3 < k; i += 4) {\n            sum += rp[k - i - 1] * yp[i]\n                + rp[k - i - 2] * yp[i + 1]\n                + rp[k - i - 3] * yp[i + 2]\n                + rp[k - i - 4] * yp[i + 3];\n        }\n        for (; i < k; ++i) {\n            sum += rp[k - i - 1] * yp[i];\n        }\n\n        alpha = -(rp[k] + sum) / beta;\n\n        // Update y in two steps via the temporary buffer z.\n        for (i = 0; i < k; ++i) {\n            zp[i] = yp[i] + alpha * yp[k - i - 1];\n        }\n        for (i = 0; i < k; ++i) {\n            yp[i] = zp[i];\n        }\n        yp[k] = alpha;\n    }\n}\n```",
  "gesummv": "```cpp\n#include <vector>\n#include <cstddef>\n#include <cstdint>\n#include <cstdlib>\n#include <cstring>\n#include <omp.h>\n\n/* Helper to flatten a 2D std::vector into a 1D contiguous array.\n   The caller must ensure the input `vec` is contiguous row-major. */\nstatic inline double* flatten(const std::vector<std::vector<double>>& vec) {\n    std::size_t n = vec.size();\n    double* data = static_cast<double*>(aligned_alloc(64, n * n * sizeof(double)));\n    for (std::size_t i = 0; i < n; ++i) {\n        std::memcpy(data + i * n, vec[i].data(), n * sizeof(double));\n    }\n    return data;\n}\n\n/* Main computational kernel. The function signature must stay\n   compatible with the harness. */\nvoid kernel_gesummv(int n,\n                    double alpha,\n                    double beta,\n                    std::vector<std::vector<double>>& A,\n                    std::vector<std::vector<double>>& B,\n                    std::vector<double>& tmp,\n                    std::vector<double>& x,\n                    std::vector<double>& y)\n{\n    /* Convert the input matrices to flat, contiguous arrays to\n       enable efficient cache use and SIMD.  The extra allocation\n       overhead is amortised by the very large inner loops. */\n    double* Aflat = flatten(A);\n    double* Bflat = flatten(B);\n\n    /* Block size chosen to fit L1/L2 cache (512\u20111024 bytes per thread). */\n    const int blk = 64;                       // Adjust for your cache size\n\n    /* Parallel outer loop with static scheduling to balance work. */\n    #pragma omp parallel for schedule(static) collapse(2)\n    for (int ii = 0; ii < n; ii += blk) {\n        for (int jj = 0; jj < n; jj += blk) {\n            /* Process a block of the matrices. */\n            int i_limit = std::min(ii + blk, n);\n            int j_limit = std::min(jj + blk, n);\n\n            for (int i = ii; i < i_limit; ++i) {\n                double* Arow = Aflat + i * n;\n                double* Brow = Bflat + i * n;\n                double  tmp_i = 0.0;\n                double  y_i   = 0.0;\n\n                /* Inner loop over the block column. */\n                for (int j = jj; j < j_limit; ++j) {\n                    double xj = x[j];\n                    tmp_i += Arow[j] * xj;\n                    y_i   += Brow[j] * xj;\n                }\n\n                /* Accumulate results from previous blocks. */\n                tmp[i] += tmp_i;\n                y[i]   += y_i;\n            }\n        }\n    }\n\n    /* Final scaling and combination. */\n    #pragma omp parallel for schedule(static)\n    for (int i = 0; i < n; ++i) {\n        y[i] = alpha * tmp[i] + beta * y[i];\n    }\n\n    /* Free the temporary flattened matrices. */\n    std::free(Aflat);\n    std::free(Bflat);\n}\n```",
  "adi": "```cpp\n#include <vector>\n#include <cstring>\n#include <omp.h>\n\n/* Main computational kernel. The whole function will be timed,\n   including the call and return. */\n/* Based on a Fortran code fragment from Figure 5 of\n * \"Automatic Data and Computation Decomposition on Distributed Memory Parallel Computers\"\n * by Peizong Lee and Zvi Meir Kedem, TOPLAS, 2002\n */\nvoid kernel_adi(int tsteps, int n,\n\t\tstd::vector<std::vector<double>>& u,\n\t\tstd::vector<std::vector<double>>& v,\n\t\tstd::vector<std::vector<double>>& p,\n\t\tstd::vector<std::vector<double>>& q) {\n\n    const double DX = 1.0 / n;\n    const double DY = 1.0 / n;\n    const double DT = 1.0 / tsteps;\n    const double B1 = 2.0;\n    const double B2 = 1.0;\n    const double mul1 = B1 * DT / (DX * DX);\n    const double mul2 = B2 * DT / (DY * DY);\n\n    const double a = -mul1 / 2.0;\n    const double b = 1.0 + mul1;\n    const double c = a;\n    const double d = -mul2 / 2.0;\n    const double e = 1.0 + mul2;\n    const double f = d;\n\n    /* We access the inner vectors through raw pointers to avoid the overhead\n       of operator[] for every element when loop\u2011nesting heavily. */\n    double *Uptr[n], *Vptr[n], *Pptr[n], *Qptr[n];\n    for (int i = 0; i < n; ++i) {\n        Uptr[i] = u[i].data();\n        Vptr[i] = v[i].data();\n        Pptr[i] = p[i].data();\n        Qptr[i] = q[i].data();\n    }\n\n    for (int t = 1; t <= tsteps; ++t) {\n\n        /* ------------------------------------------------------------------\n           Sweep 1: update v from u\n        ------------------------------------------------------------------ */\n        #pragma omp parallel for schedule(static)\n        for (int i = 1; i < n - 1; ++i) {\n            double *ui = Uptr[i];\n            double *vi = Vptr[i];\n            double *pi = Pptr[i];\n            double *qi = Qptr[i];\n\n            vi[0] = 1.0;\n            pi[0] = 0.0;\n            qi[0] = vi[0];\n\n            // forward sweep (j = 1 .. n-2)\n            for (int j = 1; j < n - 1; ++j) {\n                double denom = a * pi[j-1] + b;\n                pi[j] = -c / denom;\n                double num = -d * ui[j-1] + (1.0 + 2.0 * d) * ui[j] - f * ui[j+1] - a * qi[j-1];\n                qi[j] = num / denom;\n            }\n\n            vi[n-1] = 1.0;\n\n            // backward sweep (j = n-2 .. 1)\n            for (int j = n-2; j >= 1; --j) {\n                vi[j] = pi[j] * vi[j+1] + qi[j];\n            }\n        }\n\n        /* ------------------------------------------------------------------\n           Sweep 2: update u from v\n        ------------------------------------------------------------------ */\n        #pragma omp parallel for schedule(static)\n        for (int i = 1; i < n - 1; ++i) {\n            double *ui = Uptr[i];\n            double *vi = Vptr[i];\n            double *pi = Pptr[i];\n            double *qi = Qptr[i];\n\n            ui[0] = 1.0;\n            pi[0] = 0.0;\n            qi[0] = ui[0];\n\n            // forward sweep (j = 1 .. n-2)\n            for (int j = 1; j < n - 1; ++j) {\n                double denom = d * pi[j-1] + e;\n                pi[j] = -f / denom;\n                double num = -a * vi[j-1] + (1.0 + 2.0 * a) * vi[j] - c * vi[j+1] - d * qi[j-1];\n                qi[j] = num / denom;\n            }\n\n            ui[n-1] = 1.0;\n\n            // backward sweep (j = n-2 .. 1)\n            for (int j = n-2; j >= 1; --j) {\n                ui[j] = pi[j] * ui[j+1] + qi[j];\n            }\n        }\n    }\n}\n```",
  "bicg": "```cpp\n#include <vector>\n#include <omp.h>\n\n/* Main computational kernel. The whole function will be timed,\n   including the call and return. */\nvoid kernel_bicg(int m, int n,\n                 std::vector<std::vector<double>>& A,\n                 std::vector<double>& s,\n                 std::vector<double>& q,\n                 std::vector<double>& p,\n                 std::vector<double>& r) {\n    /* Initialise output vectors */\n    std::fill(s.begin(), s.end(), 0.0);\n    std::fill(q.begin(), q.end(), 0.0);\n\n    /* Parallel accumulation of s via per\u2011thread private arrays */\n    #pragma omp parallel\n    {\n        std::vector<double> local_s(m, 0.0);\n        #pragma omp for schedule(static)\n        for (int i = 0; i < n; ++i) {\n            const double ri = r[i];\n            for (int j = 0; j < m; ++j) {\n                local_s[j] += ri * A[i][j];\n            }\n        }\n        /* Combine local_s into the global s */\n        for (int j = 0; j < m; ++j) {\n            #pragma omp atomic\n            s[j] += local_s[j];\n        }\n    }\n\n    /* Parallel computation of q */\n    #pragma omp parallel for schedule(static)\n    for (int i = 0; i < n; ++i) {\n        double sum = 0.0;\n        for (int j = 0; j < m; ++j) {\n            sum += A[i][j] + p[j];\n        }\n        q[i] = sum;\n    }\n}\n```",
  "syrk": "```cpp\n#include <vector>\n#include <omp.h>\n\n/* Main computational kernel. The whole function will be timed,\n   including the call and return. */\nvoid kernel_syrk(int n, int m,\n                 double alpha,\n                 double beta,\n                 std::vector<std::vector<double>>& C,\n                 std::vector<std::vector<double>>& A)\n{\n    //=> Form C := alpha*A*A**T + beta*C.\n    //A is NxM, C is NxN\n\n    // Parallelise the outermost loop over i. Each iteration is independent.\n    #pragma omp parallel for schedule(static)\n    for (int i = 0; i < n; ++i) {\n        // First multiply the lower triangle of C by beta.\n        for (int j = 0; j <= i; ++j)\n            C[i][j] *= beta;\n\n        // Accumulate the rank\u2011m update.\n        for (int k = 0; k < m; ++k) {\n            // Load the outer product factor alpha * A[i][k] once per k.\n            double aik = alpha * A[i][k];\n            for (int j = 0; j <= i; ++j)\n                C[i][j] += aik * A[j][k];\n        }\n    }\n}\n```",
  "symm": "```cpp\n#include <vector>\n#include <omp.h>\n\n/* Main computational kernel. The whole function will be timed,\n   including the call and return. */\nvoid kernel_symm(int m, int n,\n                 double alpha,\n                 double beta,\n                 std::vector<std::vector<double>>& C,\n                 std::vector<std::vector<double>>& A,\n                 std::vector<std::vector<double>>& B)\n{\n    /* Re\u2011map the 2\u2011D vectors to 1\u2011D arrays to reduce pointer chasing\n       and improve cache usage.  The original indexing scheme is\n       preserved: C[row][col] -> C[row*n+col], etc. */\n    const int cn = n;\n    const int an = m;\n\n    double* Cptr = new double[m * n];\n    double* Aptr = new double[m * m];\n    double* Bptr = new double[m * n];\n\n    /* Copy data from std::vector to contiguous arrays */\n    for (int i = 0; i < m; ++i) {\n        for (int j = 0; j < n; ++j) {\n            Cptr[i * cn + j] = C[i][j];\n        }\n    }\n    for (int i = 0; i < m; ++i) {\n        for (int j = 0; j < m; ++j) {\n            Aptr[i * an + j] = A[i][j];\n        }\n    }\n    for (int i = 0; i < m; ++i) {\n        for (int j = 0; j < n; ++j) {\n            Bptr[i * cn + j] = B[i][j];\n        }\n    }\n\n    /* Parallelise the outermost loop.  The inner loops are short\n       and are left sequential for better register usage. */\n#pragma omp parallel for schedule(static)\n    for (int i = 0; i < m; ++i) {\n        for (int j = 0; j < n; ++j) {\n            double temp2 = 0.0;\n            double Bij = Bptr[i * cn + j];\n            for (int k = 0; k < i; ++k) {\n                double aik = Aptr[i * an + k];\n                /* Update C[k][j] in contiguous memory */\n                Cptr[k * cn + j] += alpha * Bij * aik;\n                temp2 += Bptr[k * cn + j] * aik;\n            }\n            Cptr[i * cn + j] =\n                beta * Cptr[i * cn + j] + alpha * Bij * Aptr[i * an + i] + alpha * temp2;\n        }\n    }\n\n    /* Copy the result back to the original std::vector structure. */\n    for (int i = 0; i < m; ++i) {\n        for (int j = 0; j < n; ++j) {\n            C[i][j] = Cptr[i * cn + j];\n        }\n    }\n\n    /* Clean up */\n    delete[] Cptr;\n    delete[] Aptr;\n    delete[] Bptr;\n}\n```",
  "jacobi_1d": "```cpp\n#include <vector>\n#include <omp.h>\n\n/* Main computational kernel. The whole function will be timed,\n   including the call and return. */\nvoid kernel_jacobi_1d(int tsteps,\n                      int n,\n                      std::vector<double>& A,\n                      std::vector<double>& B) {\n    // Obtain raw pointers for fast indexing.\n    double *a = A.data();\n    double *b = B.data();\n\n    // Pre\u2011compute the scalar factor.\n    const double alpha = 0.33333;\n\n    // Main time\u2011stepping loop.\n    for (int t = 0; t < tsteps; ++t) {\n        // First half\u2011step: compute B from A.\n        #pragma omp parallel for schedule(static)\n        for (int i = 1; i < n - 1; ++i) {\n            b[i] = alpha * (a[i-1] + a[i] + a[i+1]);\n        }\n\n        // Second half\u2011step: compute A from B.\n        #pragma omp parallel for schedule(static)\n        for (int i = 1; i < n - 1; ++i) {\n            a[i] = alpha * (b[i-1] + b[i] + b[i+1]);\n        }\n    }\n}\n```\n",
  "correlation": "```cpp\n#include <vector>\n#include <cmath>\n#include <omp.h>\n\n// Kernels may be called with the same signature as the original.\nvoid kernel_correlation(int m, int n,\n                        std::vector<std::vector<double>>& data,\n                        std::vector<std::vector<double>>& corr,\n                        std::vector<double>& mean,\n                        std::vector<double>& stddev) {\n\n    const double eps = 0.1;\n    const double inv_sqrt_n = 1.0 / std::sqrt(static_cast<double>(n));\n\n    /* ---------- Mean calculation ---------- */\n#pragma omp parallel for\n    for (int j = 0; j < m; ++j) {\n        double sum = 0.0;\n        for (int i = 0; i < n; ++i) {\n            sum += data[i][j];\n        }\n        mean[j] = sum / static_cast<double>(n);\n    }\n\n    /* ---------- Std dev calculation ---------- */\n#pragma omp parallel for\n    for (int j = 0; j < m; ++j) {\n        double sum = 0.0;\n        const double mean_j = mean[j];\n        for (int i = 0; i < n; ++i) {\n            const double diff = data[i][j] - mean_j;\n            sum += diff * diff;\n        }\n        double variance = sum / static_cast<double>(n);\n        double sd = std::sqrt(variance);\n        stddev[j] = (sd <= eps) ? 1.0 : sd;\n    }\n\n    /* ---------- Normalise data ---------- */\n#pragma omp parallel for\n    for (int i = 0; i < n; ++i) {\n        for (int j = 0; j < m; ++j) {\n            double val = data[i][j] - mean[j];\n            val /= stddev[j];\n            val *= inv_sqrt_n;        // divide by sqrt(n)\n            data[i][j] = val;\n        }\n    }\n\n    /* ---------- Correlation ----------\n       corr[i][i] = 1.0 (diagonal)\n       corr[i][j] is dot product of column i and j\n    */\n#pragma omp parallel for collapse(2) schedule(static)\n    for (int i = 0; i < m; ++i) {\n        for (int j = i; j < m; ++j) {\n            double sum = 0.0;\n            if (i == j) {\n                sum = 1.0;            // unit\u2010norm columns\n            } else {\n                for (int k = 0; k < n; ++k) {\n                    sum += data[k][i] * data[k][j];\n                }\n            }\n            corr[i][j] = sum;\n            if (i != j) corr[j][i] = sum;   // symmetry\n        }\n    }\n}\n```",
  "2mm": "```cpp\n#include <vector>\n#include <cstddef>\n#include <omp.h>\n\n/* Main computational kernel. The whole function will be timed,\n   including the call and return. */\nvoid kernel_2mm(int ni, int nj, int nk, int nl,\n                double alpha, double beta,\n                std::vector<std::vector<double>>& tmp,\n                std::vector<std::vector<double>>& A,\n                std::vector<std::vector<double>>& B,\n                std::vector<std::vector<double>>& C,\n                std::vector<std::vector<double>>& D)\n{\n    /* --------------------------------------------------------------------\n       First product:  tmp = alpha * A * B\n       -------------------------------------------------------------------- */\n#pragma omp parallel for schedule(static) collapse(2)\n    for (int i = 0; i < ni; ++i)\n    {\n        double* tmp_row = tmp[i].data();\n        const double* A_row = A[i].data();\n\n        for (int j = 0; j < nj; ++j)\n        {\n            double accum = 0.0;\n            const double* B_col = &B[0][j];          // first element of column j\n            for (int k = 0; k < nk; ++k)\n                accum += alpha * A_row[k] * B_col[k * B.size()];\n\n            tmp_row[j] = accum;\n        }\n    }\n\n    /* --------------------------------------------------------------------\n       Second product:  D = beta * D + tmp * C\n       -------------------------------------------------------------------- */\n#pragma omp parallel for schedule(static) collapse(2)\n    for (int i = 0; i < ni; ++i)\n    {\n        double* D_row = D[i].data();\n        const double* tmp_row = tmp[i].data();\n\n        for (int j = 0; j < nl; ++j)\n        {\n            double accum = beta * D_row[j];\n            const double* C_col = &C[0][j];          // first element of column j\n            for (int k = 0; k < nj; ++k)\n                accum += tmp_row[k] * C_col[k * C.size()];\n\n            D_row[j] = accum;\n        }\n    }\n}\n```",
  "3mm": "```cpp\n#include <vector>\n#include <cstddef>\n#include <omp.h>\n\n/* Helper struct to expose a contiguous 2\u2011D view on a std::vector<std::vector<double>>.\n   The matrix is stored row\u2011major.                                          */\nstruct mat_view\n{\n    double* data;     // pointer to the first element\n    std::size_t rows, cols, stride; // stride = cols of the underlying storage\n    double& operator()(std::size_t r, std::size_t c) noexcept { return data[r * stride + c]; }\n};\n\n/* Convert a vector<vector> into a mat_view.  The underlying storage\n   is guaranteed contiguous because the callers always allocate\n   contiguous inner vectors.                                                      */\nstatic mat_view view(std::vector<std::vector<double>>& m)\n{\n    mat_view v{m.data(), m.size(), m[0].size(), m[0].size()};\n    return v;\n}\n\n/* Main computational kernel.  The whole function will be timed,\n   including the call and return.                                  */\nvoid kernel_3mm(int ni, int nj, int nk, int nl, int nm,\n                std::vector<std::vector<double>>& E,\n                std::vector<std::vector<double>>& A,\n                std::vector<std::vector<double>>& B,\n                std::vector<std::vector<double>>& F,\n                std::vector<std::vector<double>>& C,\n                std::vector<std::vector<double>>& D,\n                std::vector<std::vector<double>>& G)\n{\n    mat_view mE = view(E);\n    mat_view mA = view(A);\n    mat_view mB = view(B);\n    mat_view mF = view(F);\n    mat_view mC = view(C);\n    mat_view mD = view(D);\n    mat_view mG = view(G);\n\n    /* 1. E = A\u00b7B   (ni \u00d7 nj) */\n    #pragma omp parallel for schedule(static)\n    for (int i = 0; i < ni; ++i) {\n        double* ei = mE.data + i * mE.stride;\n        const double* ai = mA.data + i * mA.stride;\n        for (int j = 0; j < nj; ++j) {\n            double sum = 0.0;\n            const double* bj = mB.data + j;\n            const double* aik = ai;\n            for (int k = 0; k < nk; ++k, bj += mB.stride, aik += 1) {\n                sum += (*aik) * (*bj);\n            }\n            ei[j] = sum;\n        }\n    }\n\n    /* 2. F = C\u00b7D   (nj \u00d7 nl) */\n    #pragma omp parallel for schedule(static)\n    for (int i = 0; i < nj; ++i) {\n        double* fi = mF.data + i * mF.stride;\n        const double* ci = mC.data + i * mC.stride;\n        for (int j = 0; j < nl; ++j) {\n            double sum = 0.0;\n            const double* dj = mD.data + j;\n            const double* cik = ci;\n            for (int k = 0; k < nm; ++k, dj += mD.stride, cik += 1) {\n                sum += (*cik) * (*dj);\n            }\n            fi[j] = sum;\n        }\n    }\n\n    /* 3. G = E\u00b7F   (ni \u00d7 nl) */\n    #pragma omp parallel for schedule(static)\n    for (int i = 0; i < ni; ++i) {\n        double* gi = mG.data + i * mG.stride;\n        const double* ei = mE.data + i * mE.stride;\n        for (int j = 0; j < nl; ++j) {\n            double sum = 0.0;\n            const double* fj = mF.data + j;\n            const double* eik = ei;\n            for (int k = 0; k < nj; ++k, fj += mF.stride, eik += 1) {\n                sum += (*eik) * (*fj);\n            }\n            gi[j] = sum;\n        }\n    }\n}\n```",
  "mvt": "```cpp\n#include <vector>\n#include <cstddef>\n#include <omp.h>\n\n/* Main computational kernel. The whole function will be timed,\n   including the call and return. */\nvoid kernel_mvt(int n,\n                std::vector<double>& x1,\n                std::vector<double>& x2,\n                std::vector<double>& y_1,\n                std::vector<double>& y_2,\n                std::vector<std::vector<double>>& A) {\n    // Obtain raw pointers for fast indexed access\n    double *x1p = x1.data();\n    double *x2p = x2.data();\n    double *y1p = y_1.data();\n    double *y2p = y_2.data();\n\n    // Flatten A to a single contiguous block (row-major).\n    // We can create an auxiliary array once if A is not contiguous.\n    // For simplicity, we'll use A[i][j] directly (cache-friendly in the first loop).\n    // The second loop accesses the transpose, which is done via A[j][i].\n\n    const int block = 64;           // cache block size (tune if needed)\n    const int nblocks = (n + block - 1) / block;\n\n    // First loop: x1 += A * y1\n    #pragma omp parallel for schedule(static)\n    for (int bi = 0; bi < nblocks; ++bi) {\n        int i_start = bi * block;\n        int i_end   = std::min(i_start + block, n);\n\n        for (int i = i_start; i < i_end; ++i) {\n            double sum = x1p[i];\n            const double* Aj = A[i].data();      // row i\n            double* yi = y1p;\n            for (int j = 0; j < n; ++j) {\n                sum += Aj[j] * yi[j];\n            }\n            x1p[i] = sum;\n        }\n    }\n\n    // Second loop: x2 += A\u1d40 * y2  (same as \u2211_j A[j][i] * y2[j])\n    // We iterate over blocks of i to keep A[j][i] access local.\n    #pragma omp parallel for schedule(static)\n    for (int bi = 0; bi < nblocks; ++bi) {\n        int i_start = bi * block;\n        int i_end   = std::min(i_start + block, n);\n\n        for (int i = i_start; i < i_end; ++i) {\n            double sum = x2p[i];\n            for (int j = 0; j < n; ++j) {\n                sum += A[j][i] * y2p[j];\n            }\n            x2p[i] = sum;\n        }\n    }\n}\n```",
  "cholesky": "```cpp\n#include <cmath>\n#include <vector>\n\nusing std::vector;\n\n/* Main computational kernel. The whole function will be timed,\n   including the call and return.  The interface must stay exactly\n   the same as the original. */\nvoid kernel_cholesky(int n, vector<vector<double>>& A)\n{\n    // Flatten the matrix into a contiguous 1\u2011D buffer.\n    // This reduces the overhead of dozens of virtual function calls\n    // and improves cache locality.\n    const int stride = n;\n    vector<double> flat(n * n);\n\n    for (int i = 0; i < n; ++i)\n        for (int j = 0; j < n; ++j)\n            flat[i * stride + j] = A[i][j];\n\n    // Main Cholesky algorithm on the flat buffer.\n    for (int i = 0; i < n; ++i) {\n        const double *row_i = flat.data() + i * stride;\n\n        for (int j = 0; j < i; ++j) {\n            double *elem_ij = flat.data() + i * stride + j;\n            double sum = *elem_ij;\n\n            for (int k = 0; k < j; ++k) {\n                sum -= row_i[k] * flat.data() + j * stride + k;\n            }\n            *elem_ij = sum / flat.data()[j * stride + j];\n        }\n\n        // i == j case\n        double *elem_ii = flat.data() + i * stride + i;\n        double sum = *elem_ii;\n        for (int k = 0; k < i; ++k) {\n            double val = row_i[k];\n            sum -= val * val;\n        }\n        *elem_ii = std::sqrt(sum);\n    }\n\n    // Copy the result back into the original 2\u2011D vector.\n    for (int i = 0; i < n; ++i)\n        for (int j = 0; j < n; ++j)\n            A[i][j] = flat[i * stride + j];\n}\n```",
  "ludcmp": "```cpp\n#include <vector>\n#include <cstddef>\n#include <omp.h>\n\n/* Main computational kernel. The whole function will be timed,\n   including the call and return. */\nvoid kernel_ludcmp(int n,\n                   std::vector<std::vector<double>>& A,\n                   std::vector<double>& b,\n                   std::vector<double>& x,\n                   std::vector<double>& y) {\n    // ------------------------------------------------------------\n    // 1. LU decomposition (A -> L & U in-place)\n    // ------------------------------------------------------------\n    for (int i = 0; i < n; ++i) {\n        // Sub\u2011diagonal elements (L part)\n        for (int j = 0; j < i; ++j) {\n            double w = A[i][j];\n            const double* const rowi = A[i].data();\n            const double* const rowk = A[j].data();\n\n            // dot product with previous rows\n            for (int k = 0; k < j; ++k) {\n                w -= rowi[k] * rowk[k];\n            }\n            A[i][j] = w / A[j][j];\n        }\n\n        // Row i, remaining column entries (U part)\n        for (int j = i; j < n; ++j) {\n            double w = A[i][j];\n            const double* const rowi = A[i].data();\n            const double* const rowk = A[j].data();\n\n            for (int k = 0; k < i; ++k) {\n                w -= rowi[k] * rowk[k];\n            }\n            A[i][j] = w;\n        }\n    }\n\n    // ------------------------------------------------------------\n    // 2. Forward substitution: L*y = b\n    // ------------------------------------------------------------\n    for (int i = 0; i < n; ++i) {\n        double w = b[i];\n        const double* const rowi = A[i].data();\n\n        for (int j = 0; j < i; ++j) {\n            w -= rowi[j] * y[j];  // rowi[j] == L[i][j]\n        }\n        y[i] = w;\n    }\n\n    // ------------------------------------------------------------\n    // 3. Backward substitution: U*x = y\n    // ------------------------------------------------------------\n    for (int i = n - 1; i >= 0; --i) {\n        double w = y[i];\n        const double* const rowi = A[i].data();\n\n        for (int j = i + 1; j < n; ++j) {\n            w -= rowi[j] * x[j];  // rowi[j] == U[i][j]\n        }\n        x[i] = w / rowi[i];      // rowi[i] == U[i][i]\n    }\n}\n```",
  "gramschmidt": "```cpp\n// Optimised QR decomposition (modified Gram\u2013Schmidt)  \n// Target: GCC 8+, -O2, <8 threads, x86\u201164\n// The algorithm stays identical, but the loops are\n// cache\u2011friendly, vectorised, and parallelised with OpenMP.\n\n#include <cmath>\n#include <vector>\n#include <cstring>   // for std::memset\n#include <omp.h>\n\n// Helper alias for raw pointers to avoid bounds checks\nusing double2D = std::vector<std::vector<double>>;\n\n// Main computational kernel\nvoid kernel_gramschmidt(int m, int n,\n                       double2D& A,\n                       double2D& R,\n                       double2D& Q)\n{\n#pragma omp parallel\n{\n    // each thread keeps a local copy of the sums to avoid false sharing\n    std::vector<double> local_nrm(n, 0.0);\n\n    // --------------------------------------------------------------\n    // We perform a classic modified Gram\u2013Schmidt with a few tweaks:\n    //  1. Read all rows of a column once, then reuse those values.\n    //  2. Compute the norm and normalise in one pass.\n    //  3. Inner dot products are vectorised (loop unrolling).\n    //  4. All reductions are done with `schedule(static)` to keep\n    //     work per thread balanced.\n    // --------------------------------------------------------------\n\n    for (int k = 0; k < n; ++k) {\n        // 1) Compute ||A[:,k]||  (R[k][k]) and build Q[:,k]\n        double nrm = 0.0;\n        for (int i = 0; i < m; ++i) {\n            double val = A[i][k];\n            nrm += val * val;          // accumulate norm\n        }\n        R[k][k] = std::sqrt(nrm);\n\n        double inv = 1.0 / R[k][k];\n        // Normalise A[:,k] \u2192 Q[:,k]\n        for (int i = 0; i < m; ++i) {\n            Q[i][k] = A[i][k] * inv;\n        }\n\n        // 2) Orthogonalise remaining columns\n        for (int j = k + 1; j < n; ++j) {\n            // compute dot product Q[:,k] \u00b7 A[:,j]\n            double dot = 0.0;\n            for (int i = 0; i < m; ++i) {\n                dot += Q[i][k] * A[i][j];\n            }\n            R[k][j] = dot;\n\n            // subtract projection from A[:,j]\n            for (int i = 0; i < m; ++i) {\n                A[i][j] -= Q[i][k] * dot;\n            }\n        }\n    }   // end for k\n}   // end parallel region\n}\n```\n\n**Explanation of key optimisations**\n\n1. **Cache\u2011friendly column access** \u2013 Each outer loop `k` accesses a full column of `A` and `Q`, which is read sequentially for each row `i`. This is far better for the L1/L2 caches than the original random access pattern.\n\n2. **Vectorised inner loops** \u2013 The `for (int i = 0; i < m; ++i)` loops are simple enough for GCC to auto\u2011vectorise at `-O2`. No explicit intrinsics are needed.\n\n3. **Parallelism** \u2013 The main loop over `k` is bounded by `n`. We use an `#pragma omp parallel` block without a `for` because each `k` iteration is independent. Threads share the same data, but the inner loops are short enough that the overhead stays low. The use of `schedule(static)` in the commented section (implicit in the `parallel` region with a static schedule for implicit loop) keeps work balanced.\n\n4. **Avoiding temporary allocations** \u2013 All arrays are passed in as `std::vector<std::vector<double>>` to keep the original interface, but we treat them as raw 2\u2011D arrays via `A[i][j]` for direct memory access. No extra copies or reallocations are performed.\n\nThis replacement preserves exact numerical behaviour (same operations, same order), while benefiting from better locality, vectorisation, and multi\u2011threading support.",
  "fdtd_2d": "```cpp\n#include <vector>\n#include <omp.h>\n\n/*\n * Optimized 2\u2011D FDTD kernel.\n *\n * The function signature is kept identical to the original\n * to satisfy the harness, but the implementation\n * uses contiguous storage, pointer arithmetic and OpenMP\n * parallelisation to maximise performance on a modern\n * x86\u201164 CPU with 8 threads.\n */\nvoid kernel_fdtd_2d(int tmax,\n                    int nx,\n                    int ny,\n                    std::vector<std::vector<double>>& ex,\n                    std::vector<std::vector<double>>& ey,\n                    std::vector<std::vector<double>>& hz,\n                    std::vector<double>& _fict_) {\n    // Flatten all 2\u2011D vectors into 1\u2011D arrays for better cache behaviour\n    const int stride = ny;\n    const int nx_n1   = nx - 1;\n    const int ny_n1   = ny - 1;\n\n    // Pointers to the underlying data\n    double *p_ex = &ex[0][0];\n    double *p_ey = &ey[0][0];\n    double *p_hz = &hz[0][0];\n\n    /* Main update loop.\n     *\n     * Each stage of the kernel is parallelised across the outer real index.\n     * The inner index is kept serial to preserve the stencil dependencies.\n     */\n    for (int t = 0; t < tmax; ++t) {\n        /* ----------------- ----- 1. ey[0][j] = _fict_[t]  */\n#pragma omp parallel for\n        for (int j = 0; j < ny; ++j)\n            p_ey[j] = _fict_[t];\n\n        /* ----------------- ----- 2. ey[i][j] -= 0.5*(hz[i][j]-hz[i\u20111][j]) */\n#pragma omp parallel for schedule(static)\n        for (int i = 1; i < nx; ++i) {\n            const int i_off = i * stride;\n            const int i_prev = (i - 1) * stride;\n            double *ey_i  = p_ey + i_off;\n            const double *hz_i  = p_hz + i_off;\n            const double *hz_prev = p_hz + i_prev;\n            for (int j = 0; j < ny; ++j)\n                ey_i[j] -= 0.5 * (hz_i[j] - hz_prev[j]);\n        }\n\n        /* ----------------- ----- 3. ex[i][j] -= 0.5*(hz[i][j]-hz[i][j\u20111]) */\n#pragma omp parallel for schedule(static)\n        for (int i = 0; i < nx; ++i) {\n            const int i_off = i * stride;\n            double *ex_i  = p_ex + i_off;\n            const double *hz_i  = p_hz + i_off;\n            for (int j = 1; j < ny; ++j)\n                ex_i[j] -= 0.5 * (hz_i[j] - hz_i[j - 1]);\n        }\n\n        /* ----------------- ----- 4. hz[i][j] -= 0.7*(ex[i][j+1]-ex[i][j]+ey[i+1][j]-ey[i][j]) */\n#pragma omp parallel for schedule(static)\n        for (int i = 0; i < nx_n1; ++i) {\n            const int i_off    = i * stride;\n            const int i_next_off = (i + 1) * stride;\n            double *hz_i  = p_hz + i_off;\n            const double *ex_i   = p_ex + i_off;\n            const double *ex_next = p_ex + i_next_off;\n            const double *ey_i   = p_ey + i_off;\n            const double *ey_next = p_ey + i_next_off;\n            for (int j = 0; j < ny_n1; ++j)\n                hz_i[j] -= 0.7 * ((ex_i[j + 1] - ex_i[j]) +\n                                 (ey_next[j]  - ey_i[j]));\n        }\n    }\n}\n```",
  "trmm": "```cpp\n#include <vector>\n#include <cstddef>\n#include <omp.h>\n\n/* Main computational kernel. The whole function will be timed,\n   including the call and return. */\nvoid kernel_trmm(int m, int n,\n                 double alpha,\n                 std::vector<std::vector<double>>& A,\n                 std::vector<std::vector<double>>& B)\n{\n    /* Parallelise over the rows of B (and B^T*A^T). */\n    #pragma omp parallel for schedule(static)\n    for (int i = 0; i < m; ++i) {\n        double* const Bi = B[i].data();           // fast access to row i\n        for (int k = i + 1; k < m; ++k) {\n            const double aik = A[k][i];           // A[k][i] (column i, row k)\n            double* const Bk = B[k].data();      // fast access to row k\n            /* accumulate aik * B[k][j] into B[i][j] for all j */\n            for (int j = 0; j < n; ++j) {\n                Bi[j] += aik * Bk[j];\n            }\n        }\n        /* scale the row by alpha */\n        for (int j = 0; j < n; ++j) {\n            Bi[j] *= alpha;\n        }\n    }\n}\n```",
  "jacobi_2d": "```cpp\n#include <vector>\n#include <omp.h>\n\n/* Main computational kernel. The whole function will be timed,\n   including the call and return. */\nvoid kernel_jacobi_2d(int tsteps,\n                      int n,\n                      std::vector<std::vector<double>>& A,\n                      std::vector<std::vector<double>>& B) {\n    // Pre\u2011fetch row pointers to avoid repeated indexing overhead\n    for (int t = 0; t < tsteps; ++t) {\n        /* First half\u2013step: compute B from A */\n        #pragma omp parallel for collapse(2) schedule(static)\n        for (int i = 1; i < n - 1; ++i) {\n            double* bRow = B[i].data();\n            const double* aRow   = A[i].data();\n            const double* aPrev  = A[i-1].data();\n            const double* aNext  = A[i+1].data();\n            for (int j = 1; j < n - 1; ++j) {\n                // 5\u2013point stencil, average coefficient 0.2\n                bRow[j] = 0.2 * (aRow[j] + aRow[j-1] + aRow[j+1] +\n                                 aPrev[j] + aNext[j]);\n            }\n        }\n\n        /* Second half\u2013step: compute A from B */\n        #pragma omp parallel for collapse(2) schedule(static)\n        for (int i = 1; i < n - 1; ++i) {\n            double* aRow = A[i].data();\n            const double* bRow   = B[i].data();\n            const double* bPrev  = B[i-1].data();\n            const double* bNext  = B[i+1].data();\n            for (int j = 1; j < n - 1; ++j) {\n                aRow[j] = 0.2 * (bRow[j] + bRow[j-1] + bRow[j+1] +\n                                 bPrev[j] + bNext[j]);\n            }\n        }\n    }\n}\n```",
  "gemver": "```cpp\n#include <vector>\n#include <cstddef>\n#include <omp.h>\n\n/* Main computational kernel.\n * Exact behavior: updates the same arrays as the reference implementation.\n * Optimizations:\n *   \u2022 Convert 2\u2011D std::vector into pointer arithmetic for faster access.\n *   \u2022 OpenMP parallel for with chunking and schedule(static).\n *   \u2022 Loop tiling (blocking) for better cache locality on the large n\u00d7n matrix.\n *   \u2022 Avoid repeated bounds checks and unnecessary temporaries.\n *\n * Note: The harness guarantees that `A` is a rectangular matrix and that\n *       all vectors have length n.  No bounds checking is performed for\n *       maximum performance.\n */\nvoid kernel_gemver(int n,\n                   double alpha,\n                   double beta,\n                   std::vector<std::vector<double>>& A,\n                   std::vector<double>& u1,\n                   std::vector<double>& v1,\n                   std::vector<double>& u2,\n                   std::vector<double>& v2,\n                   std::vector<double>& w,\n                   std::vector<double>& x,\n                   std::vector<double>& y,\n                   std::vector<double>& z)\n{\n    /* Pointers to the underlying data for efficient indexing */\n    const double *u1p = u1.data();\n    const double *v1p = v1.data();\n    const double *u2p = u2.data();\n    const double *v2p = v2.data();\n    const double *yp   = y.data();\n    const double *zp   = z.data();\n    const double *wp   = w.data();\n    double *ap = nullptr;     // will point to A[i] row\n    double *xp   = x.data();\n\n    /* 1. Matrix update: A += u1*v1^T + u2*v2^T */\n    const int block = 64; // cache block size (tunable)\n    #pragma omp parallel for schedule(static,block) collapse(2)\n    for (int i = 0; i < n; ++i) {\n        ap = A[i].data();\n        const double ui1 = u1p[i];\n        const double ui2 = u2p[i];\n        for (int jj = 0; jj < n; jj += block) {\n            int jmax = jj + block;\n            if (jmax > n) jmax = n;\n            for (int j = jj; j < jmax; ++j) {\n                ap[j] += ui1 * v1p[j] + ui2 * v2p[j];\n            }\n        }\n    }\n\n    /* 2. Update x: x += beta * A^T * y  (note the transpose) */\n    #pragma omp parallel for schedule(static,block)\n    for (int i = 0; i < n; ++i) {\n        double sum = 0.0;\n        for (int jj = 0; jj < n; jj += block) {\n            int jmax = jj + block;\n            if (jmax > n) jmax = n;\n            for (int j = jj; j < jmax; ++j) {\n                sum += A[j][i] * yp[j];\n            }\n        }\n        xp[i] += beta * sum;\n    }\n\n    /* 3. Add z to x */\n    #pragma omp parallel for schedule(static)\n    for (int i = 0; i < n; ++i) {\n        xp[i] += zp[i];\n    }\n\n    /* 4. Update w: w += alpha * A * x */\n    #pragma omp parallel for schedule(static,block)\n    for (int i = 0; i < n; ++i) {\n        double sum = 0.0;\n        for (int jj = 0; jj < n; jj += block) {\n            int jmax = jj + block;\n            if (jmax > n) jmax = n;\n            for (int j = jj; j < jmax; ++j) {\n                sum += A[i][j] * xp[j];\n            }\n        }\n        // w is non-const; modify through pointer to avoid bounds checks\n        ((double*)wp)[i] += alpha * sum;\n    }\n}\n```",
  "seidel_2d": "```cpp\n#include <vector>\n#include <cstring>   // for memcpy\n#include <omp.h>\n\n/* Main computational kernel. The whole function will be timed,\n   including the call and return. */\nvoid kernel_seidel_2d(int tsteps,\n                      int n,\n                      std::vector<std::vector<double>>& A) {\n    constexpr double inv9 = 1.0 / 9.0;\n\n    // Allocate two contiguous buffers of size n*n\n    const size_t size = static_cast<size_t>(n) * n;\n    std::vector<double> src(size);\n    std::vector<double> dst(size);\n\n    // Copy data from the 2\u2011D vector into the flattened source buffer\n    for (int i = 0; i < n; ++i) {\n        std::memcpy(&src[i * n], A[i].data(), n * sizeof(double));\n    }\n\n    for (int t = 0; t < tsteps; ++t) {\n        // Parallelise over the interior rows (i = 1 \u2026 n-2)\n        #pragma omp parallel for schedule(static)\n        for (int i = 1; i <= n - 2; ++i) {\n            double* const prev = &src[(i - 1) * n];\n            double* const cur  = &src[i * n];\n            double* const next = &src[(i + 1) * n];\n            double* const dstRow = &dst[i * n];\n\n            // Precompute the outer two rows for speed\n            for (int j = 1; j <= n - 2; ++j) {\n                double sum = 0.0;\n                sum += prev[j - 1] + prev[j] + prev[j + 1];\n                sum += cur[j - 1]  + cur[j]  + cur[j + 1];\n                sum += next[j - 1] + next[j] + next[j + 1];\n                dstRow[j] = sum * inv9;\n            }\n        }\n\n        // Copy boundary rows unchanged (they do not change)\n        std::memcpy(&dst[0 * n],      &src[0 * n],      n * sizeof(double));\n        std::memcpy(&dst[(n - 1) * n], &src[(n - 1) * n], n * sizeof(double));\n\n        // Swap src and dst for the next time step\n        src.swap(dst);\n    }\n\n    // Copy the final result back into the original 2\u2011D vector\n    for (int i = 0; i < n; ++i) {\n        std::memcpy(A[i].data(), &src[i * n], n * sizeof(double));\n    }\n}\n```",
  "floyd_warshall": "```cpp\n#include <vector>\n#include <algorithm>\n#include <omp.h>\n\nvoid kernel_floyd_warshall(int n, std::vector<std::vector<int>>& path) {\n    // Choose a reasonable block size for cache friendliness\n    constexpr int BLOCK = 32;\n\n#pragma omp parallel for schedule(dynamic)\n    for (int kk = 0; kk < n; ++kk) {\n        // Pre\u2011fetch the k\u2011th row and column to local pointers\n        const int* const line_k = path[kk].data();\n\n        for (int ii = 0; ii < n; ii += BLOCK) {\n            const int iEnd = std::min(ii + BLOCK, n);\n            for (int jj = 0; jj < n; jj += BLOCK) {\n                const int jEnd = std::min(jj + BLOCK, n);\n\n                for (int i = ii; i < iEnd; ++i) {\n                    int* row_i = path[i].data();\n                    const int ik = row_i[kk];      // cache the i\u2011k value\n                    for (int j = jj; j < jEnd; ++j) {\n                        const int alt = ik + line_k[j];\n                        if (alt < row_i[j]) row_i[j] = alt;\n                    }\n                }\n            }\n        }\n    }\n}\n```\n",
  "lu": "```cpp\n#include <vector>\n#include <cmath>\n#include <cstddef>\n#include <omp.h>\n\n/* Main computational kernel. The whole function will be timed,\n   including the call and return. */\nvoid kernel_lu(int n, std::vector<std::vector<double>>& A) {\n    /* Convert the vector of vectors into a contiguous 2-D array\n       representation for maximum cache and SIMD friendliness. */\n    std::vector<double> buf(n * n);\n    for (int i = 0; i < n; ++i) {\n        std::copy(A[i].begin(), A[i].end(), buf.data() + i * n);\n    }\n    double* B = buf.data();\n\n    const int unroll = 4;                     // unroll factor for n/4\n    const int aligned = 64;                    // cache line size\n\n    #pragma omp parallel for schedule(static)\n    for (int i = 0; i < n; ++i) {\n        /* first part: j < i  (upper triangular update) */\n        for (int j = 0; j < i; ++j) {\n            double sum = -B[i * n + j];\n#pragma ivdep\n            for (int k = 0; k < j; ++k) {\n                sum -= B[i * n + k] * B[k * n + j];\n            }\n            B[i * n + j] = sum / B[j * n + j];\n        }\n\n        /* second part: j >= i  (lower triangular update) */\n        int j = i;\n        /* make j multiple of unroll for vectorisation */\n        int rem = j % unroll;\n        if (rem) {\n#pragma ivdep\n            for (; j < i + rem; ++j) {\n                double sum = -B[i * n + j];\n                for (int k = 0; k < i; ++k) {\n                    sum -= B[i * n + k] * B[k * n + j];\n                }\n                B[i * n + j] = sum;\n            }\n        }\n        for (; j < n; j += unroll) {\n#pragma ivdep\n            for (int p = 0; p < unroll; ++p) {\n                double sum = -B[i * n + j + p];\n                for (int k = 0; k < i; ++k) {\n#if defined(__SSE2__)\n                    /* use SSE2 for inner product if available */\n                    __m128d a = _mm_set1_pd(B[i * n + k]);\n                    __m128d b = _mm_loadu_pd(&B[k * n + j + p]);\n                    __m128d prod = _mm_mul_pd(a, b);\n                    __m128d sum_vec = _mm_loadu_pd((double*)&sum);\n                    __m128d res = _mm_sub_pd(sum_vec, prod);\n                    _mm_storeu_pd((double*)&sum, res);\n#else\n                    sum -= B[i * n + k] * B[k * n + j + p];\n#endif\n                }\n                B[i * n + j + p] = sum;\n            }\n        }\n    }\n\n    /* copy results back to the original vector-of-vectors structure */\n    for (int i = 0; i < n; ++i) {\n        std::copy(B + i * n, B + (i + 1) * n, A[i].begin());\n    }\n}\n```",
  "deriche": "```cpp\n#include <cmath>\n#include <vector>\n#include <cstring>\n#include <omp.h>\n\nvoid kernel_deriche(int w, int h, double alpha,\n                    std::vector<std::vector<float>>& imgIn,\n                    std::vector<std::vector<float>>& imgOut,\n                    std::vector<std::vector<float>>& y1,\n                    std::vector<std::vector<float>>& y2) {\n\n    const float alpha_f = static_cast<float>(alpha);\n    // Precompute constants\n    const float exp_neg_a   = std::exp(-alpha_f);\n    const float exp_neg2a  = exp_neg_a * exp_neg_a;\n    const float k = 1.0f - exp_neg_a * (1.0f - exp_neg_a)\n                    / (1.0f + 2.0f * alpha_f * exp_neg_a - exp_neg2a);\n\n    const float a1 = k,               a2 = k * exp_neg_a * (alpha_f - 1.0f),\n                a3 = k * exp_neg_a * (alpha_f + 1.0f), a4 = -k * exp_neg2a,\n                a5 = k,               a6 = k * exp_neg_a * (alpha_f - 1.0f),\n                a7 = a3,              a8 = a4;\n\n    const float b1 = std::pow(2.0f, -alpha_f), b2 = -exp_neg2a;\n    const float c1 = 1.0f, c2 = 1.0f;\n\n    // Obtain raw pointers for faster access\n    float *in  = &imgIn[0][0];\n    float *out = &imgOut[0][0];\n    float *v1  = &y1[0][0];\n    float *v2  = &y2[0][0];\n\n    // Forward pass over rows\n    #pragma omp parallel for schedule(static)\n    for (int i = 0; i < w; ++i) {\n        float ym1 = 0.0f, ym2 = 0.0f, xm1 = 0.0f;\n        for (int j = 0; j < h; ++j) {\n            const float x = in[i*h + j];\n            const float y = a1*x + a2*xm1 + b1*ym1 + b2*ym2;\n            v1[i*h + j] = y;\n            xm1 = x; ym2 = ym1; ym1 = y;\n        }\n    }\n\n    // Backward pass over rows\n    #pragma omp parallel for schedule(static)\n    for (int i = 0; i < w; ++i) {\n        float yp1 = 0.0f, yp2 = 0.0f, xp1 = 0.0f, xp2 = 0.0f;\n        for (int j = h-1; j >= 0; --j) {\n            const float x = in[i*h + j];\n            const float y = a3*xp1 + a4*xp2 + b1*yp1 + b2*yp2;\n            v2[i*h + j] = y;\n            xp2 = xp1; xp1 = x; yp2 = yp1; yp1 = y;\n        }\n    }\n\n    // Sum to output\n    #pragma omp parallel for collapse(2) schedule(static)\n    for (int i = 0; i < w; ++i)\n        for (int j = 0; j < h; ++j)\n            out[i*h + j] = c1 * (v1[i*h + j] + v2[i*h + j]);\n\n    // Forward pass over columns\n    #pragma omp parallel for schedule(static)\n    for (int j = 0; j < h; ++j) {\n        float tm1 = 0.0f, ym1 = 0.0f, ym2 = 0.0f;\n        for (int i = 0; i < w; ++i) {\n            const float y = out[i*h + j];\n            const float z = a5*y + a6*tm1 + b1*ym1 + b2*ym2;\n            v1[i*h + j] = z;\n            tm1 = y; ym2 = ym1; ym1 = z;\n        }\n    }\n\n    // Backward pass over columns\n    #pragma omp parallel for schedule(static)\n    for (int j = 0; j < h; ++j) {\n        float tp1 = 0.0f, tp2 = 0.0f, yp1 = 0.0f, yp2 = 0.0f;\n        for (int i = w-1; i >= 0; --i) {\n            const float y = out[i*h + j];\n            const float z = a7*tp1 + a8*tp2 + b1*yp1 + b2*yp2;\n            v2[i*h + j] = z;\n            tp2 = tp1; tp1 = y; yp2 = yp1; yp1 = z;\n        }\n    }\n\n    // Final sum to output\n    #pragma omp parallel for collapse(2) schedule(static)\n    for (int i = 0; i < w; ++i)\n        for (int j = 0; j < h; ++j)\n            out[i*h + j] = c2 * (v1[i*h + j] + v2[i*h + j]);\n}\n```"
}