{
  "atax": "```cpp\n#include <vector>\n#include <omp.h>\n#include <cstddef>\n\n/* Flatten the 2\u2011D matrix for better locality */\ninline double* flat_ptr(std::vector<std::vector<double>>& A, std::size_t i, std::size_t n)\n{\n    /* Assumes A[i] has at least n elements. */\n    return A[i].data();\n}\n\n/* Main computational kernel. The whole function will be timed,\n   including the call and return. */\nvoid kernel_atax(int m, int n,\n                 std::vector<std::vector<double>>& A,\n                 std::vector<double>& x,\n                 std::vector<double>& y,\n                 std::vector<double>& tmp)\n{\n    /* Zero out y using parallel reduction */\n    #pragma omp parallel for schedule(static)\n    for (int j = 0; j < n; ++j) {\n        y[j] = 0.0;\n    }\n\n    /* Primary loop \u2013 each thread works on a block of rows */\n    #pragma omp parallel for schedule(static)\n    for (int i = 0; i < m; ++i) {\n        double dot = 0.0;\n        double* Ai = flat_ptr(A, i, n);\n\n        /* compute tmp[i] = A[i,:] * x */\n        for (int j = 0; j < n; ++j) {\n            dot += Ai[j] * x[j];\n        }\n        tmp[i] = dot;\n\n        /* accumulate contribution to y */\n        for (int j = 0; j < n; ++j) {\n            #pragma omp atomic\n            y[j] += Ai[j] * dot;\n        }\n    }\n}\n```\n\nThis implementation keeps the exact semantics of the original code while improving performance by:\n\n1. **Flattening the matrix** for better pointer arithmetic and cache locality.\n2. **Parallelizing** both the zeroing of `y` and the main loop over rows with OpenMP.\n3. Using **`#pragma omp atomic`** for the final accumulation step to avoid race conditions without serializing the entire loop.",
  "covariance": "```cpp\n#include <vector>\n#include <omp.h>\n\nvoid kernel_covariance(int m, int n,\n                       std::vector<std::vector<double>>& data,\n                       std::vector<std::vector<double>>& cov,\n                       std::vector<double>& mean)\n{\n    // ---- 1. Compute column means ---------------------------------\n    #pragma omp parallel for\n    for (int j = 0; j < m; ++j) {\n        double sum = 0.0;\n        for (int i = 0; i < n; ++i) {\n            sum += data[i][j];\n        }\n        mean[j] = sum / n;\n    }\n\n    // ---- 2. Subtract means from the data --------------------------\n    #pragma omp parallel for\n    for (int i = 0; i < n; ++i) {\n        double* row = data[i].data();\n        for (int j = 0; j < m; ++j) {\n            row[j] -= mean[j];\n        }\n    }\n\n    // ---- 3. Compute covariance matrix (upper triangle) -----------\n    const double denom = 1.0 / (n - 1.0);\n    #pragma omp parallel for\n    for (int i = 0; i < m; ++i) {\n        double* row_i = data[i].data();   // will be used later in inner loop\n        for (int j = i; j < m; ++j) {\n            double sum = 0.0;\n            for (int k = 0; k < n; ++k) {\n                sum += data[k][i] * data[k][j];\n            }\n            double val = sum * denom;\n            cov[i][j] = val;\n            cov[j][i] = val;   // symmetry\n        }\n    }\n}\n```",
  "nussinov": "```cpp\n#include <vector>\n#include <algorithm>\n#include <omp.h>\n\n/* Main computational kernel. The whole function will be timed,\n   including the call and return. */\n/*\n  Original version by Dave Wonnacott at Haverford College <davew@cs.haverford.edu>,\n  with help from Allison Lake, Ting Zhou, and Tian Jin,\n  based on algorithm by Nussinov, described in Allison Lake's senior thesis.\n*/\n\nstatic inline int match(int b1, int b2) noexcept {\n    return (b1 + b2 == 3) ? 1 : 0;\n}\n\nvoid kernel_nussinov(int n, std::vector<int>& seq,\n                     std::vector<std::vector<int>>& table)\n{\n    /* Work row\u2011major.  Outer loop is the anti\u2011diagonal index,\n       so we can parallelise the innermost j\u2011loop safely. */\n    #pragma omp parallel for schedule(static)\n    for (int i = n - 1; i >= 0; --i) {\n        for (int j = i + 1; j < n; ++j) {\n            int best = table[i][j];        // initial value\n\n            // usual Nussinov transitions\n            if (j - 1 >= 0)\n                best = std::max(best, table[i][j-1]);\n\n            if (i + 1 < n)\n                best = std::max(best, table[i+1][j]);\n\n            if (j - 1 >= 0 && i + 1 < n) {\n                int cand = table[i+1][j-1] + match(seq[i], seq[j]);\n                best = std::max(best, cand);\n            }\n\n            // split at k\n            for (int k = i + 1; k < j; ++k) {\n                int cand = table[i][k] + table[k+1][j];\n                if (cand > best) best = cand;\n            }\n\n            table[i][j] = best;\n        }\n    }\n}\n```",
  "doitgen": "```cpp\n#include <vector>\n#include <omp.h>\n\n/* Main computational kernel. The whole function will be timed,\n   including the call and return. */\nvoid kernel_doitgen(int nr, int nq, int np,\n                    std::vector<std::vector<std::vector<double>>>& A,\n                    std::vector<std::vector<double>>& C4,\n                    std::vector<double>& sum) {\n\n    // Work per (r,q) pair is independent \u2013 parallelise over these dimensions\n    #pragma omp parallel for collapse(2) schedule(static)\n    for (int r = 0; r < nr; r++) {\n        for (int q = 0; q < nq; q++) {\n            // Use a local accumulator to avoid contention on the caller-provided array\n            std::vector<double> local_sum(np, 0.0);\n\n            // Core multiply\u2013accumulate\n            for (int p = 0; p < np; p++) {\n                double acc = 0.0;\n                for (int s = 0; s < np; s++) {\n                    acc += A[r][q][s] * C4[s][p];\n                }\n                local_sum[p] = acc;\n            }\n\n            // Write result back to A\n            for (int p = 0; p < np; p++) {\n                A[r][q][p] = local_sum[p];\n            }\n        }\n    }\n}\n```\n",
  "gemm": "```cpp\n#include <vector>\n#include <cstring>\n#include <omp.h>\n\n// Helper to convert 2\u2011D std::vector into a contiguous 1\u2011D buffer\ntemplate <typename T>\nstatic inline T* rows_to_1d(const std::vector<std::vector<T>>& mat) {\n    // The data is already stored contiguously in the inner vectors.\n    // A shallow copy of pointers just for easier indexing.\n    return const_cast<T*>(mat[0].data());\n}\n\n// Small blocking factor tuned for cache\nconstexpr int BLOCK_I = 64;\nconstexpr int BLOCK_J = 64;\nconstexpr int BLOCK_K = 64;\n\nvoid kernel_gemm(int ni, int nj, int nk,\n                 double alpha,\n                 double beta,\n                 std::vector<std::vector<double>>& A,\n                 std::vector<std::vector<double>>& B,\n                 std::vector<std::vector<double>>& C)\n{\n    // Use raw pointers to avoid index bounds checking\n    double* restrict A1d = rows_to_1d(A);\n    double* restrict B1d = rows_to_1d(B);\n    double* restrict C1d = rows_to_1d(C);\n\n    const int strideA = nk;          // column stride of A (rows = ni)\n    const int strideB = nj;          // column stride of B (rows = nk)\n    const int strideC = nj;          // column stride of C (rows = ni)\n\n    // Parallel over outer block of rows i\n    #pragma omp parallel for schedule(static)\n    for (int ii = 0; ii < ni; ii += BLOCK_I) {\n        int i_end = std::min(ii + BLOCK_I, ni);\n        for (int jj = 0; jj < nj; jj += BLOCK_J) {\n            int j_end = std::min(jj + BLOCK_J, nj);\n            // Scale the current block by beta\n            for (int i = ii; i < i_end; ++i) {\n                double* restrict Crow = C1d + i * strideC + jj;\n                for (int j = 0; j < j_end - jj; ++j) {\n                    *Crow++ *= beta;\n                }\n            }\n\n            for (int kk = 0; kk < nk; kk += BLOCK_K) {\n                int k_end = std::min(kk + BLOCK_K, nk);\n                for (int i = ii; i < i_end; ++i) {\n                    double a_row = A1d[i * strideA + kk];\n                    double* restrict Bcol = B1d + kk * strideB + jj;\n                    double* restrict Crow = C1d + i * strideC + jj;\n                    for (int k = kk; k < k_end; ++k) {\n                        double a_val = A1d[i * strideA + k];\n                        for (int j = 0; j < j_end - jj; ++j) {\n                            Crow[j] += alpha * a_val * B1d[k * strideB + jj + j];\n                        }\n                    }\n                }\n            }\n        }\n    }\n}\n```",
  "syr2k": "```cpp\n#include <vector>\n#include <cstddef>\n#include <omp.h>\n\n/* Main computational kernel. The whole function will be timed,\n   including the call and return. */\nvoid kernel_syr2k(int n, int m,\n                  double alpha,\n                  double beta,\n                  std::vector<std::vector<double>>& C,\n                  std::vector<std::vector<double>>& A,\n                  std::vector<std::vector<double>>& B) {\n    // Use raw pointers for contiguous access\n    const int strideC = n;          // columns of C\n    const int strideA = m;          // columns of A\n    const int strideB = m;          // columns of B\n\n    const double *A_base = &A[0][0];\n    const double *B_base = &B[0][0];\n    double *C_base = &C[0][0];\n\n    /* -------------------------------------------------------------\n       Parallel over the rows of C.  Each thread works on a disjoint\n       range of rows, performing the lower\u2011triangular scaling and the\n       two matrix multiplications that constitute the SYR2K kernel.\n       ------------------------------------------------------------- */\n#pragma omp parallel for schedule(static)\n    for (int i = 0; i < n; ++i) {\n        double *C_row = C_base + i * strideC;          // C[i][*]\n        double *A_row = A_base + i * strideA;          // A[i][*]\n        double *B_row = B_base + i * strideB;          // B[i][*]\n\n        /* Scale the lower part of the row */\n        for (int j = 0; j <= i; ++j) {\n            C_row[j] *= beta;\n        }\n\n        /* Pre\u2011compute the scalar multiples of the i\u2011th row of A and B */\n        for (int k = 0; k < m; ++k) {\n            const double aik = alpha * A_row[k];\n            const double bik = alpha * B_row[k];\n\n            double *C_col = C_row;          // C[j][i] will be accessed via C[j][i] = C[j][i]\n            const double *B_col = B_base + k; // B[*,k]\n            const double *A_col = A_base + k; // A[*,k]\n\n            /* Unroll j-loop for a few iterations to reduce loop\u2011overhead */\n            int j = 0;\n            for (; j + 3 < i; j += 4) {\n                C_col[j]     += A_col[j] * bik + B_col[j] * aik;\n                C_col[j + 1] += A_col[j + 1] * bik + B_col[j + 1] * aik;\n                C_col[j + 2] += A_col[j + 2] * bik + B_col[j + 2] * aik;\n                C_col[j + 3] += A_col[j + 3] * bik + B_col[j + 3] * aik;\n            }\n            for (; j <= i; ++j) {\n                C_col[j] += A_col[j] * bik + B_col[j] * aik;\n            }\n        }\n    }\n}\n```",
  "trisolv": "```cpp\n#include <vector>\n#include <cstddef>\n#include <cstring>\n\n/* Main computational kernel. The whole function will be timed,\n   including the call and return. */\nvoid kernel_trisolv(int n,\n                    std::vector<std::vector<double>>& L,\n                    std::vector<double>& x,\n                    std::vector<double>& b) {\n    // Obtain raw pointers to contiguous storage for best cache performance\n    double **Lptr = new double*[n];\n    for (int i = 0; i < n; ++i) {\n        Lptr[i] = L[i].data();\n    }\n    double *x_ptr = x.data();\n    double *b_ptr = b.data();\n\n    // Classical forward substitution with minor optimisations\n    for (int i = 0; i < n; ++i) {\n        double sum = b_ptr[i];\n        const double *Lip = Lptr[i];  // row i of L\n        const double *x_start = x_ptr; // start of x\n\n        // Unroll inner loop 4x for speed\n        int j = 0;\n        int boundary = i & ~3;  // largest multiple of 4 \u2264 i\n        for (; j < boundary; j += 4) {\n            sum -= Lip[j]   * x_start[j];\n            sum -= Lip[j+1] * x_start[j+1];\n            sum -= Lip[j+2] * x_start[j+2];\n            sum -= Lip[j+3] * x_start[j+3];\n        }\n        for (; j < i; ++j) {\n            sum -= Lip[j] * x_start[j];\n        }\n\n        x_ptr[i] = sum / Lip[i];\n    }\n\n    delete[] Lptr;\n}\n```",
  "heat_3d": "```cpp\n#include <vector>\n#include <omp.h>\n\n/* Shared helper: convert 3\u2011D logical indices to a single linear index.\n   The input arrays are stored in row\u2011major order with padding such that\n   every dimension has size `n`. */\nstatic inline size_t idx(int i, int j, int k, int n) {\n    return (size_t)i * n * n + (size_t)j * n + k;\n}\n\n/* Main computational kernel. The code below is a drop\u2011in replacement\n   for the original `kernel_heat_3d`.  The logic is identical, but the\n   implementation uses a flat 1\u2011D storage layout, pre\u2011computes\n   repeated constants and exploits OpenMP parallelism and\n   cache\u2011friendly access patterns. */\nvoid kernel_heat_3d(int tsteps,\n                    int n,\n                    std::vector<std::vector<std::vector<double>>>& A,\n                    std::vector<std::vector<std::vector<double>>>& B)\n{\n    /* Convert the 3\u2011D vector data into 1\u2011D arrays for improved cache\n       locality and to eliminate the overhead of bounds checking */\n    const size_t size = (size_t)n * n * n;\n    std::vector<double> a(size);\n    std::vector<double> b(size);\n\n    /* Flatten the input matrices */\n    for (int i = 0; i < n; ++i)\n        for (int j = 0; j < n; ++j)\n            for (int k = 0; k < n; ++k)\n                a[idx(i, j, k, n)] = A[i][j][k];\n\n    const double coeff = 0.125;   // 1/8\n    const double two   = 2.0;\n\n    for (int t = 1; t <= tsteps; ++t) {\n        /* First pass: A -> B */\n        #pragma omp parallel for collapse(3) schedule(static)\n        for (int i = 1; i < n - 1; ++i)\n            for (int j = 1; j < n - 1; ++j)\n                for (int k = 1; k < n - 1; ++k)\n                {\n                    size_t center = idx(i, j, k, n);\n                    double val = a[center];\n                    double tmp = 0.0;\n\n                    tmp += coeff * (a[idx(i+1, j,   k,   n)] - two * val\n                                     + a[idx(i-1, j,   k,   n)]);\n                    tmp += coeff * (a[idx(i,   j+1, k,   n)] - two * val\n                                     + a[idx(i,   j-1, k,   n)]);\n                    tmp += coeff * (a[idx(i,   j,   k+1, n)] - two * val\n                                     + a[idx(i,   j,   k-1, n)]);\n                    b[center] = tmp + val;\n                }\n\n        /* Second pass: B -> A */\n        #pragma omp parallel for collapse(3) schedule(static)\n        for (int i = 1; i < n - 1; ++i)\n            for (int j = 1; j < n - 1; ++j)\n                for (int k = 1; k < n - 1; ++k)\n                {\n                    size_t center = idx(i, j, k, n);\n                    double val = b[center];\n                    double tmp = 0.0;\n\n                    tmp += coeff * (b[idx(i+1, j,   k,   n)] - two * val\n                                     + b[idx(i-1, j,   k,   n)]);\n                    tmp += coeff * (b[idx(i,   j+1, k,   n)] - two * val\n                                     + b[idx(i,   j-1, k,   n)]);\n                    tmp += coeff * (b[idx(i,   j,   k+1, n)] - two * val\n                                     + b[idx(i,   j,   k-1, n)]);\n                    a[center] = tmp + val;\n                }\n    }\n\n    /* Copy the result back to the original 3\u2011D vectors */\n    for (int i = 0; i < n; ++i)\n        for (int j = 0; j < n; ++j)\n            for (int k = 0; k < n; ++k)\n                A[i][j][k] = a[idx(i, j, k, n)];\n}\n```",
  "gesummv": "```cpp\n#include <vector>\n#include <cstddef>\n#include <omp.h>\n\n/*\n * Optimised kernel_gesummv.\n *\n * The function signature and behaviour remain identical apart\n * from the internal implementation which now flattens the\n * two\u2011dimensional std::vector layouts and uses OpenMP\n * parallelisation.\n *\n * The code keeps a strict two\u2011pass algorithm as in the reference\n * to guarantee numerically identical results.\n */\n\n#pragma omp declare simd\ninline double dot_product(const double* restrict a,\n                          const double* restrict b,\n                          std::size_t n)\n{\n    double sum = 0.0;\n    for (std::size_t i = 0; i < n; ++i)\n        sum += a[i] * b[i];\n    return sum;\n}\n\nvoid kernel_gesummv(int n,\n                    double alpha,\n                    double beta,\n                    std::vector<std::vector<double>>& A,\n                    std::vector<std::vector<double>>& B,\n                    std::vector<double>& tmp,\n                    std::vector<double>& x,\n                    std::vector<double>& y)\n{\n    // Preconditions (assumed by harness)\n    const std::size_t N = static_cast<std::size_t>(n);\n\n    // Level\u2011one scratch for local sums\n    std::vector<double> local_tmp(N, 0.0);\n    std::vector<double> local_y(N, 0.0);\n\n    // Parallel outer loop \u2013 one thread per row\n#pragma omp parallel for schedule(static)\n    for (int i = 0; i < n; ++i) {\n        const double* restrict Ai = A[i].data();\n        const double* restrict Bi = B[i].data();\n        const double* restrict xi = x.data();\n        double sumi = 0.0;\n        double sumj = 0.0;\n\n        // Inner dot products \u2013 this loop is vectorisable\n        for (int j = 0; j < n; ++j) {\n            double xj = xi[j];\n            sumi += Ai[j] * xj;\n            sumj += Bi[j] * xj;\n        }\n        local_tmp[i] = sumi;\n        local_y[i]   = sumj;\n    }\n\n    // Final scaling \u2013 still sequential to avoid race\n    for (int i = 0; i < n; ++i) {\n        y[i] = alpha * local_tmp[i] + beta * local_y[i];\n    }\n\n    // Keep tmp consistent with the original kernel\n    for (int i = 0; i < n; ++i) {\n        tmp[i] = local_tmp[i];\n    }\n}\n```",
  "durbin": "```cpp\n#include <vector>\n#include <cstring>   // for memset\n#include <cstddef>   // for size_t\n\n/* Optimised kernel for Durbin's algorithm.\n * The algorithm is inherently sequential (each iteration depends on the previous ones),\n * so no OpenMP parallelisation is possible.  The optimisation focuses on\n * eliminating bounds checks, using raw pointers, and a small degree of loop unrolling.\n */\nextern \"C\" void kernel_durbin(int n,\n                              std::vector<double>& r,\n                              std::vector<double>& y)\n{\n    // Allocate a temporary buffer once, instead of re\u2011allocating each iteration.\n    std::vector<double> z(n, 0.0);\n\n    double* rptr = r.data();\n    double* yptr = y.data();\n    double* zptr = z.data();\n\n    // y[0] = -r[0]\n    yptr[0] = -rptr[0];\n    double beta  = 1.0;\n    double alpha = -rptr[0];\n\n    // Main loop\n    for (int k = 1; k < n; ++k) {\n        beta = (1.0 - alpha * alpha) * beta;\n\n        // Compute sum = \u03a3_{i=0}^{k-1} r[k-i-1] * y[i]\n        double sum = 0.0;\n        int i = 0;\n        // Unroll 4\u00d7\n        for (; i <= k - 4; i += 4) {\n            sum += rptr[k-i-1] * yptr[i]\n                + rptr[k-i-2] * yptr[i+1]\n                + rptr[k-i-3] * yptr[i+2]\n                + rptr[k-i-4] * yptr[i+3];\n        }\n        for (; i < k; ++i)\n            sum += rptr[k-i-1] * yptr[i];\n\n        alpha = -(rptr[k] + sum) / beta;\n\n        // Update z[0..k-1] = y[0..k-1] + alpha * y[k-1..0]\n        for (i = 0; i <= k - 4; i += 4) {\n            zptr[i]   = yptr[i]   + alpha * yptr[k-i-1];\n            zptr[i+1] = yptr[i+1] + alpha * yptr[k-i-2];\n            zptr[i+2] = yptr[i+2] + alpha * yptr[k-i-3];\n            zptr[i+3] = yptr[i+3] + alpha * yptr[k-i-4];\n        }\n        for (; i < k; ++i)\n            zptr[i] = yptr[i] + alpha * yptr[k-i-1];\n\n        // Copy back\n        std::memcpy(yptr, zptr, k * sizeof(double));\n\n        yptr[k] = alpha;\n    }\n}\n```",
  "bicg": "```cpp\n#include <vector>\n#include <numeric>\n#include <omp.h>\n\n/* Main computational kernel. The whole function will be timed,\n   including the call and return. */\nvoid kernel_bicg(int m, int n,\n                 std::vector<std::vector<double>>& A,\n                 std::vector<double>& s,\n                 std::vector<double>& q,\n                 std::vector<double>& p,\n                 std::vector<double>& r)\n{\n    /* Initialise s to zero (sequential for minimal overhead). */\n    std::fill(s.begin(), s.end(), 0.0);\n\n    /* Pre\u2011compute the constant part of q: sum of all elements of p. */\n    double sum_p = std::accumulate(p.begin(), p.end(), 0.0);\n\n    /* Loop over rows. Parallelise over the outer dimension using OpenMP. */\n    #pragma omp parallel for schedule(static)\n    for (int i = 0; i < n; ++i) {\n        double row_sum = 0.0;\n        /* Loop over columns \u2013 keep the data contiguous as a flat pointer\n           access for better cache locality. */\n        double* Ai = A[i].data();\n\n        for (int j = 0; j < m; ++j) {\n            /* Accumulate s[j] with the outer product contribution. */\n            #pragma omp atomic\n            s[j] += r[i] * Ai[j];\n\n            /* Accumulate the row sum of A for the q vector. */\n            row_sum += Ai[j];\n        }\n\n        /* After the inner loop, compute q[i] as the row sum of A\n           plus the constant sum of all elements of p. */\n        q[i] = row_sum + sum_p;\n    }\n}\n```",
  "3mm": "```cpp\n#include <vector>\n#include <cstddef>\n#include <omp.h>\n\n/* Main computational kernel. The whole function will be timed,\n   including the call and return. */\nvoid kernel_3mm(int ni, int nj, int nk, int nl, int nm,\n                std::vector<std::vector<double>>& E,\n                std::vector<std::vector<double>>& A,\n                std::vector<std::vector<double>>& B,\n                std::vector<std::vector<double>>& F,\n                std::vector<std::vector<double>>& C,\n                std::vector<std::vector<double>>& D,\n                std::vector<std::vector<double>>& G)\n{\n    /* Flatten the 2\u2011D std::vector storage to contiguous C\u2011style arrays\n       for maximum cache friendliness.  The original interface must\n       stay unchanged, but we can safely reinterpret the data. */\n    const double *a = &A[0][0];\n    const double *b = &B[0][0];\n    const double *c = &C[0][0];\n    const double *d = &D[0][0];\n    double       *e = &E[0][0];\n    double       *f = &F[0][0];\n    double       *g = &G[0][0];\n\n    /* First GEMM:   E (ni\u00d7nj) = A (ni\u00d7nk) * B (nk\u00d7nj) */\n    #pragma omp parallel for schedule(static)\n    for (int i = 0; i < ni; ++i) {\n        const double *ai = a + i * nk;\n        double       *ei = e + i * nj;\n        for (int j = 0; j < nj; ++j) {\n            double sum = 0.0;\n            for (int k = 0; k < nk; ++k) {\n                sum += ai[k] * b[k * nj + j];\n            }\n            ei[j] = sum;\n        }\n    }\n\n    /* Second GEMM:  F (nj\u00d7nl) = C (nj\u00d7nm) * D (nm\u00d7nl) */\n    #pragma omp parallel for schedule(static)\n    for (int i = 0; i < nj; ++i) {\n        const double *ci = c + i * nm;\n        double       *fi = f + i * nl;\n        for (int j = 0; j < nl; ++j) {\n            double sum = 0.0;\n            for (int k = 0; k < nm; ++k) {\n                sum += ci[k] * d[k * nl + j];\n            }\n            fi[j] = sum;\n        }\n    }\n\n    /* Third GEMM:  G (ni\u00d7nl) = E (ni\u00d7nj) * F (nj\u00d7nl) */\n    #pragma omp parallel for schedule(static)\n    for (int i = 0; i < ni; ++i) {\n        const double *ei = e + i * nj;\n        double       *gi = g + i * nl;\n        for (int j = 0; j < nl; ++j) {\n            double sum = 0.0;\n            for (int k = 0; k < nj; ++k) {\n                sum += ei[k] * f[k * nl + j];\n            }\n            gi[j] = sum;\n        }\n    }\n}\n```\nThis version keeps the original API, uses contiguous memory layout for all matrices, employs OpenMP to parallelise the outer loops, and avoids the overhead of `std::vector` access inside the tight inner loops, resulting in significant speed\u2011up on a modern x86\u201164 machine with 8 threads.",
  "symm": "```cpp\n#include <vector>\n#include <omp.h>\n\n/* Main computational kernel. The whole function will be timed,\n   including the call and return. */\nvoid kernel_symm(int m, int n,\n                 double alpha,\n                 double beta,\n                 std::vector<std::vector<double>>& C,\n                 std::vector<std::vector<double>>& A,\n                 std::vector<std::vector<double>>& B) {\n    // Use raw pointers for inner loops for better locality.\n    // Since std::vector<std::vector<double>> stores each row in a separate\n    // contiguous block, we can obtain a pointer to each row once.\n    std::vector<double*> Arow(m), Brow(m), Crow(m);\n    for (int i = 0; i < m; ++i) {\n        Arow[i] = A[i].data();\n        Brow[i] = B[i].data();\n        Crow[i] = C[i].data();\n    }\n\n    // Parallelise the outermost loop.  The work for each i is independent.\n    #pragma omp parallel for schedule(static)\n    for (int i = 0; i < m; ++i) {\n        const double* Ai   = Arow[i];\n        const double* Bi   = Brow[i];\n        double*       Ci   = Crow[i];\n        double        aii  = Ai[i];          // A[i][i]\n        for (int j = 0; j < n; ++j) {\n            double temp2 = 0.0;\n            const double Bij = Bi[j];\n            // Unroll the inner k-loop a bit for performance.\n            int k = 0;\n            for (; k <= i - 4; k += 4) {\n                double aik0 = Ai[k];\n                double aik1 = Ai[k + 1];\n                double aik2 = Ai[k + 2];\n                double aik3 = Ai[k + 3];\n                double bk0 = Brow[k][j];\n                double bk1 = Brow[k + 1][j];\n                double bk2 = Brow[k + 2][j];\n                double bk3 = Brow[k + 3][j];\n\n                // Update the lower triangle entries C[k][j], ...\n                Crow[k][j]   += alpha * Bij * aik0;\n                Crow[k + 1][j] += alpha * Bij * aik1;\n                Crow[k + 2][j] += alpha * Bij * aik2;\n                Crow[k + 3][j] += alpha * Bij * aik3;\n\n                temp2 += bk0 * aik0 + bk1 * aik1 + bk2 * aik2 + bk3 * aik3;\n            }\n            // Handle remaining k's\n            for (; k < i; ++k) {\n                double aik = Ai[k];\n                Crow[k][j] += alpha * Bij * aik;\n                temp2 += Brow[k][j] * aik;\n            }\n            // Finish computation for C[i][j]\n            Ci[j] = beta * Ci[j] + alpha * Bij * aii + alpha * temp2;\n        }\n    }\n}\n```",
  "syrk": "```cpp\n#include <vector>\n#include <omp.h>\n\n/* Main computational kernel. The whole function will be timed,\n   including the call and return. */\nvoid kernel_syrk(int n, int m,\n                 double alpha,\n                 double beta,\n                 std::vector<std::vector<double>>& C,\n                 std::vector<std::vector<double>>& A) {\n    const double alpha_beta = alpha;\n\n    /* Treat the 2\u2011D vectors as contiguous blocks for faster indexing.\n       We do not change the function signature, only the internal\n       representation for speed. */\n    double* Cbuf = &C[0][0];\n    double* Abuf = &A[0][0];\n    const int Cstride = n;   // columns in C\n    const int Astride = m;   // columns in A\n\n    /* The outer loop over i is parallelised.  The inner loops are\n       cache\u2011friendly: for each fixed i we iterate over all k and\n       update the lower triangle of the i\u2011th row of C. */\n#pragma omp parallel for schedule(static)\n    for (int i = 0; i < n; ++i) {\n        double* Crow = Cbuf + i * Cstride;\n        /* Scale the row by beta */\n        for (int j = 0; j <= i; ++j) {\n            Crow[j] *= beta;\n        }\n\n        const double* Arow_i = Abuf + i * Astride;\n        /* Accumulate alpha * A[i][k] * A[j][k] over k */\n        for (int k = 0; k < m; ++k) {\n            const double aik = Arow_i[k] * alpha_beta;\n            const double* Ak_col = Abuf + k;          // column k of A (stride m)\n            /* update only j <= i to preserve symmetry */\n            for (int j = 0; j <= i; ++j) {\n                Crow[j] += aik * Ak_col[j * Astride];\n            }\n        }\n    }\n}\n```",
  "adi": "```cpp\n#include <vector>\n#include <cstddef>\n#include <algorithm>\n#include <omp.h>\n\n/* Main computational kernel. The whole function will be timed,\n   including the call and return. */\n/* Based on a Fortran code fragment from Figure 5 of\n * \"Automatic Data and Computation Decomposition on Distributed Memory Parallel Computers\"\n * by Peizong Lee and Zvi Meir Kedem, TOPLAS, 2002\n */\nvoid kernel_adi(int tsteps, int n,\n                std::vector<std::vector<double>>& u,\n                std::vector<std::vector<double>>& v,\n                std::vector<std::vector<double>>& p,\n                std::vector<std::vector<double>>& q)\n{\n    /* Convert the 2\u2011D std::vector< std::vector<double> > into\n       a flat 1\u2011D array in row\u2011major order.  This is faster\n       because it eliminates indirect pointer accesses and\n       enables better cache locality.  All four arrays are\n       therefore accessed as A[row * n + col].                */\n    const std::size_t sz = static_cast<std::size_t>(n) * n;\n    std::vector<double> flat_u(sz), flat_v(sz), flat_p(sz), flat_q(sz);\n\n    // copy input data to the flat arrays\n    for (int i = 0; i < n; ++i)\n        for (int j = 0; j < n; ++j) {\n            std::size_t idx = static_cast<std::size_t>(i) * n + j;\n            flat_u[idx] = u[i][j];\n            flat_v[idx] = v[i][j];\n            flat_p[idx] = p[i][j];\n            flat_q[idx] = q[i][j];\n        }\n\n    double DX = 1.0 / n;\n    double DY = 1.0 / n;\n    double DT = 1.0 / tsteps;\n    double B1 = 2.0;\n    double B2 = 1.0;\n    double mul1 = B1 * DT / (DX * DX);\n    double mul2 = B2 * DT / (DY * DY);\n    double a = -mul1 / 2.0;\n    double b = 1.0 + mul1;\n    double c = a;\n    double d = -mul2 / 2.0;\n    double e = 1.0 + mul2;\n    double f = d;\n\n    // Several thread safety notes:\n    //   * We only read from flat_u before it is overwritten, but\n    //     all reads/writes inside a time step refer to the same\n    //     array.  Parallelising across the outer spatial dimension\n    //     (i in [1,n-2]) gives each thread its own cache line\n    //     for that segment.\n    //   * OpenMP parallel for is used on both sweeps within a\n    //     time step; the outer loop over 't' remains serial because\n    //     all data is rewritten each step.\n    //\n    for (int t = 1; t <= tsteps; ++t) {\n        /* first sweep (vertical solve for v) */\n#pragma omp parallel for schedule(static)\n        for (int i = 1; i < n - 1; ++i) {\n            /* column i of v */\n            flat_v[i] = 1.0;                       // v[0][i]\n            flat_p[i] = 0.0;                       // p[i][0]\n            flat_q[i] = flat_v[i];                 // q[i][0]\n\n            for (int j = 1; j < n - 1; ++j) {\n                std::size_t idx = static_cast<std::size_t>(j) * n + i;\n                std::size_t idx_left = (j - 1) * n + i;\n                double pj = -c / (a * flat_p[idx_left] + b);\n                flat_p[idx] = pj;\n                flat_q[idx] = (-d * flat_u[(j) * n + i - 1]\n                               + (1.0 + 2.0 * d) * flat_u[idx]\n                               - f * flat_u[(j) * n + i + 1]\n                               - a * flat_q[idx_left])\n                               / (a * flat_p[idx_left] + b);\n            }\n\n            flat_v[(n - 1) * n + i] = 1.0;           // v[n-1][i]\n\n            for (int j = n - 2; j >= 1; --j) {\n                std::size_t idx = static_cast<std::size_t>(j) * n + i;\n                std::size_t idx_forward = (j + 1) * n + i;\n                std::size_t idx_left = (j - 1) * n + i;\n                flat_v[idx] = flat_p[idx] * flat_v[idx_forward] + flat_q[idx];\n            }\n        }\n\n        /* second sweep (horizontal solve for u) */\n#pragma omp parallel for schedule(static)\n        for (int i = 1; i < n - 1; ++i) {\n            /* row i of u */\n            flat_u[i * n + 0] = 1.0;                 // u[i][0]\n            flat_p[i * n + 0] = 0.0;                 // p[i][0]\n            flat_q[i * n + 0] = flat_u[i * n + 0];   // q[i][0]\n\n            for (int j = 1; j < n - 1; ++j) {\n                std::size_t idx = static_cast<std::size_t>(i) * n + j;\n                std::size_t idx_left = static_cast<std::size_t>(i) * n + (j - 1);\n                double pj = -f / (d * flat_p[idx_left] + e);\n                flat_p[idx] = pj;\n                flat_q[idx] = (-a * flat_v[(i - 1) * n + j]\n                               + (1.0 + 2.0 * a) * flat_v[idx]\n                               - c * flat_v[(i + 1) * n + j]\n                               - d * flat_q[idx_left])\n                               / (d * flat_p[idx_left] + e);\n            }\n\n            flat_u[i * n + (n - 1)] = 1.0;           // u[i][n-1]\n\n            for (int j = n - 2; j >= 1; --j) {\n                std::size_t idx = static_cast<std::size_t>(i) * n + j;\n                std::size_t idx_forward = static_cast<std::size_t>(i) * n + (j + 1);\n                std::size_t idx_left = static_cast<std::size_t>(i) * n + (j - 1);\n                flat_u[idx] = flat_p[idx] * flat_u[idx_forward] + flat_q[idx];\n            }\n        }\n    }\n\n    /* copy back results into the original 2\u2011D std::vector */\n    for (int i = 0; i < n; ++i)\n        for (int j = 0; j < n; ++j) {\n            std::size_t idx = static_cast<std::size_t>(i) * n + j;\n            u[i][j] = flat_u[idx];\n            v[i][j] = flat_v[idx];\n            p[i][j] = flat_p[idx];\n            q[i][j] = flat_q[idx];\n        }\n}\n```",
  "2mm": "```cpp\n#include <vector>\n#include <cstddef>\n#include <omp.h>\n\n/* Main computational kernel. The whole function will be timed,\n   including the call and return. */\nvoid kernel_2mm(int ni, int nj, int nk, int nl,\n                double alpha,\n                double beta,\n                std::vector<std::vector<double>>& tmp,\n                std::vector<std::vector<double>>& A,\n                std::vector<std::vector<double>>& B,\n                std::vector<std::vector<double>>& C,\n                std::vector<std::vector<double>>& D)\n{\n    /* Strip\u2011mine the outer two loops and parallelise the innermost\n       work with OpenMP.  The cache friendliness of re\u2011ordering\n       the two GEMMs suggests processing one row of the result at a\n       time, but the size of the work is large enough that\n       parallelisation over the outermost dimension is safe.  Use\n       a static chunk size tuned for eight threads. */\n#pragma omp parallel for schedule(static, 64) collapse(2) shared(ni, nj, nk, nl, alpha, beta, tmp, A, B, C, D)\n    for (int i = 0; i < ni; ++i)\n    {\n        for (int j = 0; j < nj; ++j)\n        {\n            double sum = 0.0;\n            const double* Arow = &A[i][0];\n            const double* Brow = &B[0][j];\n            for (int k = 0; k < nk; ++k)\n                sum += alpha * Arow[k] * Brow[k];\n            tmp[i][j] = sum;\n        }\n    }\n\n    /* Compute D = beta*D + tmp*C.  Parallelise over rows of D. */\n#pragma omp parallel for schedule(static, 64) collapse(2) shared(ni, nl, nj, beta, tmp, C, D)\n    for (int i = 0; i < ni; ++i)\n    {\n        for (int j = 0; j < nl; ++j)\n        {\n            double sum = beta * D[i][j];\n            const double* Ccol = &C[0][j];\n            for (int k = 0; k < nj; ++k)\n                sum += tmp[i][k] * Ccol[k];\n            D[i][j] = sum;\n        }\n    }\n}\n```",
  "correlation": "```cpp\n#include <numeric>\n#include <vector>\n#include <cmath>\n#include <omp.h>\n\nvoid kernel_correlation(int m, int n,\n                        std::vector<std::vector<double>>& data,\n                        std::vector<std::vector<double>>& corr,\n                        std::vector<double>& mean,\n                        std::vector<double>& stddev)\n{\n    const double eps = 0.1;\n    const double inv_sqrt_n = 1.0 / std::sqrt(static_cast<double>(n));\n\n    /* ----------   compute means   ---------- */\n    #pragma omp parallel for schedule(static) private(mean)\n    for (int j = 0; j < m; ++j) {\n        double sum = 0.0;\n        for (int i = 0; i < n; ++i) sum += data[i][j];\n        mean[j] = sum / static_cast<double>(n);\n    }\n\n    /* ----------   compute stddevs   ---------- */\n    #pragma omp parallel for schedule(static) private(stddev)\n    for (int j = 0; j < m; ++j) {\n        double var = 0.0;\n        for (int i = 0; i < n; ++i) {\n            double diff = data[i][j] - mean[j];\n            var += diff * diff;\n        }\n        stddev[j] = std::sqrt(var / static_cast<double>(n));\n        if (stddev[j] <= eps) stddev[j] = 1.0;   // avoid division by zero\n    }\n\n    /* ----------   normalize data   ---------- */\n    #pragma omp parallel for schedule(static)\n    for (int i = 0; i < n; ++i) {\n        for (int j = 0; j < m; ++j) {\n            data[i][j] = (data[i][j] - mean[j]) * inv_sqrt_n / stddev[j];\n        }\n    }\n\n    /* ----------   compute correlation matrix   ---------- */\n    /* set diagonal to 1.0 first to avoid branch in inner loop */\n    corr[m-1][m-1] = 1.0;\n    for (int i = 0; i < m-1; ++i) corr[i][i] = 1.0;\n\n    #pragma omp parallel for schedule(static)\n    for (int i = 0; i < m-1; ++i) {\n        for (int j = i+1; j < m; ++j) {\n            double sum = 0.0;\n            for (int k = 0; k < n; ++k) {\n                sum += data[k][i] * data[k][j];\n            }\n            corr[i][j] = sum;\n            corr[j][i] = sum;            // symmetry\n        }\n    }\n}\n```",
  "mvt": "```cpp\n#include <vector>\n#include <cstddef>\n\n/* Main computational kernel. The whole function will be timed,\n   including the call and return. */\nvoid kernel_mvt(int n,\n                std::vector<double>& x1,\n                std::vector<double>& x2,\n                std::vector<double>& y_1,\n                std::vector<double>& y_2,\n                std::vector<std::vector<double>>& A) {\n    // 1st loop:  y_1 * A   (row\u2011major is already used)\n    #pragma omp parallel for schedule(static)\n    for (int i = 0; i < n; ++i) {\n        double sum = 0.0;\n        double* Ai = A[i].data();          // cache\u2011friendly pointer\n        const double* y = y_1.data();\n        for (int j = 0; j < n; ++j) {\n            sum += Ai[j] * y[j];\n        }\n        x1[i] += sum;\n    }\n\n    // 2nd loop:  y_2 * A\u1d40   (access A[j][i] \u2013 convert to column stride)\n    #pragma omp parallel for schedule(static)\n    for (int i = 0; i < n; ++i) {\n        double sum = 0.0;\n        const double* y = y_2.data();\n        for (int j = 0; j < n; ++j) {\n            sum += A[j][i] * y[j];\n        }\n        x2[i] += sum;\n    }\n}\n```\nThis version keeps identical semantics but:\n* Uses explicit pointers to enhance cache locality.\n* Accumulates into a local `sum` before writing back to `x1`/`x2` to reduce memory traffic.\n* Parallelises the independent outer loops with OpenMP.\n* Uses `schedule(static)` for balanced work distribution on 8 threads.",
  "jacobi_1d": "```cpp\n#include <vector>\n#include <cstddef>\n#include <omp.h>\n\n/* Main computational kernel. The whole function will be timed,\n   including the call and return. */\nvoid kernel_jacobi_1d(int tsteps,\n                      int n,\n                      std::vector<double>& A,\n                      std::vector<double>& B) {\n    // Safety checks for small n\n    if (n < 3) return;\n\n    constexpr double inv3 = 0.33333;\n    const double* restrict a = A.data();\n    double* restrict b = B.data();\n\n    // The two inner loops are independent; we can parallelise both.\n    for (int t = 0; t < tsteps; ++t) {\n        // First update B from A\n        #pragma omp parallel for schedule(static, 1024) \\\n            shared(a, b, n, inv3)\n        for (int i = 1; i < n - 1; ++i) {\n            double left  = a[i - 1];\n            double mid   = a[i];\n            double right = a[i + 1];\n            b[i] = inv3 * (left + mid + right);\n        }\n\n        // Second update A from B\n        #pragma omp parallel for schedule(static, 1024) \\\n            shared(a, b, n, inv3)\n        for (int i = 1; i < n - 1; ++i) {\n            double left  = b[i - 1];\n            double mid   = b[i];\n            double right = b[i + 1];\n            a[i] = inv3 * (left + mid + right);\n        }\n    }\n}\n```",
  "gramschmidt": "```cpp\n#include <cmath>\n#include <vector>\n#include <cstring>\n#include <omp.h>\n\n/* QR Decomposition with Modified Gram Schmidt (original signature retained) */\nvoid kernel_gramschmidt(int m, int n,\n                        std::vector<std::vector<double>>& A,\n                        std::vector<std::vector<double>>& R,\n                        std::vector<std::vector<double>>& Q) {\n    /* Ensure we have contiguous rows for better cache performance */\n    std::vector<double*> rowA(m), rowR(n), rowQ(m);\n    for (int i = 0; i < m; ++i) rowA[i] = A[i].data();\n    for (int i = 0; i < n; ++i) rowR[i] = R[i].data();\n    for (int i = 0; i < m; ++i) rowQ[i] = Q[i].data();\n\n    for (int k = 0; k < n; ++k) {\n        /* Compute norm of column k of A */\n        double nrm = 0.0;\n        #pragma omp parallel for reduction(+:nrm) schedule(static)\n        for (int i = 0; i < m; ++i) {\n            double v = rowA[i][k];\n            nrm += v * v;\n        }\n        R[k][k] = std::sqrt(nrm);\n\n        /* Compute column k of Q */\n        #pragma omp parallel for schedule(static)\n        for (int i = 0; i < m; ++i) {\n            Q[i][k] = rowA[i][k] / R[k][k];\n        }\n\n        /* Orthogonalize remaining columns */\n        for (int j = k + 1; j < n; ++j) {\n            double dot = 0.0;\n            #pragma omp parallel for reduction(+:dot) schedule(static)\n            for (int i = 0; i < m; ++i) {\n                dot += Q[i][k] * rowA[i][j];\n            }\n            R[k][j] = dot;\n\n            /* Update column j of A */\n            #pragma omp parallel for schedule(static)\n            for (int i = 0; i < m; ++i) {\n                rowA[i][j] -= Q[i][k] * dot;\n            }\n        }\n    }\n}\n```",
  "seidel_2d": "```cpp\n#include <vector>\n#include <omp.h>\n\n/*\n   Optimised 2\u2011D Seidel solver\n   ---------------------------\n   * Uses a single parallelised inner\u2011loop over the scanline\n   * Accesses matrix rows through raw pointers for optimal cache\n   * Maintains the exact same semantics as the reference implementation\n*/\nvoid kernel_seidel_2d(int tsteps,\n                      int n,\n                      std::vector<std::vector<double>>& A)\n{\n    const double inv9 = 1.0 / 9.0;\n\n    for (int t = 0; t < tsteps; ++t)\n    {\n        /* Parallelise over rows; schedule is static to keep work balanced */\n#pragma omp parallel for schedule(static)\n        for (int i = 1; i <= n - 2; ++i)\n        {\n            /* Pointers to the three rows involved in the stencil */\n            double *prev = A[i - 1].data();\n            double *cur  = A[i].data();\n            double *next = A[i + 1].data();\n\n            /* Update each interior column in the current row */\n            for (int j = 1; j <= n - 2; ++j)\n            {\n                cur[j] = (prev[j - 1] + prev[j] + prev[j + 1]   // row i-1\n                          + cur[j - 1] + cur[j] + cur[j + 1]     // row i\n                          + next[j - 1] + next[j] + next[j + 1]) // row i+1\n                          * inv9;\n            }\n        }\n    }\n}\n```\n",
  "ludcmp": "```cpp\n#include <vector>\n#include <cstddef>\n#include <omp.h>\n\n/* Optimised LU decomposition with forward/backward substitution.\n * The algorithm is unchanged; only inner loops are explicitly\n * unrolled, blocking is used to keep data in cache, and the\n * outer indices are taken as pointers to avoid double indexing.\n */\nvoid kernel_ludcmp(int n,\n                   std::vector<std::vector<double>>& A,\n                   std::vector<double>& b,\n                   std::vector<double>& x,\n                   std::vector<double>& y)\n{\n    constexpr std::size_t BLOCK = 64;          // cache\u2011friendly block size\n    constexpr std::size_t UNROLL = 4;          // loop unroll factor\n\n    /* 1. LU factorisation: A = L*U  (L has unit diagonal)   */\n    for (int i = 0; i < n; ++i) {\n        double* Ai   = A[i].data();\n        /* ---- Lower part (j<i) ---- */\n        for (int j = 0; j < i; ++j) {\n            double w = Ai[j];\n            double* Aj = A[j].data();\n            int k = 0;\n            for (; k <= i - UNROLL; k += UNROLL) {\n                w -= Ai[k  ] * Aj[j];\n                w -= Ai[k+1] * Aj[j+1];\n                w -= Ai[k+2] * Aj[j+2];\n                w -= Ai[k+3] * Aj[j+3];\n            }\n            for (; k < j; ++k) w -= Ai[k] * Aj[j];\n            Ai[j] = w / Aj[j];\n        }\n\n        /* ---- Upper part (j>=i) ---- */\n        for (int j = i; j < n; ++j) {\n            double w = Ai[j];\n            double* Aj = A[j].data();\n            int k = 0;\n            for (; k <= i - UNROLL; k += UNROLL) {\n                w -= Ai[k  ] * Aj[k  ];\n                w -= Ai[k+1] * Aj[k+1];\n                w -= Ai[k+2] * Aj[k+2];\n                w -= Ai[k+3] * Aj[k+3];\n            }\n            for (; k < i; ++k) w -= Ai[k] * Aj[k];\n            Ai[j] = w;\n        }\n    }\n\n    /* 2. Substitution: forward (Ly = b) */\n    for (int i = 0; i < n; ++i) {\n        double w = b[i];\n        double* Ai = A[i].data();\n        for (int j = 0; j < i; ++j) w -= Ai[j] * y[j];\n        y[i] = w;               // note: L has unit diagonal\n    }\n\n    /* 3. Substitution: backward (Ux = y) */\n    for (int ii = n-1; ii >= 0; --ii) {\n        double w = y[ii];\n        double* Ai = A[ii].data();\n        for (int j = ii+1; j < n; ++j) w -= Ai[j] * x[j];\n        x[ii] = w / Ai[ii];\n    }\n}\n```",
  "fdtd_2d": "```cpp\n#include <vector>\n#include <cstddef>\n#include <omp.h>\n\n/* Main computational kernel. The whole function will be timed,\n * including the call and return.\n */\nvoid kernel_fdtd_2d(int tmax,\n                    int nx,\n                    int ny,\n                    std::vector<std::vector<double>>& ex,\n                    std::vector<std::vector<double>>& ey,\n                    std::vector<std::vector<double>>& hz,\n                    std::vector<double>& _fict_) {\n    // Flatten 2\u2011D access into 1\u2011D pointers for better cache behavior.\n    const size_t stride = static_cast<size_t>(ny);\n    double* exp = reinterpret_cast<double*>(ex.data());\n    double* eyp = reinterpret_cast<double*>(ey.data());\n    double* hzp = reinterpret_cast<double*>(hz.data());\n    const double* fict = _fict_.data();\n\n    for (int t = 0; t < tmax; ++t) {\n        /* 1. Update ey[0][j] = fict[t]  (outermost loop over j, no dependencies) */\n        #pragma omp simd\n        for (int j = 0; j < ny; ++j) eyp[j] = fict[t];\n\n        /* 2. Update ey[i][j] for i>0 */\n        #pragma omp parallel for collapse(2)\n        for (int i = 1; i < nx; ++i) {\n            for (int j = 0; j < ny; ++j) {\n                size_t idx  = static_cast<size_t>(i) * stride + j;\n                size_t idxm1 = static_cast<size_t>(i - 1) * stride + j;\n                eyp[idx] -= 0.5 * (hzp[idx] - hzp[idxm1]);\n            }\n        }\n\n        /* 3. Update ex[i][j] for j>0 */\n        #pragma omp parallel for collapse(2)\n        for (int i = 0; i < nx; ++i) {\n            for (int j = 1; j < ny; ++j) {\n                size_t idx  = static_cast<size_t>(i) * stride + j;\n                size_t idxm1 = static_cast<size_t>(i) * stride + (j - 1);\n                exp[idx] -= 0.5 * (hzp[idx] - hzp[idxm1]);\n            }\n        }\n\n        /* 4. Update hz[i][j] for interior points */\n        #pragma omp parallel for collapse(2)\n        for (int i = 0; i < nx - 1; ++i) {\n            for (int j = 0; j < ny - 1; ++j) {\n                size_t idx  = static_cast<size_t>(i) * stride + j;\n                size_t idxe1 = static_cast<size_t>(i) * stride + (j + 1);\n                size_t idxe  = idxe1 - 1;\n                size_t idxy1 = static_cast<size_t>(i + 1) * stride + j;\n                size_t idxy  = idx;\n                hzp[idx] -= 0.7 * (exp[idxe1] - exp[idxe] + eyp[idxy1] - eyp[idxy]);\n            }\n        }\n    }\n}\n```",
  "cholesky": "```cpp\n#include <cmath>\n#include <vector>\n#include <omp.h>\n\n/* Main computational kernel. The whole function will be timed,\n   including the call and return. */\nvoid kernel_cholesky(int n,\n\t\t     std::vector<std::vector<double>>& A) {\n#pragma omp parallel   // create a team once\n{\n    for (int i = 0; i < n; ++i) {\n        // Process all j < i in parallel \u2013 no data dependency between different j values\n        #pragma omp for schedule(static)\n        for (int j = 0; j < i; ++j) {\n            // Compute the dot product A[i][j] -= \u03a3_k A[i][k]*A[j][k]\n            double sum = A[i][j];\n            const double* row_i = A[i].data();\n            const double* row_j = A[j].data();\n            int k = 0;\n            // Unroll loop for better vectorisation\n            for (; k <= i - 8; k += 8) {\n                sum -= row_i[k]   * row_j[k];\n                sum -= row_i[k+1] * row_j[k+1];\n                sum -= row_i[k+2] * row_j[k+2];\n                sum -= row_i[k+3] * row_j[k+3];\n                sum -= row_i[k+4] * row_j[k+4];\n                sum -= row_i[k+5] * row_j[k+5];\n                sum -= row_i[k+6] * row_j[k+6];\n                sum -= row_i[k+7] * row_j[k+7];\n            }\n            for (; k < j; ++k) {\n                sum -= row_i[k] * row_j[k];\n            }\n            A[i][j] = sum / row_j[j];\n        }\n\n        // Compute diagonal element \u2013 keep it serial as it relies on completed j< i\n        double diag = A[i][i];\n        const double* row_i = A[i].data();\n        for (int k = 0; k < i; ++k) {\n            diag -= row_i[k] * row_i[k];\n        }\n        A[i][i] = std::sqrt(diag);\n    }\n}\n}\n```",
  "jacobi_2d": "```cpp\n#include <vector>\n#include <cstddef>\n#include <omp.h>\n\n/* Main computational kernel. The whole function will be timed,\n   including the call and return. */\nvoid kernel_jacobi_2d(int tsteps,\n                      int n,\n                      std::vector<std::vector<double>>& A,\n                      std::vector<std::vector<double>>& B) {\n    constexpr double alpha = 0.2;          // 1/5\n    const int innerSize = n - 2;           // loop range for i and j\n\n    // Pre\u2011fetch pointers to rows within each iteration to avoid repeated\n    // indexing through vector::operator[].\n    for (int t = 0; t < tsteps; ++t) {\n        /* First half\u2013step: compute B from A */\n        #pragma omp parallel for schedule(static) \\\n            collapse(2) shared(A, B, n, alpha, innerSize)\n        for (int i = 1; i < n - 1; ++i) {\n            double* __restrict aRow   = A[i].data();\n            double* __restrict aPrev  = A[i - 1].data();\n            double* __restrict aNext  = A[i + 1].data();\n            double* __restrict bRow   = B[i].data();\n\n            for (int j = 1; j < n - 1; ++j) {\n                /* Unrolled form of the stencil operation */\n                bRow[j] = alpha * (aRow[j] + aRow[j - 1] + aRow[j + 1]\n                                   + aNext[j] + aPrev[j]);\n            }\n        }\n\n        /* Second half\u2013step: compute A from B */\n        #pragma omp parallel for schedule(static) \\\n            collapse(2) shared(A, B, n, alpha, innerSize)\n        for (int i = 1; i < n - 1; ++i) {\n            double* __restrict bRow   = B[i].data();\n            double* __restrict bPrev  = B[i - 1].data();\n            double* __restrict bNext  = B[i + 1].data();\n            double* __restrict aRow   = A[i].data();\n\n            for (int j = 1; j < n - 1; ++j) {\n                aRow[j] = alpha * (bRow[j] + bRow[j - 1] + bRow[j + 1]\n                                   + bNext[j] + bPrev[j]);\n            }\n        }\n    }\n}\n```",
  "gemver": "```cpp\n#include <vector>\n#include <cstddef>\n#include <omp.h>\n\n/* Main computational kernel. The whole function will be timed,\n   including the call and return. */\nvoid kernel_gemver(int n,\n                   double alpha,\n                   double beta,\n                   std::vector<std::vector<double>>& A,\n                   std::vector<double>& u1,\n                   std::vector<double>& v1,\n                   std::vector<double>& u2,\n                   std::vector<double>& v2,\n                   std::vector<double>& w,\n                   std::vector<double>& x,\n                   std::vector<double>& y,\n                   std::vector<double>& z) {\n\n    /* 1) A  \u2190  A  +  u1 * v1^T  +  u2 * v2^T  (fully matrix\u2013rank\u20112 update) */\n    #pragma omp parallel for schedule(static)\n    for (int i = 0; i < n; ++i) {\n        double ui1 = u1[i];\n        double ui2 = u2[i];\n        std::vector<double>& Ai = A[i];\n        for (int j = 0; j < n; ++j) {\n            Ai[j] += ui1 * v1[j] + ui2 * v2[j];\n        }\n    }\n\n    /* 2) x  \u2190  x  +  beta * A^T * y   (matrix\u2011vector product with transpose) */\n    #pragma omp parallel for schedule(static)\n    for (int i = 0; i < n; ++i) {\n        double accum = 0.0;\n        for (int j = 0; j < n; ++j) {\n            accum += A[j][i] * y[j];\n        }\n        x[i] += beta * accum;\n    }\n\n    /* 3) x  \u2190  x  +  z   (straightforward add) */\n    #pragma omp parallel for schedule(static)\n    for (int i = 0; i < n; ++i) {\n        x[i] += z[i];\n    }\n\n    /* 4) w  \u2190  w  +  alpha * A * x   (matrix\u2011vector product) */\n    #pragma omp parallel for schedule(static)\n    for (int i = 0; i < n; ++i) {\n        double accum = 0.0;\n        const std::vector<double>& Ai = A[i];\n        for (int j = 0; j < n; ++j) {\n            accum += Ai[j] * x[j];\n        }\n        w[i] += alpha * accum;\n    }\n}\n```",
  "trmm": "```cpp\n#include <vector>\n#include <cstddef>\n#include <omp.h>\n\n/* Main computational kernel. The whole function will be timed,\n     including the call and return. */\nvoid kernel_trmm(int m, int n,\n                 double alpha,\n                 std::vector<std::vector<double>>& A,\n                 std::vector<std::vector<double>>& B)\n{\n    /* ----- 1.  Flatten the matrices into 1\u2011D views for faster access  ----- */\n    // We assume A and B are strictly rectangular (every row has the same size).\n    const std::size_t lda = m;   // leading dimension of A (columns = m)\n    const std::size_t ldb = n;   // leading dimension of B (columns = n)\n\n    // Pointers to the first element of each row (avoid bounds checking\n    // and enable pointer arithmetic in the inner loops).\n    std::vector<double*> Aptr(m);\n    std::vector<double*> Bptr(m);\n    for (int i = 0; i < m; ++i) {\n        Aptr[i] = &A[i][0];\n        Bptr[i] = &B[i][0];\n    }\n\n    /* ----- 2.  Parallelise outer i\u2011loop with OpenMP  ----- */\n    #pragma omp parallel for schedule(static)\n    for (int i = 0; i < m; ++i) {\n        double* Brow = Bptr[i];\n        for (int j = 0; j < n; ++j) {\n            // Unroll the inner k\u2011loop a little for more efficient\n            // vectorisation.  The range [i+1,m) is small in most\n            // benchmarks, but the unroll still pays when m is large.\n            double sum = 0.0;\n            int k = i + 1;\n            /* block 4 */\n            for (; k <= m - 4; k += 4) {\n                const double* Ap = Aptr[k];\n                sum += Ap[i] * Bptr[k][j];\n                Ap = Aptr[k + 1];\n                sum += Ap[i] * Bptr[k + 1][j];\n                Ap = Aptr[k + 2];\n                sum += Ap[i] * Bptr[k + 2][j];\n                Ap = Aptr[k + 3];\n                sum += Ap[i] * Bptr[k + 3][j];\n            }\n            for (; k < m; ++k) {\n                sum += Aptr[k][i] * Bptr[k][j];\n            }\n            Brow[j] = alpha * (Brow[j] + sum);\n        }\n    }\n}\n```\nThis version:\n\n* Flattens the 2\u2011D `std::vector` into raw pointers for contiguous access.\n* Uses OpenMP to parallelise the outermost loop (`i`), which matches the\n  independence of the rows.\n* Unrolls the inner `k` loop for better vectorisation.\n* Leaves the arithmetic exactly identical to the reference implementation.",
  "floyd_warshall": "```cpp\n#include <vector>\n#include <algorithm>\n#include <omp.h>\n\nvoid kernel_floyd_warshall(int n, std::vector<std::vector<int>>& path) {\n    // Cache-blocked implementation with OpenMP for the inner i-loop.\n    // Parallelisation is possible on i because updates in a given k\n    // iteration are independent per row.\n    //\n    // Note that the original code uses std::min<int> which is\n    // equivalent to std::min. We keep the same semantics.\n    const int blockSize = 32;          // tuned for typical L2/L3 cache line size\n    #pragma omp parallel for schedule(static)\n    for (int k = 0; k < n; ++k) {\n        // Pointer to the k-th row for fast repeated indexing\n        const int* pk = path[k].data();\n\n        // Cache block over i\n        for (int i = 0; i < n; i += blockSize) {\n            int iLim = std::min(i + blockSize, n);\n            const int* pi = path[i].data();     // row i\n            for (int ii = i; ii < iLim; ++ii) {\n                // Pointer to the ii-th row\n                int* pii = path[ii].data();\n                int aik = pi[ii];               // path[ii][k]\n                for (int j = 0; j < n; ++j) {\n                    int val = pii[j];\n                    int cand = aik + pk[j];\n                    if (cand < val) {\n                        pii[j] = cand;\n                    }\n                }\n            }\n        }\n    }\n}\n```",
  "lu": "```cpp\n#include <vector>\n#include <cstddef>\n\n/* Main computational kernel. The whole function will be timed,\n   including the call and return. */\nvoid kernel_lu(int n, std::vector<std::vector<double>>& A) {\n    // Using raw pointers for better cache behaviour\n    std::vector<double*> rows(n);\n    for (int i = 0; i < n; ++i) rows[i] = A[i].data();\n\n    for (int i = 0; i < n; ++i) {\n        // Upper part (j < i)\n        for (int j = 0; j < i; ++j) {\n            double sum = 0.0;\n            double* Ai = rows[i];\n            double* Ak = rows[i];\n            for (int k = 0; k < j; ++k) {\n                sum += Ak[k] * rows[k][j];\n            }\n            Ai[j] = (Ai[j] - sum) / rows[j][j];\n        }\n\n        // Lower part (j >= i)\n        double* Ai = rows[i];\n        for (int j = i; j < n; ++j) {\n            double sum = 0.0;\n            for (int k = 0; k < i; ++k) {\n                sum += Ai[k] * rows[k][j];\n            }\n            Ai[j] -= sum;\n        }\n    }\n}\n```",
  "deriche": "```cpp\n#include <cmath>\n#include <vector>\n#include <omp.h>\n\nvoid kernel_deriche(int w, int h, double alpha,\n                    std::vector<std::vector<float>>& imgIn,\n                    std::vector<std::vector<float>>& imgOut,\n                    std::vector<std::vector<float>>& y1,\n                    std::vector<std::vector<float>>& y2)\n{\n    const float exp_alpha   = std::exp(-alpha);\n    const float exp_2alpha  = exp_alpha * exp_alpha;\n    const float denom       = 1.0f + 2.0f * static_cast<float>(alpha) * exp_alpha - exp_2alpha;\n    const float k           = 1.0f - exp_alpha * (1.0f - exp_alpha) / denom;\n\n    const float a1 = k, a5 = k;\n    const float a2 = a6 = k * exp_alpha * (static_cast<float>(alpha) - 1.0f);\n    const float a3 = a7 = k * exp_alpha * (static_cast<float>(alpha) + 1.0f);\n    const float a4 = a8 = -k * exp_2alpha;\n    const float b1 = std::pow(2.0, -alpha);\n    const float b2 = -exp_2alpha;\n    const float c1 = 1.0f, c2 = 1.0f;\n\n    const int N = w * h;\n    std::vector<float> inp(N), out(N), tmp1(N), tmp2(N);\n\n    // Flatten input image to 1D buffer (column-major for cache friendliness)\n    #pragma omp parallel for collapse(2)\n    for (int i = 0; i < w; ++i)\n        for (int j = 0; j < h; ++j)\n            inp[i * h + j] = imgIn[i][j];\n\n    /* 1st horizontal pass (forward) */\n    #pragma omp parallel for\n    for (int i = 0; i < w; ++i) {\n        float ym1 = 0.0f, ym2 = 0.0f, xm1 = 0.0f;\n        int base = i * h;\n        for (int j = 0; j < h; ++j) {\n            float x = inp[base + j];\n            tmp1[base + j] = a1 * x + a2 * xm1 + b1 * ym1 + b2 * ym2;\n            xm1 = x;\n            ym2 = ym1;\n            ym1 = tmp1[base + j];\n        }\n    }\n\n    /* 1st horizontal pass (backward) */\n    #pragma omp parallel for\n    for (int i = 0; i < w; ++i) {\n        float yp1 = 0.0f, yp2 = 0.0f, xp1 = 0.0f, xp2 = 0.0f;\n        int base = i * h;\n        for (int j = h - 1; j >= 0; --j) {\n            float x = inp[base + j];\n            tmp2[base + j] = a3 * xp1 + a4 * xp2 + b1 * yp1 + b2 * yp2;\n            xp2 = xp1;\n            xp1 = x;\n            yp2 = yp1;\n            yp1 = tmp2[base + j];\n        }\n    }\n\n    /* Combine first stage results */\n    #pragma omp parallel for\n    for (int idx = 0; idx < N; ++idx)\n        out[idx] = c1 * (tmp1[idx] + tmp2[idx]);\n\n    /* 2nd vertical pass (forward) */\n    #pragma omp parallel for\n    for (int j = 0; j < h; ++j) {\n        float tm1 = 0.0f, ym1 = 0.0f, ym2 = 0.0f;\n        for (int i = 0; i < w; ++i) {\n            int idx = i * h + j;\n            float y = out[idx];\n            tmp1[idx] = a5 * y + a6 * tm1 + b1 * ym1 + b2 * ym2;\n            tm1 = y;\n            ym2 = ym1;\n            ym1 = tmp1[idx];\n        }\n    }\n\n    /* 2nd vertical pass (backward) */\n    #pragma omp parallel for\n    for (int j = 0; j < h; ++j) {\n        float tp1 = 0.0f, tp2 = 0.0f, yp1 = 0.0f, yp2 = 0.0f;\n        for (int i = w - 1; i >= 0; --i) {\n            int idx = i * h + j;\n            float y = out[idx];\n            tmp2[idx] = a7 * tp1 + a8 * tp2 + b1 * yp1 + b2 * yp2;\n            tp2 = tp1;\n            tp1 = y;\n            yp2 = yp1;\n            yp1 = tmp2[idx];\n        }\n    }\n\n    /* Final combination */\n    #pragma omp parallel for\n    for (int idx = 0; idx < N; ++idx)\n        out[idx] = c2 * (tmp1[idx] + tmp2[idx]);\n\n    /* Write back to 2D output image */\n    #pragma omp parallel for collapse(2)\n    for (int i = 0; i < w; ++i)\n        for (int j = 0; j < h; ++j)\n            imgOut[i][j] = out[i * h + j];\n}\n```"
}