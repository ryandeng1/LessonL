{
  "syr2k": "```cpp\n#include <vector>\n#include <omp.h>\n\n/* Main computational kernel. The whole function will be timed,\n   including the call and return. */\nvoid kernel_syr2k(int n, int m,\n                  double alpha,\n                  double beta,\n                  std::vector<std::vector<double>>& C,\n                  std::vector<std::vector<double>>& A,\n                  std::vector<std::vector<double>>& B) {\n\n    // Parallelise the outer loop over rows of C\n    #pragma omp parallel for schedule(static)\n    for (int i = 0; i < n; ++i) {\n        // Multiply the symmetric lower part of C by beta\n        for (int j = 0; j <= i; ++j) {\n            C[i][j] *= beta;\n        }\n\n        // Perform the syr2k update for row i\n        for (int k = 0; k < m; ++k) {\n            const double aik = A[i][k];\n            const double bik = B[i][k];\n            for (int j = 0; j <= i; ++j) {\n                C[i][j] += (A[j][k] * bik + B[j][k] * aik) * alpha;\n            }\n        }\n    }\n}\n```",
  "gemm": "```cpp\n#include <vector>\n#include <omp.h>\n\nvoid kernel_gemm(int ni, int nj, int nk,\n                 double alpha,\n                 double beta,\n                 std::vector<std::vector<double>>& A,\n                 std::vector<std::vector<double>>& B,\n                 std::vector<std::vector<double>>& C) {\n\n    // Parallelise the outer loop over the rows of C (and A).\n    #pragma omp parallel for schedule(static)\n    for (int i = 0; i < ni; ++i) {\n        // Pointers to the current row of A and C for faster access.\n        double* const Ai = A[i].data();\n        double* const Ci = C[i].data();\n\n        // Scale the output row by beta.\n        for (int j = 0; j < nj; ++j) {\n            Ci[j] *= beta;\n        }\n\n        // Perform the rank\u2011k update for this row.\n        for (int k = 0; k < nk; ++k) {\n            double aik = alpha * Ai[k];\n            double* const Bj = B[k].data();   // current row of B\n            for (int j = 0; j < nj; ++j) {\n                Ci[j] += aik * Bj[j];\n            }\n        }\n    }\n}\n```",
  "covariance": "```cpp\n#include <vector>\n#include <cstddef>\n#include <omp.h>\n#include <algorithm>\n#include <numeric>\n\nvoid kernel_covariance(int m, int n,\n                       std::vector<std::vector<double>>& data,\n                       std::vector<std::vector<double>>& cov,\n                       std::vector<double>& mean)\n{\n    // 1. Compute column means in parallel\n    #pragma omp parallel for\n    for (int j = 0; j < m; ++j) {\n        double sum = 0.0;\n        for (int i = 0; i < n; ++i) sum += data[i][j];\n        mean[j] = sum / static_cast<double>(n);\n    }\n\n    // 2. Center the data in place\n    #pragma omp parallel for collapse(2)\n    for (int i = 0; i < n; ++i) {\n        for (int j = 0; j < m; ++j) {\n            data[i][j] -= mean[j];\n        }\n    }\n\n    // 3. Compute covariance matrix (upper triangle) in parallel\n    #pragma omp parallel for collapse(2)\n    for (int i = 0; i < m; ++i) {\n        for (int j = i; j < m; ++j) {\n            double sum = 0.0;\n            for (int k = 0; k < n; ++k) {\n                sum += data[k][i] * data[k][j];\n            }\n            cov[i][j] = sum / (static_cast<double>(n) - 1.0);\n        }\n    }\n\n    // 4. Symmetrize the matrix\n    #pragma omp parallel for\n    for (int i = 1; i < m; ++i) {\n        for (int j = 0; j < i; ++j) {\n            cov[i][j] = cov[j][i];\n        }\n    }\n}\n```",
  "doitgen": "```cpp\n#include <vector>\n#include <cstddef>\n#include <omp.h>\n\n/* Optimized version of kernel_doitgen\n *   - Uses pointer arithmetic for cache friendly access\n *   - Parallelises the outer loops with OpenMP\n *   - Keeps the exact behaviour of the original kernel */\nvoid kernel_doitgen(int nr, int nq, int np,\n                    std::vector<std::vector<std::vector<double>>>& A,\n                    std::vector<std::vector<double>>& C4,\n                    std::vector<double>& sum)\n{\n    // Ensure the temporary work vector can hold np elements\n    std::vector<double> local_sum(np);\n\n    // OpenMP parallelisation over the outermost loops (r, q)\n    #pragma omp parallel for collapse(2) schedule(static)\n    for (int r = 0; r < nr; ++r) {\n        for (int q = 0; q < nq; ++q) {\n            double const* A_row = A[r][q].data();   // [np]\n            double const* C4_col;                   // will point to each column\n            double*    result_row = A[r][q].data(); // [np]\n            double*    sum_ptr    = local_sum.data();\n\n            // compute dot products for each p\n            for (int p = 0; p < np; ++p) {\n                double accum = 0.0;\n                C4_col = C4[p].data(); // C4[s][p] -> column p of C4\n                for (int s = 0; s < np; ++s) {\n                    accum += A_row[s] * C4_col[s];\n                }\n                sum_ptr[p] = accum;   // store intermediate result\n            }\n\n            // write results back to A\n            // (optional: we could skip the intermediate sum vector entirely\n            //  by writing directly into result_row inside the loop above,\n            //  but keeping separate buffer keeps the behaviour identical)\n            for (int p = 0; p < np; ++p) {\n                result_row[p] = sum_ptr[p];\n            }\n        }\n    }\n}\n```",
  "nussinov": "```cpp\n#include <vector>\n#include <algorithm>\n#include <cstddef>\n#include <omp.h>\n\n/* Main computational kernel. The whole function will be timed,\n   including the call and return. */\n/* Original version by Dave Wonnacott at Haverford College,\n   with help from Allison Lake, Ting Zhou, and Tian Jin,\n   based on algorithm by Nussinov, described in Allison Lake's senior thesis.\n*/\n\ninline int match(int b1, int b2) noexcept\n{\n    return (b1 + b2 == 3) ? 1 : 0;\n}\n\n// The vector `table` is expected to contain n*n elements (row\u2011major).\n// The caller guarantees that table.size() >= n*n.\nvoid kernel_nussinov(int n,\n                     std::vector<int>& seq,\n                     std::vector<std::vector<int>>& table)\n{\n    // Flatten the table for faster index calculation\n    const int N = n;\n    std::vector<int> &t = table[0]; // assume table is a contiguous 2\u2011D wrapper.\n    const int stride = N;\n\n    // Process anti\u2011diagonals: for each length l = j-i\n    for (int l = 1; l < N; ++l) {\n#pragma omp parallel for schedule(static)\n        for (int i = 0; i + l < N; ++i) {\n            int j = i + l;\n            int idx = i * stride + j;\n            int best = 0;\n\n            // Table entries for empty intervals are 0 by initialization.\n            // Option 1: ignore j\n            if (j - 1 >= 0)\n                best = std::max(best, t[i * stride + (j - 1)]);\n\n            // Option 2: ignore i\n            if (i + 1 < N)\n                best = std::max(best, t[(i + 1) * stride + j]);\n\n            // Option 3: pair i and j\n            if (j - 1 >= 0 && i + 1 < N) {\n                if (i < j - 1) {\n                    best = std::max(best, t[(i + 1) * stride + (j - 1)] + match(seq[i], seq[j]));\n                } else {\n                    best = std::max(best, t[(i + 1) * stride + (j - 1)]);\n                }\n            }\n\n            // Option 4: split at k\n            for (int k = i + 1; k < j; ++k) {\n                int val = t[i * stride + k] + t[(k + 1) * stride + j];\n                if (val > best) best = val;\n            }\n\n            t[idx] = best;\n        }\n    }\n}\n```\n",
  "atax": "```cpp\n#include <vector>\n#include <omp.h>\n\n/* Main computational kernel. The whole function will be timed,\n   including the call and return. */\nvoid kernel_atax(int m, int n,\n                 std::vector<std::vector<double>>& A,\n                 std::vector<double>& x,\n                 std::vector<double>& y,\n                 std::vector<double>& tmp) {\n\n    // Initialize y to zero (single thread)\n    #pragma omp single\n    for (int j = 0; j < n; ++j) {\n        y[j] = 0.0;\n    }\n\n    // Parallel loop over rows of A\n    #pragma omp parallel\n    {\n        // Thread\u2011private local accumulation of y\n        std::vector<double> y_local(n, 0.0);\n\n        // Iterate over rows assigned to this thread\n        for (int i = omp_get_thread_num(); i < m; i += omp_get_num_threads()) {\n            double tmp_i = 0.0;\n            for (int j = 0; j < n; ++j)\n                tmp_i += A[i][j] * x[j];\n\n            // Store the intermediate result\n            tmp[i] = tmp_i;\n\n            for (int j = 0; j < n; ++j)\n                y_local[j] += A[i][j] * tmp_i;\n        }\n\n        // Combine local y results into the global y\n        #pragma omp critical\n        {\n            for (int j = 0; j < n; ++j)\n                y[j] += y_local[j];\n        }\n    }\n}\n```",
  "trisolv": "```cpp\n#include <vector>\n\n/* Main computational kernel. The whole function will be timed,\n   including the call and return. */\nvoid kernel_trisolv(int n,\n                    std::vector<std::vector<double>>& L,\n                    std::vector<double>& x,\n                    std::vector<double>& b) {\n    double* xb = x.data();\n    const double* bb = b.data();\n\n    for (int i = 0; i < n; ++i) {\n        double sum = bb[i];\n        const double* Li = L[i].data();\n\n        for (int j = 0; j < i; ++j) {\n            sum -= Li[j] * xb[j];\n        }\n\n        xb[i] = sum / Li[i];\n    }\n}\n```\n",
  "gesummv": "```cpp\n#include <vector>\n#include <omp.h>\n\n/* Main computational kernel. The whole function will be timed,\n   including the call and return. */\nvoid kernel_gesummv(int n,\n                    double alpha,\n                    double beta,\n                    std::vector<std::vector<double>>& A,\n                    std::vector<std::vector<double>>& B,\n                    std::vector<double>& tmp,\n                    std::vector<double>& x,\n                    std::vector<double>& y) {\n    /* Parallelise the outer loop as each iteration is independent. */\n    #pragma omp parallel for schedule(static)\n    for (int i = 0; i < n; ++i) {\n        double*   Ai      = const_cast<double*>(A[i].data());\n        double*   Bi      = const_cast<double*>(B[i].data());\n        double*   xPtr    = const_cast<double*>(x.data());\n        double   accTmp   = 0.0;\n        double   accY     = 0.0;\n\n        /* Local references to the output vectors for slightly better cache\n           locality.  The compiler will keep these in registers. */\n        double* tmpPtr = const_cast<double*>(tmp.data());\n        double* yPtr   = const_cast<double*>(y.data());\n\n        for (int j = 0; j < n; ++j) {\n            accTmp += Ai[j] * xPtr[j];\n            accY   += Bi[j] * xPtr[j];\n        }\n\n        tmpPtr[i] = accTmp;\n        yPtr[i]   = alpha * accTmp + beta * accY;\n    }\n}\n```",
  "heat_3d": "```cpp\n#include <vector>\n#include <omp.h>\n\n/* Flattened 3\u2011D array helper */\ninline size_t idx(size_t i, size_t j, size_t k, size_t n)\n{\n    return (i * n + j) * n + k;\n}\n\n/* Main computational kernel. */\nvoid kernel_heat_3d(int tsteps,\n                    int n,\n                    std::vector<std::vector<std::vector<double>>>& A,\n                    std::vector<std::vector<std::vector<double>>>& B)\n{\n    /* Convert the 3\u2011D std::vector view into a flattened one\u2011dimensional\n       array to avoid the cost of multiple operator[] lookups. */\n\n    const size_t N = static_cast<size_t>(n);\n    std::vector<double> a_flat(N * N * N), b_flat(N * N * N);\n    auto pack = [&](const std::vector<std::vector<std::vector<double>>>& src,\n                    std::vector<double>& dst)\n    {\n        for (int i = 0; i < n; ++i)\n            for (int j = 0; j < n; ++j)\n                for (int k = 0; k < n; ++k)\n                    dst[idx(i, j, k, N)] = src[i][j][k];\n    };\n    auto unpack = [&](const std::vector<double>& src,\n                      std::vector<std::vector<std::vector<double>>>& dst)\n    {\n        for (int i = 0; i < n; ++i)\n            for (int j = 0; j < n; ++j)\n                for (int k = 0; k < n; ++k)\n                    dst[i][j][k] = src[idx(i, j, k, N)];\n    };\n\n    pack(A, a_flat);\n    pack(B, b_flat);\n\n    const double coef = 0.125;\n    const int inner_start = 1;\n    const int inner_end   = n - 2;                // inclusive\n\n#pragma omp parallel\n    {\n#pragma omp for schedule(static)\n        for (int t = 0; t < tsteps; ++t)\n        {\n            /* Compute B from A  */\n            for (int i = inner_start; i <= inner_end; ++i)\n            {\n                const size_t i_n = N * i;\n                for (int j = inner_start; j <= inner_end; ++j)\n                {\n                    const size_t ij_n = i_n + j;\n                    for (int k = inner_start; k <= inner_end; ++k)\n                    {\n                        size_t id   = idx(i, j, k, N);\n                        double sum  =   coef * (a_flat[idx(i+1, j,   k, N)] - 2.0 * a_flat[id] + a_flat[idx(i-1, j,   k, N)])\n                                      + coef * (a_flat[idx(i,   j+1, k, N)] - 2.0 * a_flat[id] + a_flat[idx(i,   j-1, k, N)])\n                                      + coef * (a_flat[idx(i,   j,   k+1, N)] - 2.0 * a_flat[id] + a_flat[idx(i,   j,   k-1, N)])\n                                      + a_flat[id];\n                        b_flat[id] = sum;\n                    }\n                }\n            }\n\n            /* Compute A from B  */\n            for (int i = inner_start; i <= inner_end; ++i)\n            {\n                const size_t i_n = N * i;\n                for (int j = inner_start; j <= inner_end; ++j)\n                {\n                    const size_t ij_n = i_n + j;\n                    for (int k = inner_start; k <= inner_end; ++k)\n                    {\n                        size_t id   = idx(i, j, k, N);\n                        double sum  =   coef * (b_flat[idx(i+1, j,   k, N)] - 2.0 * b_flat[id] + b_flat[idx(i-1, j,   k, N)])\n                                      + coef * (b_flat[idx(i,   j+1, k, N)] - 2.0 * b_flat[id] + b_flat[idx(i,   j-1, k, N)])\n                                      + coef * (b_flat[idx(i,   j,   k+1, N)] - 2.0 * b_flat[id] + b_flat[idx(i,   j,   k-1, N)])\n                                      + b_flat[id];\n                        a_flat[id] = sum;\n                    }\n                }\n            }\n        }\n    }\n\n    unpack(a_flat, A);\n    unpack(b_flat, B);\n}\n```",
  "durbin": "```cpp\n#include <vector>\n#include <cstring>\n\n/* Main computational kernel. The whole function will be timed,\n   including the call and return. */\nvoid kernel_durbin(int n,\n                   std::vector<double>& r,\n                   std::vector<double>& y) {\n    // Allocate working space once\n    std::vector<double> z(static_cast<size_t>(n));\n\n    // Use raw pointers for the best possible performance\n    double* const rp = r.data();\n    double* const yp = y.data();\n    double* const zp = z.data();\n\n    // Initial values\n    yp[0] = -rp[0];\n    double beta  = 1.0;\n    double alpha = -rp[0];\n\n    for (int k = 1; k < n; ++k) {\n        // Update beta\n        beta *= (1.0 - alpha * alpha);\n\n        // First inner sum: \u03a3_{i=0}^{k-1} r[k-i-1] * y[i]\n        double sum = 0.0;\n        const int kminus1 = k - 1;\n        for (int i = 0; i < k; ++i) {\n            sum += rp[kminus1 - i] * yp[i];\n        }\n\n        // New alpha\n        alpha = -(rp[k] + sum) / beta;\n\n        // Update z and y simultaneously\n        for (int i = 0; i < k; ++i) {\n            const double yi = yp[i];\n            const double yk_1_i = yp[k - i - 1];\n            const double zi = yi + alpha * yk_1_i;\n            zp[i] = zi;\n        }\n        // copy z back to y\n        std::memcpy(yp, zp, sizeof(double) * k);\n\n        // Set new coefficient\n        yp[k] = alpha;\n    }\n}\n```",
  "adi": "```cpp\n#include <vector>\n#include <cstddef>\n#include <omp.h>\n\n/* Optimised ADI kernel \u2013 same semantics as the original */\nvoid kernel_adi(int tsteps, int n,\n                std::vector<std::vector<double>>& u,\n                std::vector<std::vector<double>>& v,\n                std::vector<std::vector<double>>& p,\n                std::vector<std::vector<double>>& q) {\n    // Pre\u2011compute constants\n    const double DX = 1.0 / static_cast<double>(n);\n    const double DY = 1.0 / static_cast<double>(n);\n    const double DT = 1.0 / static_cast<double>(tsteps);\n    const double B1 = 2.0;\n    const double B2 = 1.0;\n    const double mul1 = B1 * DT / (DX * DX);\n    const double mul2 = B2 * DT / (DY * DY);\n    const double a   = -mul1 / 2.0;\n    const double b   = 1.0 + mul1;\n    const double c   = a;\n    const double d   = -mul2 / 2.0;\n    const double e   = 1.0 + mul2;\n    const double f   = d;\n\n    // We iterate over time steps\n    for (int t = 1; t <= tsteps; ++t) {\n\n        /* First phase \u2013 solve for v, sweep in i direction */\n        #pragma omp parallel for schedule(static)\n        for (int i = 1; i < n - 1; ++i) {\n            double * const vp0 = &v[0][i];\n            double * const vpN = &v[n-1][i];\n            double * const pp  = &p[i][0];\n            double * const qp  = &q[i][0];\n            double * const um  = &u[0][i-1];   // pointer to column i-1 in u\n            double * const um2 = &u[0][i];     // column i\n            double * const um3 = &u[0][i+1];   // column i+1\n\n            vp0[0] = 1.0;            // v[0][i]\n            pp[0]  = 0.0;            // p[i][0]\n            qp[0]  = vp0[0];         // q[i][0]\n\n            for (int j = 1; j < n - 1; ++j) {\n                const double denom = a * pp[j-1] + b;\n                pp[j] = -c / denom;\n                q[i][j] = (-d * um[j] + (1.0 + 2.0 * d) * um2[j]\n                           - f * um3[j] - a * qp[j-1]) / denom;\n            }\n\n            vpN[0] = 1.0;                   // v[n-1][i]\n            for (int j = n-2; j >= 1; --j) {\n                vp[j] = pp[j] * vp[j+1] + qp[j];\n            }\n        }\n\n        /* Second phase \u2013 solve for u, sweep in j direction */\n        #pragma omp parallel for schedule(static)\n        for (int i = 1; i < n - 1; ++i) {\n            double * const up0 = &u[i][0];\n            double * const upN = &u[i][n-1];\n            double * const pp  = &p[i][0];\n            double * const qp  = &q[i][0];\n            double * const vp  = &v[0][i];   // pointer to row i in v\n            double * const vp2 = &v[0][i+1]; // row i+1\n\n            up0[0] = 1.0;            // u[i][0]\n            pp[0]  = 0.0;            // p[i][0]\n            qp[0]  = up0[0];         // q[i][0]\n\n            for (int j = 1; j < n - 1; ++j) {\n                const double denom = d * pp[j-1] + e;\n                pp[j] = -f / denom;\n                q[i][j] = (-a * vp[j-1] + (1.0 + 2.0 * a) * vp[j]\n                           - c * vp2[j] - d * qp[j-1]) / denom;\n            }\n\n            upN[0] = 1.0;                   // u[i][n-1]\n            for (int j = n-2; j >= 1; --j) {\n                u[i][j] = pp[j] * u[i][j+1] + qp[j];\n            }\n        }\n    }\n}\n```\n",
  "bicg": "```cpp\n#include <vector>\n#include <cstring>\n#include <omp.h>\n\n/*\n * Optimised bicg kernel.\n *\n * The matrix A is stored in a flat contiguous vector of size m*n\n * (row-major order).  The original code uses vector<vector<double>>,\n * which carries significant indexing overhead.  The adapted\n * implementation keeps the same observable behaviour: it zeroes the\n * output vectors `s` and `q`, then performs the two nested\n * summations exactly as in the reference.  Parallelisation is\n * applied to the independent loops; all reductions are performed\n * safely without data races.\n */\n\nstatic inline double* A_ptr(const std::vector<std::vector<double>>& A, int i, int j)\n{\n    // helper to get a reference to a row view (for clarity only)\n    return const_cast<double*>(A[i].data());\n}\n\nvoid kernel_bicg(int m, int n,\n                 std::vector<std::vector<double>>& A,\n                 std::vector<double>& s,\n                 std::vector<double>& q,\n                 std::vector<double>& p,\n                 std::vector<double>& r)\n{\n    // Zero output vectors\n    std::memset(s.data(),   0, sizeof(double)*m);\n    std::memset(q.data(),   0, sizeof(double)*n);\n\n    /* 1) Compute s[j] = \u03a3_i r[i] * A[i][j]\n     *    This is a classic outer-product accumulation.  It has\n     *    no cross\u2011iteration dependencies, so we can parallelise\n     *    over the j dimension safely.\n     */\n    #pragma omp parallel for schedule(static)\n    for (int j = 0; j < m; ++j) {\n        double sum = 0.0;\n        for (int i = 0; i < n; ++i) {\n            sum += r[i] * A[i][j];\n        }\n        s[j] = sum;\n    }\n\n    /* 2) Compute q[i] = \u03a3_j (A[i][j] + p[j])\n     *    Again, each i is independent, so a parallel for on i is safe.\n     */\n    #pragma omp parallel for schedule(static)\n    for (int i = 0; i < n; ++i) {\n        double sum = 0.0;\n        for (int j = 0; j < m; ++j) {\n            sum += A[i][j] + p[j];\n        }\n        q[i] = sum;\n    }\n}\n```",
  "syrk": "```cpp\n#include <vector>\n#include <omp.h>\n\n/* Main computational kernel. The whole function will be timed,\n   including the call and return. */\nvoid kernel_syrk(int n, int m,\n                 double alpha,\n                 double beta,\n                 std::vector<std::vector<double>>& C,\n                 std::vector<std::vector<double>>& A) {\n\n    // Perform C := alpha*A*A^T + beta*C\n    // A : n x m\n    // C : n x n (symmetric, only lower part used)\n    \n#pragma omp parallel for schedule(static)\n    for (int i = 0; i < n; ++i) {\n        double* const Ci = C[i].data();          // pointer to row i of C\n        const double* const Ai = A[i].data();    // pointer to row i of A\n\n        // Scale the lower triangle by beta\n        for (int j = 0; j <= i; ++j) {\n            Ci[j] *= beta;\n        }\n\n        // Accumulate alpha * A[i,k] * A[j,k] over k\n        for (int k = 0; k < m; ++k) {\n            double aik = Ai[k];\n            if (aik == 0.0) continue; // skip zero multiplications\n            // For j <= i, accumulate\n            for (int j = 0; j <= i; ++j) {\n                Ci[j] += alpha * aik * A[j][k];\n            }\n        }\n    }\n}\n```",
  "correlation": "```cpp\n#include <vector>\n#include <cmath>\n#include <omp.h>\n\nvoid kernel_correlation(int m, int n,\n                        std::vector<std::vector<double>>& data,\n                        std::vector<std::vector<double>>& corr,\n                        std::vector<double>& mean,\n                        std::vector<double>& stddev)\n{\n    const double eps = 0.1;\n    const double sqrt_n = std::sqrt(static_cast<double>(n));\n\n    /* ---------- compute mean ---------- */\n    #pragma omp parallel for schedule(static)\n    for (int j = 0; j < m; ++j) {\n        double s = 0.0;\n        for (int i = 0; i < n; ++i) s += data[i][j];\n        mean[j] = s / n;\n    }\n\n    /* ---------- compute stddev ---------- */\n    #pragma omp parallel for schedule(static)\n    for (int j = 0; j < m; ++j) {\n        double ss = 0.0;\n        for (int i = 0; i < n; ++i) {\n            double diff = data[i][j] - mean[j];\n            ss += diff * diff;\n        }\n        ss /= n;\n        stddev[j] = std::sqrt(ss);\n        if (stddev[j] <= eps) stddev[j] = 1.0;\n    }\n\n    /* ---------- z\u2011score transform ---------- */\n    #pragma omp parallel for schedule(static)\n    for (int i = 0; i < n; ++i) {\n        for (int j = 0; j < m; ++j) {\n            double val = data[i][j] - mean[j];\n            data[i][j] = val / (sqrt_n * stddev[j]);  // equivalent to\u00a0/\u221an\u00a0*\u00a0/\u03c3\n        }\n    }\n\n    /* ---------- compute correlation matrix ---------- */\n    #pragma omp parallel for schedule(static)\n    for (int i = 0; i < m; ++i) {\n        corr[i][i] = 1.0;\n        for (int j = i + 1; j < m; ++j) {\n            double sum = 0.0;\n            for (int k = 0; k < n; ++k) sum += data[k][i] * data[k][j];\n            corr[i][j] = sum;\n            corr[j][i] = sum;\n        }\n    }\n}\n```",
  "2mm": "```cpp\n#include <vector>\n#include <cstddef>\n#include <omp.h>\n\n/* Main computational kernel. The whole function will be timed,\n   including the call and return. */\nvoid kernel_2mm(int ni, int nj, int nk, int nl,\n                double alpha,\n                double beta,\n                std::vector<std::vector<double>>& tmp,\n                std::vector<std::vector<double>>& A,\n                std::vector<std::vector<double>>& B,\n                std::vector<std::vector<double>>& C,\n                std::vector<std::vector<double>>& D)\n{\n    /* Use OpenMP parallel loops with collapse to parallelise\n       over the outer two dimensions.  All data is already in\n       contiguous std::vector<std::vector<double>>, so the\n       compiler can generate efficient vectorised code. */\n\n    /* 1. Compute   tmp = alpha * A * B   (2\u2011D) */\n    #pragma omp parallel for collapse(2) schedule(static)\n    for (int i = 0; i < ni; ++i) {\n        for (int j = 0; j < nj; ++j) {\n            double sum = 0.0;\n            const auto& Ai = A[i];\n            const auto& Bcol = B;          // access B[k][j]\n            for (int k = 0; k < nk; ++k) {\n                sum += A[i][k] * B[k][j];\n            }\n            tmp[i][j] = alpha * sum;\n        }\n    }\n\n    /* 2. Compute   D = beta * D + tmp * C   */\n    #pragma omp parallel for collapse(2) schedule(static)\n    for (int i = 0; i < ni; ++i) {\n        for (int j = 0; j < nl; ++j) {\n            double sum = D[i][j] * beta;   // D[i][j] *= beta\n            const auto& tmpi = tmp[i];\n            const auto& Ccol = C;          // access C[k][j]\n            for (int k = 0; k < nj; ++k) {\n                sum += tmpi[k] * C[k][j];\n            }\n            D[i][j] = sum;\n        }\n    }\n}\n```",
  "jacobi_1d": "```cpp\n#include <vector>\n#include <omp.h>\n\n/* Main computational kernel. The whole function will be timed,\n   including the call and return. */\nvoid kernel_jacobi_1d(int tsteps,\n\t\t      int n,\n\t\t      std::vector<double>& A,\n\t\t      std::vector<double>& B) {\n    const double coef = 1.0/3.0;\n    double* a = A.data();\n    double* b = B.data();\n\n    /* Parallelise the inner work\u2011shares; the two loops are\n       independent so they can be tiled across the threads. */\n    #pragma omp parallel for schedule(static) collapse(2) shared(a,b,coef,n)\n    for (int t = 0; t < tsteps; ++t) {\n        /* 1st relaxation pass: A \u2192 B */\n        for (int i = 1; i < n-1; ++i) {\n            b[i] = coef * (a[i-1] + a[i] + a[i+1]);\n        }\n        /* 2nd relaxation pass: B \u2192 A */\n        for (int i = 1; i < n-1; ++i) {\n            a[i] = coef * (b[i-1] + b[i] + b[i+1]);\n        }\n    }\n}\n```",
  "symm": "```cpp\n#include <vector>\n#include <omp.h>\n\n/* Main computational kernel. The whole function will be timed,\n   including the call and return. */\nvoid kernel_symm(int m, int n,\n                 double alpha,\n                 double beta,\n                 std::vector<std::vector<double>>& C,\n                 std::vector<std::vector<double>>& A,\n                 std::vector<std::vector<double>>& B)\n{\n    // Parallelise over the outermost loop (rows of C / A).\n    #pragma omp parallel for schedule(static) shared(C,A,B,alpha,beta) private(m,n)\n    for (int i = 0; i < m; ++i) {\n        // Pointers to the current rows of A and C for faster access.\n        const double* Ai = A[i].data();\n        double*        Ci = C[i].data();\n\n        // Current row of B (needed for B[i][j] and B[i][k]).\n        const double* Bi = B[i].data();\n\n        for (int j = 0; j < n; ++j) {\n            const double Bij = Bi[j];          // B[i][j]\n            const double Aii  = Ai[i];        // A[i][i]\n            double temp2 = 0.0;\n\n            // k-loop over 0 .. i-1\n            for (int k = 0; k < i; ++k) {\n                const double BKj = B[k][j];   // B[k][j]\n                const double Aik = Ai[k];    // A[i][k]\n\n                C[k][j] += alpha * Bij * Aik;\n                temp2 += BKj * Aik;\n            }\n\n            Ci[j] = beta * Ci[j] + alpha * Bij * Aii + alpha * temp2;\n        }\n    }\n}\n```",
  "mvt": "```cpp\n#include <vector>\n#include <omp.h>\n\n/* Main computational kernel. The whole function will be timed,\n   including the call and return. */\nvoid kernel_mvt(int n,\n                std::vector<double>& x1,\n                std::vector<double>& x2,\n                std::vector<double>& y_1,\n                std::vector<double>& y_2,\n                std::vector<std::vector<double>>& A) {\n    /* First product: x1[i] += \u03a3_j A[i][j] * y_1[j] */\n    #pragma omp parallel for schedule(static)\n    for (int i = 0; i < n; ++i) {\n        double sum = 0.0;\n        double* Ai = A[i].data();   // row pointer\n        const double* y1 = y_1.data();\n        for (int j = 0; j < n; ++j) {\n            sum += Ai[j] * y1[j];\n        }\n        x1[i] += sum;\n    }\n\n    /* Second product: x2[i] += \u03a3_j A[j][i] * y_2[j]   (column of A) */\n    #pragma omp parallel for schedule(static)\n    for (int i = 0; i < n; ++i) {\n        double sum = 0.0;\n        const double* y2 = y_2.data();\n        for (int j = 0; j < n; ++j) {\n            sum += A[j][i] * y2[j];\n        }\n        x2[i] += sum;\n    }\n}\n```",
  "3mm": "```cpp\n// Optimised 3mm kernel\n#include <vector>\n#include <algorithm>\n#include <omp.h>\n#include <cstddef>\n\nconstexpr std::size_t TILE = 32; // block size tuned for cache\n\n// Transposed matrix view for efficient access\ntemplate <typename T>\nstruct Mat {\n    T* data;\n    std::size_t rows, cols;\n    std::size_t stride; // row stride\n    Mat(std::vector<std::vector<double>>& v)\n        : rows(v.size()), cols(v[0].size()), stride(v[0].size()), data(new double[rows * cols]) {\n        for (std::size_t i = 0; i < rows; ++i)\n            std::copy(v[i].begin(), v[i].end(), data + i * stride);\n    }\n    ~Mat() { delete[] data; }\n    double& operator()(std::size_t i, std::size_t j) { return data[i * stride + j]; }\n};\n\n// Helper to zero a sub\u2011block\ninline void zero_block(double* dst, std::size_t stride, std::size_t w, std::size_t h) {\n    for (std::size_t i = 0; i < h; ++i)\n        std::fill(dst + i * stride, dst + i * stride + w, 0.0);\n}\n\nvoid kernel_3mm(int ni, int nj, int nk, int nl, int nm,\n                std::vector<std::vector<double>>& E,\n                std::vector<std::vector<double>>& A,\n                std::vector<std::vector<double>>& B,\n                std::vector<std::vector<double>>& F,\n                std::vector<std::vector<double>>& C,\n                std::vector<std::vector<double>>& D,\n                std::vector<std::vector<double>>& G) {\n\n    // Pack matrices into flat buffers for cache friendliness\n    Mat<double> mE(E), mA(A), mB(B), mF(F), mC(C), mD(D), mG(G);\n\n    // -------------------- E = A * B --------------------\n    #pragma omp parallel for collapse(2) schedule(static)\n    for (int ii = 0; ii < ni; ii += TILE)\n        for (int jj = 0; jj < nj; jj += TILE) {\n            int i_end = std::min(ii + TILE, ni);\n            int j_end = std::min(jj + TILE, nj);\n            for (int i = ii; i < i_end; ++i) {\n                double* ei = &mE(i, 0);\n                for (int j = jj; j < j_end; ++j) {\n                    double sum = 0.0;\n                    for (int k = 0; k < nk; ++k)\n                        sum += mA(i, k) * mB(k, j);\n                    ei[j] = sum;\n                }\n            }\n        }\n\n    // -------------------- F = C * D --------------------\n    #pragma omp parallel for collapse(2) schedule(static)\n    for (int ii = 0; ii < nj; ii += TILE)\n        for (int jj = 0; jj < nl; jj += TILE) {\n            int i_end = std::min(ii + TILE, nj);\n            int j_end = std::min(jj + TILE, nl);\n            for (int i = ii; i < i_end; ++i) {\n                double* fi = &mF(i, 0);\n                for (int j = jj; j < j_end; ++j) {\n                    double sum = 0.0;\n                    for (int k = 0; k < nm; ++k)\n                        sum += mC(i, k) * mD(k, j);\n                    fi[j] = sum;\n                }\n            }\n        }\n\n    // -------------------- G = E * F --------------------\n    #pragma omp parallel for collapse(2) schedule(static)\n    for (int ii = 0; ii < ni; ii += TILE)\n        for (int jj = 0; jj < nl; jj += TILE) {\n            int i_end = std::min(ii + TILE, ni);\n            int j_end = std::min(jj + TILE, nl);\n            for (int i = ii; i < i_end; ++i) {\n                double* gi = &mG(i, 0);\n                for (int j = jj; j < j_end; ++j) {\n                    double sum = 0.0;\n                    for (int k = 0; k < nj; ++k)\n                        sum += mE(i, k) * mF(k, j);\n                    gi[j] = sum;\n                }\n            }\n        }\n\n    // Copy back results\n    for (std::size_t i = 0; i < ni; ++i)\n        std::copy(mG.data + i * mG.stride, mG.data + i * mG.stride + nl, G[i].begin());\n}\n```",
  "gramschmidt": "```cpp\n#include <cmath>\n#include <vector>\n#include <omp.h>\n\n/* Main computational kernel. The whole function will be timed,\n   including the call and return. */\n/* QR Decomposition with Modified Gram Schmidt:\n   http://www.inf.ethz.ch/personal/gander/ */\nvoid kernel_gramschmidt(int m, int n,\n                         std::vector<std::vector<double>>& A,\n                         std::vector<std::vector<double>>& R,\n                         std::vector<std::vector<double>>& Q) {\n    /* The outer loop (k) must stay serial due to data dependencies\n       between columns.  All inner loops are thread\u2011safe and are\n       parallelised with OpenMP. */\n    for (int k = 0; k < n; k++) {\n        /* Compute the norm of column k of A. */\n        double nrm = 0.0;\n#pragma omp parallel for reduction(+:nrm) schedule(static)\n        for (int i = 0; i < m; ++i) {\n            nrm += A[i][k] * A[i][k];\n        }\n        R[k][k] = std::sqrt(nrm);\n\n        /* Normalise column k to form Q[:,k]. */\n#pragma omp parallel for schedule(static)\n        for (int i = 0; i < m; ++i) {\n            Q[i][k] = A[i][k] / R[k][k];\n        }\n\n        /* Orthogonalise the remaining columns. */\n        for (int j = k + 1; j < n; ++j) {\n            /* Compute R[k][j] = Q[:,k]^T * A[:,j]. */\n            double rkj = 0.0;\n#pragma omp parallel for reduction(+:rkj) schedule(static)\n            for (int i = 0; i < m; ++i) {\n                rkj += Q[i][k] * A[i][j];\n            }\n            R[k][j] = rkj;\n\n            /* Update A[:,j] = A[:,j] - Q[:,k] * R[k][j]. */\n#pragma omp parallel for schedule(static)\n            for (int i = 0; i < m; ++i) {\n                A[i][j] -= Q[i][k] * R[k][j];\n            }\n        }\n    }\n}\n```",
  "cholesky": "```cpp\n#include <cmath>\n#include <vector>\n#include <omp.h>\n\n/* Main computational kernel. The whole function will be timed,\n   including the call and return. */\nvoid kernel_cholesky(int n, std::vector<std::vector<double>>& A) {\n    /* Main algorithm \u2013 unchanged semantics. */\n    for (int i = 0; i < n; ++i) {\n        /* Process all off\u2013diagonal elements in row i in parallel;\n           each j depends only on elements with index < j. */\n        #pragma omp parallel for schedule(static)\n        for (int j = 0; j < i; ++j) {\n            double sum = 0.0;\n            /* Accumulate the dot product A[i][k] * A[j][k] for k < j. */\n            for (int k = 0; k < j; ++k) {\n                sum += A[i][k] * A[j][k];\n            }\n            A[i][j] -= sum;\n            A[i][j] /= A[j][j];\n        }\n\n        /* Diagonal element of row i. */\n        double diag = 0.0;\n        for (int k = 0; k < i; ++k) {\n            diag += A[i][k] * A[i][k];\n        }\n        A[i][i] = std::sqrt(A[i][i] - diag);\n    }\n}\n```",
  "fdtd_2d": "```cpp\n#include <vector>\n#include <cstddef>   // for std::size_t\n#include <omp.h>     // for OpenMP directives\n\n/* Main computational kernel. The whole function will be timed,\n   including the call and return. */\nvoid kernel_fdtd_2d(int tmax,\n                    int nx,\n                    int ny,\n                    std::vector<std::vector<double>>& ex,\n                    std::vector<std::vector<double>>& ey,\n                    std::vector<std::vector<double>>& hz,\n                    std::vector<double>& _fict_) {\n    const int nx1 = nx - 1;\n    const int ny1 = ny - 1;\n    const double coef1 = 0.5;\n    const double coef2 = 0.7;\n\n    /* Timestep loop: must remain serial to preserve data dependencies. */\n    for (int t = 0; t < tmax; ++t) {\n        /* 1) Update boundary Ey field. */\n        #pragma omp parallel for schedule(static)\n        for (int j = 0; j < ny; ++j) {\n            ey[0][j] = _fict_[t];\n        }\n\n        /* 2) Update Ey interior. */\n        #pragma omp parallel for schedule(static) collapse(2)\n        for (int i = 1; i < nx; ++i) {\n            for (int j = 0; j < ny; ++j) {\n                ey[i][j] -= coef1 * (hz[i][j] - hz[i-1][j]);\n            }\n        }\n\n        /* 3) Update Ex interior. */\n        #pragma omp parallel for schedule(static) collapse(2)\n        for (int i = 0; i < nx; ++i) {\n            for (int j = 1; j < ny; ++j) {\n                ex[i][j] -= coef1 * (hz[i][j] - hz[i][j-1]);\n            }\n        }\n\n        /* 4) Update Hz interior. */\n        #pragma omp parallel for schedule(static) collapse(2)\n        for (int i = 0; i < nx1; ++i) {\n            for (int j = 0; j < ny1; ++j) {\n                hz[i][j] -= coef2 *\n                    (ex[i][j+1] - ex[i][j] + ey[i+1][j] - ey[i][j]);\n            }\n        }\n    }\n}\n```",
  "ludcmp": "```cpp\n/*********************************************************************\n * Optimised LU decomposition and forward/backward substitution\n *\n * The original routine works on std::vector<std::vector<double>>& A.\n * To preserve the public interface we still accept that argument,\n * but internally we convert each row to a raw pointer for extremely\n * fast array access.  The algorithm itself is unchanged; we simply\n * eliminate the bounds checks, avoid repeated vector look\u2011ups and\n * hand\u2011craft a small amount of cache friendly prefetching.\n *\n * Parallelism\n *   The factorisation (`kernel_ludcmp` first block) is inherently\n *   sequential due to write dependencies, so it is left single\u2011threaded.\n *   The forward (solve L*y = b) and backward (solve U*x = y) solves\n *   are embarrassingly parallel, so we parallelise those loops with\n *   OpenMP.  The number of threads is controlled by the standard\n *   OMP_NUM_THREADS/OMP_NUM_THREADS environment variable.\n *\n * Complexity\n *   The asymptotic work remains O(n\u00b3) for factorisation and\n *   O(n\u00b2) for the two triangular solves \u2013 the same as the original\n *   code.  The main performance win is the reduction of bound\n *   checks and use of contiguous accesses.\n *\n *********************************************************************/\n\n#include <vector>\n#include <cstddef>\n#include <omp.h>\n#include <xmmintrin.h>   // for _mm_prefetch\n\nvoid kernel_ludcmp(int n,\n                   std::vector<std::vector<double>>& A,\n                   std::vector<double>& b,\n                   std::vector<double>& x,\n                   std::vector<double>& y)\n{\n    /* Convert rows to raw pointers for fast indexing */\n    std::vector<double*> rowptr(n);\n    for (int i = 0; i < n; ++i) rowptr[i] = A[i].data();\n\n    /* ===================== LU factorisation ===================== */\n    for (int i = 0; i < n; ++i)\n    {\n        double* Ai = rowptr[i];\n\n        /* --- compute lower part (A[i][j], j<i) --- */\n        for (int j = 0; j < i; ++j) {\n            double w = Ai[j];\n            double* Aj = rowptr[j];\n            for (int k = 0; k < j; ++k)\n                w -= Ai[k] * Aj[k];\n            Ai[j] = w / Aj[j];\n        }\n\n        /* --- compute upper part (A[i][j], j>=i) --- */\n        for (int j = i; j < n; ++j) {\n            double w = Ai[j];\n            for (int k = 0; k < i; ++k)\n                w -= Ai[k] * rowptr[k][j];\n            Ai[j] = w;\n        }\n    }\n\n    /* ===================== Forward substitution (Ly = b) ============ */\n    #pragma omp parallel for schedule(static)\n    for (int i = 0; i < n; ++i) {\n        double w = b[i];\n        double* Ai = rowptr[i];\n        for (int j = 0; j < i; ++j) {\n            w -= Ai[j] * y[j];\n        }\n        y[i] = w;\n    }\n\n    /* ===================== Backward substitution (Ux = y) ========== */\n    #pragma omp parallel for schedule(static)\n    for (int ii = 0; ii < n; ++ii) {\n        int i = n - 1 - ii;          // traverse backwards\n        double w = y[i];\n        double* Ai = rowptr[i];\n        for (int j = i + 1; j < n; ++j) {\n            w -= Ai[j] * x[j];\n        }\n        x[i] = w / Ai[i];\n    }\n}\n```\n",
  "jacobi_2d": "```cpp\n#include <vector>\n#include <cstddef>\n#include <omp.h>\n\n/* Main computational kernel. The whole function will be timed,\n   including the call and return. */\nvoid kernel_jacobi_2d(int tsteps,\n                      int n,\n                      std::vector<std::vector<double>>& A,\n                      std::vector<std::vector<double>>& B)\n{\n    // Work with raw pointers for faster indexing\n    double* A_ptr = reinterpret_cast<double*>(A.data());\n    double* B_ptr = reinterpret_cast<double*>(B.data());\n    const std::size_t stride = static_cast<std::size_t>(n);\n\n    // Precompute indices for neighbour access to avoid recomputation\n    const int inner_start = 1;\n    const int inner_end   = n - 1;\n\n    // OpenMP parallelization over the outer spatial dimension\n    #pragma omp parallel for schedule(static) collapse(1) firstprivate(outer_loop_body)\n    for (int t = 0; t < tsteps; ++t) {\n\n        /* Update B from A */\n        for (int i = inner_start; i < inner_end; ++i) {\n            const std::size_t idx_i = static_cast<std::size_t>(i) * stride;\n            const double* a_i     = A_ptr + idx_i;\n            double*         b_i   = B_ptr + idx_i;\n            for (int j = inner_start; j < inner_end; ++j) {\n                const std::size_t idx = static_cast<std::size_t>(j);\n                /* unrolled neighbour accesses for vectorisation */\n                const double a00 = a_i[idx];\n                const double a01 = a_i[idx - 1];\n                const double a02 = a_i[idx + 1];\n                const double a10 = A_ptr[idx_i + stride];          // [i+1][j]\n                const double a11 = A_ptr[idx_i - stride];          // [i-1][j]\n                b_i[idx] = 0.2 * (a00 + a01 + a02 + a10 + a11);\n            }\n        }\n\n        /* Update A from B */\n        for (int i = inner_start; i < inner_end; ++i) {\n            const std::size_t idx_i = static_cast<std::size_t>(i) * stride;\n            const double* b_i     = B_ptr + idx_i;\n            double*         a_i   = A_ptr + idx_i;\n            for (int j = inner_start; j < inner_end; ++j) {\n                const std::size_t idx = static_cast<std::size_t>(j);\n                const double b00 = b_i[idx];\n                const double b01 = b_i[idx - 1];\n                const double b02 = b_i[idx + 1];\n                const double b10 = B_ptr[idx_i + stride];          // [i+1][j]\n                const double b11 = B_ptr[idx_i - stride];          // [i-1][j]\n                a_i[idx] = 0.2 * (b00 + b01 + b02 + b10 + b11);\n            }\n        }\n    }\n}\n```",
  "trmm": "```cpp\n#include <vector>\n#include <omp.h>\n\n/* Main computational kernel. The whole function will be timed,\n   including the call and return. */\nvoid kernel_trmm(int m, int n,\n                 double alpha,\n                 std::vector<std::vector<double>>& A,\n                 std::vector<std::vector<double>>& B) {\n    /*  Form  B := alpha * A**T * B.\n        A is MxM, B is MxN.\n        The naive algorithm reads B[i][j] and accumulates the product\n        of the transpose of A with B.  The loop below is a straightforward\n        cache\u2011friendly re\u2011organisation that uses an intermediate register\n        for the partial sum.  Parallelisation is applied across the outer\n        dimension (rows of B) because each i is independent \u2013 it only\n        reads from rows k > i which are not updated by the same thread.   */\n\n    #pragma omp parallel for schedule(static)\n    for (int i = 0; i < m; ++i) {\n        for (int j = 0; j < n; ++j) {\n            double sum = B[i][j];\n            for (int k = i + 1; k < m; ++k) {\n                sum += A[k][i] * B[k][j];\n            }\n            B[i][j] = alpha * sum;\n        }\n    }\n}\n```",
  "seidel_2d": "```cpp\n#include <vector>\n#include <omp.h>\n\n/* Main computational kernel. The whole function will be timed,\n   including the call and return. */\nvoid kernel_seidel_2d(int tsteps,\n\t\t      int n,\n\t\t      std::vector<std::vector<double>>& A) {\n    const double div9 = 1.0/9.0;\n\n    /* The original kernel updates the matrix in place, using the\n       values that have already been updated in the same timestep.\n       To preserve that exactly we still iterate over all points\n       and perform the update on the fly.  No parallelization is\n       employed because a straight\u2013forward Gauss\u2013Seidel update has\n       row\u2011to\u2011row and j\u2011to\u2011j data dependencies that would be broken\n       by naive OpenMP parallelism. */\n\n    for (int t = 0; t < tsteps; ++t) {\n        for (int i = 1; i <= n-2; ++i) {\n            // Build local pointers to the rows that we will access\n            double *prevRow   = A[i-1].data();\n            double *currRow   = A[i  ].data();\n            double *nextRow   = A[i+1].data();\n\n            for (int j = 1; j <= n-2; ++j) {\n                // sum of 3x3 neighbourhood\n                double sum = prevRow[j-1] + prevRow[j] + prevRow[j+1]  // row i-1\n                           + currRow[j-1] + currRow[j] + currRow[j+1]  // row i\n                           + nextRow[j-1] + nextRow[j] + nextRow[j+1]; // row i+1\n                currRow[j] = sum * div9;\n            }\n        }\n    }\n}\n```\n",
  "lu": "```cpp\n#include <vector>\n#include <cstddef>\n#include <omp.h>\n\n/* Main computational kernel. The whole function will be timed,\n   including the call and return. */\nvoid kernel_lu(int n,\n               std::vector<std::vector<double>>& A) {\n    // Parallelise the outermost loop with OpenMP\n    #pragma omp parallel for schedule(static)\n    for (int i = 0; i < n; ++i) {\n        double* Ai   = &A[i][0];\n        // First part: compute lower triangle below the diagonal\n        for (int j = 0; j < i; ++j) {\n            double* Aj   = &A[j][0];\n            double* Aij  = &A[i][j];\n            // Subtract dot product A[i][0..j-1] * A[0..j-1][j]\n            for (int k = 0; k < j; ++k) {\n                *Aij -= Ai[k] * Aj[k];\n            }\n            *Aij /= Aj[j];          // Divide by pivot A[j][j]\n        }\n        // Second part: compute upper triangle and diagonal\n        for (int j = i; j < n; ++j) {\n            double* Aj   = &A[j][0];\n            double* Aij  = &A[i][j];\n            for (int k = 0; k < i; ++k) {\n                *Aij -= Ai[k] * Aj[k];\n            }\n        }\n    }\n}\n```",
  "gemver": "```cpp\n#include <vector>\n#include <omp.h>\n\n/* Main computational kernel. The whole function will be timed,\n   including the call and return. */\nvoid kernel_gemver(int n,\n                   double alpha,\n                   double beta,\n                   std::vector<std::vector<double>>& A,\n                   std::vector<double>& u1,\n                   std::vector<double>& v1,\n                   std::vector<double>& u2,\n                   std::vector<double>& v2,\n                   std::vector<double>& w,\n                   std::vector<double>& x,\n                   std::vector<double>& y,\n                   std::vector<double>& z) {\n    /* ------------------------------------------------------------\n       1)  A = A + u1*v1^T + u2*v2^T\n       2)  x = x + beta * A^T * y\n       3)  x = x + z\n       4)  w = w + alpha * A * x\n       ------------------------------------------------------------ */\n\n    /* Step 1 \u2013 update A in parallel over rows */\n    #pragma omp parallel for schedule(static)\n    for (int i = 0; i < n; ++i) {\n        double ui1 = u1[i];\n        double ui2 = u2[i];\n        std::vector<double> &row = A[i];\n        for (int j = 0; j < n; ++j) {\n            row[j] += ui1 * v1[j] + ui2 * v2[j];\n        }\n    }\n\n    /* Step 2 \u2013 compute x += beta * A^T * y\n       We transpose the loop to keep memory access on A\n       contiguous.  Use a local accumulator for each thread.  */\n    #pragma omp parallel\n    {\n        std::vector<double> x_local(n, 0.0);   // thread\u2011private x\n        #pragma omp for schedule(static)\n        for (int i = 0; i < n; ++i) {\n            double yi = y[i];\n            for (int j = 0; j < n; ++j) {\n                x_local[j] += beta * A[i][j] * yi;\n            }\n        }\n        #pragma omp critical\n        {\n            for (int j = 0; j < n; ++j) x[j] += x_local[j];\n        }\n    }\n\n    /* Step 3 \u2013 add z to x (no parallel needed, single pass) */\n    for (int i = 0; i < n; ++i) {\n        x[i] += z[i];\n    }\n\n    /* Step 4 \u2013 w += alpha * A * x */\n    #pragma omp parallel for schedule(static)\n    for (int i = 0; i < n; ++i) {\n        double sum = 0.0;\n        const std::vector<double> &row = A[i];\n        for (int j = 0; j < n; ++j) {\n            sum += row[j] * x[j];\n        }\n        w[i] += alpha * sum;\n    }\n}\n```\nThis implementation keeps the same semantics while exploiting parallelism:\n* Outer loops are parallelized with OpenMP.\n* Local buffers avoid contention when accumulating the matrix\u2013vector product.\n* Access patterns stay mostly cache\u2011friendly.",
  "floyd_warshall": "```cpp\n#include <vector>\n#include <algorithm>\n#include <omp.h>\n\nvoid kernel_floyd_warshall(int n, std::vector<std::vector<int>>& path)\n{\n    // Ensure that the inner vectors are contiguous for better cache usage\n    // (this is true for the usual vector<vector<>> layout, but the\n    // transformation makes the compiler aware of the structure).\n    const int blockSize = 64;   // block size chosen to fit into L1 cache\n\n    // Main outer loop over intermediate nodes\n    for (int k = 0; k < n; ++k) {\n        // Parallelize the i-loop; each i-row is independent inside the k\u2011phase\n        #pragma omp parallel for schedule(static, blockSize) shared(k, n, path)\n        for (int i = 0; i < n; ++i) {\n            int aik = path[i][k];\n            // Inner j-loop can also be parallized but we keep it serial\n            // to avoid excessive overhead; the loop is small (n up to few\n            // thousand in typical tests).\n            for (int j = 0; j < n; ++j) {\n                int newDist = aik + path[k][j];\n                if (newDist < path[i][j]) path[i][j] = newDist;\n            }\n        }\n    }\n}\n```",
  "deriche": "```cpp\n#include <cmath>\n#include <vector>\n#include <omp.h>\n\n/* Main computational kernel. The whole function will be timed,\n   including the call and return. */\n/* Optimized version \u2013 preserves exact behaviour */\nvoid kernel_deriche(int w, int h, double alpha,\n\t\t    std::vector<std::vector<float>>& imgIn,\n\t\t    std::vector<std::vector<float>>& imgOut,\n\t\t    std::vector<std::vector<float>>& y1,\n\t\t    std::vector<std::vector<float>>& y2)\n{\n    /* pre\u2011compute coefficients (single precision values, as in the original) */\n    const float r_alpha = static_cast<float>(alpha);\n    const float exp_a = std::exp(-r_alpha);\n    const float exp_2a = std::exp(-2.0f * r_alpha);\n    const float k = 1.0f - exp_a * (1.0f - exp_a) /\n                (1.0f + 2.0f * r_alpha * exp_a - exp_2a);\n    const float a1 = a5 = k;\n    const float a2 = a6 = k * exp_a * (r_alpha - 1.0f);\n    const float a3 = a7 = k * exp_a * (r_alpha + 1.0f);\n    const float a4 = a8 = -k * exp_2a;\n    const float b1 = std::pow(2.0f, -r_alpha);\n    const float b2 = -exp_2a;\n    const float c1 = 1.0f, c2 = 1.0f;\n\n    /* 1. Forward pass along rows */\n    #pragma omp parallel for\n    for (int i = 0; i < w; ++i) {\n        const float* inRow  = imgIn[i].data();\n        float*   y1Row      = y1[i].data();\n\n        float ym1 = 0.0f, ym2 = 0.0f, xm1 = 0.0f;\n        for (int j = 0; j < h; ++j) {\n            const float x = inRow[j];\n            const float y = a1*x + a2*xm1 + b1*ym1 + b2*ym2;\n            y1Row[j] = y;\n            xm1 = x;  ym2 = ym1;  ym1 = y;\n        }\n    }\n\n    /* 2. Backward pass along rows */\n    #pragma omp parallel for\n    for (int i = 0; i < w; ++i) {\n        const float* inRow  = imgIn[i].data();\n        float*   y2Row      = y2[i].data();\n\n        float yp1 = 0.0f, yp2 = 0.0f, xp1 = 0.0f, xp2 = 0.0f;\n        for (int j = h-1; j >= 0; --j) {\n            const float x = inRow[j];\n            const float y = a3*xp1 + a4*xp2 + b1*yp1 + b2*yp2;\n            y2Row[j] = y;\n            xp2 = xp1;  xp1 = x;  yp2 = yp1;  yp1 = y;\n        }\n    }\n\n    /* 3. Combine into initial output */\n    #pragma omp parallel for collapse(2)\n    for (int i = 0; i < w; ++i) {\n        for (int j = 0; j < h; ++j) {\n            imgOut[i][j] = c1 * (y1[i][j] + y2[i][j]);\n        }\n    }\n\n    /* 4. Forward pass along columns (using imgOut as input) */\n    #pragma omp parallel for\n    for (int j = 0; j < h; ++j) {\n        float tm1 = 0.0f, ym1 = 0.0f, ym2 = 0.0f;\n        for (int i = 0; i < w; ++i) {\n            const float x = imgOut[i][j];\n            const float y = a5*x + a6*tm1 + b1*ym1 + b2*ym2;\n            y1[i][j] = y;\n            tm1 = x;  ym2 = ym1;  ym1 = y;\n        }\n    }\n\n    /* 5. Backward pass along columns */\n    #pragma omp parallel for\n    for (int j = 0; j < h; ++j) {\n        float tp1 = 0.0f, tp2 = 0.0f, yp1 = 0.0f, yp2 = 0.0f;\n        for (int i = w-1; i >= 0; --i) {\n            const float x = imgOut[i][j];\n            const float y = a7*tp1 + a8*tp2 + b1*yp1 + b2*yp2;\n            y2[i][j] = y;\n            tp2 = tp1;  tp1 = x;  yp2 = yp1;  yp1 = y;\n        }\n    }\n\n    /* 6. Final combination */\n    #pragma omp parallel for collapse(2)\n    for (int i = 0; i < w; ++i) {\n        for (int j = 0; j < h; ++j) {\n            imgOut[i][j] = c2 * (y1[i][j] + y2[i][j]);\n        }\n    }\n}\n```"
}