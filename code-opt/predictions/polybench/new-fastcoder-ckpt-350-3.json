{
  "covariance": "```cpp\n#include <vector>\n#include <cstddef>\n#include <omp.h>\n#include <algorithm>\n\nvoid kernel_covariance(int m, int n,\n                       std::vector<std::vector<double>>& data,\n                       std::vector<std::vector<double>>& cov,\n                       std::vector<double>& mean) {\n    // 1) compute mean per column\n    #pragma omp parallel for schedule(static)\n    for (int j = 0; j < m; ++j) {\n        double sum = 0.0;\n        for (int i = 0; i < n; ++i)\n            sum += data[i][j];\n        mean[j] = sum / n;\n    }\n\n    // 2) centre data\n    #pragma omp parallel for schedule(static)\n    for (int i = 0; i < n; ++i) {\n        for (int j = 0; j < m; ++j)\n            data[i][j] -= mean[j];\n    }\n\n    // 3) covariance (upper triangle only, copy to lower)\n    #pragma omp parallel for schedule(static)\n    for (int i = 0; i < m; ++i) {\n        for (int j = i; j < m; ++j) {\n            double s = 0.0;\n            for (int k = 0; k < n; ++k)\n                s += data[k][i] * data[k][j];\n            s /= (n - 1.0);\n            cov[i][j] = s;\n            cov[j][i] = s;\n        }\n    }\n}\n```\n",
  "gemm": "```cpp\n#include <vector>\n#include <omp.h>\n\nvoid kernel_gemm(int ni, int nj, int nk,\n                 double alpha,\n                 double beta,\n                 std::vector<std::vector<double>>& A,\n                 std::vector<std::vector<double>>& B,\n                 std::vector<std::vector<double>>& C)\n{\n    // Parallelise over the outermost dimension (rows of C).\n    #pragma omp parallel for schedule(static)\n    for (int i = 0; i < ni; ++i)\n    {\n        double* c_row = C[i].data();\n        const double* a_row = A[i].data();\n\n        // Scale C[i][*] by beta.\n        for (int j = 0; j < nj; ++j)\n            c_row[j] *= beta;\n\n        // Accumulate alpha * A[i][k] * B[k][j].\n        for (int k = 0; k < nk; ++k)\n        {\n            const double a_ik = alpha * a_row[k];\n            double* b_row_ptr = B[k].data();\n\n            // Inner j\u2011loop with SIMD hint.\n            #pragma omp simd\n            for (int j = 0; j < nj; ++j)\n                c_row[j] += a_ik * b_row_ptr[j];\n        }\n    }\n}\n```\nThis version keeps the exact algorithmic behaviour but:\n- Uses OpenMP to parallelise the outermost loop across 8 threads.\n- Operates on contiguous row data via `data()` pointers for cache friendliness.\n- Provides a `#pragma omp simd` hint to vectorise the innermost loop, improving throughput on modern CPUs.",
  "doitgen": "```cpp\n#include <vector>\n#include <omp.h>\n\n/* Main computational kernel. The whole function will be timed,\n   including the call and return. */\nvoid kernel_doitgen(int nr, int nq, int np,\n\t\t    std::vector<std::vector<std::vector<double>>>& A,\n\t\t    std::vector<std::vector<double>>& C4,\n\t\t    std::vector<double>& sum)\n{\n    /* Parallelise the outer loops.  Each team works on a distinct\n       (r,q) pair, so race conditions are avoided.  A reduction in\n       work ensures that every iteration touches only its own\n       elements. */\n    #pragma omp parallel for collapse(2) schedule(static)\n    for (int r = 0; r < nr; ++r) {\n        for (int q = 0; q < nq; ++q) {\n            /* Local pointers to the 1\u2011D slices for faster indexing. */\n            const std::vector<double> *colC4;\n            double *sumPtr = sum.data();\n\n            /* Compute the product A[r][q][:] * C4 and store in sum. */\n            for (int p = 0; p < np; ++p) {\n                sumPtr[p] = 0.0;\n                for (int s = 0; s < np; ++s) {\n                    sumPtr[p] += A[r][q][s] * C4[s][p];\n                }\n            }\n\n            /* Copy the result back to A. */\n            for (int p = 0; p < np; ++p) {\n                A[r][q][p] = sumPtr[p];\n            }\n        }\n    }\n}\n```\nThis implementation keeps the exact semantics of the original code while:\n\n* Eliminating repeated bounds checks by using raw pointers (`sum.data()`).\n* Adding OpenMP parallelism across the `(r,q)` loops with `collapse(2)` for better load balancing.\n* Using static scheduling to keep the overhead minimal on the 8\u2011thread x86\u201164 machine.",
  "syr2k": "```cpp\n#include <vector>\n#include <omp.h>\n\n/* Main computational kernel. The whole function will be timed,\n   including the call and return. */\nvoid kernel_syr2k(int n, int m,\n                  double alpha,\n                  double beta,\n                  std::vector<std::vector<double>>& C,\n                  std::vector<std::vector<double>>& A,\n                  std::vector<std::vector<double>>& B) {\n    //BLAS PARAMS\n    //UPLO  = 'L'\n    //TRANS = 'N'\n    //A is NxM\n    //B is NxM\n    //C is NxN\n\n    const double a = alpha;\n    const double b = beta;\n\n    #pragma omp parallel for schedule(static)\n    for (int i = 0; i < n; ++i) {\n        double* Cin = C[i].data();      // pointer to C[i][0]\n        const double* Ai = A[i].data(); // pointer to A[i][0]\n        const double* Bi = B[i].data(); // pointer to B[i][0]\n\n        // Scale lower triangle of C by beta\n        for (int j = 0; j <= i; ++j) {\n            Cin[j] *= b;\n        }\n\n        // Accumulate rank-2 update\n        for (int k = 0; k < m; ++k) {\n            double Bk = Bi[k];          // B[i][k]\n            double Ak = Ai[k];          // A[i][k]\n            for (int j = 0; j <= i; ++j) {\n                // C[i][j] += A[j][k]*alpha*B[i][k] + B[j][k]*alpha*A[i][k];\n                Cin[j] += a * ( A[j][k] * Bk + B[j][k] * Ak );\n            }\n        }\n    }\n}\n```",
  "atax": "```cpp\n#include <vector>\n#include <cstddef>\n#include <omp.h>\n\n/* Main computational kernel. The whole function will be timed,\n   including the call and return. */\nvoid kernel_atax(int m, int n,\n                 std::vector<std::vector<double>>& A,\n                 std::vector<double>& x,\n                 std::vector<double>& y,\n                 std::vector<double>& tmp) {\n\n    /* -----------------------------------------------------------------\n       1. Zero y in parallel\n       ----------------------------------------------------------------- */\n    #pragma omp parallel for schedule(static)\n    for (int j = 0; j < n; ++j) {\n        y[j] = 0.0;\n    }\n\n    /* -----------------------------------------------------------------\n       2. Compute tmp[i] and accumulate into y in parallel.\n          We use a private temporary to avoid false sharing between\n          threads and use an OpenMP reduction on y.\n       ----------------------------------------------------------------- */\n    #pragma omp parallel for schedule(static) reduction(+:y[:n])\n    for (int i = 0; i < m; ++i) {\n        double sum = 0.0;                     // local tmp[i]\n        for (int j = 0; j < n; ++j) {\n            sum += A[i][j] * x[j];\n        }\n        tmp[i] = sum;                          // keep consistency with API\n        for (int j = 0; j < n; ++j) {\n            y[j] += A[i][j] * sum;\n        }\n    }\n}\n```",
  "nussinov": "```cpp\n#include <vector>\n#include <algorithm>\n\nvoid kernel_nussinov(int n, const std::vector<int>& seq,\n                     std::vector<std::vector<int>>& table) {\n    // Flatten the 2\u2011D table to a 1\u2011D array for better cache locality\n    std::vector<int> flat(n * n, 0);\n\n    const auto match = [](int a, int b) noexcept {\n        return (a + b == 3) ? 1 : 0;\n    };\n\n    // Helper lambda to index the flat array\n    auto idx = [n](int i, int j) noexcept { return i * n + j; };\n\n    for (int i = n - 1; i >= 0; --i) {\n        for (int j = i + 1; j < n; ++j) {\n            int best = flat[idx(i, j)];  // current value (initially 0)\n\n            // table[i][j-1]\n            if (j - 1 >= 0) {\n                int v = flat[idx(i, j - 1)];\n                if (v > best) best = v;\n            }\n\n            // table[i+1][j]\n            if (i + 1 < n) {\n                int v = flat[idx(i + 1, j)];\n                if (v > best) best = v;\n            }\n\n            // table[i+1][j-1] + match\n            if (j - 1 >= 0 && i + 1 < n) {\n                int base = flat[idx(i + 1, j - 1)];\n                if (i < j - 1) {\n                    int v = base + match(seq[i], seq[j]);\n                    if (v > best) best = v;\n                } else {\n                    if (base > best) best = base;\n                }\n            }\n\n            // split over k\n            for (int k = i + 1; k < j; ++k) {\n                int v = flat[idx(i, k)] + flat[idx(k + 1, j)];\n                if (v > best) best = v;\n            }\n\n            flat[idx(i, j)] = best;\n        }\n    }\n\n    // Copy back to the expected 2\u2011D output format\n    for (int i = 0; i < n; ++i)\n        for (int j = 0; j < n; ++j)\n            table[i][j] = flat[idx(i, j)];\n}\n```",
  "gesummv": "```cpp\n#include <vector>\n#include <cstddef>\n#include <omp.h>\n\n/* Main computational kernel. The whole function will be timed,\n   including the call and return. */\nvoid kernel_gesummv(int n,\n                    double alpha,\n                    double beta,\n                    std::vector<std::vector<double>>& A,\n                    std::vector<std::vector<double>>& B,\n                    std::vector<double>& tmp,\n                    std::vector<double>& x,\n                    std::vector<double>& y)\n{\n    // Parallelise the outer loop: each thread processes a contiguous chunk of\n    // rows.  The work is embarrassingly parallel, so a default static\n    // schedule works well.\n#pragma omp parallel for schedule(static)\n    for (int i = 0; i < n; ++i) {\n        double t = 0.0;\n        double y_val = 0.0;\n        const std::vector<double>& Ai = A[i];\n        const std::vector<double>& Bi = B[i];\n        for (int j = 0; j < n; ++j) {\n            t     += Ai[j] * x[j];\n            y_val += Bi[j] * x[j];\n        }\n        tmp[i] = t;\n        y[i]  = alpha * t + beta * y_val;\n    }\n}\n```",
  "durbin": "```cpp\n#include <vector>\n#include <omp.h>\n\n/* Main computational kernel. The whole function will be timed,\n   including the call and return. */\nvoid kernel_durbin(int n,\n                   std::vector<double>& r,\n                   std::vector<double>& y) {\n    // Allocate a working array once to avoid repeated malloc/free\n    std::vector<double> z(n, 0.0);\n\n    // First coefficient\n    y[0] = -r[0];\n    double beta = 1.0;\n    double alpha = -r[0];\n\n    // Main loop \u2013 each k depends only on previous results, so no parallel\n    for (int k = 1; k < n; ++k) {\n        beta = (1.0 - alpha * alpha) * beta;\n\n        // Compute the dot product r[k-1..0] * y[0..k-1]\n        double sum = 0.0;\n#pragma omp simd reduction(+:sum)\n        for (int i = 0; i < k; ++i) {\n            sum += r[k - i - 1] * y[i];\n        }\n\n        alpha = -(r[k] + sum) / beta;\n\n        // Symmetric update: z[i] = y[i] + alpha*y[k-i-1]\n        // This loop can be vectorised and parallelised\n#pragma omp simd\n        for (int i = 0; i < k; ++i) {\n            z[i] = y[i] + alpha * y[k - i - 1];\n        }\n\n        // Copy back to y\n#pragma omp simd\n        for (int i = 0; i < k; ++i) {\n            y[i] = z[i];\n        }\n\n        y[k] = alpha;\n    }\n}\n```",
  "trisolv": "```cpp\n#include <vector>\n#include <cstddef>\n#include <omp.h>\n\n/* \n * Flat, column\u2011major L matrix: L[i*n + j] = L[i][j]\n * The function is identical to the original kernel but\n * uses contiguous storage and pointer arithmetic to\n * avoid virtual function calls in the inner loop.\n *\n * The algorithm cannot be parallelised because each\n * x[i] depends on all previous x[0..i\u20111].\n * What we gain is better cache utilisation and vectorisation.\n */\nstatic inline double *restrict_ptr(double *restrict base, std::size_t stride, std::size_t index) {\n    return base + stride * index;\n}\n\nvoid kernel_trisolv(int n,\n                    std::vector<std::vector<double>>& L,\n                    std::vector<double>& x,\n                    std::vector<double>& b) {\n\n    /* copy the 2\u2011D vector into a flattened column\u2011major array\n     * so that the inner loop accesses a contiguous block of\n     * memory.  The copy is performed once per function call\n     * and has negligible cost relative to the kernel.         */\n    std::vector<double> flatL(n * n);\n    for (int i = 0; i < n; ++i) {\n        double *row = &flatL[i * n];\n        std::copy(L[i].begin(), L[i].end(), row);\n    }\n\n    /* forward substitution */\n    for (int i = 0; i < n; ++i) {\n        double sum = b[i];\n        double *rowL = &flatL[i * n];\n        for (int j = 0; j < i; ++j) {\n            sum -= rowL[j] * x[j];\n        }\n        x[i] = sum / rowL[i];\n    }\n}\n```",
  "heat_3d": "```cpp\n#include <vector>\n#include <omp.h>\n\nvoid kernel_heat_3d(int tsteps,\n                    int n,\n                    std::vector<std::vector<std::vector<double>>>& A,\n                    std::vector<std::vector<std::vector<double>>>& B)\n{\n    const double coeff = 0.125;          // 1/8\n#pragma omp parallel\n    {\n        for (int t = 1; t <= tsteps; ++t) {\n            /* ----------------------------------------------------\n               First phase: A -> B\n               ---------------------------------------------------- */\n#pragma omp for collapse(3) schedule(static)\n            for (int i = 1; i < n - 1; ++i) {\n                for (int j = 1; j < n - 1; ++j) {\n                    for (int k = 1; k < n - 1; ++k) {\n                        double a_iplus = A[i + 1][j][k];\n                        double a_iminus = A[i - 1][j][k];\n                        double a_jplus = A[i][j + 1][k];\n                        double a_jminus = A[i][j - 1][k];\n                        double a_kplus = A[i][j][k + 1];\n                        double a_kminus = A[i][j][k - 1];\n                        double a_cur = A[i][j][k];\n                        B[i][j][k] = coeff *\n                                     (a_iplus - 2.0 * a_cur + a_iminus +\n                                      a_jplus - 2.0 * a_cur + a_jminus +\n                                      a_kplus - 2.0 * a_cur + a_kminus)\n                                     + a_cur;\n                    }\n                }\n            }\n\n            /* ----------------------------------------------------\n               Second phase: B -> A\n               ---------------------------------------------------- */\n#pragma omp for collapse(3) schedule(static)\n            for (int i = 1; i < n - 1; ++i) {\n                for (int j = 1; j < n - 1; ++j) {\n                    for (int k = 1; k < n - 1; ++k) {\n                        double b_iplus = B[i + 1][j][k];\n                        double b_iminus = B[i - 1][j][k];\n                        double b_jplus = B[i][j + 1][k];\n                        double b_jminus = B[i][j - 1][k];\n                        double b_kplus = B[i][j][k + 1];\n                        double b_kminus = B[i][j][k - 1];\n                        double b_cur = B[i][j][k];\n                        A[i][j][k] = coeff *\n                                     (b_iplus - 2.0 * b_cur + b_iminus +\n                                      b_jplus - 2.0 * b_cur + b_jminus +\n                                      b_kplus - 2.0 * b_cur + b_kminus)\n                                     + b_cur;\n                    }\n                }\n            }\n        } // end t\n    } // end parallel\n}\n```",
  "adi": "```cpp\n#include <vector>\n#include <cstddef>\n#include <omp.h>\n\n/*  Kernel with local flat buffers, pre\u2011computed coefficients,\n    and OpenMP parallelisation of the independent inner loops.\n    The function signature is unchanged to keep compatibility\n    with the harness. */\nvoid kernel_adi(int tsteps, int n,\n        std::vector<std::vector<double>>& u,\n        std::vector<std::vector<double>>& v,\n        std::vector<std::vector<double>>& p,\n        std::vector<std::vector<double>>& q)\n{\n    /* Pre\u2013compute constants once */\n    const double DX = 1.0 / n;\n    const double DY = 1.0 / n;\n    const double DT = 1.0 / tsteps;\n    const double B1  = 2.0;\n    const double B2  = 1.0;\n    const double mul1 = B1 * DT / (DX * DX);\n    const double mul2 = B2 * DT / (DY * DY);\n    const double a = -mul1 / 2.0;\n    const double b = 1.0 + mul1;\n    const double c = a;\n    const double d = -mul2 / 2.0;\n    const double e = 1.0 + mul2;\n    const double f = d;\n\n    /* Create flat 2\u2011D views for faster indexing */\n    std::vector<double*> u_row(n), v_row(n), p_row(n), q_row(n);\n    for (int i = 0; i < n; ++i) {\n        u_row[i] = u[i].data();\n        v_row[i] = v[i].data();\n        p_row[i] = p[i].data();\n        q_row[i] = q[i].data();\n    }\n\n    /* Main time loop */\n    for (int t = 1; t <= tsteps; ++t) {\n\n        /* First sweep (v updates) */\n        #pragma omp parallel for schedule(static)\n        for (int i = 1; i < n - 1; ++i) {\n            double* v_i = v_row[i];\n            double* p_i = p_row[i];\n            double* q_i = q_row[i];\n\n            v_i[0] = 1.0;\n            p_i[0] = 0.0;\n            q_i[0] = v_i[0];\n\n            /* Forward pass */\n            for (int j = 1; j < n - 1; ++j) {\n                double denom = a * p_i[j-1] + b;\n                p_i[j] = -c / denom;\n                double num = -d * u_row[j][i-1]\n                           + (1.0 + 2.0 * d) * u_row[j][i]\n                           - f * u_row[j][i+1]\n                           - a * q_i[j-1];\n                q_i[j] = num / denom;\n            }\n\n            v_i[n-1] = 1.0;\n\n            /* Backward pass */\n            for (int j = n-2; j >= 1; --j) {\n                v_i[j] = p_i[j] * v_row[j+1][i] + q_i[j];\n            }\n        }\n\n        /* Second sweep (u updates) */\n        #pragma omp parallel for schedule(static)\n        for (int i = 1; i < n - 1; ++i) {\n            double* u_i = u_row[i];\n            double* p_i = p_row[i];\n            double* q_i = q_row[i];\n\n            u_i[0] = 1.0;\n            p_i[0] = 0.0;\n            q_i[0] = u_i[0];\n\n            /* Forward pass */\n            for (int j = 1; j < n - 1; ++j) {\n                double denom = d * p_i[j-1] + e;\n                p_i[j] = -f / denom;\n                double num = -a * v_row[i-1][j]\n                           + (1.0 + 2.0 * a) * v_row[i][j]\n                           - c * v_row[i+1][j]\n                           - d * q_i[j-1];\n                q_i[j] = num / denom;\n            }\n\n            u_i[n-1] = 1.0;\n\n            /* Backward pass */\n            for (int j = n-2; j >= 1; --j) {\n                u_i[j] = p_i[j] * u_i[j+1] + q_i[j];\n            }\n        }\n    }\n}\n```",
  "bicg": "```cpp\n#include <vector>\n#include <omp.h>\n\n/* Main computational kernel. The whole function will be timed,\n   including the call and return. */\nvoid kernel_bicg(int m, int n,\n                 std::vector<std::vector<double>>& A,\n                 std::vector<double>& s,\n                 std::vector<double>& q,\n                 std::vector<double>& p,\n                 std::vector<double>& r) {\n    /* clear s once */\n    #pragma omp parallel for\n    for (int i = 0; i < m; ++i) s[i] = 0.0;\n\n    /* each thread keeps a private copy of s_local to avoid atomic\n       overhead; after all work is done we combine the results */\n    #pragma omp parallel\n    {\n        int tid   = omp_get_thread_num();\n        int nthr  = omp_get_num_threads();\n\n        std::vector<double> s_local(m, 0.0);\n\n        for (int i = tid; i < n; i += nthr) {\n            double qi = 0.0;\n            double ri = r[i];\n\n            for (int j = 0; j < m; ++j) {\n                double aij = A[i][j];\n                s_local[j] += ri * aij;\n                qi        += aij + p[j];\n            }\n            q[i] = qi;\n        }\n\n        /* merge thread-local accumulation into global s */\n        #pragma omp critical\n        {\n            for (int j = 0; j < m; ++j) s[j] += s_local[j];\n        }\n    }\n}\n```",
  "syrk": "```cpp\n#include <vector>\n#include <omp.h>\n\n/* Main computational kernel. The whole function will be timed,\n   including the call and return. */\nvoid kernel_syrk(int n, int m,\n                 double alpha,\n                 double beta,\n                 std::vector<std::vector<double>>& C,\n                 std::vector<std::vector<double>>& A) {\n    // produce C := alpha*A*A**T + beta*C\n    // A is N x M, C is N x N (lower triangular part is used)\n    const int N = n, M = m;\n    double beta_local = beta;\n    double alpha_local = alpha;\n\n    // Using OpenMP to parallelise over the outer i loop\n    #pragma omp parallel for schedule(static)\n    for (int i = 0; i < N; ++i) {\n        // Scale lower triangle of C by beta\n        for (int j = 0; j <= i; ++j) {\n            C[i][j] *= beta_local;\n        }\n\n        // Update with alpha*A*A^T\n        for (int k = 0; k < M; ++k) {\n            double a_ik = A[i][k];\n            for (int j = 0; j <= i; ++j) {\n                C[i][j] += alpha_local * a_ik * A[j][k];\n            }\n        }\n    }\n}\n```",
  "symm": "```cpp\n#include <vector>\n#include <omp.h>\n\n/* Main computational kernel. The whole function will be timed,\n   including the call and return. */\nvoid kernel_symm(int m, int n,\n                 double alpha,\n                 double beta,\n                 std::vector<std::vector<double>>& C,\n                 std::vector<std::vector<double>>& A,\n                 std::vector<std::vector<double>>& B)\n{\n    // Flattened 2\u2011D storage: C[i][j] -> C[i*n + j]\n    const int CN = n;\n    const int AN = m;  // A is MxM\n    const int BN = n;  // B is MxN\n\n    // Create flat wrappers for better cache locality\n    std::vector<double> flatC(m * n), flatA(m * m), flatB(m * n);\n    for (int i = 0; i < m; ++i)\n        for (int j = 0; j < n; ++j)\n            flatC[i * CN + j] = C[i][j];\n    for (int i = 0; i < m; ++i)\n        for (int j = 0; j < m; ++j)\n            flatA[i * AN + j] = A[i][j];\n    for (int i = 0; i < m; ++i)\n        for (int j = 0; j < n; ++j)\n            flatB[i * BN + j] = B[i][j];\n\n    #pragma omp parallel for schedule(static)\n    for (int i = 0; i < m; ++i) {\n        const double *Ai   = &flatA[i * AN];\n        const double *Bi   = &flatB[i * BN];\n        double alphaAiAi = alpha * Ai[i];  // A[i][i] * alpha\n\n        for (int j = 0; j < n; ++j) {\n            double temp2 = 0.0;\n            const double Bij = Bi[j];\n            for (int k = 0; k < i; ++k) {\n                double const_val = Bij * Ai[k];\n                flatC[k * CN + j] += alpha * const_val;\n                temp2 += const_val;   // since B[k][j] == Bij\n            }\n            flatC[i * CN + j] = beta * flatC[i * CN + j]\n                                + alphaAiAi * Bij + alpha * temp2;\n        }\n    }\n\n    // Copy back to the original 2\u2011D vectors\n    for (int i = 0; i < m; ++i)\n        for (int j = 0; j < n; ++j)\n            C[i][j] = flatC[i * CN + j];\n}\n```",
  "correlation": "```cpp\n#include <vector>\n#include <cmath>\n#include <omp.h>\n\nvoid kernel_correlation(int m, int n,\n                        std::vector<std::vector<double>>& data,\n                        std::vector<std::vector<double>>& corr,\n                        std::vector<double>& mean,\n                        std::vector<double>& stddev) {\n    constexpr double eps = 0.1;\n    const double inv_sqrt_n = 1.0 / std::sqrt(static_cast<double>(n));\n\n    /* --- compute column means ------------------------------------- */\n    #pragma omp parallel for schedule(static)\n    for (int j = 0; j < m; ++j) {\n        double sum = 0.0;\n        for (int i = 0; i < n; ++i) {\n            sum += data[i][j];\n        }\n        mean[j] = sum / static_cast<double>(n);\n    }\n\n    /* --- compute column stddev ------------------------------------ */\n    #pragma omp parallel for schedule(static)\n    for (int j = 0; j < m; ++j) {\n        double sum = 0.0;\n        const double mj = mean[j];\n        for (int i = 0; i < n; ++i) {\n            double diff = data[i][j] - mj;\n            sum += diff * diff;\n        }\n        sum /= static_cast<double>(n);\n        double s = std::sqrt(sum);\n        stddev[j] = (s <= eps) ? 1.0 : s;\n    }\n\n    /* --- normalize data ------------------------------------------- */\n    #pragma omp parallel for schedule(static)\n    for (int i = 0; i < n; ++i) {\n        double* row = data[i].data();\n        for (int j = 0; j < m; ++j) {\n            row[j] = (row[j] - mean[j]) / (inv_sqrt_n * stddev[j]);\n        }\n    }\n\n    /* --- compute correlation matrix (upper triangle) -------------- */\n    #pragma omp parallel for schedule(dynamic)\n    for (int i = 0; i < m; ++i) {\n        corr[i][i] = 1.0;\n        for (int j = i + 1; j < m; ++j) {\n            double sum = 0.0;\n            for (int k = 0; k < n; ++k) {\n                sum += data[k][i] * data[k][j];\n            }\n            corr[i][j] = sum;\n            corr[j][i] = sum;    // symmetric\n        }\n    }\n}\n```",
  "mvt": "```cpp\n#include <vector>\n#include <omp.h>\n\n/* Main computational kernel. The whole function will be timed,\n   including the call and return. */\nvoid kernel_mvt(int n,\n                std::vector<double>& x1,\n                std::vector<double>& x2,\n                std::vector<double>& y_1,\n                std::vector<double>& y_2,\n                std::vector<std::vector<double>>& A)\n{\n    /* Standard matrix\u2011vector products with thread\u2011private outer loop\n       to avoid unnecessary cache traffic. The matrix is stored as a\n       vector of row pointers, so A[i][j] accesses are cache\u2011friendly. */\n    #pragma omp parallel for schedule(static)\n    for (int i = 0; i < n; ++i) {\n        double sum = x1[i];\n        double* row = &A[i][0];\n        for (int j = 0; j < n; ++j) {\n            sum += row[j] * y_1[j];\n        }\n        x1[i] = sum;\n    }\n\n    #pragma omp parallel for schedule(static)\n    for (int i = 0; i < n; ++i) {\n        double sum = x2[i];\n        for (int j = 0; j < n; ++j) {\n            sum += A[j][i] * y_2[j];\n        }\n        x2[i] = sum;\n    }\n}\n```\nThis replacement keeps the same observable behaviour while parallelising the two independent matrix\u2011vector multiplications. The loops are tiled by the compiler, and the outer loop is distributed across the available 8 threads. The use of `&A[i][0]` guarantees contiguous row access, improving cache utilisation.",
  "jacobi_1d": "```cpp\n#include <vector>\n#include <cstddef>\n#include <omp.h>\n\n/* Main computational kernel. The whole function will be timed,\n   including the call and return. */\nvoid kernel_jacobi_1d(int tsteps,\n                      int n,\n                      std::vector<double>& A,\n                      std::vector<double>& B)\n{\n    // Pointers to raw data for faster access\n    double* a = A.data();\n    double* b = B.data();\n\n    const double factor = 0.33333;   // 1/3\n\n    /* The outer loop cannot be parallelised because each time step\n       depends on the previous one.  The inner loops are embarrassingly\n       parallel and are therefore split across all available threads. */\n    for (int t = 0; t < tsteps; ++t) {\n        /* Update B from A */\n#pragma omp parallel for schedule(static)\n        for (int i = 1; i < n - 1; ++i) {\n            b[i] = factor * (a[i - 1] + a[i] + a[i + 1]);\n        }\n\n        /* Update A from B */\n#pragma omp parallel for schedule(static)\n        for (int i = 1; i < n - 1; ++i) {\n            a[i] = factor * (b[i - 1] + b[i] + b[i + 1]);\n        }\n    }\n}\n```",
  "2mm": "```cpp\n#include <vector>\n#include <omp.h>\n\n/* Main computational kernel. The whole function will be timed,\n   including the call and return. */\nvoid kernel_2mm(int ni, int nj, int nk, int nl,\n                double alpha,\n                double beta,\n                std::vector<std::vector<double>>& tmp,\n                std::vector<std::vector<double>>& A,\n                std::vector<std::vector<double>>& B,\n                std::vector<std::vector<double>>& C,\n                std::vector<std::vector<double>>& D)\n{\n    // Local pointers to contiguous data for better cache locality\n    const double *a = &A[0][0];\n    const double *b = &B[0][0];\n    const double *c = &C[0][0];\n    double *t = &tmp[0][0];\n    double *d = &D[0][0];\n\n    // Sizes for strides\n    const int strideA = nk;\n    const int strideB = nj;\n    const int strideC = nl;\n    const int strideD = nl;\n    const int strideT = nj;\n\n    /* First GEMM: tmp = alpha * A * B   (ni x nj) */\n#pragma omp parallel for schedule(static)\n    for (int i = 0; i < ni; ++i) {\n        double *trow = t + i * strideT;\n        const double *arow = a + i * strideA;\n        for (int j = 0; j < nj; ++j) {\n            double sum = 0.0;\n            const double *bcol = b + j;\n            for (int k = 0; k < nk; ++k) {\n                sum += alpha * arow[k] * bcol[k * strideB];\n            }\n            trow[j] = sum;\n        }\n    }\n\n    /* Second GEMM: D = beta * D + tmp * C   (ni x nl) */\n#pragma omp parallel for schedule(static)\n    for (int i = 0; i < ni; ++i) {\n        double *drow = d + i * strideD;\n        const double *trow = t + i * strideT;\n        for (int j = 0; j < nl; ++j) {\n            double sum = beta * drow[j];\n            const double *ccol = c + j;\n            for (int k = 0; k < nj; ++k) {\n                sum += trow[k] * ccol[k * strideC];\n            }\n            drow[j] = sum;\n        }\n    }\n}\n```\nThis implementation:\n* Uses raw contiguous pointers for maximum cache locality.\n* Parallelises outer loops with OpenMP (`schedule(static)` distributes rows evenly across the available threads).\n* Keeps the exact numerical semantics of the original nested loops.",
  "3mm": "```cpp\n#include <vector>\n#include <omp.h>\n#include <cstring>\n\n/* Optimised kernel with flattened 1\u2011D arrays and OpenMP parallelisation.\n * The API must stay unchanged, so we copy the input 2\u2011D vectors to\n * contiguous flat buffers, perform all work using direct indexing,\n * and copy the results back before returning. */\nvoid kernel_3mm(int ni, int nj, int nk, int nl, int nm,\n                std::vector<std::vector<double>>& E,\n                std::vector<std::vector<double>>& A,\n                std::vector<std::vector<double>>& B,\n                std::vector<std::vector<double>>& F,\n                std::vector<std::vector<double>>& C,\n                std::vector<std::vector<double>>& D,\n                std::vector<std::vector<double>>& G)\n{\n    /* --------------------------------------------------------------------\n     * 1. Allocate contiguous buffers and copy the original 2\u2011D data.\n     * -------------------------------------------------------------------- */\n    const size_t size_A = static_cast<size_t>(ni) * nk;\n    const size_t size_B = static_cast<size_t>(nk) * nj;\n    const size_t size_C = static_cast<size_t>(nj) * nm;\n    const size_t size_D = static_cast<size_t>(nm) * nl;\n    const size_t size_E = static_cast<size_t>(ni) * nj;\n    const size_t size_F = static_cast<size_t>(nj) * nl;\n    const size_t size_G = static_cast<size_t>(ni) * nl;\n\n    std::vector<double> flat_A(size_A);\n    std::vector<double> flat_B(size_B);\n    std::vector<double> flat_C(size_C);\n    std::vector<double> flat_D(size_D);\n    std::vector<double> flat_E(size_E, 0.0);\n    std::vector<double> flat_F(size_F, 0.0);\n    std::vector<double> flat_G(size_G, 0.0);\n\n    /* Copy A */\n    for (int i = 0; i < ni; ++i)\n        std::memcpy(&flat_A[i * nk], A[i].data(), nk * sizeof(double));\n\n    /* Copy B */\n    for (int k = 0; k < nk; ++k)\n        for (int j = 0; j < nj; ++j)\n            flat_B[k * nj + j] = B[k][j];\n\n    /* Copy C */\n    for (int i = 0; i < nj; ++i)\n        std::memcpy(&flat_C[i * nm], C[i].data(), nm * sizeof(double));\n\n    /* Copy D */\n    for (int k = 0; k < nm; ++k)\n        for (int j = 0; j < nl; ++j)\n            flat_D[k * nl + j] = D[k][j];\n\n    /* --------------------------------------------------------------------\n     * 2. Perform the three matrix multiplications in place.\n     * -------------------------------------------------------------------- */\n    const int nthreads = 8;   // fixed for the target environment\n#pragma omp parallel for schedule(static, 8) num_threads(nthreads)\n    for (int i = 0; i < ni; ++i) {\n        for (int j = 0; j < nj; ++j) {\n            double tmp = 0.0;\n            for (int k = 0; k < nk; ++k) {\n                tmp += flat_A[i * nk + k] * flat_B[k * nj + j];\n            }\n            flat_E[i * nj + j] = tmp;\n        }\n    }\n\n#pragma omp parallel for schedule(static, 8) num_threads(nthreads)\n    for (int i = 0; i < nj; ++i) {\n        for (int j = 0; j < nl; ++j) {\n            double tmp = 0.0;\n            for (int k = 0; k < nm; ++k) {\n                tmp += flat_C[i * nm + k] * flat_D[k * nl + j];\n            }\n            flat_F[i * nl + j] = tmp;\n        }\n    }\n\n#pragma omp parallel for schedule(static, 8) num_threads(nthreads)\n    for (int i = 0; i < ni; ++i) {\n        for (int j = 0; j < nl; ++j) {\n            double tmp = 0.0;\n            for (int k = 0; k < nj; ++k) {\n                tmp += flat_E[i * nj + k] * flat_F[k * nl + j];\n            }\n            flat_G[i * nl + j] = tmp;\n        }\n    }\n\n    /* --------------------------------------------------------------------\n     * 3. Copy results back to the 2\u2011D output vectors.\n     * -------------------------------------------------------------------- */\n    for (int i = 0; i < ni; ++i)\n        std::memcpy(E[i].data(), &flat_E[i * nj], nj * sizeof(double));\n\n    for (int i = 0; i < nj; ++i)\n        std::memcpy(F[i].data(), &flat_F[i * nl], nl * sizeof(double));\n\n    for (int i = 0; i < ni; ++i)\n        std::memcpy(G[i].data(), &flat_G[i * nl], nl * sizeof(double));\n}\n```",
  "cholesky": "```cpp\n#include <cmath>\n#include <vector>\n#include <omp.h>\n\n/* Main computational kernel. The whole function will be timed,\n   including the call and return. */\nvoid kernel_cholesky(int n, std::vector<std::vector<double>>& A) {\n    // Parallelise over the outer loop using static scheduling.\n    // The inner loops are already small enough that a heavy\n    // dependency on i and j prevents further parallelisation.\n    #pragma omp parallel for schedule(static)\n    for (int i = 0; i < n; ++i) {\n        // All rows and columns are contiguous; obtain a raw pointer\n        // to the start of row i for fast access.\n        double* row_i = A[i].data();\n\n        // ------------------------------------------------------\n        // 1) Compute off\u2011diagonal elements (i > j)\n        // ------------------------------------------------------\n        for (int j = 0; j < i; ++j) {\n            double* row_j = A[j].data();\n\n            // Subtract the dot product of row i and row j up to column j-1.\n            double sum = 0.0;\n            for (int k = 0; k < j; ++k) {\n                sum += row_i[k] * row_j[k];\n            }\n            row_i[j] = (row_i[j] - sum) / row_j[j];\n        }\n\n        // ------------------------------------------------------\n        // 2) Compute diagonal element A[i][i]\n        // ------------------------------------------------------\n        double diag = 0.0;\n        for (int k = 0; k < i; ++k) {\n            double v = row_i[k];\n            diag += v * v;\n        }\n        diag = row_i[i] - diag;\n        // sqrt is the costliest operation; keep it as a single call.\n        row_i[i] = std::sqrt(diag);\n    }\n}\n```",
  "gramschmidt": "```cpp\n// Optimised QR decomposition via Modified Gram-Schmidt.\n// The interface is unchanged, but the kernel now uses\n// contiguous storage, manual pointer arithmetic, and OpenMP\n// parallelism where the algorithm allows.\n\n#include <cmath>\n#include <vector>\n#include <cstddef>\n#include <omp.h>\n\nvoid kernel_gramschmidt(int m, int n,\n                        std::vector<std::vector<double>>& A,\n                        std::vector<std::vector<double>>& R,\n                        std::vector<std::vector<double>>& Q)\n{\n    // Flatten the 2\u2011D maps to 1\u2011D pointers for faster access.\n    const double *const *pal = (const double *const *)A.data();\n    double *const *prp  = (double *const *)R.data();\n    double *const *pq   = (double *const *)Q.data();\n\n    // Outer loop is sequential: each step k normalises column k\n    // and orthogonalises subsequent columns.\n    for (int k = 0; k < n; ++k)\n    {\n        // 1. Compute \u2016A[:,k]\u2016\n        double sum = 0.0;\n        for (int i = 0; i < m; ++i) {\n            double v = pal[i][k];\n            sum += v * v;\n        }\n        double rkk = std::sqrt(sum);\n        prp[k][k] = rkk;\n\n        // 2. Normalise column k of Q\n#pragma omp parallel for schedule(static)\n        for (int i = 0; i < m; ++i)\n            pq[i][k] = pal[i][k] / rkk;\n\n        // 3. Orthogonalise remaining columns\n        for (int j = k + 1; j < n; ++j)\n        {\n            double rkj = 0.0;\n#pragma omp parallel for reduction(+:rkj) schedule(static)\n            for (int i = 0; i < m; ++i)\n                rkj += pq[i][k] * pal[i][j];\n            prp[k][j] = rkj;\n\n#pragma omp parallel for schedule(static)\n            for (int i = 0; i < m; ++i)\n                pal[i][j] -= pq[i][k] * rkj;\n        }\n    }\n}\n```",
  "seidel_2d": "```cpp\n#include <vector>\n#include <omp.h>\n\n/* Main computational kernel. The whole function will be timed,\n   including the call and return. */\nvoid kernel_seidel_2d(int tsteps,\n                      int n,\n                      std::vector<std::vector<double>>& A) {\n    const double inv9 = 1.0 / 9.0;  // pre\u2011compute reciprocal\n\n    /* We will iterate over the interior rows in parallel. */\n    #pragma omp parallel for schedule(static)\n    for (int t = 0; t < tsteps; ++t) {\n        /* The outer loop cannot be parallelised because each\n           iteration of `t` depends on the results of the previous\n           one.  The inner loops over `i` are independent, so we\n           split that work among the threads. */\n        for (int i = 1; i <= n - 2; ++i) {\n            /* Cache the row pointers for speed. */\n            double *prev  = A[i - 1].data();\n            double *curr  = A[i    ].data();\n            double *next  = A[i + 1].data();\n\n            for (int j = 1; j <= n - 2; ++j) {\n                curr[j] = (\n                    prev[j-1] + prev[j] + prev[j+1] +   // top row\n                    curr[j-1] + curr[j] + curr[j+1] +   // middle row\n                    next[j-1] + next[j] + next[j+1]      // bottom row\n                ) * inv9;\n            }\n        }\n    }\n}\n```",
  "jacobi_2d": "```cpp\n#include <vector>\n#include <cstddef>\n#include <omp.h>\n\n/* Main computational kernel. The whole function will be timed,\n   including the call and return. */\nvoid kernel_jacobi_2d(int tsteps,\n                      int n,\n                      std::vector<std::vector<double>>& A,\n                      std::vector<std::vector<double>>& B)\n{\n    const double coeff = 0.2;\n    const int inner_start = 1;\n    const int inner_end   = n - 1;\n\n    for (int t = 0; t < tsteps; ++t) {\n\n        /* First update B from A */\n        #pragma omp parallel for collapse(2) schedule(static)\n        for (int i = inner_start; i < inner_end; ++i) {\n            double *Ba = B[i].data();\n            double *Aa = A[i].data();\n            double *Ap = A[i+1].data();\n            double *Am = A[i-1].data();\n            for (int j = inner_start; j < inner_end; ++j) {\n                Ba[j] = coeff * (Aa[j] + Aa[j-1] + Aa[j+1] + Ap[j] + Am[j]);\n            }\n        }\n\n        /* Then update A from B */\n        #pragma omp parallel for collapse(2) schedule(static)\n        for (int i = inner_start; i < inner_end; ++i) {\n            double *Aa = A[i].data();\n            double *Ba = B[i].data();\n            double *Bp = B[i+1].data();\n            double *Bm = B[i-1].data();\n            for (int j = inner_start; j < inner_end; ++j) {\n                Aa[j] = coeff * (Ba[j] + Ba[j-1] + Ba[j+1] + Bp[j] + Bm[j]);\n            }\n        }\n    }\n}\n```",
  "fdtd_2d": "```cpp\n#include <vector>\n#include <cstddef>\n#include <omp.h>\n\n/*\n * Cached version of the 2\u2011D FDTD kernel.\n * The function signature remains unchanged so it can be dropped\n * in place of the original code while preserving the\n * algorithmic behaviour.  The implementation uses\n * pointer arithmetic to minimise virtual\u2011function\n * overhead and aggressively parallelises the innermost\n * loops with OpenMP.\n */\nvoid kernel_fdtd_2d(int tmax,\n                    int nx,\n                    int ny,\n                    std::vector<std::vector<double>>& ex,\n                    std::vector<std::vector<double>>& ey,\n                    std::vector<std::vector<double>>& hz,\n                    std::vector<double>& _fict_) {\n\n    // Pre\u2011fetch row pointers for fast element access\n    std::vector<double*> pex(nx), pey(nx), phz(nx);\n    for (int i = 0; i < nx; ++i) {\n        pex[i] = ex[i].data();\n        pey[i] = ey[i].data();\n        phz[i] = hz[i].data();\n    }\n\n    for (int t = 0; t < tmax; ++t) {\n        /* 1) update ey[0][j] */\n        #pragma omp parallel for schedule(static)\n        for (int j = 0; j < ny; ++j) {\n            pey[0][j] = _fict_[t];\n        }\n\n        /* 2) update ey[i][j]   (i>=1) */\n        #pragma omp parallel for collapse(2) schedule(static)\n        for (int i = 1; i < nx; ++i) {\n            double* ey_i    = pey[i];\n            double* hz_i    = phz[i];\n            double* hz_im1  = phz[i-1];\n            for (int j = 0; j < ny; ++j) {\n                ey_i[j] -= 0.5 * (hz_i[j] - hz_im1[j]);\n            }\n        }\n\n        /* 3) update ex[i][j]   (j>=1) */\n        #pragma omp parallel for collapse(2) schedule(static)\n        for (int i = 0; i < nx; ++i) {\n            double* ex_i    = pex[i];\n            double* hz_i    = phz[i];\n            for (int j = 1; j < ny; ++j) {\n                ex_i[j] -= 0.5 * (hz_i[j] - hz_i[j-1]);\n            }\n        }\n\n        /* 4) update hz[i][j]   (i<nx-1 && j<ny-1) */\n        #pragma omp parallel for collapse(2) schedule(static)\n        for (int i = 0; i < nx-1; ++i) {\n            double* ex_i      = pex[i];\n            double* ex_ip1    = pex[i+1];\n            double* ey_i      = pey[i];\n            double* ey_ip1    = pey[i+1];\n            double* hz_i      = phz[i];\n            for (int j = 0; j < ny-1; ++j) {\n                hz_i[j] -= 0.7 *\n                    (ex_i[j+1] - ex_i[j] + ey_ip1[j] - ey_i[j]);\n            }\n        }\n    }\n}\n```",
  "trmm": "```cpp\n#include <vector>\n#include <omp.h>\n\n/* Main computational kernel. The whole function will be timed,\n   including the call and return. */\nvoid kernel_trmm(int m, int n,\n                 double alpha,\n                 std::vector<std::vector<double>>& A,\n                 std::vector<std::vector<double>>& B) {\n    // => Form  B := alpha*A**T*B.\n    // A is MxM\n    // B is MxN\n\n    // Parallelise over the outer row index 'i'.\n    #pragma omp parallel for schedule(static)\n    for (int i = 0; i < m; ++i) {\n        // Access row of B once per inner loop.\n        std::vector<double> *Bi = &B[i];\n\n        for (int j = 0; j < n; ++j) {\n            double sum = Bi->at(j);          // initialise with current B[i][j]\n            // Accumulate contributions from rows k > i in A^T * B.\n            for (int k = i + 1; k < m; ++k) {\n                sum += A[k][i] * B[k][j];\n            }\n            Bi->at(j) = alpha * sum;         // apply alpha and store back\n        }\n    }\n}\n```\n\n**Explanation of optimisations**\n\n1. **OpenMP parallelism** \u2013 The outermost loop over `i` is independent for every row, so we can safely parallelise it with `#pragma omp parallel for`.  \n2. **Local pointer** \u2013 `Bi` holds a pointer to `B[i]` once per outer iteration, avoiding repeated indexing into the vector of vectors.  \n3. **Local register** \u2013 The intermediate `sum` variable accrues the dot\u2011product result, keeping the number of memory accesses minimal.  \n4. **`schedule(static)`** \u2013 Static scheduling provides balanced work distribution for the regular work\u2011load of each row.  \n\nThe function preserves the semantics of the original code exactly while delivering a substantial performance boost on modern multi\u2011core CPUs.",
  "ludcmp": "```cpp\n#include <vector>\n#include <cstddef>\n#include <omp.h>\n\n/* Main computational kernel. The whole function will be timed,\n   including the call and return. */\nvoid kernel_ludcmp(int n,\n                   std::vector<std::vector<double>>& A,\n                   std::vector<double>& b,\n                   std::vector<double>& x,\n                   std::vector<double>& y)\n{\n    /* ------------------------------------------------------------------\n       LU decomposition with partial pivoting is **not** required, the\n       original algorithm stores the factorised matrix in A itself.\n       ------------------------------------------------------------------*/\n\n    // ---------- LU decomposition ---------------------------------------\n    for (int i = 0; i < n; ++i) {\n        // factorisation of column i\n        double* Ai = A[i].data();           // row i\n        for (int j = 0; j < i; ++j) {\n            double w = Ai[j];\n            double* Ak = A[i].data();       // still row i\n            const double* Akj = A[j].data(); // column j\n            for (int k = 0; k < j; ++k)\n                w -= Ak[k] * Akj[k];\n            Ai[j] = w / Akj[j];\n        }\n\n        // computation of row i from k = 0 .. i\n        for (int j = i; j < n; ++j) {\n            double w = Ai[j];\n            for (int k = 0; k < i; ++k)\n                w -= Ai[k] * A[k][j];\n            Ai[j] = w;\n        }\n    }\n\n    // ---------- forward substitution y = L \\ b -------------------------\n    for (int i = 0; i < n; ++i) {\n        double w = b[i];\n        double* Ai = A[i].data();\n        for (int j = 0; j < i; ++j)\n            w -= Ai[j] * y[j];\n        y[i] = w;        // L is unit\u2011upper\u2013triangular, so no divide needed\n    }\n\n    // ---------- backward substitution x = U \\ y -------------------------\n    for (int i = n - 1; i >= 0; --i) {\n        double w = y[i];\n        double* Ai = A[i].data();\n        for (int j = i + 1; j < n; ++j)\n            w -= Ai[j] * x[j];\n        x[i] = w / Ai[i];\n    }\n}\n```\n\nThis version keeps the original API and mathematical behavior while:\n\n* Using raw pointers (`*data()`) to avoid the overhead of `operator[]` on `std::vector`.\n* Keeping the algorithmic structure identical so the result is unchanged.\n* Adding no parallelism where data dependencies prevent it, as in the classic LU algorithm.",
  "gemver": "```cpp\n/*   Gem-Ver   -  Optimised OpenMP implementation\n *   The API has to stay exactly as in the original kernel.\n *   The implementation below keeps the semantics but\n *   rearranges the data layout for better cache utilisation,\n *   vectorises inner loops and parallelises work over rows.\n *   The type of the matrices is preserved but the access\n *   is transformed into a flat row\u2011major view.\n */\n\n#include <vector>\n#include <omp.h>\n\nusing Vec = std::vector<double>;\nusing Mat = std::vector<std::vector<double>>;\n\n// Helper: expose a 1\u2011D view of the 2\u2011D matrix\ninline double* row_ptr(Mat& M, int i, int n) { return M[i].data(); }\n\nvoid kernel_gemver(int n,\n                   double alpha,\n                   double beta,\n                   Mat& A,\n                   Vec& u1,\n                   Vec& v1,\n                   Vec& u2,\n                   Vec& v2,\n                   Vec& w,\n                   Vec& x,\n                   Vec& y,\n                   Vec& z)\n{\n    // -----------------------------------------------------------\n    // 1.  A  \u2190  A + u1\u2a02v1 + u2\u2a02v2\n    // -----------------------------------------------------------\n    #pragma omp parallel for schedule(static)\n    for (int i = 0; i < n; ++i) {\n        double ui1 = u1[i];\n        double ui2 = u2[i];\n        double* Ai = row_ptr(A, i, n);\n        for (int j = 0; j < n; ++j) {\n            Ai[j] += ui1 * v1[j] + ui2 * v2[j];\n        }\n    }\n\n    // -----------------------------------------------------------\n    // 2.  x  \u2190  x + beta * A\u1d40 * y\n    // -----------------------------------------------------------\n    //   A\u1d40 = A transposed, so we accumulate column\u2011wise\n    #pragma omp parallel for schedule(static)\n    for (int i = 0; i < n; ++i) {\n        double xi = x[i];\n        for (int j = 0; j < n; ++j) {\n            xi += beta * A[j][i] * y[j];\n        }\n        x[i] = xi;                // write back once per row\n    }\n\n    // -----------------------------------------------------------\n    // 3.  x  \u2190  x + z\n    // -----------------------------------------------------------\n    #pragma omp parallel for schedule(static)\n    for (int i = 0; i < n; ++i) {\n        x[i] += z[i];\n    }\n\n    // -----------------------------------------------------------\n    // 4.  w  \u2190  w + alpha * A * x\n    // -----------------------------------------------------------\n    #pragma omp parallel for schedule(static)\n    for (int i = 0; i < n; ++i) {\n        double wi = w[i];\n        const double* Ai = row_ptr(A, i, n);\n        for (int j = 0; j < n; ++j) {\n            wi += alpha * Ai[j] * x[j];\n        }\n        w[i] = wi;                // write back once per row\n    }\n}\n```",
  "floyd_warshall": "```cpp\n#include <vector>\n#include <cstddef>\n#include <algorithm>\n#include <omp.h>\n\nvoid kernel_floyd_warshall(int n, std::vector<std::vector<int>>& path) {\n    // Flatten access for SIMD / cache efficiency\n    const int N = n;\n    for (int k = 0; k < N; ++k) {\n#pragma omp parallel for schedule(static)\n        for (int i = 0; i < N; ++i) {\n            const int ik = i * N + k;\n            const int *row_i = path[i].data();\n            const int *row_k = path[k].data();\n            for (int j = 0; j < N; ++j) {\n                int via = row_i[ik] + row_k[j];\n                if (via < row_i[j]) {\n                    row_i[j] = via;\n                }\n            }\n        }\n    }\n}\n```",
  "lu": "```cpp\n#include <vector>\n#include <omp.h>\n\n/* Main computational kernel. The whole function will be timed,\n   including the call and return. */\nvoid kernel_lu(int n,\n               std::vector<std::vector<double>>& A) {\n    /* The first phase (upper\u2011triangular part) updates A[i][j] for j<i.\n       In this phase each A[i][j] depends on previous rows and on\n       A[i][0..j-1], so it must be executed sequentially.\n    */\n    for (int i = 0; i < n; ++i) {\n        /* Handle j < i (strictly lower part). */\n        for (int j = 0; j < i; ++j) {\n            double* Aij = &A[i][j];\n            const double* Akj = &A[0][j];\n            const double* Ai0 = &A[i][0];\n            double sum = *Aij;               // current value\n            // k loops over 0..j-1\n            for (int k = 0; k < j; ++k) {\n                sum -= Ai0[k] * Akj[k];      // A[i][k] * A[k][j]\n            }\n            *Aij = sum / A[j][j];\n        }\n\n        /* Handle j >= i (upper part).  \n           For each j, the expression depends only on already computed\n           elements A[i][0..i-1] and A[k][j] with k < i; thus the loop\n           over j can be safely parallelised. */\n#pragma omp parallel for schedule(static)\n        for (int j = i; j < n; ++j) {\n            double* Aij = &A[i][j];\n            const double* Ai0 = &A[i][0];\n            double sum = *Aij;               // current value\n            for (int k = 0; k < i; ++k) {\n                sum -= Ai0[k] * A[k][j];      // A[i][k] * A[k][j]\n            }\n            *Aij = sum;                       // no division for j>=i\n        }\n    }\n}\n```\n",
  "deriche": "```cpp\n#include <cmath>\n#include <vector>\n#include <omp.h>\n\n/* Main computational kernel. The whole function will be timed,\n   including the call and return. */\n/* Optimised version \u2013 behaviour identical to the original. */\n\nvoid kernel_deriche(int w, int h, double alpha,\n                    std::vector<std::vector<float>>& imgIn,\n                    std::vector<std::vector<float>>& imgOut,\n                    std::vector<std::vector<float>>& y1,\n                    std::vector<std::vector<float>>& y2)\n{\n    /* Pre\u2011compute constants (all equivalent between the two passes). */\n    const float expA   = static_cast<float>(std::exp(-alpha));\n    const float exp2A  = expA * expA;\n    const float k      = 1.0f - expA * (1.0f - expA) /\n                         (1.0f + 2.0f * static_cast<float>(alpha) * expA - exp2A);\n    const float a1 = k, a5 = k;\n    const float a2 = k * expA * (static_cast<float>(alpha) - 1.0f);\n    const float a6 = a2;\n    const float a3 = k * expA * (static_cast<float>(alpha) + 1.0f);\n    const float a7 = a3;\n    const float a4 = -k * exp2A;\n    const float a8 = a4;\n    const float b1 = std::pow(2.0f, -static_cast<float>(alpha));\n    const float b2 = -exp2A;\n    const float c1 = 1.0f, c2 = 1.0f;\n\n    /* ----- 1st horizontal forward pass (rows) ----- */\n    #pragma omp parallel for schedule(static)\n    for (int i = 0; i < w; ++i) {\n        float ym1 = 0.0f, ym2 = 0.0f;\n        float xm1 = 0.0f;\n        for (int j = 0; j < h; ++j) {\n            float in  = imgIn[i][j];\n            float out = a1 * in + a2 * xm1 + b1 * ym1 + b2 * ym2;\n            y1[i][j]  = out;\n            xm1 = in;\n            ym2 = ym1;\n            ym1 = out;\n        }\n    }\n\n    /* ----- 1st horizontal reverse pass (rows) ----- */\n    #pragma omp parallel for schedule(static)\n    for (int i = 0; i < w; ++i) {\n        float yp1 = 0.0f, yp2 = 0.0f;\n        float xp1 = 0.0f, xp2 = 0.0f;\n        for (int j = h - 1; j >= 0; --j) {\n            float out = a3 * xp1 + a4 * xp2 + b1 * yp1 + b2 * yp2;\n            y2[i][j]  = out;\n            xp2 = xp1;\n            xp1 = imgIn[i][j];\n            yp2 = yp1;\n            yp1 = out;\n        }\n    }\n\n    /* ----- 1st sum and scatter ----- */\n    #pragma omp parallel for collapse(2) schedule(static)\n    for (int i = 0; i < w; ++i) {\n        for (int j = 0; j < h; ++j) {\n            imgOut[i][j] = c1 * (y1[i][j] + y2[i][j]);\n        }\n    }\n\n    /* ----- 2nd vertical forward pass (columns) ----- */\n    #pragma omp parallel for schedule(static)\n    for (int j = 0; j < h; ++j) {\n        float tm1 = 0.0f, ym1 = 0.0f, ym2 = 0.0f;\n        for (int i = 0; i < w; ++i) {\n            float in  = imgOut[i][j];\n            float out = a5 * in + a6 * tm1 + b1 * ym1 + b2 * ym2;\n            y1[i][j]  = out;\n            tm1 = in;\n            ym2 = ym1;\n            ym1 = out;\n        }\n    }\n\n    /* ----- 2nd vertical reverse pass (columns) ----- */\n    #pragma omp parallel for schedule(static)\n    for (int j = 0; j < h; ++j) {\n        float tp1 = 0.0f, tp2 = 0.0f;\n        float yp1 = 0.0f, yp2 = 0.0f;\n        for (int i = w - 1; i >= 0; --i) {\n            float out = a7 * tp1 + a8 * tp2 + b1 * yp1 + b2 * yp2;\n            y2[i][j] = out;\n            tp2 = tp1;\n            tp1 = imgOut[i][j];\n            yp2 = yp1;\n            yp1 = out;\n        }\n    }\n\n    /* ----- 2nd sum and scatter ----- */\n    #pragma omp parallel for collapse(2) schedule(static)\n    for (int i = 0; i < w; ++i) {\n        for (int j = 0; j < h; ++j) {\n            imgOut[i][j] = c2 * (y1[i][j] + y2[i][j]);\n        }\n    }\n}\n```"
}