{
  "gemm": "```cpp\n#include <vector>\n#include <omp.h>\n\nvoid kernel_gemm(int ni, int nj, int nk,\n                 double alpha,\n                 double beta,\n                 std::vector<std::vector<double>>& A,\n                 std::vector<std::vector<double>>& B,\n                 std::vector<std::vector<double>>& C) {\n    #pragma omp parallel for schedule(static)\n    for (int i = 0; i < ni; ++i) {\n        double* c = C[i].data();\n        const double* a_row = A[i].data();\n\n        /* Scale the i\u2011th row of C by beta. */\n        #pragma omp simd\n        for (int j = 0; j < nj; ++j) {\n            c[j] *= beta;\n        }\n\n        /* Main GEMM computation. */\n        for (int k = 0; k < nk; ++k) {\n            const double aik = alpha * a_row[k];\n            const double* b_row = B[k].data();\n\n            #pragma omp simd\n            for (int j = 0; j < nj; ++j) {\n                c[j] += aik * b_row[j];\n            }\n        }\n    }\n}\n```\n",
  "trisolv": "```cpp\n#include <vector>\n\nvoid kernel_trisolv(int n,\n                    std::vector<std::vector<double>>& L,\n                    std::vector<double>& x,\n                    std::vector<double>& b) {\n    double* x_ptr = x.data();\n    const double* b_ptr = b.data();\n\n    for (int i = 0; i < n; ++i) {\n        double val = b_ptr[i];\n        const double* row = L[i].data();\n\n        int j = 0;\n        int limit = i - (i & 3);   // i - (i % 4)\n        for (; j < limit; j += 4) {\n            val -= row[j]   * x_ptr[j];\n            val -= row[j+1] * x_ptr[j+1];\n            val -= row[j+2] * x_ptr[j+2];\n            val -= row[j+3] * x_ptr[j+3];\n        }\n        for (; j < i; ++j) {\n            val -= row[j] * x_ptr[j];\n        }\n        x_ptr[i] = val / row[i];\n    }\n}\n```",
  "gesummv": "```cpp\n#include <vector>\n#include <omp.h>\n\n/* Main computational kernel. The whole function will be timed,\n   including the call and return. */\nvoid kernel_gesummv(int n,\n                    double alpha,\n                    double beta,\n                    std::vector<std::vector<double>>& A,\n                    std::vector<std::vector<double>>& B,\n                    std::vector<double>& tmp,\n                    std::vector<double>& x,\n                    std::vector<double>& y)\n{\n    /* Parallelise the outer loop over the rows of A and B.  Each thread works\n       on a distinct subset of rows, which keeps all accesses independent\n       and free of data races.  The inner loop remains sequential to\n       preserve the exact accumulation order. */\n#pragma omp parallel for schedule(static)\n    for (int i = 0; i < n; ++i) {\n        double sumA = 0.0;\n        double sumB = 0.0;\n\n        const double* aRow = A[i].data();\n        const double* bRow = B[i].data();\n        const double* xPtr = x.data();\n\n        /* Compute the two dot products for this row.  The inner loop\n           operates over contiguous memory and can be vectorised by\n           the compiler. */\n        for (int j = 0; j < n; ++j) {\n            double xj = xPtr[j];\n            sumA += aRow[j] * xj;\n            sumB += bRow[j] * xj;\n        }\n\n        /* Store the intermediate result and apply the final linear\n           combination exactly as done in the original serial code. */\n        tmp[i] = sumA;\n        y[i]   = alpha * sumA + beta * sumB;\n    }\n}\n```",
  "covariance": "```cpp\n#include <vector>\n#include <omp.h>\n\nvoid kernel_covariance(int m, int n,\n                       std::vector<std::vector<double>>& data,\n                       std::vector<std::vector<double>>& cov,\n                       std::vector<double>& mean)\n{\n    // ----- compute column means --------------------------------------------\n    std::vector<double> sum_col(m, 0.0);\n    for (int i = 0; i < n; ++i)\n    {\n        const std::vector<double>& row = data[i];\n        for (int j = 0; j < m; ++j)\n            sum_col[j] += row[j];\n    }\n    for (int j = 0; j < m; ++j)\n        mean[j] = sum_col[j] / static_cast<double>(n);\n\n    // ----- subtract means (in place) ---------------------------------------\n    #pragma omp parallel for schedule(static)\n    for (int i = 0; i < n; ++i)\n    {\n        std::vector<double>& row = data[i];\n        for (int j = 0; j < m; ++j)\n            row[j] -= mean[j];\n    }\n\n    // ----- compute covariance ---------------------------------------------\n    const double inv = 1.0 / static_cast<double>(n - 1);\n    #pragma omp parallel for schedule(static)\n    for (int i = 0; i < m; ++i)\n    {\n        for (int j = i; j < m; ++j)\n        {\n            double sum = 0.0;\n            #pragma omp simd reduction(+:sum)\n            for (int k = 0; k < n; ++k)\n                sum += data[k][i] * data[k][j];\n            double val = sum * inv;\n            cov[i][j] = val;\n            if (j > i)\n                cov[j][i] = val;\n        }\n    }\n}\n```",
  "3mm": "```cpp\n#include <vector>\n#ifdef _OPENMP\n#include <omp.h>\n#endif\n\nvoid kernel_3mm(int ni, int nj, int nk, int nl, int nm,\n        std::vector<std::vector<double>>& E,\n        std::vector<std::vector<double>>& A,\n        std::vector<std::vector<double>>& B,\n        std::vector<std::vector<double>>& F,\n        std::vector<std::vector<double>>& C,\n        std::vector<std::vector<double>>& D,\n        std::vector<std::vector<double>>& G) {\n\n    /* E = A * B  (ni \u00d7 nj) */\n    #pragma omp parallel for schedule(static)\n    for (int i = 0; i < ni; ++i) {\n        double*        Erow = E[i].data();\n        const double*  Arow = A[i].data();\n\n        /* Zero the result row */\n        for (int j = 0; j < nj; ++j) {\n            Erow[j] = 0.0;\n        }\n\n        /* Accumulate */\n        for (int k = 0; k < nk; ++k) {\n            double          aik = Arow[k];\n            const double*   Brow = B[k].data();\n\n            const double* bp = Brow;\n            double*       ep = Erow;\n            #pragma omp simd\n            for (int j = 0; j < nj; ++j) {\n                *ep++ += aik * *bp++;\n            }\n        }\n    }\n\n    /* F = C * D  (nj \u00d7 nl) */\n    #pragma omp parallel for schedule(static)\n    for (int i = 0; i < nj; ++i) {\n        double*        Frow = F[i].data();\n        const double*  Crow = C[i].data();\n\n        /* Zero the result row */\n        for (int j = 0; j < nl; ++j) {\n            Frow[j] = 0.0;\n        }\n\n        /* Accumulate */\n        for (int k = 0; k < nm; ++k) {\n            double          cik = Crow[k];\n            const double*   Drow = D[k].data();\n\n            const double* dp = Drow;\n            double*       fp = Frow;\n            #pragma omp simd\n            for (int j = 0; j < nl; ++j) {\n                *fp++ += cik * *dp++;\n            }\n        }\n    }\n\n    /* G = E * F  (ni \u00d7 nl) */\n    #pragma omp parallel for schedule(static)\n    for (int i = 0; i < ni; ++i) {\n        double*        Grow = G[i].data();\n        const double*  Erow = E[i].data();\n\n        /* Zero the result row */\n        for (int j = 0; j < nl; ++j) {\n            Grow[j] = 0.0;\n        }\n\n        /* Accumulate */\n        for (int k = 0; k < nj; ++k) {\n            double          eik = Erow[k];\n            const double*   Frow = F[k].data();\n\n            const double* fp = Frow;\n            double*       gp = Grow;\n            #pragma omp simd\n            for (int j = 0; j < nl; ++j) {\n                *gp++ += eik * *fp++;\n            }\n        }\n    }\n}\n```",
  "symm": "```cpp\n#include <vector>\n#include <omp.h>\n\nvoid kernel_symm(int m, int n,\n                 double alpha,\n                 double beta,\n                 std::vector<std::vector<double>>& C,\n                 std::vector<std::vector<double>>& A,\n                 std::vector<std::vector<double>>& B)\n{\n    // Cache raw pointers for faster row access\n    std::vector<double*> C_rows(m);\n    std::vector<double*> A_rows(m);\n    std::vector<double*> B_rows(m);\n    for (int i = 0; i < m; ++i) {\n        C_rows[i] = C[i].data();\n        A_rows[i] = A[i].data();\n        B_rows[i] = B[i].data();\n    }\n\n    // Parallelise over columns; each column can be processed independently.\n    #pragma omp parallel for schedule(static)\n    for (int j = 0; j < n; ++j) {\n        for (int i = 0; i < m; ++i) {\n            double temp2 = 0.0;\n            double Bi_j = B_rows[i][j];\n            double factor = alpha * Bi_j;              // \u03b1 * B[i][j]\n            double* ci_row = C_rows[i];\n\n            for (int k = 0; k < i; ++k) {\n                double aik = A_rows[i][k];\n                C_rows[k][j] += factor * aik;          // \u03b1 * B[i][j] * A[i][k]\n                temp2 += B_rows[k][j] * aik;          // B[k][j] * A[i][k]\n            }\n\n            ci_row[j] = beta * ci_row[j] + factor * A_rows[i][i] + alpha * temp2;\n        }\n    }\n}\n```",
  "2mm": "```cpp\n#include <vector>\n#include <omp.h>\n#include <cstring>\n\nvoid kernel_2mm(int ni, int nj, int nk, int nl,\n                double alpha,\n                double beta,\n                std::vector<std::vector<double>>& tmp,\n                std::vector<std::vector<double>>& A,\n                std::vector<std::vector<double>>& B,\n                std::vector<std::vector<double>>& C,\n                std::vector<std::vector<double>>& D)\n{\n    /* Compute tmp = alpha * A * B */\n    #pragma omp parallel for schedule(static)\n    for (int i = 0; i < ni; ++i) {\n        double* tmp_row = tmp[i].data();\n        std::memset(tmp_row, 0, nj * sizeof(double));\n\n        const double* a_row = A[i].data();\n        for (int k = 0; k < nk; ++k) {\n            double aik = alpha * a_row[k];\n            const double* b_row = B[k].data();\n\n            for (int j = 0; j < nj; ++j) {\n                tmp_row[j] += aik * b_row[j];\n            }\n        }\n    }\n\n    /* Compute D = beta * D + tmp * C */\n    #pragma omp parallel for schedule(static)\n    for (int i = 0; i < ni; ++i) {\n        double* d_row = D[i].data();\n\n        for (int j = 0; j < nl; ++j) {\n            d_row[j] *= beta;\n        }\n\n        const double* tmp_row = tmp[i].data();\n        for (int k = 0; k < nj; ++k) {\n            double t = tmp_row[k];\n            const double* c_row = C[k].data();\n\n            for (int j = 0; j < nl; ++j) {\n                d_row[j] += t * c_row[j];\n            }\n        }\n    }\n}\n```",
  "heat_3d": "```cpp\n#include <vector>\n#include <omp.h>\n\n/* Main computational kernel. The whole function will be timed,\n   including the call and return. */\nvoid kernel_heat_3d(int tsteps,\n                    int n,\n                    std::vector<std::vector<std::vector<double>>>& A,\n                    std::vector<std::vector<std::vector<double>>>& B) {\n    const double inv8   = 0.125;   // 1/8\n    const double quarter = 0.25;    // 1/4\n    const int nMinus1 = n - 1;\n\n    #pragma omp parallel\n    {\n        for (int t = 1; t <= tsteps; ++t) {\n            /* First update: compute B from A */\n            #pragma omp for collapse(2) schedule(static)\n            for (int i = 1; i < nMinus1; ++i) {\n                for (int j = 1; j < nMinus1; ++j) {\n                    double* Acur    = A[i][j].data();\n                    double* Aprev   = A[i-1][j].data();\n                    double* Anext   = A[i+1][j].data();\n                    double* Ajprev  = A[i][j-1].data();\n                    double* Ajnext  = A[i][j+1].data();\n                    double* Bcur    = B[i][j].data();\n\n                    #pragma omp simd\n                    for (int k = 1; k < nMinus1; ++k) {\n                        double center = Acur[k];\n                        double sumNeighbors =\n                            Aprev[k] + Anext[k] + Ajprev[k] + Ajnext[k] + Acur[k-1] + Acur[k+1];\n                        Bcur[k] = inv8 * sumNeighbors + quarter * center;\n                    }\n                }\n            }\n\n            /* Second update: compute A from B */\n            #pragma omp for collapse(2) schedule(static)\n            for (int i = 1; i < nMinus1; ++i) {\n                for (int j = 1; j < nMinus1; ++j) {\n                    double* Bcur    = B[i][j].data();\n                    double* Bprev   = B[i-1][j].data();\n                    double* Bnext   = B[i+1][j].data();\n                    double* Bjprev  = B[i][j-1].data();\n                    double* Bjnext  = B[i][j+1].data();\n                    double* Acur    = A[i][j].data();\n\n                    #pragma omp simd\n                    for (int k = 1; k < nMinus1; ++k) {\n                        double center = Bcur[k];\n                        double sumNeighbors =\n                            Bprev[k] + Bnext[k] + Bjprev[k] + Bjnext[k] + Bcur[k-1] + Bcur[k+1];\n                        Acur[k] = inv8 * sumNeighbors + quarter * center;\n                    }\n                }\n            }\n        }\n    }\n}\n```",
  "atax": "```cpp\n#include <vector>\n#include <omp.h>\n\nvoid kernel_atax(int m, int n,\n                 std::vector<std::vector<double>>& A,\n                 std::vector<double>& x,\n                 std::vector<double>& y,\n                 std::vector<double>& tmp) {\n    // Per\u2011thread accumulators for y to avoid write conflicts.\n    int num_threads = omp_get_max_threads();\n    std::vector<std::vector<double>> local_y(\n        num_threads, std::vector<double>(n));\n\n    const double *x_ptr = x.data();\n\n    #pragma omp parallel\n    {\n        int tid = omp_get_thread_num();\n        std::vector<double> &y_local = local_y[tid];\n\n        #pragma omp for schedule(static)\n        for (int i = 0; i < m; ++i) {\n            const double *arow = A[i].data();\n\n            // Compute tmp[i] = A[i] \u00b7 x\n            double tmp_val = 0.0;\n            #pragma omp simd\n            for (int j = 0; j < n; ++j) {\n                tmp_val += arow[j] * x_ptr[j];\n            }\n            tmp[i] = tmp_val;\n\n            // Accumulate contribution to y_local\n            #pragma omp simd\n            for (int j = 0; j < n; ++j) {\n                y_local[j] += arow[j] * tmp_val;\n            }\n        }\n    }\n\n    // Combine thread results into the final y vector.\n    for (int j = 0; j < n; ++j) {\n        double sum = 0.0;\n        for (int t = 0; t < num_threads; ++t) {\n            sum += local_y[t][j];\n        }\n        y[j] = sum;\n    }\n}\n```\n",
  "correlation": "```cpp\n#include <vector>\n#include <cmath>\n#include <omp.h>\n\nvoid kernel_correlation(int m, int n,\n                        std::vector<std::vector<double>>& data,\n                        std::vector<std::vector<double>>& corr,\n                        std::vector<double>& mean,\n                        std::vector<double>& stddev) {\n    const double eps = 0.1;\n\n    /* Compute the mean of each column */\n    #pragma omp parallel for schedule(static)\n    for (int j = 0; j < m; ++j) {\n        double sum = 0.0;\n        for (int i = 0; i < n; ++i) {\n            sum += data[i][j];\n        }\n        mean[j] = sum / n;\n    }\n\n    /* Compute the standard deviation of each column (biased, denominator n) */\n    #pragma omp parallel for schedule(static)\n    for (int j = 0; j < m; ++j) {\n        double ssq = 0.0;\n        double mj = mean[j];\n        for (int i = 0; i < n; ++i) {\n            double d = data[i][j] - mj;\n            ssq += d * d;\n        }\n        double var = ssq / n;\n        double sd  = std::sqrt(var);\n        if (sd <= eps) sd = 1.0;        // avoid near-zero stddev\n        stddev[j] = sd;\n    }\n\n    /* Pre\u2011compute denominators for standardisation */\n    const double sqrt_n = std::sqrt(static_cast<double>(n));\n    std::vector<double> denom(m);\n    for (int j = 0; j < m; ++j) {\n        denom[j] = sqrt_n * stddev[j];\n    }\n\n    /* Standardise the data in place */\n    #pragma omp parallel for schedule(static)\n    for (int i = 0; i < n; ++i) {\n        double* row = data[i].data();\n        for (int j = 0; j < m; ++j) {\n            row[j] = (row[j] - mean[j]) / denom[j];\n        }\n    }\n\n    /* Compute the correlation matrix */\n    #pragma omp parallel for schedule(static)\n    for (int i = 0; i < m - 1; ++i) {\n        corr[i][i] = 1.0;\n        for (int j = i + 1; j < m; ++j) {\n            double sum = 0.0;\n            for (int k = 0; k < n; ++k) {\n                const double* row = data[k].data();\n                sum += row[i] * row[j];\n            }\n            corr[i][j] = sum;\n            corr[j][i] = sum;\n        }\n    }\n    if (m > 0) {\n        corr[m - 1][m - 1] = 1.0;\n    }\n}\n```\n",
  "nussinov": "```cpp\n#include <vector>\n#include <omp.h>\n\nvoid kernel_nussinov(int n, std::vector<int>& seq,\n                     std::vector<std::vector<int>>& table) {\n    if (n <= 1) return;         // nothing to compute\n\n    #pragma omp parallel\n    {\n        for (int d = 1; d < n; ++d) {\n            const int nd = n - d;                     // number of rows for this diagonal\n            #pragma omp for schedule(static)\n            for (int i = 0; i < nd; ++i) {\n                int j = i + d;                       // column index\n\n                // start with the current (possibly pre\u2011initialised) value\n                int best = table[i][j];\n\n                // left neighbour\n                if (table[i][j-1] > best) best = table[i][j-1];\n\n                // upper neighbour\n                if (table[i+1][j] > best) best = table[i+1][j];\n\n                // diagonal neighbour with possible match\n                int diag = table[i+1][j-1] + ((i < j-1) && (seq[i] + seq[j] == 3));\n                if (diag > best) best = diag;\n\n                // split contributions\n                for (int k = i + 1; k < j; ++k) {\n                    int split = table[i][k] + table[k+1][j];\n                    if (split > best) best = split;\n                }\n\n                table[i][j] = best;\n            }\n        }\n    }\n}\n```\n",
  "mvt": "```cpp\n#include <vector>\n#include <omp.h>\n\nvoid kernel_mvt(int n,\n                std::vector<double>& x1,\n                std::vector<double>& x2,\n                std::vector<double>& y_1,\n                std::vector<double>& y_2,\n                std::vector<std::vector<double>>& A) {\n    const int N = n;\n    const double* const y1 = y_1.data();\n    const double* const y2 = y_2.data();\n\n    #pragma omp parallel\n    {\n        /* First matrix\u2011vector product: x1 += A * y_1 */\n        #pragma omp for schedule(static) nowait\n        for (int i = 0; i < N; ++i) {\n            double sum = x1[i];\n            const double* const row = A[i].data();\n            for (int j = 0; j < N; ++j) {\n                sum += row[j] * y1[j];\n            }\n            x1[i] = sum;\n        }\n\n        /* Second matrix\u2011vector product: x2 += A^T * y_2 */\n        #pragma omp for schedule(static) nowait\n        for (int i = 0; i < N; ++i) {\n            double sum = x2[i];\n            for (int j = 0; j < N; ++j) {\n                sum += A[j][i] * y2[j];\n            }\n            x2[i] = sum;\n        }\n    }\n}\n```",
  "syr2k": "```cpp\n#include <vector>\n#include <omp.h>\n\nvoid kernel_syr2k(int n, int m,\n                  double alpha, double beta,\n                  std::vector<std::vector<double>>& C,\n                  std::vector<std::vector<double>>& A,\n                  std::vector<std::vector<double>>& B)\n{\n    /* Pre\u2011compute row pointers to avoid repeated indirection. */\n    std::vector<const double*> a_rows(n);\n    std::vector<const double*> b_rows(n);\n    for (int i = 0; i < n; ++i) {\n        a_rows[i] = A[i].data();\n        b_rows[i] = B[i].data();\n    }\n\n    /* Parallelise over the outermost index (rows of C). */\n    #pragma omp parallel for schedule(static)\n    for (int i = 0; i < n; ++i) {\n        double *Ci = C[i].data();        /* current row of C */\n        const double *Ai = a_rows[i];    /* current row of A   */\n        const double *Bi = b_rows[i];    /* current row of B   */\n\n        /* Scale the lower triangular part of C by \u03b2. */\n        for (int j = 0; j <= i; ++j) {\n            Ci[j] *= beta;\n        }\n\n        /* Rank\u20112k update: C[i][j] += A[j][k]*\u03b1*B[i][k] + B[j][k]*\u03b1*A[i][k] */\n        for (int k = 0; k < m; ++k) {\n            double aik = Ai[k];   /* A[i][k] \u2013 constant for this k */\n            double bik = Bi[k];   /* B[i][k] \u2013 constant for this k */\n\n            for (int j = 0; j <= i; ++j) {\n                Ci[j] += (a_rows[j][k] * alpha) * bik\n                       + (b_rows[j][k] * alpha) * aik;\n            }\n        }\n    }\n}\n```",
  "adi": "```cpp\n#include <vector>\n#include <omp.h>\n\n/* Main computational kernel. The whole function will be timed,\n   including the call and return. */\n/* Based on a Fortran code fragment from Figure 5 of\n * \"Automatic Data and Computation Decomposition on Distributed Memory Parallel Computers\"\n * by Peizong Lee and Zvi Meir Kedem, TOPLAS, 2002\n */\nvoid kernel_adi(int tsteps, int n,\n                std::vector<std::vector<double>>& u,\n                std::vector<std::vector<double>>& v,\n                std::vector<std::vector<double>>& p,\n                std::vector<std::vector<double>>& q) {\n    const double DX = 1.0 / n;\n    const double DY = 1.0 / n;\n    const double DT = 1.0 / tsteps;\n    const double B1 = 2.0;\n    const double B2 = 1.0;\n    const double mul1 = B1 * DT / (DX * DX);\n    const double mul2 = B2 * DT / (DY * DY);\n    const double a = -mul1 / 2.0;\n    const double b = 1.0 + mul1;\n    const double c = a;\n    const double d = -mul2 / 2.0;\n    const double e = 1.0 + mul2;\n    const double f = d;\n\n    const int N = n;                     // local alias for size\n\n    /* Prepare row pointers for fast 2\u2011D indexing */\n    std::vector<double*> u_rows(N);\n    std::vector<double*> v_rows(N);\n    std::vector<double*> p_rows(N);\n    std::vector<double*> q_rows(N);\n    for (int i = 0; i < N; ++i) {\n        u_rows[i] = u[i].data();\n        v_rows[i] = v[i].data();\n        p_rows[i] = p[i].data();\n        q_rows[i] = q[i].data();\n    }\n\n    /* Parallel outer loop over time steps \u2013 serial inside each thread */\n#pragma omp parallel\n    {\n        for (int t = 1; t <= tsteps; ++t) {\n\n            /* ---------- Forward sweep: solve for v ---------- */\n#pragma omp for schedule(static)\n            for (int i = 1; i < N - 1; ++i) {\n                v_rows[0][i] = 1.0;\n                p_rows[i][0] = 0.0;\n                q_rows[i][0] = v_rows[0][i];   // q[i][0] = v[0][i]\n\n                /* forward sweep over j */\n                for (int j = 1; j < N - 1; ++j) {\n                    double* u_j = u_rows[j];\n                    double denom = a * p_rows[i][j - 1] + b;\n                    p_rows[i][j] = -c / denom;\n                    double numer = -d * u_j[i - 1] +\n                                   (1.0 + 2.0 * d) * u_j[i] -\n                                   f * u_j[i + 1] -\n                                   a * q_rows[i][j - 1];\n                    q_rows[i][j] = numer / denom;\n                }\n\n                v_rows[N - 1][i] = 1.0;\n\n                /* backward sweep over j */\n                for (int j = N - 2; j >= 1; --j) {\n                    double* v_jp1 = v_rows[j + 1];\n                    v_rows[j][i] = p_rows[i][j] * v_jp1[i] + q_rows[i][j];\n                }\n            }\n\n            /* ---------- Backward sweep: solve for u ---------- */\n#pragma omp for schedule(static)\n            for (int i = 1; i < N - 1; ++i) {\n                u_rows[i][0] = 1.0;\n                p_rows[i][0] = 0.0;\n                q_rows[i][0] = u_rows[i][0];   // q[i][0] = u[i][0]\n\n                for (int j = 1; j < N - 1; ++j) {\n                    double* v_im1 = v_rows[i - 1];\n                    double* v_i   = v_rows[i];\n                    double* v_ip1 = v_rows[i + 1];\n                    double denom = d * p_rows[i][j - 1] + e;\n                    p_rows[i][j] = -f / denom;\n                    double numer = -a * v_im1[j] +\n                                   (1.0 + 2.0 * a) * v_i[j] -\n                                   c * v_ip1[j] -\n                                   d * q_rows[i][j - 1];\n                    q_rows[i][j] = numer / denom;\n                }\n\n                u_rows[i][N - 1] = 1.0;\n\n                for (int j = N - 2; j >= 1; --j) {\n                    u_rows[i][j] = p_rows[i][j] * u_rows[i][j + 1] + q_rows[i][j];\n                }\n            }\n        }   // end t loop\n    }   // end parallel region\n}\n```",
  "cholesky": "```cpp\n#include <cmath>\n#include <vector>\n\nvoid kernel_cholesky(int n, std::vector<std::vector<double>>& A) {\n    for (int i = 0; i < n; ++i) {\n        // Pointer to the i\u2011th row (restrict hints no aliasing)\n        double * __restrict__ ai = A[i].data();\n\n        /* Off\u2011diagonal entries */\n        for (int j = 0; j < i; ++j) {\n            double * __restrict__ aj = A[j].data();   // j\u2011th row\n            double val   = ai[j];                     // current A[i][j]\n            double ajj   = aj[j];                     // A[j][j]\n            int k = 0;\n\n            /* Unroll inner loop in steps of 4 for better instruction\n               density while preserving exact arithmetic order. */\n            for (; k <= j - 4; k += 4) {\n                val -= ai[k]   * aj[k];\n                val -= ai[k+1] * aj[k+1];\n                val -= ai[k+2] * aj[k+2];\n                val -= ai[k+3] * aj[k+3];\n            }\n            for (; k < j; ++k) {\n                val -= ai[k] * aj[k];\n            }\n\n            val /= ajj;\n            ai[j] = val;\n        }\n\n        /* Diagonal entry */\n        double di = ai[i];\n        int k = 0;\n        for (; k <= i - 4; k += 4) {\n            di -= ai[k]   * ai[k];\n            di -= ai[k+1] * ai[k+1];\n            di -= ai[k+2] * ai[k+2];\n            di -= ai[k+3] * ai[k+3];\n        }\n        for (; k < i; ++k) {\n            di -= ai[k] * ai[k];\n        }\n        ai[i] = std::sqrt(di);\n    }\n}\n```",
  "gemver": "```cpp\n#include <vector>\n#include <omp.h>\n\nvoid kernel_gemver(int n,\n                   double alpha,\n                   double beta,\n                   std::vector<std::vector<double>>& A,\n                   std::vector<double>& u1,\n                   std::vector<double>& v1,\n                   std::vector<double>& u2,\n                   std::vector<double>& v2,\n                   std::vector<double>& w,\n                   std::vector<double>& x,\n                   std::vector<double>& y,\n                   std::vector<double>& z)\n{\n    /* ------------------------------------------------------------\n       Use raw pointers for fastest access.  The vector of\n       pointers to each row is built once, then used in all loops.\n       ------------------------------------------------------------ */\n    std::vector<double*> Adata(n);\n    for (int i = 0; i < n; ++i) {\n        Adata[i] = A[i].data();\n    }\n\n    double *u1p = u1.data();\n    double *v1p = v1.data();\n    double *u2p = u2.data();\n    double *v2p = v2.data();\n\n    double *wp = w.data();\n    double *xp = x.data();\n    double *yp = y.data();\n    double *zp = z.data();\n\n    /* ------------------------------------------------------------\n       1.  A += u1*v1\u1d40 + u2*v2\u1d40\n       ------------------------------------------------------------ */\n    #pragma omp parallel for schedule(static)\n    for (int i = 0; i < n; ++i) {\n        double *Ap = Adata[i];\n        double u1i = u1p[i];\n        double u2i = u2p[i];\n        for (int j = 0; j < n; ++j) {\n            Ap[j] += u1i * v1p[j] + u2i * v2p[j];\n        }\n    }\n\n    /* ------------------------------------------------------------\n       2.  x += beta * A\u1d40 * y\n       ------------------------------------------------------------ */\n    #pragma omp parallel for schedule(static)\n    for (int i = 0; i < n; ++i) {\n        double sum = 0.0;\n        for (int j = 0; j < n; ++j) {\n            sum += beta * Adata[j][i] * yp[j];\n        }\n        xp[i] += sum;\n    }\n\n    /* ------------------------------------------------------------\n       3.  x += z\n       ------------------------------------------------------------ */\n    #pragma omp parallel for schedule(static)\n    for (int i = 0; i < n; ++i) {\n        xp[i] += zp[i];\n    }\n\n    /* ------------------------------------------------------------\n       4.  w += alpha * A * x\n       ------------------------------------------------------------ */\n    #pragma omp parallel for schedule(static)\n    for (int i = 0; i < n; ++i) {\n        double sum = 0.0;\n        double *Ap = Adata[i];\n        for (int j = 0; j < n; ++j) {\n            sum += Ap[j] * xp[j];\n        }\n        wp[i] += alpha * sum;\n    }\n}\n```\n",
  "bicg": "```cpp\n#include <vector>\n#include <omp.h>\n\nvoid kernel_bicg(int m, int n,\n                 std::vector<std::vector<double>>& A,\n                 std::vector<double>& s,\n                 std::vector<double>& q,\n                 std::vector<double>& p,\n                 std::vector<double>& r)\n{\n    // Handle the trivial case where there are no columns.\n    if (m == 0) {\n        for (int i = 0; i < n; ++i) q[i] = 0.0;\n        return;\n    }\n\n    // Compute the sum of all elements of p once.\n    double sum_p = 0.0;\n#pragma omp parallel for reduction(+:sum_p)\n    for (int j = 0; j < m; ++j) {\n        sum_p += p[j];\n    }\n\n    // Allocate per\u2011thread buffers for partial results of s.\n    int num_threads = omp_get_max_threads();\n    std::vector<double> local_s(num_threads * m, 0.0);\n\n    // Main computation: compute s = A\u1d40 * r and q[i] = \u03a3_j A[i][j] + \u03a3_j p[j].\n#pragma omp parallel\n    {\n        int tid = omp_get_thread_num();\n        double* s_local = &local_s[tid * m];\n\n#pragma omp for schedule(static)\n        for (int i = 0; i < n; ++i) {\n            const double* row = A[i].data();\n            double r_i = r[i];\n            double row_sum = 0.0;\n\n            for (int j = 0; j < m; ++j) {\n                double aij = row[j];\n                s_local[j] += r_i * aij;\n                row_sum += aij;\n            }\n            q[i] = row_sum + sum_p;\n        }\n    }\n\n    // Reduce the per\u2011thread partial sums into the global s vector.\n#pragma omp parallel for\n    for (int j = 0; j < m; ++j) {\n        double total = 0.0;\n        for (int t = 0; t < num_threads; ++t) {\n            total += local_s[t * m + j];\n        }\n        s[j] = total;\n    }\n}\n```",
  "jacobi_1d": "```cpp\n#include <vector>\n#include <omp.h>\n\nvoid kernel_jacobi_1d(int tsteps, int n,\n                      std::vector<double>& A,\n                      std::vector<double>& B)\n{\n    constexpr double coeff = 0.33333;          // same constant as in original\n    if (tsteps <= 0 || n <= 2) return;        // nothing to do\n\n    const double* A_src = A.data();           // read\u2011only copy of A\n    double*       A_dst = A.data();           // write to A\n    double*       B_mem = B.data();           // write/read from B\n    const int last = n - 1;                   // loop upper bound (exclusive)\n\n    #pragma omp parallel\n    {\n        for (int t = 0; t < tsteps; ++t)\n        {\n            /* first phase: compute B from current A */\n            #pragma omp for schedule(static)\n            for (int i = 1; i < last; ++i)\n            {\n                B_mem[i] = coeff * (A_src[i-1] + A_src[i] + A_src[i+1]);\n            }\n\n            /* second phase: update A from updated B */\n            #pragma omp for schedule(static)\n            for (int i = 1; i < last; ++i)\n            {\n                A_dst[i] = coeff * (B_mem[i-1] + B_mem[i] + B_mem[i+1]);\n            }\n        }\n    }\n}\n```\n",
  "durbin": "```cpp\n#include <vector>\n\nvoid kernel_durbin(int n,\n                   std::vector<double>& r,\n                   std::vector<double>& y) {\n    // Direct pointer access for speed\n    const double* const rp = r.data();\n    double* const yp = y.data();\n\n    // Base case initialization\n    yp[0] = -rp[0];\n    double beta  = 1.0;\n    double alpha = -rp[0];\n\n    for (int k = 1; k < n; ++k) {\n        // Update beta using the previous alpha\n        beta *= (1.0 - alpha * alpha);\n\n        // Correlation sum: sum_{i=0}^{k-1} r[k-1-i] * y[i]\n        double sum = 0.0;\n        for (int i = 0; i < k; ++i) {\n            sum += rp[k - 1 - i] * yp[i];\n        }\n\n        // New predictor coefficient\n        alpha = -(rp[k] + sum) / beta;\n\n        // In-place, pairwise update of the reflection coefficients (y)\n        int half = k >> 1;                      // floor(k/2)\n        for (int i = 0; i < half; ++i) {\n            double yi = yp[i];\n            double yj = yp[k - 1 - i];\n            yp[i]        = yi + alpha * yj;\n            yp[k - 1 - i] = yj + alpha * yi;\n        }\n\n        // Handle the middle element when k is odd\n        if (k & 1) {\n            int mid = half;\n            double ym = yp[mid];\n            yp[mid] = ym + alpha * ym;\n        }\n\n        // Store the newest coefficient\n        yp[k] = alpha;\n    }\n}\n```",
  "jacobi_2d": "```cpp\n#include <vector>\n#include <omp.h>\n\nvoid kernel_jacobi_2d(int tsteps,\n                      int n,\n                      std::vector<std::vector<double>>& A,\n                      std::vector<std::vector<double>>& B) {\n    const double w = 0.2;\n\n    /* Create row pointers for fast access */\n    std::vector<double*> A_rows(n);\n    std::vector<double*> B_rows(n);\n    for (int i = 0; i < n; ++i) {\n        A_rows[i] = A[i].data();\n        B_rows[i] = B[i].data();\n    }\n\n    for (int t = 0; t < tsteps; ++t) {\n        /* Compute B from A */\n#pragma omp parallel for schedule(static)\n        for (int i = 1; i < n - 1; ++i) {\n            double *Ai   = A_rows[i];\n            double *Aip1 = A_rows[i + 1];\n            double *Aim1 = A_rows[i - 1];\n            double *Bi   = B_rows[i];\n#pragma ivdep\n#pragma omp simd\n            for (int j = 1; j < n - 1; ++j) {\n                Bi[j] = w * (Ai[j] + Ai[j - 1] + Ai[j + 1] + Aip1[j] + Aim1[j]);\n            }\n        }\n\n        /* Compute A from B */\n#pragma omp parallel for schedule(static)\n        for (int i = 1; i < n - 1; ++i) {\n            double *Bi   = B_rows[i];\n            double *Bip1 = B_rows[i + 1];\n            double *Bim1 = B_rows[i - 1];\n            double *Ai   = A_rows[i];\n#pragma ivdep\n#pragma omp simd\n            for (int j = 1; j < n - 1; ++j) {\n                Ai[j] = w * (Bi[j] + Bi[j - 1] + Bi[j + 1] + Bip1[j] + Bim1[j]);\n            }\n        }\n    }\n}\n```",
  "doitgen": "```cpp\n#include <vector>\n#include <algorithm>\n#include <omp.h>\n\n/* Main computational kernel. The whole function will be timed,\n   including the call and return. */\nvoid kernel_doitgen(int nr, int nq, int np,\n                    std::vector<std::vector<std::vector<double>>>& A,\n                    std::vector<std::vector<double>>& C4,\n                    std::vector<double>& sum) {\n\n    /* Quick exit for empty dimensions \u2013 matches the original no\u2011op behaviour */\n    if (nr <= 0 || nq <= 0 || np <= 0)\n        return;\n\n    /* Parallelise over the (r,q) pairs. Each thread keeps its own\n       temporary buffer to avoid race conditions. */\n    #pragma omp parallel\n    {\n        /* Thread\u2011local buffer for the intermediate result of one (r,q) pair */\n        std::vector<double> buf(np, 0.0);\n\n        /* Pre\u2011compute raw pointers to the rows of C4 for faster inner access */\n        std::vector<const double*> cptr(np);\n        for (int s = 0; s < np; ++s)\n            cptr[s] = C4[s].data();\n\n        /* Collapse the two outer loops so that work is evenly distributed */\n        #pragma omp for collapse(2) schedule(static)\n        for (int r = 0; r < nr; ++r) {\n            for (int q = 0; q < nq; ++q) {\n\n                /* Reset the temporary buffer for this (r,q) */\n                std::fill(buf.begin(), buf.end(), 0.0);\n\n                /* Pointer to the current row of A */\n                const double* a_row = A[r][q].data();\n\n                /* Core kernel:  A[r][q] = A[r][q] * C4  (row\u2011by\u2011matrix product) */\n                for (int s = 0; s < np; ++s) {\n                    double a_val  = a_row[s];\n                    const double* c_row = cptr[s];\n\n                    /* SIMD\u2011friendly inner loop */\n                    #pragma omp simd\n                    for (int p = 0; p < np; ++p) {\n                        buf[p] += a_val * c_row[p];\n                    }\n                }\n\n                /* Store the result back into A */\n                std::copy(buf.begin(), buf.end(), A[r][q].data());\n\n                /* The final value of 'sum' must match the last processed (r,q) pair.\n                   Write it only for the last iteration. */\n                if (r == nr - 1 && q == nq - 1) {\n                    std::copy(buf.begin(), buf.end(), sum.begin());\n                }\n            }\n        }\n    }   /* end parallel region */\n}\n```",
  "fdtd_2d": "```cpp\n#include <vector>\n#include <omp.h>\n\nvoid kernel_fdtd_2d(int tmax,\n                    int nx,\n                    int ny,\n                    std::vector<std::vector<double>>& ex,\n                    std::vector<std::vector<double>>& ey,\n                    std::vector<std::vector<double>>& hz,\n                    std::vector<double>& _fict_) {\n    constexpr double coef1 = 0.5;\n    constexpr double coef2 = 0.7;\n\n    #pragma omp parallel\n    {\n        for (int t = 0; t < tmax; ++t) {\n            const double fict = _fict_[t];\n\n            /* Update first row of ey */\n            #pragma omp for schedule(static)\n            for (int j = 0; j < ny; ++j)\n                ey[0][j] = fict;\n\n            /* Update ey using hz */\n            #pragma omp for schedule(static)\n            for (int i = 1; i < nx; ++i) {\n                double* ey_i = ey[i].data();\n                const double* hz_i   = hz[i].data();\n                const double* hz_im1 = hz[i - 1].data();\n\n                #pragma omp simd\n                for (int j = 0; j < ny; ++j)\n                    ey_i[j] -= coef1 * (hz_i[j] - hz_im1[j]);\n            }\n\n            /* Update ex using hz */\n            #pragma omp for schedule(static)\n            for (int i = 0; i < nx; ++i) {\n                double* ex_i = ex[i].data();\n                const double* hz_i = hz[i].data();\n\n                #pragma omp simd\n                for (int j = 1; j < ny; ++j)\n                    ex_i[j] -= coef1 * (hz_i[j] - hz_i[j - 1]);\n            }\n\n            /* Update hz using ex and ey */\n            #pragma omp for schedule(static)\n            for (int i = 0; i < nx - 1; ++i) {\n                double* hz_i   = hz[i].data();\n                const double* ex_i   = ex[i].data();\n                const double* ey_i   = ey[i].data();\n                const double* ey_ip1 = ey[i + 1].data();\n\n                #pragma omp simd\n                for (int j = 0; j < ny - 1; ++j)\n                    hz_i[j] -= coef2 *\n                               ((ex_i[j + 1] - ex_i[j]) + (ey_ip1[j] - ey_i[j]));\n            }\n        }\n    }\n}\n```",
  "syrk": "```cpp\n#include <vector>\n#include <omp.h>\n\nvoid kernel_syrk(int n, int m,\n                 double alpha,\n                 double beta,\n                 std::vector<std::vector<double>>& C,\n                 std::vector<std::vector<double>>& A) {\n    // Fast path when no rank\u2011k term is present.\n    if (m == 0) {\n#pragma omp parallel for schedule(static)\n        for (int i = 0; i < n; ++i) {\n            double* Ci = C[i].data();\n            for (int j = 0; j <= i; ++j) {\n                Ci[j] *= beta;\n            }\n        }\n        return;\n    }\n\n    /* Transpose A to have contiguous access to columns.\n       AT[k][j] == A[j][k]                                      */\n    std::vector<std::vector<double>> AT(m, std::vector<double>(n));\n#pragma omp parallel for schedule(static)\n    for (int j = 0; j < n; ++j) {\n        const double* Aj = A[j].data();\n        for (int k = 0; k < m; ++k) {\n            AT[k][j] = Aj[k];\n        }\n    }\n\n    /* Main kernel: lower\u2011triangular SYRK with cache\u2011friendly layout. */\n#pragma omp parallel\n    {\n        std::vector<double> tmp(n);   // thread\u2011private temporary buffer\n\n#pragma omp for schedule(static)\n        for (int i = 0; i < n; ++i) {\n            double* Ci = C[i].data();\n\n            /* Copy beta\u2011scaled original values into tmp. */\n            for (int j = 0; j <= i; ++j) {\n                tmp[j] = beta * Ci[j];\n            }\n\n            /* Accumulate contributions from each k. */\n            for (int k = 0; k < m; ++k) {\n                const double* At_row = AT[k].data(); // pointer to row k of AT\n                double aik   = AT[k][i];             // equals A[i][k]\n                double mult  = alpha * aik;          // pre\u2011compute\n\n#pragma omp simd\n                for (int j = 0; j <= i; ++j) {\n                    tmp[j] += mult * At_row[j];\n                }\n            }\n\n            /* Write the accumulated result back to C. */\n            for (int j = 0; j <= i; ++j) {\n                Ci[j] = tmp[j];\n            }\n        }\n    }\n}\n```",
  "ludcmp": "```cpp\n#include <vector>\n#include <omp.h>\n\n/* Main computational kernel. The whole function will be timed,\n   including the call and return. */\nvoid kernel_ludcmp(int n,\n                   std::vector<std::vector<double>>& A,\n                   std::vector<double>& b,\n                   std::vector<double>& x,\n                   std::vector<double>& y)\n{\n    /* Build raw row pointers for faster access */\n    std::vector<double*> rows(n);\n    for (int i = 0; i < n; ++i)\n        rows[i] = A[i].data();\n\n    /* Temporary storage for diagonal elements (U[i][i]) */\n    std::vector<double> diag(n);\n\n    /* --- LU decomposition ------------------------------------------------- */\n    #pragma omp parallel\n    {\n        for (int i = 0; i < n; ++i)\n        {\n            double* row_i = rows[i];\n\n            /* Compute the lower part L[i][j] for j < i   (single thread) */\n            #pragma omp single\n            {\n                for (int j = 0; j < i; ++j)\n                {\n                    double w = row_i[j];\n                    for (int k = 0; k < j; ++k)\n                        w -= row_i[k] * rows[k][j];\n                    row_i[j] = w / diag[j];\n                }\n            }\n\n            /* Compute the upper part U[i][j] for j \u2265 i   (all threads) */\n            #pragma omp for schedule(static)\n            for (int j = i; j < n; ++j)\n            {\n                double w = row_i[j];\n                for (int k = 0; k < i; ++k)\n                    w -= row_i[k] * rows[k][j];\n                row_i[j] = w;\n            }\n\n            /* Store the updated diagonal value for later use */\n            #pragma omp single\n            {\n                diag[i] = row_i[i];\n            }\n        }\n    }\n    /* --------------------------------------------------------------------- */\n\n    /* --- Forward substitution: y = L\u207b\u00b9 b -------------------------------- */\n    for (int i = 0; i < n; ++i)\n    {\n        double w = b[i];\n        double* row_i = rows[i];\n        for (int j = 0; j < i; ++j)\n            w -= row_i[j] * y[j];\n        y[i] = w;\n    }\n    /* --------------------------------------------------------------------- */\n\n    /* --- Backward substitution: x = U\u207b\u00b9 y -------------------------------- */\n    for (int i = n - 1; i >= 0; --i)\n    {\n        double w = y[i];\n        double* row_i = rows[i];\n        for (int j = i + 1; j < n; ++j)\n            w -= row_i[j] * x[j];\n        x[i] = w / row_i[i];\n    }\n    /* --------------------------------------------------------------------- */\n}\n```",
  "deriche": "```cpp\n#include <cmath>\n#include <vector>\n#include <omp.h>\n\nvoid kernel_deriche(int w, int h, double alpha,\n            std::vector<std::vector<float>>& imgIn,\n            std::vector<std::vector<float>>& imgOut,\n            std::vector<std::vector<float>>& y1,\n            std::vector<std::vector<float>>& y2) {\n\n    /* Pre\u2011compute filter coefficients in double precision, then cast to float */\n    const double eAlpha_d   = std::exp(-alpha);\n    const double e2Alpha_d  = eAlpha_d * eAlpha_d;          // exp(-2*alpha)\n    const double pow2alpha_d = std::pow(2.0, -alpha);       // 2^(-alpha)\n\n    const double k_d = 1.0 - eAlpha_d * (1.0 - eAlpha_d) /\n                       (1.0 + 2.0 * alpha * eAlpha_d - e2Alpha_d);\n\n    const float k  = static_cast<float>(k_d);                                 // a1 = a5\n    const float a1 = k, a5 = k;\n    const float a2 = static_cast<float>(k_d * eAlpha_d * (alpha - 1.0));\n    const float a6 = a2;\n    const float a3 = static_cast<float>(k_d * eAlpha_d * (alpha + 1.0));\n    const float a7 = a3;\n    const float a4 = static_cast<float>(-k_d * e2Alpha_d);\n    const float a8 = a4;\n    const float b1 = static_cast<float>(pow2alpha_d);\n    const float b2 = static_cast<float>(-e2Alpha_d);\n    /* c1 and c2 are both 1.0f, used only in the final summation */\n\n    /* --------------------------------------------------------------------- */\n    /* Pass 1 \u2013 forward horizontal filtering (into y1)                         */\n    /* --------------------------------------------------------------------- */\n    #pragma omp parallel for schedule(static)\n    for (int i = 0; i < w; ++i) {\n        const float* src = imgIn[i].data();\n        float* dst = y1[i].data();\n\n        float xm1 = 0.0f, ym1 = 0.0f, ym2 = 0.0f;\n        for (int j = 0; j < h; ++j) {\n            float val = src[j];\n            float y = a1 * val + a2 * xm1 + b1 * ym1 + b2 * ym2;\n            dst[j] = y;\n\n            xm1 = val;\n            ym2 = ym1;\n            ym1 = y;\n        }\n    }\n\n    /* --------------------------------------------------------------------- */\n    /* Pass 2 \u2013 backward horizontal filtering (into y2)                       */\n    /* --------------------------------------------------------------------- */\n    #pragma omp parallel for schedule(static)\n    for (int i = 0; i < w; ++i) {\n        const float* src = imgIn[i].data();\n        float* dst = y2[i].data();\n\n        float xp1 = 0.0f, xp2 = 0.0f, yp1 = 0.0f, yp2 = 0.0f;\n        for (int j = h - 1; j >= 0; --j) {\n            float val = src[j];\n            float y = a3 * xp1 + a4 * xp2 + b1 * yp1 + b2 * yp2;\n            dst[j] = y;\n\n            xp2 = xp1;\n            xp1 = val;\n            yp2 = yp1;\n            yp1 = y;\n        }\n    }\n\n    /* --------------------------------------------------------------------- */\n    /* Pass 3 \u2013 combine horizontal results into imgOut                        */\n    /* --------------------------------------------------------------------- */\n    #pragma omp parallel for schedule(static)\n    for (int i = 0; i < w; ++i) {\n        const float* src1 = y1[i].data();\n        const float* src2 = y2[i].data();\n        float* dst = imgOut[i].data();\n\n        for (int j = 0; j < h; ++j) {\n            dst[j] = src1[j] + src2[j];   // c1 = 1.0f\n        }\n    }\n\n    /* --------------------------------------------------------------------- */\n    /* Pass 4 \u2013 forward vertical filtering (into y1) using imgOut            */\n    /* --------------------------------------------------------------------- */\n    #pragma omp parallel for schedule(static)\n    for (int j = 0; j < h; ++j) {\n        float tm1 = 0.0f, ym1 = 0.0f, ym2 = 0.0f;\n        for (int i = 0; i < w; ++i) {\n            const float* src = imgOut[i].data();\n            float val = src[j];\n            float y = a5 * val + a6 * tm1 + b1 * ym1 + b2 * ym2;\n            y1[i][j] = y;\n\n            tm1 = val;\n            ym2 = ym1;\n            ym1 = y;\n        }\n    }\n\n    /* --------------------------------------------------------------------- */\n    /* Pass 5 \u2013 backward vertical filtering (into y2) using imgOut            */\n    /* --------------------------------------------------------------------- */\n    #pragma omp parallel for schedule(static)\n    for (int j = 0; j < h; ++j) {\n        float tp1 = 0.0f, tp2 = 0.0f, yp1 = 0.0f, yp2 = 0.0f;\n        for (int i = w - 1; i >= 0; --i) {\n            const float* src = imgOut[i].data();\n            float val = src[j];\n            float y = a7 * tp1 + a8 * tp2 + b1 * yp1 + b2 * yp2;\n            y2[i][j] = y;\n\n            tp2 = tp1;\n            tp1 = val;\n            yp2 = yp1;\n            yp1 = y;\n        }\n    }\n\n    /* --------------------------------------------------------------------- */\n    /* Pass 6 \u2013 combine vertical results back into imgOut                    */\n    /* --------------------------------------------------------------------- */\n    #pragma omp parallel for schedule(static)\n    for (int i = 0; i < w; ++i) {\n        const float* src1 = y1[i].data();\n        const float* src2 = y2[i].data();\n        float* dst = imgOut[i].data();\n\n        for (int j = 0; j < h; ++j) {\n            dst[j] = src1[j] + src2[j];   // c2 = 1.0f\n        }\n    }\n}\n```",
  "seidel_2d": "```cpp\n#include <vector>\n\nvoid __attribute__((optimize(\"O3\"))) kernel_seidel_2d(int tsteps,\n                                                    int n,\n                                                    std::vector<std::vector<double>>& A) {\n    // No interior cells \u2013 nothing to do\n    if (n <= 2 || tsteps <= 0) return;\n\n    const double inv9        = 1.0 / 9.0;\n    const int    interior_end = n - 1;          // upper exclusive bound\n\n    for (int t = 0; t < tsteps; ++t) {\n        for (int i = 1; i < interior_end; ++i) {\n            double * __restrict__ up   = A[i - 1].data();\n            double * __restrict__ cur  = A[i].data();\n            double * __restrict__ down = A[i + 1].data();\n\n            double left  = cur[0];\n\n            /* Initialise the three\u2011column window for the neighbours */\n            double upPrev   = up[0];\n            double upCurr   = up[1];\n            double upNext   = up[2];\n            double downPrev = down[0];\n            double downCurr = down[1];\n            double downNext = down[2];\n\n            for (int j = 1; j < interior_end; ++j) {\n                double sum = upPrev + upCurr + upNext + left +\n                             cur[j] + cur[j + 1] +\n                             downPrev + downCurr + downNext;\n                double new_val = sum * inv9;\n\n                left = cur[j] = new_val;\n\n                if (j + 1 < interior_end) {\n                    /* Advance the neighbour window by one column */\n                    upPrev   = upCurr;\n                    upCurr   = upNext;\n                    upNext   = up[j + 2];\n                    downPrev = downCurr;\n                    downCurr = downNext;\n                    downNext = down[j + 2];\n                }\n            }\n        }\n    }\n}\n```",
  "gramschmidt": "```cpp\n#include <cmath>\n#include <vector>\n#include <omp.h>\n\nvoid kernel_gramschmidt(int m, int n,\n                        std::vector<std::vector<double>>& A,\n                        std::vector<std::vector<double>>& R,\n                        std::vector<std::vector<double>>& Q)\n{\n    // Temporary buffer to hold the current Q column\n    std::vector<double> qcol(m);\n\n    for (int k = 0; k < n; ++k) {\n        // ----- 1. Compute norm of column k ----------\n        double nrm = 0.0;\n        for (int i = 0; i < m; ++i) {\n            double a = A[i][k];\n            nrm += a * a;\n        }\n        double rkk = std::sqrt(nrm);\n        R[k][k] = rkk;\n\n        // ----- 2. Build Q(:,k) and cache it -------\n        for (int i = 0; i < m; ++i) {\n            double q = A[i][k] / rkk;\n            Q[i][k] = q;\n            qcol[i] = q;\n        }\n\n        // If no remaining columns, skip\n        const int offset = k + 1;\n        const int len    = n - offset;\n        if (len <= 0) continue;\n\n        // ----- 3. Process columns j > k in parallel ----\n        #pragma omp parallel for schedule(static)\n        for (int j = offset; j < n; ++j) {\n            // Compute R[k][j] as a dot product\n            double sum = 0.0;\n            for (int i = 0; i < m; ++i) {\n                const double* arow = A[i].data() + j;  // A[i][j]\n                sum += qcol[i] * arow[0];\n            }\n            R[k][j] = sum;\n\n            // Update the remaining column of A\n            for (int i = 0; i < m; ++i) {\n                double* arow = A[i].data() + j;\n                arow[0] -= qcol[i] * sum;\n            }\n        }\n    }\n}\n```",
  "lu": "```cpp\n#include <vector>\n#include <omp.h>\n\nvoid kernel_lu(int n, std::vector<std::vector<double>>& A) {\n    #pragma omp parallel\n    {\n        for (int i = 0; i < n; ++i) {\n            double* row_i = A[i].data();\n\n            // Compute lower\u2010triangular part (L[i][j]) sequentially\n            #pragma omp single\n            {\n                for (int j = 0; j < i; ++j) {\n                    double val = row_i[j];\n                    for (int k = 0; k < j; ++k) {\n                        val -= row_i[k] * A[k][j];\n                    }\n                    val /= A[j][j];\n                    row_i[j] = val;\n                }\n            }\n\n            // Compute upper\u2010triangular part (U[i][j]) in parallel\n            #pragma omp for schedule(static)\n            for (int j = i; j < n; ++j) {\n                double val = row_i[j];\n                for (int k = 0; k < i; ++k) {\n                    val -= row_i[k] * A[k][j];\n                }\n                row_i[j] = val;\n            }\n        }\n    }\n}\n```",
  "floyd_warshall": "```cpp\n#include <vector>\n#include <omp.h>\n\n// Perform the inner work for a single row.\nstatic inline void process_row(int n, int k, int* row, int* row_k) {\n    int aik = row[k];\n\n    // Update columns before k.\n    #pragma omp simd\n    for (int j = 0; j < k; ++j) {\n        int cand = aik + row_k[j];\n        if (cand < row[j]) row[j] = cand;\n    }\n\n    // Update column k.\n    int cand = aik + row_k[k];\n    if (cand < row[k]) row[k] = cand;\n\n    // Update columns after k using the potentially updated value.\n    int aik2 = row[k];\n    #pragma omp simd\n    for (int j = k + 1; j < n; ++j) {\n        int cand2 = aik2 + row_k[j];\n        if (cand2 < row[j]) row[j] = cand2;\n    }\n}\n\nvoid kernel_floyd_warshall(int n, std::vector<std::vector<int>>& path) {\n    // Pointers to the beginning of each row for fast indexing.\n    std::vector<int*> rows(n);\n    for (int i = 0; i < n; ++i) rows[i] = path[i].data();\n\n    #pragma omp parallel\n    {\n        for (int k = 0; k < n; ++k) {\n            int* row_k = rows[k];\n\n            // i < k\n            #pragma omp for schedule(static)\n            for (int i = 0; i < k; ++i) {\n                process_row(n, k, rows[i], row_k);\n            }\n\n            // i == k\n            #pragma omp single\n            {\n                process_row(n, k, row_k, row_k);\n            }\n\n            // i > k\n            #pragma omp for schedule(static)\n            for (int i = k + 1; i < n; ++i) {\n                process_row(n, k, rows[i], row_k);\n            }\n        }\n    }\n}\n```\n",
  "trmm": "```cpp\n#include <vector>\n#include <omp.h>\n\n/* Main computational kernel.\n   The whole function will be timed, including the call and return. */\nvoid kernel_trmm(int m, int n,\n                 double alpha,\n                 std::vector<std::vector<double>>& A,\n                 std::vector<std::vector<double>>& B)\n{\n    if (m <= 0 || n <= 0) return;\n\n    // Each thread keeps its own temporary buffers.\n    #pragma omp parallel\n    {\n        // Temporary storage for one column of A and the corresponding B row pointers\n        std::vector<double> a_col(m);          // will hold A[k][i] for k>i\n        std::vector<const double*> b_rows(m);  // will hold B[k].data() for k>i\n\n        for (int i = 0; i < m; ++i) {\n            double* bi = B[i].data();           // row of B being updated\n            const int ksize = m - i - 1;        // number of rows below i\n\n            // Load column i of A (for k>i) and pointers to the rows of B that are used\n            for (int k = i + 1; k < m; ++k) {\n                const int idx = k - i - 1;\n                a_col[idx] = A[k][i];\n                b_rows[idx] = B[k].data();\n            }\n\n            // Parallelise across the columns of B[i]\n            #pragma omp for schedule(static)\n            for (int j = 0; j < n; ++j) {\n                double sum = bi[j];              // start with the original B[i][j]\n                for (int idx = 0; idx < ksize; ++idx) {\n                    sum += a_col[idx] * b_rows[idx][j];\n                }\n                bi[j] = alpha * sum;             // final scaling by alpha\n            }\n        }\n    }\n}\n```"
}