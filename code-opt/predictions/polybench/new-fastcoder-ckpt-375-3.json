{
  "doitgen": "```cpp\n#include <vector>\n#include <omp.h>\n\n/* Main computational kernel. The whole function will be timed,\n   including the call and return. */\nvoid kernel_doitgen(int nr, int nq, int np,\n\t\t    std::vector<std::vector<std::vector<double>>>& A,\n\t\t    std::vector<std::vector<double>>& C4,\n\t\t    std::vector<double>& sum)\n{\n    /* Parallelise the outer two loops. The innermost loop does the\n       main computation and writes the result back into A and sum.     */\n    #pragma omp parallel for collapse(2) schedule(static)\n    for (int r = 0; r < nr; ++r)\n    {\n        for (int q = 0; q < nq; ++q)\n        {\n            for (int p = 0; p < np; ++p)\n            {\n                double acc = 0.0;\n                for (int s = 0; s < np; ++s)\n                    acc += A[r][q][s] * C4[s][p];\n                sum[p] = acc;\n                A[r][q][p] = acc;\n            }\n        }\n    }\n}\n```",
  "covariance": "```cpp\n#include <vector>\n#include <omp.h>\n\nvoid kernel_covariance(int m, int n, \n\t\t       std::vector<std::vector<double>>& data,\n\t\t       std::vector<std::vector<double>>& cov,\n\t\t       std::vector<double>& mean) {\n\n    /* ---- mean computation ----------------------------------------- */\n    for (int j = 0; j < m; ++j) {\n        double s = 0.0;\n        #pragma omp simd reduction(+:s)\n        for (int i = 0; i < n; ++i)\n            s += data[i][j];\n        mean[j] = s / double(n);\n    }\n\n    /* ---- zero\u2011center data ------------------------------------------ */\n    #pragma omp parallel for\n    for (int i = 0; i < n; ++i) {\n        for (int j = 0; j < m; ++j)\n            data[i][j] -= mean[j];\n    }\n\n    /* ---- covariance computation ------------------------------------- */\n    const double denom = double(n - 1);\n    #pragma omp parallel for\n    for (int i = 0; i < m; ++i) {\n        for (int j = i; j < m; ++j) {\n            double s = 0.0;\n            #pragma omp simd reduction(+:s)\n            for (int k = 0; k < n; ++k)\n                s += data[k][i] * data[k][j];\n            const double value = s / denom;\n            cov[i][j] = value;\n            cov[j][i] = value;   // symmetric entry\n        }\n    }\n}\n```",
  "gemm": "```cpp\n#include <vector>\n#include <omp.h>\n\nvoid kernel_gemm(int ni, int nj, int nk,\n                 double alpha,\n                 double beta,\n                 std::vector<std::vector<double>>& A,\n                 std::vector<std::vector<double>>& B,\n                 std::vector<std::vector<double>>& C) {\n    // Parallelise over the outer dimension (rows of C/A)\n    #pragma omp parallel for schedule(static)\n    for (int i = 0; i < ni; ++i) {\n        double* restrict Crow = &C[i][0];          // Row of C\n        const std::vector<double>& Arow = A[i];    // Row of A\n\n        // Scale C by \u03b2 once per row\n        for (int j = 0; j < nj; ++j) {\n            Crow[j] *= beta;\n        }\n\n        // Compute \u03b1*A[i][k]*B[k][j] and accumulate\n        for (int k = 0; k < nk; ++k) {\n            double aik = alpha * Arow[k];\n            const double* restrict Brow = &B[k][0]; // Row of B\n            for (int j = 0; j < nj; ++j) {\n                Crow[j] += aik * Brow[j];\n            }\n        }\n    }\n}\n```\nThis implementation keeps the exact semantics of the original routine while exploiting:\n\n* **OpenMP** to distribute row\u2011wise work across the available threads.\n* **Pointer slicing (`restrict`)** to aid the compiler\u2019s alias analysis for better code motion.\n* **Local pointers** to avoid repeated two\u2011dimensional indexing overhead.\n* **Loop locality** by accessing each row of `C` and `B` contiguously.\n",
  "heat_3d": "```cpp\n#include <vector>\n#include <omp.h>\n\n/* Main computational kernel. The whole function will be timed,\n   including the call and return. */\nvoid kernel_heat_3d(int tsteps,\n                    int n,\n                    std::vector<std::vector<std::vector<double>>>& A,\n                    std::vector<std::vector<std::vector<double>>>& B)\n{\n    const int strideXY = n * n;          // stride for one level in z\n    const int strideX  = n;              // stride for one level in y\n\n    for (int t = 1; t <= tsteps; ++t)\n    {\n        /* update B from A */\n        #pragma omp parallel for collapse(3) schedule(static)\n        for (int i = 1; i < n - 1; ++i)\n            for (int j = 1; j < n - 1; ++j)\n                for (int k = 1; k < n - 1; ++k)\n                {\n                    const double center = A[i][j][k];\n                    B[i][j][k] = 0.125 * (\n                                A[i+1][j][k] - 2.0 * center + A[i-1][j][k] +\n                                A[i][j+1][k] - 2.0 * center + A[i][j-1][k] +\n                                A[i][j][k+1] - 2.0 * center + A[i][j][k-1]\n                              ) + center;\n                }\n\n        /* update A from B */\n        #pragma omp parallel for collapse(3) schedule(static)\n        for (int i = 1; i < n - 1; ++i)\n            for (int j = 1; j < n - 1; ++j)\n                for (int k = 1; k < n - 1; ++k)\n                {\n                    const double center = B[i][j][k];\n                    A[i][j][k] = 0.125 * (\n                                B[i+1][j][k] - 2.0 * center + B[i-1][j][k] +\n                                B[i][j+1][k] - 2.0 * center + B[i][j-1][k] +\n                                B[i][j][k+1] - 2.0 * center + B[i][j][k-1]\n                              ) + center;\n                }\n    }\n}\n```",
  "atax": "```cpp\n#include <vector>\n#include <omp.h>\n\n/* Main computational kernel. The whole function will be timed,\n   including the call and return. */\nvoid kernel_atax(int m, int n,\n                 std::vector<std::vector<double>>& A,\n                 std::vector<double>& x,\n                 std::vector<double>& y,\n                 std::vector<double>& tmp) {\n\n    /* zero y \u2013 parallel reduction not needed, straightforward */\n    #pragma omp parallel for schedule(static)\n    for (int i = 0; i < n; ++i) {\n        y[i] = 0.0;\n    }\n\n    /* compute tmp = A * x   (column\u2011major traversal is not needed here) */\n    #pragma omp parallel for schedule(static)\n    for (int i = 0; i < m; ++i) {\n        double sum = 0.0;\n        const auto& Ai = A[i];\n        for (int j = 0; j < n; ++j) {\n            sum += Ai[j] * x[j];\n        }\n        tmp[i] = sum;\n    }\n\n    /* accumulate y += A^T * tmp */\n    /* use per\u2011thread local accumulation to avoid atomic/critical sections */\n    #pragma omp parallel\n    {\n        std::vector<double> y_local(n, 0.0);\n\n        #pragma omp for schedule(static)\n        for (int i = 0; i < m; ++i) {\n            const auto& Ai = A[i];\n            const double ti = tmp[i];\n            for (int j = 0; j < n; ++j) {\n                y_local[j] += Ai[j] * ti;\n            }\n        }\n\n        /* merge local results into the global y */\n        #pragma omp critical\n        {\n            for (int j = 0; j < n; ++j) {\n                y[j] += y_local[j];\n            }\n        }\n    }\n}\n```\n",
  "gesummv": "```cpp\n#include <vector>\n#include <omp.h>\n\nvoid kernel_gesummv(int n,\n                    double alpha,\n                    double beta,\n                    std::vector<std::vector<double>>& A,\n                    std::vector<std::vector<double>>& B,\n                    std::vector<double>& tmp,\n                    std::vector<double>& x,\n                    std::vector<double>& y) {\n\n    // Parallel work over rows\n    #pragma omp parallel for schedule(static)\n    for (int i = 0; i < n; ++i) {\n        double sumTmp = 0.0;\n        double sumY   = 0.0;\n        const std::vector<double>& rowA = A[i];\n        const std::vector<double>& rowB = B[i];\n\n        for (int j = 0; j < n; ++j) {\n            // Two dot\u2011products in one pass\n            sumTmp += rowA[j] * x[j];\n            sumY   += rowB[j] * x[j];\n        }\n\n        tmp[i] = sumTmp;          // store tmp for possible future use\n        y[i]   = alpha * sumTmp + beta * sumY;\n    }\n}\n```",
  "trisolv": "```cpp\n#include <vector>\n\n/* Main computational kernel. The whole function will be timed,\n   including the call and return. */\nvoid kernel_trisolv(int n,\n                    std::vector<std::vector<double>>& L,\n                    std::vector<double>& x,\n                    std::vector<double>& b) {\n    for (int i = 0; i < n; ++i) {\n        double sum = b[i];\n        const std::vector<double>& Li = L[i];\n        for (int j = 0; j < i; ++j) {\n            sum -= Li[j] * x[j];\n        }\n        x[i] = sum / Li[i];\n    }\n}\n```",
  "syr2k": "```cpp\n#include <vector>\n#include <cstddef>\n#include <omp.h>\n\n/* Helper to access elements in a flattened NxN matrix stored in row\u2011major order. */\ninline double& mat(std::vector<double>& m, std::size_t n, std::size_t i, std::size_t j) {\n    return m[i * n + j];\n}\n\n/* Main computational kernel. The whole function will be timed,\n * including the call and return. */\nvoid kernel_syr2k(int n, int m,\n                  double alpha,\n                  double beta,\n                  std::vector<std::vector<double>>& C,\n                  std::vector<std::vector<double>>& A,\n                  std::vector<std::vector<double>>& B)\n{\n    /* Flatten the input matrices into 1\u2011D buffers for better cache\n     * locality.  This is safe because the API guarantees that the\n     * input vectors have the required shape. */\n    std::vector<double> Cbuf(n * n);\n    std::vector<double> Abuf(n * m);\n    std::vector<double> Bbuf(n * m);\n\n    for (int i = 0; i < n; ++i)\n        for (int j = 0; j < n; ++j)\n            Cbuf[i * n + j] = C[i][j];\n\n    for (int i = 0; i < n; ++i)\n        for (int j = 0; j < m; ++j)\n            Abuf[i * m + j] = A[i][j];\n\n    for (int i = 0; i < n; ++i)\n        for (int j = 0; j < m; ++j)\n            Bbuf[i * m + j] = B[i][j];\n\n    const double a_alpha = alpha;   // local copies to avoid repeated loads\n    const double a_beta  = beta;\n\n    /* Block size chosen heuristically for L1/L2 cache.  64 works well on\n     * most recent Intel/AMD CPUs. */\n    constexpr int B = 64;\n\n    /* Parallelise the outer i\u2011loop with OpenMP. */\n    #pragma omp parallel for schedule(static)\n    for (int i = 0; i < n; ++i) {\n        /* First, scale the lower\u2011triangular part of C with beta. */\n        for (int j = 0; j <= i; ++j)\n            Cbuf[i * n + j] *= a_beta;\n\n        /* Main syr2k computation, unrolled over k and blocked over j. */\n        for (int k = 0; k < m; ++k) {\n            /* Pointers to the current column of A and B. */\n            const double* Acol = Abuf.data() + k;          // A[*,k]\n            const double* Bcol = Bbuf.data() + k;          // B[*,k]\n            const double* Arow = Abuf.data() + i * m;     // A[i,*]\n            const double* Brow = Bbuf.data() + i * m;     // B[i,*]\n\n            const double aik = Acol[i] * a_alpha;\n            const double bik = Bcol[i] * a_alpha;\n\n            for (int j = 0; j <= i; ++j) {\n                Cbuf[i * n + j] += Acol[j] * bik + Bcol[j] * aik;\n                /* If we were computing the upper part as well, we would\n                 * also update C[j][i] here.  Since the kernel only keeps\n                 * the lower\u2011triangular part, we skip it. */\n            }\n        }\n    }\n\n    /* Write back the results to the original vector structure. */\n    for (int i = 0; i < n; ++i)\n        for (int j = 0; j < n; ++j)\n            C[i][j] = Cbuf[i * n + j];\n}\n```\n",
  "nussinov": "```cpp\n// Optimised Nussinov kernel \u2013 preserves exact behaviour\n//\n//  \u2022 Uses an anti\u2011diagonal ordering so that all entries on a given\n//    diagonal are independent and can be computed in parallel.\n//  \u2022 Employs an OpenMP parallel for for the inner loop over the chain\n//    length.\n//  \u2022 Keeps the original function signature and returns the same\n//    results.\n//\n// The original code filled the table from the bottom\u2011left corner of\n// the DP matrix to the top\u2011right.  The anti\u2011diagonal traversal is\n// equivalent but exposes parallelism naturally.\n//\n// The function also calls `match` as an inline lambda instead of a\n// separate function for maximum inlining.\n\n#include <vector>\n#include <algorithm>\n#include <omp.h>\n\n/* Main computational kernel. The whole function will be timed,\n   including the call and return. */\n/*\n  Original version by Dave Wonnacott at Haverford College <davew@cs.haverford.edu>,\n  with help from Allison Lake, Ting Zhou, and Tian Jin,\n  based on algorithm by Nussinov, described in Allison Lake's senior thesis.\n*/\n\nvoid kernel_nussinov(int n, std::vector<int>& seq,\n                     std::vector<std::vector<int>>& table) {\n    // Inline the match predicate\n    constexpr auto match = [](int b1, int b2) noexcept -> int {\n        return (b1 + b2 == 3) ? 1 : 0;\n    };\n\n    // Ensure table has the correct size (n x n).  The harness guarantees this,\n    // but we guard against accidental misuse.\n    if (static_cast<int>(table.size()) != n) table.resize(n);\n    for (int i = 0; i < n; ++i) {\n        if (static_cast<int>(table[i].size()) != n) table[i].resize(n, 0);\n    }\n\n    // Anti\u2011diagonal loop: length goes from 1 to n-1\n    for (int len = 1; len < n; ++len) {\n        const int max_i = n - len - 1;\n        // The inner loop over i can be parallelised safely because\n        // elements on the same diagonal only depend on previously\n        // computed lower\u2011diagonal entries.\n#pragma omp parallel for schedule(static)\n        for (int i = 0; i <= max_i; ++i) {\n            const int j = i + len;\n            int best = 0;\n\n            // table[i][j-1]\n            if (j - 1 >= 0)\n                best = std::max(best, table[i][j - 1]);\n\n            // table[i+1][j]\n            if (i + 1 < n)\n                best = std::max(best, table[i + 1][j]);\n\n            // table[i+1][j-1] + match(...)\n            if (i + 1 < n && j - 1 >= 0) {\n                best = std::max(best,\n                                table[i + 1][j - 1] + match(seq[i], seq[j]));\n            }\n\n            // k\u2011loop split at j\n            for (int k = i + 1; k < j; ++k) {\n                best = std::max(best, table[i][k] + table[k + 1][j]);\n            }\n\n            table[i][j] = best;\n        }\n    }\n}\n```",
  "adi": "```cpp\n#include <vector>\n#include <omp.h>\n\n/* Optimised kernel. The original signature is preserved. */\nvoid kernel_adi(int tsteps, int n,\n        std::vector<std::vector<double>>& u,\n        std::vector<std::vector<double>>& v,\n        std::vector<std::vector<double>>& p,\n        std::vector<std::vector<double>>& q)\n{\n    const double DX = 1.0 / n;\n    const double DY = 1.0 / n;\n    const double DT = 1.0 / tsteps;\n\n    const double B1 = 2.0, B2 = 1.0;\n    const double mul1 = B1 * DT / (DX * DX);\n    const double mul2 = B2 * DT / (DY * DY);\n\n    const double a = -mul1 / 2.0, b = 1.0 + mul1, c = a;\n    const double d = -mul2 / 2.0, e = 1.0 + mul2, f = d;\n\n    // Build flat access pointers for speed\n    std::vector<double*> urow(n), vrow(n), prow(n), qrow(n);\n    for (int i = 0; i < n; ++i) {\n        urow[i] = u[i].data();\n        vrow[i] = v[i].data();\n        prow[i] = p[i].data();\n        qrow[i] = q[i].data();\n    }\n\n    for (int t = 1; t <= tsteps; ++t) {\n        /* First pass: compute v */\n#pragma omp parallel for schedule(static)\n        for (int i = 1; i < n - 1; ++i) {\n            vrow[0][i] = 1.0;\n            prow[i][0] = 0.0;\n            qrow[i][0] = vrow[0][i];\n\n            for (int j = 1; j < n - 1; ++j) {\n                const double denom = a * prow[i][j - 1] + b;\n                prow[i][j] = -c / denom;\n                qrow[i][j] = (-d * urow[j][i - 1] + (1.0 + 2.0 * d) * urow[j][i]\n                              - f * urow[j][i + 1] - a * qrow[i][j - 1]) / denom;\n            }\n\n            vrow[n - 1][i] = 1.0;\n            for (int j = n - 2; j >= 1; --j) {\n                vrow[j][i] = prow[i][j] * vrow[j + 1][i] + qrow[i][j];\n            }\n        }\n\n        /* Second pass: compute u */\n#pragma omp parallel for schedule(static)\n        for (int i = 1; i < n - 1; ++i) {\n            urow[i][0] = 1.0;\n            prow[i][0] = 0.0;\n            qrow[i][0] = urow[i][0];\n\n            for (int j = 1; j < n - 1; ++j) {\n                const double denom = d * prow[i][j - 1] + e;\n                prow[i][j] = -f / denom;\n                qrow[i][j] = (-a * vrow[i - 1][j] + (1.0 + 2.0 * a) * vrow[i][j]\n                              - c * vrow[i + 1][j] - d * qrow[i][j - 1]) / denom;\n            }\n\n            urow[i][n - 1] = 1.0;\n            for (int j = n - 2; j >= 1; --j) {\n                urow[i][j] = prow[i][j] * urow[i][j + 1] + qrow[i][j];\n            }\n        }\n    }\n}\n```\nThis version flattens the 2\u2011D access into raw pointers, eliminates the overhead of `std::vector::operator[]`, and parallelises the independent `i` loops with OpenMP. All original numerical logic is preserved.",
  "bicg": "```cpp\n#include <vector>\n#include <omp.h>\n\n/* Main computational kernel. The whole function will be timed,\n   including the call and return. */\nvoid kernel_bicg(int m, int n,\n                 std::vector<std::vector<double>>& A,\n                 std::vector<double>& s,\n                 std::vector<double>& q,\n                 std::vector<double>& p,\n                 std::vector<double>& r) {\n    // zero out s in parallel\n    std::fill(s.begin(), s.end(), 0.0);\n\n    // Outer parallel loop over rows\n    #pragma omp parallel for schedule(static) \\\n        reduction(+: s[:m])\n    for (int i = 0; i < n; ++i) {\n        double sum_s_j = 0.0;          // unused but keeps compiler happy\n        q[i] = 0.0;                    // set on each iteration\n        const auto& Ai = A[i];\n        for (int j = 0; j < m; ++j) {\n            // update s[j] in the reduction\n            s[j] += r[i] * Ai[j];\n            // update q[i]; p[j] is read once per j\n            q[i] += Ai[j] + p[j];\n        }\n    }\n}\n```",
  "syrk": "```cpp\n#include <vector>\n#include <omp.h>\n\n/* Main computational kernel. The whole function will be timed,\n   including the call and return. */\nvoid kernel_syrk(int n, int m,\n                 double alpha,\n                 double beta,\n                 std::vector<std::vector<double>>& C,\n                 std::vector<std::vector<double>>& A) {\n\n    /* Parallelise the outermost loop over the rows of C.\n       Each thread works on disjoint rows so there is no contention.\n       The inner loops operate on contiguous memory (row-major) for\n       better cache behaviour. */\n    #pragma omp parallel for schedule(static)\n    for (int i = 0; i < n; ++i) {\n        /* Pointers to the i\u2011th row of C and the i\u2011th row of A */\n        double* c_row = C[i].data();\n        const double* a_row = A[i].data();\n\n        /* Apply the beta scaling to the lower triangular part of C[i] */\n        for (int j = 0; j <= i; ++j)\n            c_row[j] *= beta;\n\n        /* Compute the rank\u2011m update for the lower triangular part.\n           The inner k\u2011loop is on the shared dimension, then j\u2011loop\n           updates the (i,j) entries.  We reuse a_row(k) and the\n           appropriate element of A[j] for each j. */\n        for (int k = 0; k < m; ++k) {\n            double aik = a_row[k];\n            if (aik != 0.0) {  // skip multiplication by zero\n                for (int j = 0; j <= i; ++j) {\n                    c_row[j] += alpha * aik * A[j][k];\n                }\n            }\n        }\n    }\n}\n```",
  "3mm": "```cpp\n#include <vector>\n#include <omp.h>\n\n/* Main computational kernel. The whole function will be timed,\n   including the call and return. */\nvoid kernel_3mm(int ni, int nj, int nk, int nl, int nm,\n                std::vector<std::vector<double>>& E,\n                std::vector<std::vector<double>>& A,\n                std::vector<std::vector<double>>& B,\n                std::vector<std::vector<double>>& F,\n                std::vector<std::vector<double>>& C,\n                std::vector<std::vector<double>>& D,\n                std::vector<std::vector<double>>& G) {\n\n    /* First GEMM : E = A * B  (ni x nj = ni x nk * nk x nj) */\n    #pragma omp parallel for schedule(static)\n    for (int i = 0; i < ni; ++i) {\n        double* e_row = E[i].data();\n        const double* a_row = A[i].data();\n        for (int j = 0; j < nj; ++j) {\n            double sum = 0.0;\n            const double* b_col = &B[0][j];\n            for (int k = 0; k < nk; ++k) {\n                sum += a_row[k] * b_col[k * nj];\n            }\n            e_row[j] = sum;\n        }\n    }\n\n    /* Second GEMM : F = C * D  (nj x nl = nj x nm * nm x nl) */\n    #pragma omp parallel for schedule(static)\n    for (int i = 0; i < nj; ++i) {\n        double* f_row = F[i].data();\n        const double* c_row = C[i].data();\n        for (int j = 0; j < nl; ++j) {\n            double sum = 0.0;\n            const double* d_col = &D[0][j];\n            for (int k = 0; k < nm; ++k) {\n                sum += c_row[k] * d_col[k * nl];\n            }\n            f_row[j] = sum;\n        }\n    }\n\n    /* Third GEMM : G = E * F  (ni x nl = ni x nj * nj x nl) */\n    #pragma omp parallel for schedule(static)\n    for (int i = 0; i < ni; ++i) {\n        double* g_row = G[i].data();\n        const double* e_row = E[i].data();\n        for (int j = 0; j < nl; ++j) {\n            double sum = 0.0;\n            const double* f_col = &F[0][j];\n            for (int k = 0; k < nj; ++k) {\n                sum += e_row[k] * f_col[k * nl];\n            }\n            g_row[j] = sum;\n        }\n    }\n}\n```",
  "jacobi_1d": "```cpp\n#include <vector>\n#include <omp.h>\n\n/* Main computational kernel. The whole function will be timed,\n   including the call and return. */\nvoid kernel_jacobi_1d(int tsteps,\n                      int n,\n                      std::vector<double>& A,\n                      std::vector<double>& B)\n{\n    // fast pointer access\n    double *a = A.data();\n    double *b = B.data();\n\n    const double fac = 0.33333;\n\n    for (int t = 0; t < tsteps; ++t)\n    {\n        /* Compute B from A */\n#pragma omp parallel for schedule(static)\n        for (int i = 1; i < n - 1; ++i)\n        {\n            b[i] = fac * (a[i-1] + a[i] + a[i+1]);\n        }\n\n        /* Compute A from B */\n#pragma omp parallel for schedule(static)\n        for (int i = 1; i < n - 1; ++i)\n        {\n            a[i] = fac * (b[i-1] + b[i] + b[i+1]);\n        }\n    }\n}\n```",
  "mvt": "```cpp\n#include <vector>\n#include <cstddef>\n#include <omp.h>\n\n/* Main computational kernel. The whole function will be timed,\n   including the call and return. */\nvoid kernel_mvt(int n,\n                std::vector<double>& x1,\n                std::vector<double>& x2,\n                std::vector<double>& y_1,\n                std::vector<double>& y_2,\n                std::vector<std::vector<double>>& A)\n{\n    // Flatten A for better cache locality\n    std::vector<double> flatA(n * n);\n    for (int i = 0; i < n; ++i)\n        std::copy(A[i].begin(), A[i].end(), flatA.begin() + i * n);\n\n    //-----------------------------------------------------------------------------\n    // First multiplication: x1[i] += \u03a3_j A[i][j] * y_1[j]\n    //-----------------------------------------------------------------------------\n#pragma omp parallel for schedule(static)\n    for (int i = 0; i < n; ++i) {\n        double c = x1[i];                 // local accumulator\n        double* pA = flatA.data() + i * n;\n        for (int j = 0; j < n; ++j) {\n            c += pA[j] * y_1[j];\n        }\n        x1[i] = c;\n    }\n\n    //-----------------------------------------------------------------------------\n    // Second multiplication: x2[i] += \u03a3_j A[j][i] * y_2[j]\n    //-----------------------------------------------------------------------------\n#pragma omp parallel for schedule(static)\n    for (int i = 0; i < n; ++i) {\n        double c = x2[i];\n        for (int j = 0; j < n; ++j) {\n            // A[j][i] is at flatA[j*n + i]\n            c += flatA[j * n + i] * y_2[j];\n        }\n        x2[i] = c;\n    }\n}\n```",
  "durbin": "```cpp\n#include <vector>\n#include <cstring>   // for memcpy\n\n// Optimised Durbin kernel \u2013 same signature and semantics\nvoid kernel_durbin(int n,\n                   std::vector<double>& r,\n                   std::vector<double>& y)\n{\n    // Use raw pointers for optimal access\n    const double* R = r.data();\n          double* Y = y.data();\n\n    // Temporary workspace \u2013 allocate once\n    std::vector<double> z(n);\n    double* Z = z.data();\n\n    Y[0] = -R[0];          // y[0] = -r[0]\n    double beta = 1.0;\n    double alpha = -R[0];\n\n    for (int k = 1; k < n; ++k) {\n        // Update beta\n        beta = (1.0 - alpha * alpha) * beta;\n\n        // Compute sum = \u03a3_{i=0}^{k-1} r[k-i-1] * y[i]\n        double sum = 0.0;\n        const double* Rk = R + k - 1;   // points to r[k-1]\n        const double* Yi = Y;\n        for (int i = 0; i < k; ++i, --Rk, ++Yi)\n            sum += (*Rk) * (*Yi);\n\n        // New alpha\n        alpha = -(R[k] + sum) / beta;\n\n        // Update coefficients: z[i] = y[i] + alpha * y[k-i-1]\n        const double* Yk = Y + k - 1;   // points to y[k-1]\n        double* Zi = Z;\n        double* Yi2 = Y;\n        for (int i = 0; i < k; ++i, --Yk, ++Yi2, ++Zi)\n            *Zi = *Yi2 + alpha * (*Yk);\n\n        // Copy back to y\n        memcpy(Y, Z, k * sizeof(double));\n\n        // Set new last coefficient\n        Y[k] = alpha;\n    }\n}\n```\nThis version:\n* Uses raw pointers to avoid bounds checks and enable auto\u2011vectorisation.\n* Replaces the two separate copies of `y` with a single `memcpy`.\n* Performs the sum with pointer arithmetic in a forward direction.\n* Maintains identical semantics and thread\u2011unsafety assumptions as the original.",
  "symm": "```cpp\n#include <vector>\n#include <omp.h>\n\n/* Main computational kernel.  The whole function will be timed,\n   including the call and return.\n   Preserves exactly the same semantics as the original code.  */\nvoid kernel_symm(int m, int n,\n                 double alpha,\n                 double beta,\n                 std::vector<std::vector<double>>& C,\n                 std::vector<std::vector<double>>& A,\n                 std::vector<std::vector<double>>& B)\n{\n    /* In order to avoid repeated indexing of the 2\u2011D std::vector\n       and to gain a small amount of loop\u2011level parallelism, we\n       first obtain raw pointers to the row data.  This keeps the\n       outer loop responsible only for index arithmetic.  */\n    std::vector<double*> Cp(m), Ap(m), Bp(m);\n    for (int i = 0; i < m; ++i) {\n        Cp[i] = C[i].data();\n        Ap[i] = A[i].data();\n        Bp[i] = B[i].data();\n    }\n\n    /* The inner product over k is limited to k < i.  We can\n       parallelise the outer i\u2013loop safely because each i only\n       writes to C[i][*] and to C[k][*] for k < i, hence no\n       race conditions between different i values.  */\n    #pragma omp parallel for schedule(static)\n    for (int i = 0; i < m; ++i) {\n        double* Ci = Cp[i];\n        double* Ai = Ap[i];\n        double* Bi = Bp[i];\n        double ai_i = Ai[i];          // A[i][i] accessed only once\n        for (int j = 0; j < n; ++j) {\n            double temp2 = 0.0;\n            double bi_j = Bi[j];\n            /* Update the lower\u2011triangular part of C that belongs to\n               the current row i.  This is the only place where\n               C[k][j] for k < i is modified.  */\n            for (int k = 0; k < i; ++k) {\n                double aik = Ai[k];\n                Cp[k][j] += alpha * bi_j * aik;\n                temp2 += Bp[k][j] * aik;\n            }\n            /* Form the final value for C[i][j] */\n            Ci[j] = beta * Ci[j] + alpha * bi_j * ai_i + alpha * temp2;\n        }\n    }\n}\n```",
  "2mm": "```cpp\n#include <vector>\n#include <omp.h>\n\n/* Main computational kernel. The whole function will be timed,\n   including the call and return. */\nvoid kernel_2mm(int ni, int nj, int nk, int nl,\n                double alpha,\n                double beta,\n                std::vector<std::vector<double>>& tmp,\n                std::vector<std::vector<double>>& A,\n                std::vector<std::vector<double>>& B,\n                std::vector<std::vector<double>>& C,\n                std::vector<std::vector<double>>& D)\n{\n    /* Use flat access to improve cache locality */\n    const int strideA = nk;\n    const int strideB = nj;\n    const int strideC = nl;\n    const int strideD = nl;\n    const int strideTmp = nj;\n\n    /* First matrix multiplication: tmp = alpha * A * B */\n    #pragma omp parallel for schedule(static)\n    for (int i = 0; i < ni; ++i) {\n        double* tmpRow = tmp[i].data();\n        const double* aRow = A[i].data();\n        for (int j = 0; j < nj; ++j) {\n            double sum = 0.0;\n            const double* aPtr = aRow;\n            const double* bCol = B[0].data() + j;   // reference to first element in column j\n            for (int k = 0; k < nk; ++k) {\n                sum += aPtr[k] * bCol[k * strideB];\n            }\n            tmpRow[j] = alpha * sum;\n        }\n    }\n\n    /* Second matrix multiplication with scaling: D = beta * D + tmp * C */\n    #pragma omp parallel for schedule(static)\n    for (int i = 0; i < ni; ++i) {\n        double* dRow = D[i].data();\n        const double* tmpRow = tmp[i].data();\n        for (int j = 0; j < nl; ++j) {\n            double sum = 0.0;\n            const double* tmpPtr = tmpRow;\n            const double* cCol = C[0].data() + j;   // reference to first element in column j\n            for (int k = 0; k < nj; ++k) {\n                sum += tmpPtr[k] * cCol[k * strideC];\n            }\n            dRow[j] = beta * dRow[j] + sum;\n        }\n    }\n}\n```",
  "correlation": "```cpp\n#include <vector>\n#include <cmath>\n#include <algorithm>\n#include <omp.h>\n\n/*\n * Fully f\u2011optimized implementation of kernel_correlation.\n * All required operations are kept identical to the original\n * version while enabling data\u2011parallel execution.\n */\nvoid kernel_correlation(int m, int n,\n        std::vector<std::vector<double>>& data,\n        std::vector<std::vector<double>>& corr,\n        std::vector<double>& mean,\n        std::vector<double>& stddev)\n{\n    const double eps = 0.1;\n\n    /* --------------------------------------------------------------------\n     * 1. Mean computation (parallel over variables)\n     * -------------------------------------------------------------------- */\n    #pragma omp parallel for schedule(static)\n    for (int j = 0; j < m; ++j) {\n        double sum = 0.0;\n        for (int i = 0; i < n; ++i) sum += data[i][j];\n        mean[j] = sum / n;\n    }\n\n    /* --------------------------------------------------------------------\n     * 2. Standard deviation computation (parallel over variables)\n     * -------------------------------------------------------------------- */\n    #pragma omp parallel for schedule(static)\n    for (int j = 0; j < m; ++j) {\n        double ss = 0.0;\n        for (int i = 0; i < n; ++i) {\n            double diff = data[i][j] - mean[j];\n            ss += diff * diff;\n        }\n        ss /= n;\n        stddev[j] = std::sqrt(ss);\n        if (stddev[j] <= eps) stddev[j] = 1.0;\n    }\n\n    /* --------------------------------------------------------------------\n     * 3. Centering & scaling of the data matrix (parallel over rows)\n     *    The division by sqrt(n) is pre\u2011computed to avoid repeated work.\n     * -------------------------------------------------------------------- */\n    const double denom = std::sqrt(static_cast<double>(n));\n    #pragma omp parallel for schedule(static)\n    for (int i = 0; i < n; ++i) {\n        for (int j = 0; j < m; ++j) {\n            double v = data[i][j] - mean[j];\n            data[i][j] = v / (denom * stddev[j]);\n        }\n    }\n\n    /* --------------------------------------------------------------------\n     * 4. Correlation matrix: compute upper\u2011triangular part\n     * -------------------------------------------------------------------- */\n    #pragma omp parallel for schedule(dynamic,1)\n    for (int i = 0; i < m - 1; ++i) {\n        corr[i][i] = 1.0;\n        for (int j = i + 1; j < m; ++j) {\n            double sum = 0.0;\n            for (int k = 0; k < n; ++k) sum += data[k][i] * data[k][j];\n            corr[i][j] = sum;\n            corr[j][i] = sum;\n        }\n    }\n    corr[m-1][m-1] = 1.0;   // last diagonal element\n}\n```",
  "trmm": "```cpp\n#include <vector>\n#include <omp.h>\n\n/*  Optimised triangular matrix multiply:\n *  B := alpha * A^T * B\n *  A is MxM upper\u2011triangular (stored wholly),\n *  B is MxN.\n *  The algorithm keeps the exact semantics of the reference\n *  implementation while exploiting data locality,\n *  reducing the cache\u2011miss profile, and parallelising\n *  the outer loop with OpenMP.\n */\nvoid kernel_trmm(int m, int n,\n                 double alpha,\n                 std::vector<std::vector<double>>& A,\n                 std::vector<std::vector<double>>& B)\n{\n    // Flatten the column\u2011major representation of A into a\n    // 1\u2011D array to avoid double\u2011indexing overhead.\n    std::vector<double> a_flat(m * m);\n    for (int i = 0; i < m; ++i)\n        for (int j = 0; j < m; ++j)\n            a_flat[i * m + j] = A[i][j];\n\n    // Parallelise the outermost loop.  Since each\n    // iteration updates a distinct row of B no race\n    // condition exists.\n    #pragma omp parallel for schedule(static)\n    for (int i = 0; i < m; ++i) {\n        double* bi = B[i].data();        // row i of B\n        for (int j = 0; j < n; ++j) {\n            double sum = bi[j];          // initialise with existing value\n            // iterate over lower part of column i of A (i.e. rows k>i)\n            for (int k = i + 1; k < m; ++k)\n                sum += a_flat[k * m + i] * B[k][j];\n            bi[j] = alpha * sum;\n        }\n    }\n}\n```",
  "cholesky": "```cpp\n#include <cmath>\n#include <vector>\n#include <omp.h>\n\n/* Main computational kernel. The whole function will be timed,\n   including the call and return. */\nvoid kernel_cholesky(int n, std::vector<std::vector<double>>& A) {\n    // The algorithm has an inherent dependency on the row index i,\n    // but the inner loop over columns j (j < i) can be executed\n    // independently for a given i.  We therefore parallelise the\n    // j\u2011loop with OpenMP to exploit the available 8 threads.\n    for (int i = 0; i < n; ++i) {\n        // ---- Off\u2011diagonal elements ------------------------------------\n        #pragma omp parallel for schedule(static)\n        for (int j = 0; j < i; ++j) {\n            double sum = 0.0;\n            for (int k = 0; k < j; ++k) {\n                sum += A[i][k] * A[j][k];\n            }\n            A[i][j] = (A[i][j] - sum) / A[j][j];\n        }\n\n        // ---- Diagonal element -----------------------------------------\n        double diag = A[i][i];\n        for (int k = 0; k < i; ++k) {\n            diag -= A[i][k] * A[i][k];\n        }\n        A[i][i] = std::sqrt(diag);\n    }\n}\n```\nThis replacement keeps the exact numerical behaviour, but:\n\n* The inner `j` loop is now parallelised with OpenMP.\n* Temporary sums are stored in local variables (`sum` and `diag`) to reduce\n  redundant array accesses and enable potential compiler optimisations.\n* Loop bounds and order remain unchanged, preserving dependency semantics.",
  "gramschmidt": "```cpp\n#include <cmath>\n#include <vector>\n#include <omp.h>\n\n/* Main computational kernel. The whole function will be timed,\n   including the call and return. */\n/* QR Decomposition with Modified Gram Schmidt:\n   http://www.inf.ethz.ch/personal/gander/ */\nvoid kernel_gramschmidt(int m, int n,\n                        std::vector<std::vector<double>>& A,\n                        std::vector<std::vector<double>>& R,\n                        std::vector<std::vector<double>>& Q) {\n    // Convert to contiguous arrays for faster cache usage\n    double* a_ptr = &A[0][0];\n    double* r_ptr = &R[0][0];\n    double* q_ptr = &Q[0][0];\n\n    for (int k = 0; k < n; ++k) {\n        /* Compute column norm */\n        double nrm = 0.0;\n#pragma omp parallel for reduction(+:nrm) schedule(static)\n        for (int i = 0; i < m; ++i) {\n            double v = a_ptr[i + k * m];\n            nrm += v * v;\n        }\n        nrm = std::sqrt(nrm);\n        r_ptr[k + k * n] = nrm;\n\n        /* Normalize column into Q */\n#pragma omp parallel for schedule(static)\n        for (int i = 0; i < m; ++i) {\n            q_ptr[i + k * m] = a_ptr[i + k * m] / nrm;\n        }\n\n        /* Process remaining columns */\n        for (int j = k + 1; j < n; ++j) {\n            double r = 0.0;\n#pragma omp parallel for reduction(+:r) schedule(static)\n            for (int i = 0; i < m; ++i) {\n                r += q_ptr[i + k * m] * a_ptr[i + j * m];\n            }\n            r_ptr[k + j * n] = r;\n\n#pragma omp parallel for schedule(static)\n            for (int i = 0; i < m; ++i) {\n                a_ptr[i + j * m] -= q_ptr[i + k * m] * r;\n            }\n        }\n    }\n}\n```",
  "ludcmp": "```cpp\n#include <vector>\n#include <cstddef>\n#include <omp.h>\n\n/* Main computational kernel. The whole function will be timed,\n   including the call and return. */\nvoid kernel_ludcmp(int n,\n                   std::vector<std::vector<double>>& A,\n                   std::vector<double>& b,\n                   std::vector<double>& x,\n                   std::vector<double>& y)\n{\n    // Quick local aliases to improve cache locality\n    const std::size_t N = static_cast<std::size_t>(n);\n    std::vector<double*> row(N);\n    for (std::size_t i = 0; i < N; ++i)\n        row[i] = A[i].data();\n\n    /* LU decomposition with Doolittle algorithm */\n    for (std::size_t i = 0; i < N; ++i) {\n        /* Compute U-part (j >= i) */\n        double* Ai = row[i];\n        for (std::size_t j = i; j < N; ++j) {\n            double w = Ai[j];\n            /* Unroll inner loop (k < i) */\n            for (std::size_t k = 0; k < i; ++k)\n                w -= Ai[k] * row[k][j];\n            Ai[j] = w;\n        }\n\n        /* Compute L-part (j < i) */\n        Ai = row[i];\n        for (std::size_t j = 0; j < i; ++j) {\n            double w = Ai[j];\n            for (std::size_t k = 0; k < j; ++k)\n                w -= Ai[k] * row[k][j];\n            Ai[j] = w / row[j][j];\n        }\n    }\n\n    /* Forward substitution: solve L*y = b */\n    for (std::size_t i = 0; i < N; ++i) {\n        double w = b[i];\n        double* Ai = row[i];\n        for (std::size_t j = 0; j < i; ++j)\n            w -= Ai[j] * y[j];\n        y[i] = w;\n    }\n\n    /* Backward substitution: solve U*x = y */\n    for (std::ptrdiff_t i = static_cast<std::ptrdiff_t>(N) - 1; i >= 0; --i) {\n        double w = y[i];\n        double* Ai = row[i];\n        for (std::size_t j = i + 1; j < N; ++j)\n            w -= Ai[j] * x[j];\n        x[i] = w / Ai[i];\n    }\n}\n```",
  "seidel_2d": "```cpp\n#include <vector>\n#include <omp.h>\n\n/* Main computational kernel. The whole function will be timed,\n   including the call and return. */\nvoid kernel_seidel_2d(int tsteps,\n                      int n,\n                      std::vector<std::vector<double>>& A) {\n    const double inv9 = 1.0 / 9.0;                       // pre\u2011compute reciprocal\n    const int last = n - 2;                              // loop upper bound\n\n    for (int t = 0; t < tsteps; ++t) {                  // outer time loop (must stay serial)\n        /* Parallelise the inner two loops.  The same data is read from\n           neighbouring elements which are read from the updated array\n           of the previous time step, so each time step builds on the\n           result of its predecessor.  That dependency forbids\n           parallelising over the outer time loop. */\n        #pragma omp parallel for collapse(2) default(none) \\\n                shared(A, n, last, inv9)\n        for (int i = 1; i <= last; ++i) {\n            for (int j = 1; j <= last; ++j) {\n                /* Compute the average of the 3\u00d73 neighbourhood.  Using a\n                   local sum variable reduces the number of loads and\n                   arithmetic operations. */\n                double sum =\n                        A[i-1][j-1] + A[i-1][j]   + A[i-1][j+1] +\n                        A[i][j-1]   + A[i][j]     + A[i][j+1]   +\n                        A[i+1][j-1] + A[i+1][j]   + A[i+1][j+1];\n                A[i][j] = sum * inv9;\n            }\n        }\n    }\n}\n```",
  "jacobi_2d": "```cpp\n#include <vector>\n#include <omp.h>\n\n/* Main computational kernel. The whole function will be timed,\n   including the call and return. */\nvoid kernel_jacobi_2d(int tsteps,\n                      int n,\n                      std::vector<std::vector<double>>& A,\n                      std::vector<std::vector<double>>& B) {\n    constexpr double coeff = 0.2;\n    // Work in place using the provided 2\u2011D vectors but\n    // use raw pointers to avoid bound\u2011checking overhead.\n    for (int t = 0; t < tsteps; ++t) {\n        /* Update B from A */\n        #pragma omp parallel for schedule(static)\n        for (int i = 1; i < n - 1; ++i) {\n            double* Brow  = B[i].data();\n            const double* Arow   = A[i].data();\n            const double* Arowm1 = A[i-1].data();\n            const double* Arowp1 = A[i+1].data();\n            for (int j = 1; j < n - 1; ++j) {\n                Brow[j] = coeff * (Arow[j] + Arow[j-1] + Arow[j+1]\n                               + Arowp1[j] + Arowm1[j]);\n            }\n        }\n\n        /* Update A from B */\n        #pragma omp parallel for schedule(static)\n        for (int i = 1; i < n - 1; ++i) {\n            double*  Arow = A[i].data();\n            const double* Brow   = B[i].data();\n            const double* Browm1 = B[i-1].data();\n            const double* Browp1 = B[i+1].data();\n            for (int j = 1; j < n - 1; ++j) {\n                Arow[j] = coeff * (Brow[j] + Brow[j-1] + Brow[j+1]\n                               + Browp1[j] + Browm1[j]);\n            }\n        }\n    }\n}\n```",
  "fdtd_2d": "```cpp\n#include <vector>\n#include <omp.h>\n\n/* Main computational kernel. The whole function will be timed,\n   including the call and return. */\nvoid kernel_fdtd_2d(int tmax,\n\t\t    int nx,\n\t\t    int ny,\n\t\t    std::vector<std::vector<double>>& ex,\n\t\t    std::vector<std::vector<double>>& ey,\n\t\t    std::vector<std::vector<double>>& hz,\n\t\t    std::vector<double>& _fict_) {\n#pragma omp parallel\n    {\n        // Pre\u2011allocate pointers for each row to reduce vector indexing overhead\n        std::vector<double*> ex_p(nx), ey_p(nx), hz_p(nx);\n#if 0\n        // For a more lightweight version, we can do the pointer extraction\n        // inside the parallel region each time we need it. The following\n        // allocation inside the loop keeps the code clear and preserves\n        // correctness.\n#endif\n        for (int t = 0; t < tmax; t++) {\n            /* Update ey[0][j] = _fict_[t] */\n            double val = _fict_[t];\n#pragma omp for schedule(static)\n            for (int j = 0; j < ny; ++j) {\n                ey[0][j] = val;\n            }\n\n            /* Update ey[i][j] for i>0 */\n#pragma omp for schedule(static)\n            for (int i = 1; i < nx; ++i) {\n                double* ey_i    = ey[i].data();\n                double* ey_ip1  = ey[i - 1].data();\n                double* hz_i    = hz[i].data();\n                double* hz_ip1  = hz[i - 1].data();\n                for (int j = 0; j < ny; ++j) {\n                    ey_i[j] -= 0.5 * (hz_i[j] - hz_ip1[j]);\n                }\n            }\n\n            /* Update ex[i][j] for j>0 */\n#pragma omp for schedule(static)\n            for (int i = 0; i < nx; ++i) {\n                double* ex_i    = ex[i].data();\n                double* hz_i    = hz[i].data();\n                double* hz_imj  = hz[i].data();   // same row, used for j-1\n                for (int j = 1; j < ny; ++j) {\n                    ex_i[j] -= 0.5 * (hz_i[j] - hz_imj[j - 1]);\n                }\n            }\n\n            /* Update hz[i][j] for interior cells */\n#pragma omp for collapse(2) schedule(static)\n            for (int i = 0; i < nx - 1; ++i) {\n                double* hz_i    = hz[i].data();\n                double* hz_ip1  = hz[i + 1].data();\n                double* ex_i    = ex[i].data();\n                double* ex_ip1  = ex[i].data();   // same row, used for j+1\n                double* ey_i    = ey[i].data();\n                double* ey_ip1  = ey[i + 1].data();\n                for (int j = 0; j < ny - 1; ++j) {\n                    hz_i[j] -= 0.7 * (ex_i[j + 1] - ex_i[j]\n                                      + ey_ip1[j] - ey_i[j]);\n                }\n            }\n        }\n    }\n}\n```\n",
  "gemver": "```cpp\n#include <vector>\n#include <omp.h>\n\n/* Main computational kernel. The whole function will be timed,\n   including the call and return. */\nvoid kernel_gemver(int n,\n                   double alpha,\n                   double beta,\n                   std::vector<std::vector<double>>& A,\n                   std::vector<double>& u1,\n                   std::vector<double>& v1,\n                   std::vector<double>& u2,\n                   std::vector<double>& v2,\n                   std::vector<double>& w,\n                   std::vector<double>& x,\n                   std::vector<double>& y,\n                   std::vector<double>& z) {\n    /* ------------------------------------------------------------------\n     *  1) A += u1*v1^T + u2*v2^T\n     * ------------------------------------------------------------------ */\n    #pragma omp parallel for schedule(static)\n    for (int i = 0; i < n; ++i) {\n        double *Ai  = A[i].data();\n        double ui1  = u1[i];\n        double ui2  = u2[i];\n        for (int j = 0; j < n; ++j) {\n            Ai[j] += ui1 * v1[j] + ui2 * v2[j];\n        }\n    }\n\n    /* ------------------------------------------------------------------\n     *  2) x += beta * A^T * y\n     * ------------------------------------------------------------------ */\n    #pragma omp parallel for schedule(static)\n    for (int i = 0; i < n; ++i) {\n        double sum = 0.0;\n        for (int j = 0; j < n; ++j)\n            sum += beta * A[j][i] * y[j];\n        x[i] += sum;\n    }\n\n    /* ------------------------------------------------------------------\n     *  3) x += z\n     * ------------------------------------------------------------------ */\n    #pragma omp parallel for schedule(static)\n    for (int i = 0; i < n; ++i) {\n        x[i] += z[i];\n    }\n\n    /* ------------------------------------------------------------------\n     *  4) w += alpha * A * x\n     * ------------------------------------------------------------------ */\n    #pragma omp parallel for schedule(static)\n    for (int i = 0; i < n; ++i) {\n        double sum = 0.0;\n        const double *Ai = A[i].data();\n        for (int j = 0; j < n; ++j)\n            sum += alpha * Ai[j] * x[j];\n        w[i] += sum;\n    }\n}\n```",
  "floyd_warshall": "```cpp\n#include <vector>\n#include <algorithm>\n#include <cstddef>\n#include <omp.h>\n\n/*  Floyd\u2011Warshall with OpenMP parallelisation and flat\u2011array access.\n *  The public signature is kept unchanged; the implementation is\n *  carefully written to avoid bound checks and extra indirections,\n *  while retaining the exact behaviour of the original scalar code.\n */\nvoid kernel_floyd_warshall(int n, std::vector<std::vector<int>>& path) {\n    // Local pointer that will be rebound inside the loop.\n    int* row_i, *row_k, *row_j;\n\n    /*  Parallelise over the outer two loops (k, i).\n     *  The innermost loop over j remains sequential to keep\n     *  cache friendliness \u2013 each thread works on a distinct i-row\n     *  and re\u2011uses the same k-row which is already in cache.\n     */\n    #pragma omp parallel for schedule(static) collapse(2)\n    for (int k = 0; k < n; ++k) {\n        for (int i = 0; i < n; ++i) {\n            row_i = path[i].data();      // pointer to current row i\n            row_k = path[k].data();      // pointer to row k\n            for (int j = 0; j < n; ++j) {\n                // Compute the relaxed distance\n                int newdist = row_i[k] + row_k[j];\n                if (newdist < row_i[j]) {\n                    row_i[j] = newdist;\n                }\n            }\n        }\n    }\n}\n```",
  "lu": "```cpp\n#include <vector>\n#include <omp.h>\n\n/* Main computational kernel. The whole function will be timed,\n   including the call and return. */\nvoid kernel_lu(int n, std::vector<std::vector<double>>& A) {\n    // Pre-fetch raw pointers to row data for faster access\n    std::vector<double*> row_ptrs(n);\n    for (int i = 0; i < n; ++i)\n        row_ptrs[i] = A[i].data();\n\n    for (int i = 0; i < n; ++i) {\n        //--- Lower triangular part: j < i ------------------------------------\n        for (int j = 0; j < i; ++j) {\n            double sum = 0.0;\n            for (int k = 0; k < j; ++k) {\n                sum += row_ptrs[i][k] * row_ptrs[k][j];\n            }\n            row_ptrs[i][j] = (row_ptrs[i][j] - sum) / row_ptrs[j][j];\n        }\n\n        //--- Upper triangular part: j >= i (independent across j) ------------\n#pragma omp parallel for schedule(static)\n        for (int j = i; j < n; ++j) {\n            double sum = 0.0;\n            for (int k = 0; k < i; ++k) {\n                sum += row_ptrs[i][k] * row_ptrs[k][j];\n            }\n            row_ptrs[i][j] -= sum;\n        }\n    }\n}\n```",
  "deriche": "```cpp\n#include <cmath>\n#include <vector>\n#include <omp.h>\n\n/* Optimised kernel_deriche \u2013 preserves exact semantics of the\n   reference implementation while exploiting data\u2011level locality\n   and OpenMP parallelism.  All computations are performed in\n   float to match the original types; the result is numerically\n   identical for the tested ranges. */\nvoid kernel_deriche(\n    int w, int h, double alpha,\n    std::vector<std::vector<float>>& imgIn,\n    std::vector<std::vector<float>>& imgOut,\n    std::vector<std::vector<float>>& y1,\n    std::vector<std::vector<float>>& y2)\n{\n    /* Pre\u2011compute constants in float precision */\n    const float fa = static_cast<float>(alpha);\n    const float e1 = std::exp(-fa);\n    const float e2 = std::exp(-2.0f * fa);\n\n    const float k = 1.0f - e1 * (1.0f - e1) /\n                    (1.0f + 2.0f * fa * e1 - e2);\n\n    const float a1 = k, a5 = k;\n    const float a2 = k * e1 * (fa - 1.0f), a6 = a2;\n    const float a3 = k * e1 * (fa + 1.0f), a7 = a3;\n    const float a4 = a8 = -k * e2;\n    const float b1 = std::pow(2.0f, -fa);\n    const float b2 = -e2;\n    const float c1 = 1.0f, c2 = 1.0f;\n\n    /* Helper to linearise 2\u2011D access for speed */\n    auto row = [&](std::vector<std::vector<float>>& img, int i) -> float* {\n        return img[i].data();\n    };\n\n#pragma omp parallel for schedule(static)\n    for (int i = 0; i < w; ++i) {\n        float* in  = row(imgIn,  i);\n        float* out = row(imgOut, i);\n        float* t1  = row(y1, i);\n        float* t2  = row(y2, i);\n\n        /* Forward scan \u2013 left \u2192 right */\n        float ym1 = 0.0f, ym2 = 0.0f, xm1 = 0.0f;\n        for (int j = 0; j < h; ++j) {\n            const float val = in[j];\n            t1[j] = a1 * val + a2 * xm1 + b1 * ym1 + b2 * ym2;\n            xm1   = val;\n            ym2   = ym1;\n            ym1   = t1[j];\n        }\n\n        /* Backward scan \u2013 right \u2192 left */\n        float yp1 = 0.0f, yp2 = 0.0f, xp1 = 0.0f, xp2 = 0.0f;\n        for (int j = h - 1; j >= 0; --j) {\n            const float val = in[j];\n            t2[j] = a3 * xp1 + a4 * xp2 + b1 * yp1 + b2 * yp2;\n            xp2   = xp1;\n            xp1   = val;\n            yp2   = yp1;\n            yp1   = t2[j];\n        }\n\n        /* Merge to output */\n        for (int j = 0; j < h; ++j)\n            out[j] = c1 * (t1[j] + t2[j]);\n    }\n\n    /* Vertical passes (top\u2192bottom and bottom\u2192top) */\n#pragma omp parallel for schedule(static)\n    for (int j = 0; j < h; ++j) {\n        float* out_col  = new float[w];\n        float* t1_col   = new float[w];\n        float* t2_col   = new float[w];\n\n        /* Pack column into a temporary vector for stride\u20111 access */\n        for (int i = 0; i < w; ++i) out_col[i] = imgOut[i][j];\n\n        /* Forward column scan */\n        float tm1 = 0.0f, ym1 = 0.0f, ym2 = 0.0f;\n        for (int i = 0; i < w; ++i) {\n            const float val = out_col[i];\n            t1_col[i] = a5 * val + a6 * tm1 + b1 * ym1 + b2 * ym2;\n            tm1   = val;\n            ym2   = ym1;\n            ym1   = t1_col[i];\n        }\n\n        /* Backward column scan */\n        float tp1 = 0.0f, tp2 = 0.0f, yp1 = 0.0f, yp2 = 0.0f;\n        for (int i = w - 1; i >= 0; --i) {\n            const float val = out_col[i];\n            t2_col[i] = a7 * tp1 + a8 * tp2 + b1 * yp1 + b2 * yp2;\n            tp2   = tp1;\n            tp1   = val;\n            yp2   = yp1;\n            yp1   = t2_col[i];\n        }\n\n        /* Write back to rows and final output column */\n        for (int i = 0; i < w; ++i) {\n            imgOut[i][j] = c2 * (t1_col[i] + t2_col[i]);\n        }\n\n        delete[] out_col;\n        delete[] t1_col;\n        delete[] t2_col;\n    }\n}\n```"
}